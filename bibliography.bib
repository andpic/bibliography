% Encoding: UTF-8

@Article{DeOliveira2019,
  author    = {Gonzaga De Oliveira, S. L. and Abreu, A. A. A. M.},
  title     = {An Experimental Analysis of Three Pseudo-peripheral Vertex Finders in conjunction with the Reverse Cuthill-McKee Method for Bandwidth Reduction},
  journal   = {TEMA (São Carlos)},
  year      = {2019},
  language  = {en},
  volume    = {20},
  month     = {12},
  pages     = {497--507},
  issn      = {2179-8451},
  doi       = {10.5540/tema.2019.020.03.0497},
  url       = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S2179-84512019000300497&nrm=iso},
  abstract  = {The need to determine pseudoperipheral vertices arises from several graph-theoretical approaches for ordering sparse matrix equations. The results of two algorithms for finding such vertices, namely, the George-Liu and Kaveh-Bondarabady algorithms, are evaluated in this work along with a variant of the Kaveh-Bondarabady algorithm. The results suggest that the well-know George-Liu algorithm dominates the other two pseudoperipheral vertex finders mainly when considering the computational times of the algorithms.},
  publisher = {scielo},
}

@InProceedings{Dufrechou2018,
  author    = {Dufrechou, Ernesto and Ezzatti, Pablo},
  title     = {Solving Sparse Triangular Linear Systems in Modern {GPUs}: A Synchronization-Free Algorithm},
  booktitle = {Proceedings of the 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
  date      = {2018},
  series    = {PDP},
  location  = {Cambridge, UK},
  pages     = {196--203},
  doi       = {10.1109/PDP2018.2018.00034},
  abstract  = {Sparse triangular linear systems are ubiquitous in a wide range of science and engineering fields, and represent one of the most important building blocks of Sparse Numerical Lineal Algebra methods. For this reason, their parallel solution has been subject of exhaustive study, and efficient implementations of this kernel can be found for almost every hardware platform. However, the strong data dependencies that serialize a great deal of the execution and the load imbalance inherent to the triangular structure poses serious difficulties for its parallel performance, specially in the context of massively- parallel processors such as GPUs. To this day, the most widespread GPU implementation of this kernel is the one distributed in NVIDIA CUSPARSE library, which relies on a preprocessing stage to determine the parallel execution schedule. Although the solution phase is highly efficient, this strategy pays the cost of constant synchronizations with the CPU. In this work, we present a synchronization-free GPU al- gorithm to solve sparse triangular linear systems for the CSR format. The experimental evaluation shows performance improvements over CUSPARSE and a recently proposed synchronization-free method for the CSC matrix format.},
  timestamp = {Tue, 03 Jul 2018 17:32:39 +0200},
}

@PhdThesis{Achterberg2008,
  author      = {Achterberg, Tobias},
  title       = {Constraint Integer Programming},
  institution = {Technische Universität Berlin},
  date        = {2008},
  url         = {https://opus4.kobv.de/opus4-zib/frontdoor/index/index/docId/1112},
  abstract    = {This thesis introduces the novel paradigm of constraint integer programming (CIP), which integrates constraint programming (CP) and mixed integer programming (MIP) modeling and solving techniques. It is supplemented by the software SCIP, which is a solver and framework for constraint integer programming that also features SAT solving techniques. SCIP is freely available in source code for academic and non-commercial purposes. \\ Our constraint integer programming approach is a generalization of MIP that allows for the inclusion of arbitrary constraints, as long as they turn into linear constraints on the continuous variables after all integer variables have been fixed. The constraints, may they be linear or more complex, are treated by any combination of CP and MIP techniques: the propagation of the domains by constraint specific algorithms, the generation of a linear relaxation and its solving by LP methods, and the strengthening of the LP by cutting plane separation. \\ The current version of SCIP comes with all of the necessary components to solve mixed integer programs. In the thesis, we cover most of these ingredients and present extensive computational results to compare different variants for the individual building blocks of a MIP solver. We focus on the algorithms and their impact on the overall performance of the solver. \\ In addition to mixed integer programming, the thesis deals with chip design verification, which is an important topic of electronic design automation. Chip manufacturers have to make sure that the logic design of a circuit conforms to the specification of the chip. Otherwise, the chip would show an erroneous behavior that may cause failures in the device where it is employed. An important subproblem of chip design verification is the property checking problem, which is to verify whether a circuit satisfies a specified property. We show how this problem can be modeled as constraint integer program and provide a number of problem-specific algorithms that exploit the structure of the individual constraints and the circuit as a whole. Another set of extensive computational benchmarks compares our CIP approach to the current state-of-the-art SAT methodology and documents the success of our method.},
  owner       = {Andrea},
  timestamp   = {2013.12.28},
}

@InCollection{Alvarado1993,
  author    = {Alvarado, Fernando L. and Pothen, Alex and Schreiber, Robert},
  title     = {Highly Parallel Sparse Triangular Solution},
  booktitle = {{G}raph {T}heory and {S}parse {M}atrix {C}omputation},
  date      = {1993},
  editor    = {George, Alan and Gilbert, John R. and Liu, Joseph W. H.},
  language  = {English},
  volume    = {56},
  series    = {The IMA Volumes in Mathematics and its Applications},
  publisher = {Springer New York},
  isbn      = {978-1-4613-8371-0},
  pages     = {141--157},
  doi       = {10.1007/978-1-4613-8369-7_7},
  abstract  = {In this paper we survey a recent approach for solving sparse triangular systems of equations on highly parallel computers. This approach employs a partitioned representation of the inverse of the triangular matrix so that the solution can be computed by matrix-vector multiplication. The number of factors in the partitioned inverse is proportional to the number of general communication steps (router steps on a CM-2) required in a highly parallel algorithm. We describe partitioning algorithms that minimize the number of factors in the partitioned inverse over all symmetric permutations of the triangular matrix such that the permuted matrix continues to be triangular. For a Cholesky factor we describe an O(n) time and space algorithm to solve the partitioning problem above, where n is the order of the matrix. Our computational results on a CM-2 demonstrate the potential superiority of the partitioned inverse approach over the conventional substitution algorithm for highly parallel sparse triangular solution. Finally we describe current and future extensions of these results.},
  keywords  = {chordal graph; directed acyclic graph; elimination tree; graph partitioning; massively parallel computers; partitioned inverse; sparse triangular systems; transitive closure},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@TechReport{Alvarez2015,
  author      = {Alvarez, Alejandro Marcos and Wehenkel, Louis and Louveaux, Quentin},
  title       = {Machine Learning to Balance the Load in Parallel Branch-and-Bound},
  institution = {Department of Electrical Engineering and Computer Science, University of Liege},
  date        = {2015},
  month       = mar,
  url         = {http://www.optimization-online.org/DB_FILE/2015/03/4832.pdf},
  abstract    = {We describe in this paper a new approach to parallelize branch-and-bound on a certain number of processors. We propose to split the optimization of the original problem into the optimization of several subproblems that can be optimized separately with the goal that the amount of work that each processor carries out is balanced between the processors, while achieving interesting speedups. The main innovation of our approach consists in the use of machine learning to create a function able to estimate the difficulty (number of nodes) of a subproblem of the original problem. We also present a set of features that we developed in order to characterize the encountered subproblems. These features are used as input of the function learned with machine learning in order to estimate the difficulty of a subproblem. The estimates of the numbers of nodes are then used to decide how to partition the original optimization tree into a given number of subproblems, and to decide how to distribute them among the available processors. The experiments that we carry out show that our approach succeeds in balancing the amount of work between the processors, and that interesting speedups can be achieved with little effort.},
  owner       = {andrea},
  timestamp   = {2015.04.07},
}

@Book{Anderson1999,
  author    = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
  title     = {{LAPACK} Users' Guide},
  date      = {1999},
  edition   = {Third},
  publisher = {Society for Industrial and Applied Mathematics},
  location  = {Philadelphia, PA},
  isbn      = {0-89871-447-8 (paperback)},
  owner     = {andrea},
  timestamp = {2016.11.07},
}

@PhdThesis{Anderson2014,
  author      = {Anderson, Michael},
  title       = {A Framework for Composing High-Performance {OpenCL} from Python Descriptions},
  institution = {University of California at Berkeley},
  date        = {2014},
  note        = {UCB/EECS-2014-177},
  month       = nov,
  url         = {http://digitalassets.lib.berkeley.edu/etd/ucb/text/Anderson_berkeley_0028E_14773.pdf},
  abstract    = {Parallel processors have become ubiquitous; most programmers today have access to parallel hardware such as multi-core processors and graphics processors. This has created an implementation gap, where efficiency programmers with knowledge of hardware details can attain high performance by exploiting parallel hardware, while productivity programmers with application-level knowledge may not understand low-level performance trade-offs. Ideally, we would like to be able to write programs in productivity languages such as Python or MATLAB, and achieve performance comparable to the best hand-tuned code. \\One approach toward achieving this ideal is to write libraries that get high efficiency on certain operations, and call these libraries from the productivity environment. We propose a framework that addresses two problems with this approach: that it fails to fuse operations for efficiency, and that it may not consider runtime information such as shapes and sizes of data structures. With our framework, efficiency programmers write and/or generate customized OpenCL snippets at runtime and the framework automatically fuses, compiles, and executes these operations based on a Python description. \\We evaluate the framework with case studies of two very different applications: space-time adaptive radar processing and optical flow. For a space-time adaptive radar processing application, our framework's implementation is competitive with a hand-coded implementation that uses a vendor-optimized library. For optical flow, a computer vision application, the framework achieves frame rates that are between 0.5$\times$ and 0.97$\times$ hand-coded OpenCL performance.},
  owner       = {andrea},
  timestamp   = {2015.01.29},
}

@Article{Asanovic2006,
  author       = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine},
  title        = {The Landscape of Parallel Computing Research : A View from {B}erkeley},
  journaltitle = {Communications of the {ACM}},
  date         = {2006},
  volume       = {52},
  number       = {10},
  pages        = {56--67},
  issn         = {0001-0782},
  doi          = {10.1145/1562764.1562783},
 abstract = {Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.},
  institution  = {EECS Department, University of California Berkeley},
  owner        = {andrea},
  timestamp    = {2016.09.07},
}

@Article{Aykanat2004,
  author       = {Aykanat, Cevdet and Pinar, Ali and Çatalyürek, {\"U}mit V.},
  title        = {Permuting Sparse Rectangular Matrices into Block-Diagonal Form},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2004},
  volume       = {25},
  number       = {6},
  pages        = {1860--1879},
  doi          = {10.1137/S1064827502401953},
  abstract     = {We investigate the problem of permuting a sparse rectangular matrix into block-diagonal form. Block-diagonal form of a matrix grants an inherent parallelism for solving the deriving problem, as recently investigated in the context of mathematical programming, LU factorization, and QR factorization. To represent the nonzero structure of a matrix, we propose bipartite graph and hypergraph models that reduce the permutation problem to those of graph partitioning by vertex separator and hypergraph partitioning, respectively. Our experiments on a wide range of matrices, using the state-of-the-art graph and hypergraph partitioning tools MeTiS and PaToH, revealed that the proposed methods yield very effective solutions both in terms of solution quality and runtime.},
  owner        = {andrea},
  timestamp    = {2014.12.15},
}

@Article{Azad2016,
  author       = {Azad, Ariful and Ballard, Grey and Buluç, Aydın and Demmel, James and Grigori, Laura and Schwartz, Oded and Toledo, Sivan and Williams, Samuel},
  title        = {Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix Multiplication},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2016},
  volume       = {38},
  number       = {6},
  pages        = {C624-C651},
  doi          = {10.1137/15M104253X},
  abstract     = {Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high-performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. The scaling of existing parallel implementations of SpGEMM is heavily bound by communication. Even though 3D (or 2.5D) algorithms have been proposed and theoretically analyzed in the flat MPI model on Erdős--Rényi matrices, those algorithms had not been implemented in practice and their complexities had not been analyzed for the general case. In this work, we present the first implementation of the 3D SpGEMM formulation that exploits multiple (intranode and internode) levels of parallelism, achieving significant speedups over the state-of-the-art publicly available codes at all levels of concurrencies. We extensively evaluate our implementation and identify bottlenecks that should be subject to further research.},
}

@InProceedings{Bakhoda2009,
  author    = {Bakhoda, Ali and Yuan, George L. and Fung, Wilson W. L. and Wong, Henry and Aamodt, Tor M.},
  title     = {Analyzing {CUDA} workloads using a detailed {GPU} simulator},
  booktitle = {{P}roceedings of the {IEEE} {I}nternational {S}ymposium on {P}erformance {A}nalysis of {S}ystems and {S}oftware},
  date      = {2009},
  series    = {ISPASS '09},
  location  = {Boston, MA, USA},
  month     = {4},
  pages     = {163--174},
  doi       = {10.1109/ISPASS.2009.4919648},
  abstract  = {Modern Graphic Processing Units (GPUs) provide sufficiently flexible programming models that understanding their performance can provide insight in designing tomorrow's manycore processors, whether those are GPUs or otherwise. The combination of multiple, multithreaded, SIMD cores makes studying these GPUs useful in understanding tradeoffs among memory, data, and thread level parallelism. While modern GPUs offer orders of magnitude more raw computing power than contemporary CPUs, many important applications, even those with abundant data level parallelism, do not achieve peak performance. This paper characterizes several non-graphics applications written in NVIDIA's CUDA programming model by running them on a novel detailed microarchitecture performance simulator that runs NVIDIA's parallel thread execution (PTX) virtual instruction set. For this study, we selected twelve non-trivial CUDA applications demonstrating varying levels of performance improvement on GPU hardware (versus a CPU-only sequential version of the application). We study the performance of these applications on our GPU performance simulator with configurations comparable to contemporary high-end graphics cards. We characterize the performance impact of several microarchitecture design choices including choice of interconnect topology, use of caches, design of memory controller, parallel workload distribution mechanisms, and memory request coalescing hardware. Two observations we make are (1) that for the applications we study, performance is more sensitive to interconnect bisection bandwidth rather than latency, and (2) that, for some applications, running fewer threads concurrently than on-chip resources might otherwise allow can improve performance by reducing contention in the memory system.},
  keywords  = {cache storage;computer graphic equipment;instruction sets;multi-threading;multiprocessing systems;parallel architectures;CUDA programming;CUDA workload;GPU hardware;GPU simulator;caches;flexible programming model;graphic processing unit;high-end graphics card;interconnect topology;memory controller;memory request coalescing hardware;microarchitecture design;microarchitecture performance simulator;parallel thread execution;parallel workload distribution;virtual instruction set;Analytical models;Computational modeling;Concurrent computing;Graphics;Hardware;Microarchitecture;Parallel processing;Parallel programming;Process design;Yarn},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@TechReport{Baskaran2008,
  author      = {Baskaran, Muthu Manikandan and Bordawekar, Rajesh},
  title       = {Optimizing Sparse Matrix-Vector Multiplication on {GPUs} Using Compile-time and Run-time Strategies},
  institution = {IBM Research Division},
  date        = {2008},
  url         = {http://domino.watson.ibm.com/library/CyberDig.nsf/1e4115aea78b6e7c85256b360066f0d4/1d32f6d23b99f7898525752200618339?OpenDocument},
  abstract    = {We are witnessing the emergence of Graphics Processor units (GPUs) as powerful massively parallel systems. Furthermore, the introduction of new APIs for general-purpose computations on GPUs, namely CUDA from NVIDIA, Stream SDK from AMD, and OpenCL, makes GPUs an attractive choice for high-performance numerical and scientific computing. Sparse Matrix-Vector multiplication (SpMV) is one of the most important and heavily used kernels in scientific computing. However with indirect and irregular memory accesses resulting in more memory accesses per floating point operation, optimization of SpMV kernel is a significant challenge in any architecture. \\ In this paper, we evaluate the various challenges in developing a high-performance SpMV kernel on NVIDIA GPUs using the CUDA programming model and propose a framework that employs both compile-time and run-time optimizations. The compile-time optimizations include: (1) exploiting synchronization-free parallelism, (2) optimized thread mapping based on the affinity towards optimal memory access pattern, (3) optimized off-chip memory access to tolerate the high latency, and (4) exploiting data reuse. The runtime optimizations involve a runtime inspection of the sparse matrix to determine dense non-zero sub-blocks, which facilitate the reuse of input vector elements while execution. We propose a new blocked storage format for storing and accessing elements of a sparse matrix in an optimized manner from the GPU memories. We evaluate our optimizations over two classes of NVIDIA GPU chips, namely, GeForce 8800 GTX and GeForce GTX 280, and we compare the performance of our approach with that of existing parallel SpMV implementations such as the one from NVIDIA's CUDPP library and the one implemented using optimal segmented scan primitive. Our approach outperforms the other existing implementations by a factor of 2 to 4. Using our framework, we achieve a peak SpMV performance that is 70\% of the performance observed for SpMV computations using dense matrices stored in sparse format.},
  owner       = {andrea},
  timestamp   = {2017.05.07},
}

@InProceedings{Bayliss2006,
  author    = {Bayliss, Samuel and Bouganis, Christos-S. and Constantinides, George A. and Luk, Wayne},
  title     = {An {FPGA} Implementation of the Simplex Algorithm},
  booktitle = {{P}roceedings of the {IEEE} {I}nternational {C}onference on {F}ield {P}rogrammable {T}echnology},
  date      = {2006},
  series    = {FPT '06},
  location  = {Bangkok, TH},
  month     = dec,
  pages     = {49--56},
  doi       = {10.1109/FPT.2006.270294},
  abstract  = {Linear programming is applied to a large variety of scientific computing applications and industrial optimization problems. The Simplex algorithm is widely used for solving linear programs due to its robustness and scalability properties. However, application of the current software implementations of the Simplex algorithm to real-life optimization problems are time consuming when used as the bounding engine within an integer linear programming framework. This work aims to accelerate the Simplex algorithm by proposing a novel parameterizable hardware implementation of the algorithm on an FPGA. Evaluation of the proposed design using real problems demonstrates a speedup of up to 20 times over a highly optimized commercial software implementation running on a 3.4GHz Pentium 4 processor, which is itself 100 times faster than one of the main public domain solvers},
  keywords  = {field programmable gate arrays;integer programming;linear programming;logic design;microprocessor chips;3.4 GHz;FPGA;Pentium 4 processor;Simplex algorithm;bounding engine;industrial optimization problems;integer linear programming framework;scientific computing applications;Application software;Computer industry;Engines;Field programmable gate arrays;Integer linear programming;Linear programming;Robustness;Scalability;Scientific computing;Software algorithms},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Book{Bazaraa2010,
  author    = {Bazaraa, Mokhtar S. and Jarvis, John J. and Sherali, Hanif D.},
  title     = {Linear Programming and Network Flows},
  date      = {2010},
  publisher = {Wiley},
  owner     = {andrea},
  timestamp = {2015.09.28},
}

@Article{Belotti2013,
  author       = {Belotti, Pietro and Kirches, Christian and Leyffer, Sven and Linderoth, Jeff and Luedtke, James and Mahajan, Ashutosh},
  title        = {Mixed-integer nonlinear optimization},
  journaltitle = {Acta Numerica},
  date         = {2013},
  volume       = {22},
  month        = may,
  pages        = {1--131},
  issn         = {1474-0508},
  doi          = {10.1017/S0962492913000032},
  abstract     = {Many optimal decision problems in scientific, engineering, and public sector applications involve both discrete decisions and nonlinear system dynamics that affect the quality of the final design or plan. These decision problems lead to mixed-integer nonlinear programming (MINLP) problems that combine the combinatorial difficulty of optimizing over discrete variable sets with the challenges of handling nonlinear functions. We review models and applications of MINLP, and survey the state of the art in methods for solving this challenging class of problems. \\Most solution methods for MINLP apply some form of tree search. We distinguish two broad classes of methods: single-tree and multitree methods. We discuss these two classes of methods first in the case where the underlying problem functions are convex. Classical single-tree methods include nonlinear branch-and-bound and branch-and-cut methods, while classical multitree methods include outer approximation and Benders decomposition. The most efficient class of methods for convex MINLP are hybrid methods that combine the strengths of both classes of classical techniques. \\Non-convex MINLPs pose additional challenges, because they contain non-convex functions in the objective function or the constraints; hence even when the integer variables are relaxed to be continuous, the feasible region is generally non-convex, resulting in many local minima. We discuss a range of approaches for tackling this challenging class of problems, including piecewise linear approximations, generic strategies for obtaining convex relaxations for non-convex functions, spatial branch-and-bound methods, and a small sample of techniques that exploit particular types of non-convex structures to obtain improved convex relaxations. \\We finish our survey with a brief discussion of three important aspects of MINLP. First, we review heuristic techniques that can obtain good feasible solution in situations where the search-tree has grown too large or we require real-time solutions. Second, we describe an emerging area of mixed-integer optimal control that adds systems of ordinary differential equations to MINLP. Third, we survey the state of the art in software for MINLP.},
  numpages     = {131},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Manual{Berkelaar2013,
  author    = {Berkelaar, M. and Eikland, K. and Notebaert, P.},
  title     = {{lp\_solve} reference guide},
  date      = {2013},
  edition   = {5.5.2.0},
  note      = {Accessed 30 March 2014},
  url       = {http://lpsolve.sourceforge.net/5.5/},
  month     = {4},
  owner     = {andrea},
  timestamp = {2014.03.30},
}

@Book{Bertsekas1999,
  author    = {Bertsekas, Dimitri P.},
  title     = {Nonlinear Programming},
  date      = {1999},
  publisher = {Athena Scientific},
  isbn      = {1-886529-00-0},
  owner     = {andrea},
  timestamp = {2015.09.28},
}

@InProceedings{Bieling2010,
  author     = {Bieling, Jakob and Peschlow, Patrick and Martini, Peter},
  title      = {An Efficient {GPU} implementation of the Revised Simplex Method},
  booktitle  = {{P}roceedings of the 2011 {IEEE} {I}nternational {P}arallel and {D}istributed {P}rocessing {S}ymposium},
  date       = {2010},
  series     = {IPDPS '10},
  location   = {Atlanta, GE, USA},
  pages      = {1--8},
  doi        = {10.1109/IPDPSW.2010.5470831},
  abstract   = {The computational power provided by the massive parallelism of modern graphics processing units (GPUs) has moved increasingly into focus over the past few years. In particular, general purpose computing on GPUs (GPGPU) is attracting attention among researchers and practitioners alike. Yet GPGPU research is still in its infancy, and a major challenge is to rearrange existing algorithms so as to obtain a significant performance gain from the execution on a GPU. In this paper, we address this challenge by presenting an efficient GPU implementation of a very popular algorithm for linear programming, the revised simplex method. We describe how to carry out the steps of the revised simplex method to take full advantage of the parallel processing capabilities of a GPU. Our experiments demonstrate considerable speedup over a widely used CPU implementation, thus underlining the tremendous potential of GPGPU.},
  annotation = {{I}mplementation of the two-phase tableau simplex method with steepest edge heuristics instead of pseudocost. {A}lways find the edge from the current vertex with the steepest local slope under the objective function leading to an adjacent vertex. {I}mplemented with {C}-for-graphics. {T}ested against randomly generated instances, max speedup $\times$10 compared to the same problem on {GPLK}. {PC} {I}ntel {C}ore 2 {D}uo {E}8400 {CPU} (3 {GH}z, {FSB} 1333 {MH}z, 6 {MB} {L}2 cache), {N}vidia {G}e{F}orce 9600 {GT} {GPU} (1 {GB} {RAM}, 64 shader processors).},
  file       = {:home/andrea/Dropbox/PhD/Papers/Bieling et al. - An Efficient GPU Implementation of the Revised Simplex Method.pdf:PDF},
  keywords   = {coprocessors;linear programming;mathematics computing;parallel processing;general purpose computing;graphics processing units;linear programming;parallel processing;revised simplex method;Central Processing Unit;Computer graphics;Computer science;Concurrent computing;Hardware;Linear algebra;Linear programming;Parallel processing;Performance gain;Scalability},
  owner      = {ap8213},
  timestamp  = {2014.10.09},
}

@Article{Bixby2002,
  author       = {Bixby, Robert E.},
  title        = {Solving Real-World Linear Programs: A Decade And More Of Progress},
  journaltitle = {Operations Research},
  date         = {2002},
  volume       = {50},
  number       = {1},
  month        = {1},
  pages        = {3--15},
  doi          = {10.1287/opre.50.1.3.17780},
  abstract     = {This paper is an invited contribution to the 50th anniversary issue of the journalOperations Research, published by the Institute of Operations Research and Management Science (INFORMS). It describes one person's perspective on the development of computational tools for linear programming. The paper begins with a short personal history, followed by historical remarks covering the some 40 years of linear-programming developments that predate my own involvement in this subject. It concludes with a more detailed look at the evolution of computational linear programming since 1987.},
  owner        = {andrea},
  timestamp    = {2015.09.28},
}

@InBook{Bixby2004,
  author    = {Bixby, Robert E. and Fenelon, Mary and Gu, Zonghao and Rothberg, Ed and Wunderling, Roland},
  title     = {Mixed-Integer Programming: A Progress Report},
  booktitle = {{T}he {S}harpest {C}ut},
  date      = {2004},
  editor    = {Martin Grötschel},
  series    = {MOS-SIAM Series on Optimization},
  chapter   = {18},
  pages     = {309--325},
  doi       = {10.1137/1.9780898718805.ch18},
  eprint    = {http://epubs.siam.org/doi/pdf/10.1137/1.9780898718805.ch18},
  abstract  = {Over the last several decades, from the early 1970s to as recently as 1998, the underlying solution technology in commercial mixed-integer programming codes remained essentially unchanged. In spite of important advances in the theory, many of these advances have clear computational value. In the last several years, that situation has changed. The result has been a major step forward in our ability to solve real-world mixed-integer programming problems.},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Blackford2001,
  author       = {Blackford, L. S. and Demmel, J. and Dongarra, J. and Duff, I. and Hammarling, S. and Henry, G. and Heroux, M. and Kaufman, L. and Lumsdaine, A. and Petitet, A. and Pozo, R. and Remington, K. and Whaley, R. C.},
  title        = {An Updated Set of Basic Linear Algebra Subprograms ({BLAS})},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {2001},
  volume       = {28},
  pages        = {135--151},
  doi          = {10.1145/567806.567807},
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@InProceedings{Bleris2006,
  author    = {Bleris, L. G. and Vouzis, P. D. and Arnold, M. G. and Kothare, M. V.},
  title     = {A co-processor {FPGA} platform for the implementation of real-time model predictive control},
  booktitle = {{A}merican {C}ontrol {C}onference},
  date      = {2006},
  series    = {ACC '06},
  location  = {Minneapolis, MN, USA},
  month     = {6},
  pages     = {6},
  doi       = {10.1109/ACC.2006.1656499},
  abstract  = {In order to effectively control nonlinear and multivariable models, and to incorporate constraints on system states, inputs and outputs (bounds, rate of change), a suitable (sometimes necessary) controller is model predictive control (MPC). MPC is an optimization-based control scheme that requires abundant matrix operations for the calculation of the optimal control moves. In this work we propose a mixed software and hardware embedded MPC implementation. Using a codesign step and based on profiling results, we decompose the optimization algorithm into two parts: one that fits into a host processor and one that fits into a custom made unit that performs the computationally demanding arithmetic operations. The profiling results and information on the co-processor design are provided},
  keywords  = {control engineering computing;coprocessors;field programmable gate arrays;matrix algebra;multivariable control systems;nonlinear control systems;optimal control;predictive control;coprocessor FPGA platform;coprocessor design;matrix operations;multivariable model control;nonlinear model control;optimal control;optimization-based control;real-time model predictive control;Arithmetic;Coprocessors;Embedded software;Field programmable gate arrays;Hardware;Matrix decomposition;Nonlinear control systems;Optimal control;Predictive control;Predictive models},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Blumofe1995,
  Title                    = {Cilk: An Efficient Multithreaded Runtime System},
  Author                   = {Blumofe, Robert D. and Joerg, Christopher F. and Kuszmaul, Bradley C. and Leiserson, Charles E. and Randall, Keith H. and Zhou, Yuli},
  Booktitle                = {{P}roceedings of the 5th {ACM} {SIGPLAN} {S}ymposium on {P}rinciples and {P}ractice of {P}arallel {P}rogramming},

  Address                  = {New York, NY, USA},
  Pages                    = {207--216},
  Publisher                = {ACM},
  Series                   = {PPOPP '95},

 abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical path” of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of optimal.The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the *Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship.},
  Acmid                    = {209958},
  Date                     = {1995},
  Doi                      = {10.1145/209936.209958},
  ISBN                     = {0-89791-700-6},
  Location                 = {Santa Barbara, California, USA},
  Numpages                 = {10}
}

@InProceedings{Boechat2013,
  author    = {Boéchat, Marc-Alexandre and Liu, Junyi and Peyril, Helfried and Zanarini, Alessandro and Besselmann, Thomas},
  title     = {An Architecture for Solving Quadratic Programs with the Fast Gradient Method on a Field Programmable Gate Array},
  booktitle = {{P}roceedings of the 21st {M}editerranean {C}onference on {C}ontrol {A}utomation},
  date      = {2013},
  series    = {MED '13},
  location  = {Platanias-Chania, GR},
  month     = {6},
  pages     = {1557--1562},
  doi       = {10.1109/MED.2013.6608929},
  abstract  = {In this paper an architecture for the implementation of gradient-based optimisation methods on a Field Programmable Gate Array (FPGA) is proposed. Combining the algorithmic advantages of gradient-based algorithms with the computational strengths of a tailored FPGA implementation allows to solve quadratic programs occurring, for example, in Model Predictive Control (MPC) applications in the microsecond range. The experimental comparisons show a computational advantage of the proposed FPGA implementation against parallel software versions ranging between one and two orders of magnitude. The proposed FPGA-based solution can broaden the applicability of MPC to problems that were considered out-of-reach till recent years.},
  keywords  = {field programmable gate arrays;gradient methods;predictive control;quadratic programming;FPGA;MPC applications;architecture;computational strengths;fast gradient method;field programmable gate array;gradient-based algorithms;gradient-based optimisation methods;microsecond range;model predictive control;quadratic programs;Adders;Clocks;Computer architecture;Field programmable gate arrays;Gradient methods;Vectors},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Boisvert1997,
  Title                    = {Matrix Market: A Web Resource for Test Matrix Collections},
  Author                   = {Boisvert, Ronald F. and Pozo, Roldan and Remington, Karin and Barrett, Richard F. and Dongarra, Jack J.},
  Booktitle                = {{P}roceedings of the {IFIP} {TC2/WG2.5} {W}orking {C}onference on {Q}uality of {N}umerical {S}oftware: {A}ssessment and {E}nhancement},

  Address                  = {London, UK},
  Pages                    = {125--137},
  Publisher                = {Chapman \& Hall, Ltd.},

  Date                     = {1997},
  ISBN                     = {0-412-80530-8},
  Location                 = {Oxford, UK},
  Numpages                 = {13},
  Owner                    = {andrea},
  Timestamp                = {2016.11.07},
  Url                      = {http://dl.acm.org/citation.cfm?id=265834.265854}
}

@InProceedings{Boland2008,
  author    = {Boland, D. and Constantinides, G. A.},
  title     = {An {FPGA}-based implementation of the {MINRES} algorithm},
  booktitle = {{P}roceedings of the {I}nternational {C}onference on {F}ield {P}rogrammable {L}ogic and {A}pplications},
  date      = {2008},
  series    = {FPL '08},
  location  = {Heidelberg, DEU},
  month     = sep,
  pages     = {379--384},
  doi       = {10.1109/FPL.2008.4629967},
  abstract  = {Due to continuous improvements in the resources available on FPGAs, it is becoming increasingly possible to accelerate floating point algorithms. The solution of a system of linear equations forms the basis of many problems in engineering and science, but its calculation is highly time consuming. The minimum residual algorithm (MINRES) is one method to solve this problem, and is highly effective provided the matrix exhibits certain characteristics. This paper examines an IEEE 754 single precision floating point implementation of the MINRES algorithm on an FPGA. It demonstrates that through parallelisation and heavy pipelining of all floating point components it is possible to achieve a sustained performance of up to 53 GFLOPS on the Virtex5-330T. This compares favourably to other hardware implementations of floating point matrix inversion algorithms, and corresponds to an improvement of nearly an order of magnitude compared to a software implementation.},
  keywords  = {field programmable gate arrays;floating point arithmetic;FPGA-based implementation;IEEE 754;MINRES algorithm;Virtex5-330T;floating point algorithms;floating point matrix inversion;linear equations;minimum residual algorithm;parallelisation;Continuous improvement;Educational institutions;Equations;Field programmable gate arrays;Hardware;Iterative algorithms;Iterative methods;Scientific computing;Sparse matrices;Symmetric matrices},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InBook{Bosch2005,
  author    = {Bosch, Robert and Tick, Michael},
  title     = {Integer programming},
  booktitle = {{S}earch {M}ethodologies: {I}ntroductory {T}utorials in {O}ptimization and {D}ecision {S}upport {T}echniques},
  date      = {2005},
  editor    = {Edmund K. Burke and Graham Kendall},
  publisher = {Springer, US},
  chapter   = {9},
  pages     = {69--95},
  doi       = {10.1007/0-387-28356-0_3},
  abstract  = {Over the last 20 years, the combination of faster computers, more reliable data, and improved algorithms has resulted in the near-routine solution of many integer programs of practical interest. Integer programming models are used in a wide variety of applications, including scheduling, resource assignment, planning, supply chain design, auction design, and many, many others. In this tutorial, we outline some of the major themes involved in creating and solving integer programming models.},
  owner     = {andrea},
  timestamp = {2015.04.24},
}

@InProceedings{Boukedjar2012,
  author    = {Boukedjar, Abdelamine and Lalami, Mohammed Esseghir and El-Baz, Didier},
  title     = {Parallel Branch and Bound on a {CPU}--{GPU} System},
  booktitle = {{P}roceedings of the 20th {E}uromicro {I}nternational {C}onference on {P}arallel, {D}istributed and {N}etwork-{B}ased {P}rocessing},
  date      = {2012},
  series    = {PDP '12},
  location  = {Garching, DE},
  month     = feb,
  pages     = {392--398},
  doi       = {10.1109/PDP.2012.23},
  abstract  = {Hybrid implementation via CUDA of a branch and bound method for knapsack problems is proposed. Branch and bound computations can be carried out either on the CPU or on the GPU according to the size of the branch and bound list, i.e. the number of nodes. Tests are carried out on a Tesla C2050 GPU. A first series of computational results showing a substantial speedup is displayed and analyzed.},
  issn      = {1066-6192},
  keywords  = {graphics processing units;knapsack problems;parallel architectures;tree searching;CPU-GPU system;CUDA;knapsack problems;parallel branch and bound;Computer architecture;Graphics processing unit;Instruction sets;Kernel;Optimization;Parallel algorithms;Upper bound;CUDA;branch and bound;combinatorial optimization;computing;hybrid computing;knapsack problems},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Bouwmeester2015,
  author       = {Bouwmeester, H. and Dougherty, B. and Knyazev, A.},
  title        = {Nonsymmetric Preconditioning for Conjugate Gradient and Steepest Descent Methods},
  journaltitle = {Procedia Computer Science},
  date         = {2015},
  volume       = {51},
  pages        = {276--285},
  issn         = {1877-0509},
  doi          = {10.1016/j.procs.2015.05.241},
  abstract     = {We analyze a possibility of turning off post-smoothing (relaxation) in geometric multigrid when used as a preconditioner in preconditioned conjugate gradient (PCG) linear and eigenvalue solvers for the 3D Laplacian. The geometric Semicoarsening Multigrid (SMG) method is provided by the hypre parallel software package. We solve linear systems using two variants (standard and flexible) of PCG and preconditioned steepest descent (PSD) methods. The eigen-value problems are solved using the locally optimal block preconditioned conjugate gradient (LOBPCG) method available in hypre through BLOPEX software. We observe that turning off the post-smoothing in SMG dramatically slows down the standard PCG-SMG. For flexible PCG and LOBPCG, our numerical tests show that removing the post-smoothing results in overall 40--50 percent acceleration, due to the high costs of smoothing and relatively insignificant decrease in convergence speed. We demonstrate that PSD-SMG and flexible PCG-SMG converge similarly if SMG post-smoothing is off. A theoretical justification is provided.},
  keywords     = {linear equations},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InProceedings{Boyer2013,
  author    = {Boyer, Vincent and El Baz, Didier},
  title     = {Recent Advances on {GPU} Computing in Operations Research},
  booktitle = {{P}roceedings of the 27th {IEEE} {I}nternational {S}ymposium on {P}arallel \& {D}istributed {P}rocessing {W}orkshops and {P}h{D} {F}orum},
  date      = {2013},
  series    = {IPDPSW '13},
  publisher = {IEEE Computer Society},
  location  = {Boston, MA, USA},
  month     = may,
  pages     = {1778--1787},
  doi       = {10.1109/IPDPSW.2013.45},
  abstract  = {In the last decade, Graphics Processing Units(GPUs) have gained an increasing popularity as accelerators for High Performance Computing (HPC) applications. Recent GPUs are not only powerful graphics engines but also highly threaded parallel computing processors that can achieve sustainable speedup as compared with CPUs. In this context, researchers try to exploit the capability of this architecture to solve difficult problems in many domains in science and engineering. In this article, we present recent advances on GPU Computing in Operations Research. We focus in particular on Integer Programming and Linear Programming.},
  address   = {Los Alamitos, CA, USA},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@TechReport{Brandes2016,
  author      = {Brandes, Thomas and Schricker, Eric and Soddemann, Thomas},
  title       = {The {LAMA} Approach for Writing Portable Applications on Heterogenous Architectures},
  institution = {Fraunhofer Institute for Algorithms and Scientific Computing},
  date        = {2016},
  location    = {Sankt Augustin, Germany,},
  url         = {http://www.libama.org/assets/lamawhitepaper.pdf},
  abstract    = {Ensuring longevity and maintainability of modern software applications is mandatory for a proper return on investment. Since the hardware landscape is changing rapidly and will continue to do so, it is imperative to take on those topics also in the HPC domain where applications traditionally have a long live-span. For recent years, we have observed a trend towards more and more heterogeneous systems. Realizing the performance promises of the hardware vendors is a huge challenge to the software developer. Portability is the second challenge to be met in this context. In this paper we present our library LAMA. We created this library to address both challenges successfully in the realm of linear algebra and numerical mathematics. We introduce our solutions to heterogeneous memory and kernel management as well as our solutions to task parallelism. In the end we do performance and scalability benchmarks drawing a comparison to PETSc for the example of a CG solver.},
  owner       = {andrea},
  timestamp   = {2017.05.09},
}

@InCollection{Braun2001,
  author    = {Braun, Tracy D. and Siegel, Howard Jay and Maciejewski, Anthony A.},
  title     = {Heterogeneous Computing: Goals, Methods, and Open Problems},
  booktitle = {{H}igh {P}erformance {C}omputing ({HiPC} 2001)},
  date      = {2001},
  editor    = {Monien, Burkhard and Prasanna, Viktor K. and Vajapeyam, Sriram},
  language  = {English},
  volume    = {2228},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-43009-4},
  pages     = {307--318},
  doi       = {10.1007/3-540-45307-5_27},
  abstract  = {This paper discusses the material tob e presented by H. J. Siegel in his keynote talk. Distributed high-performance heterogeneous computing (HC) environments are composed of machines with varied computational capabilities interconnected by high-speed links. These environments are well suited to meet the computational demands of large, diverse groups of applications. One key factor in achieving the best performance possible from HC environments is the ability to assign effectively the applications to machines and schedule their execution. Several factors must be considered during this assignment. A conceptual model for the automatic decomposition of an application into tasks and assignment of tasks to machines is presented. An example of a static matching and scheduling approach for an HC environment is summarized. Some examples of current HC technology and open research problems are discussed.},
  month     = dec,
  owner     = {andrea},
  timestamp = {2015.07.13},
}

@InProceedings{Buss2010,
  Title                    = {{STAPL}: Standard Template Adaptive Parallel Library},
  Author                   = {Buss, Antal and Harshvardhan and Papadopoulos, Ioannis and Pearce, Olga and Smith, Timmie and Tanase, Gabriel and Thomas, Nathan and Xu, Xiabing and Bianco, Mauro and Amato, Nancy M. and Rauchwerger, Lawrence},
  Booktitle                = {{P}roceedings of the 3rd {A}nnual {H}aifa {E}xperimental {S}ystems {C}onference},

  Address                  = {New York, NY, USA},
  Pages                    = {14:1--14:10},
  Publisher                = {ACM},
  Series                   = {SYSTOR '10},

 abstract = {The Standard Template Adaptive Parallel Library (stapl) is a high-productivity parallel programming framework that extends C++ and stl with unified support for shared and distributed memory parallelism. stapl provides distributed data structures (pContainers) and parallel algorithms (pAlgorithms) and a generic methodology for extending them to provide customized functionality. The stapl runtime system provides the abstraction for communication and program execution. In this paper, we describe the major components of stapl and present performance results for both algorithms and data structures showing scalability up to tens of thousands of processors.},
  Acmid                    = {1815713},
  Articleno                = {14},
  Date                     = {2010},
  Doi                      = {10.1145/1815695.1815713},
  ISBN                     = {978-1-60558-908-4},
  Keywords                 = {high productivity parallel programming, library, parallel data structures},
  Location                 = {Haifa, IL},
  Numpages                 = {10}
}

@Article{Buttari2009,
  author       = {Buttari, A. and Langou, J. and Kurzak, J. and Dongarra, J.},
  title        = {A class of parallel tiled linear algebra algorithms for multicore architectures},
  journaltitle = {Parallel Computing},
  date         = {2009},
  volume       = {35},
  number       = {1},
  pages        = {38--53},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2008.10.002},
  abstract     = {As multicore systems continue to gain ground in the high performance computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents algorithms for the Cholesky, LU and QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data. These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in out of order execution of tasks which will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with LAPACK algorithms where parallelism can only be exploited at the level of the BLAS operations and vendor implementations.},
  keywords     = {Linear algebra},
  owner        = {ap8213},
  timestamp    = {2014.10.09},
}

@Article{Cafieri2007,
  Title                    = {On the iterative solution of {KKT} systems in potential reduction software for large-scale quadratic problems},
  Author                   = {Cafieri, S. and D'Apuzzo, M. and De Simone, V. and di Serafino, D.},
  Number                   = {1},
  Pages                    = {27--45},
  Volume                   = {38},

 abstract = {Iterative solvers appear to be very promising in the development of efficient software, based on Interior Point methods, for large-scale nonlinear optimization problems. In this paper we focus on the use of preconditioned iterative techniques to solve the KKT system arising at each iteration of a Potential Reduction method for convex Quadratic Programming. We consider the augmented system approach and analyze the behaviour of the Constraint Preconditioner with the Conjugate Gradient algorithm. Comparisons with a direct solution of the augmented system and with MOSEK show the effectiveness of the iterative approach on large-scale sparse problems.},
  Date                     = {2007},
  Doi                      = {10.1007/s10589-007-9035-y},
  ISSN                     = {1573-2894},
  Journaltitle             = {Computational Optimization and Applications},
  Owner                    = {andrea},
  Timestamp                = {2016.05.27}
}

@Article{Calgaro2010,
  Title                    = {Incremental incomplete {LU} factorizations with applications},
  Author                   = {Calgaro, Caterina and Chehab, Jean-Paul and Saad, Yousef},
  Number                   = {5},
  Pages                    = {811--837},
  Volume                   = {17},

 abstract = {This paper addresses the problem of computing preconditioners for solving linear systems of equations with a sequence of slowly varying matrices. This problem arises in many important applications. For example, a common situation in computational fluid dynamics, is when the equations change only slightly, possibly in some parts of the physical domain. In such situations it is wasteful to recompute entirely any LU or ILU factorizations computed for the previous coefficient matrix. A number of techniques for computing incremental ILU factorizations are examined. For example we consider methods based on approximate inverses as well as alternating techniques for updating the factors L and U of the factorization.},
  Date                     = {2010},
  Doi                      = {10.1002/nla.756},
  ISSN                     = {1099--1506},
  Journaltitle             = {Numerical Linear Algebra with Applications},
  Publisher                = {John Wiley \& Sons, Ltd.}
}

@InProceedings{Candel2015,
  author    = {Candel, F. and Petit, S. and Sahuquillo, J. and Duato, J.},
  title     = {Accurately modeling the {GPU} memory subsystem},
  booktitle = {{P}roceedings of the 2015 {I}nternational {C}onference on {H}igh {P}erformance {C}omputing {S}imulation},
  date      = {2015},
  series    = {HPCS '15},
  location  = {Amsterdam, NL},
  month     = jul,
  pages     = {179--186},
  doi       = {10.1109/HPCSim.2015.7237038},
  abstract  = {Nowadays, research on GPU processor architecture is extraordinarily active since these architectures offer much more performance per watt than CPU architectures. This is the main reason why massive deployment of GPU multiprocessors is considered one of the most feasible solutions to attain exascale computing capabilities. In this context, ongoing GPU architecture research is required to improve GPU programmability as well as to integrate CPU and GPU cores in the same die. One of the most important research topics in current GPUs, is the GPU memory hierarchy, since its design goals are very different from those of conventional CPU memory hierarchies. To explore novel designs to better support General Purpose computing in GPUs (GPGPU computing) as well as to improve the performance of GPU and CPU/GPU systems, researchers often require advanced microarchitectural simulators with detailed models of the memory subsystem. Nevertheless, due to fast speed at which current GPU architectures evolve, simulation accuracy of existing state-of-the-art simulators suffers. This paper focuses on accurately modeling the GPU memory subsystem. We identified three main aspects that should be modeled with more accuracy: i) miss status holding registers, ii) coalescing vector memory requests, and iii) non-blocking GPU stores. In this sense, we extend the Multi2Sim heterogeneous CPU/GPU processor simulator to model these aspects with enough accuracy. Experimental results show that if these aspects are not considered in the simulation framework, performance deviations can rise in some applications up to 70\%, 75\%, and 60\%, respectively.},
  keywords  = {graphics processing units;memory architecture;multiprocessing systems;CPU memory hierarchies;GPGPU computing;GPU cores;GPU memory hierarchy;GPU memory subsystem modeling;GPU multiprocessors;GPU processor architecture;GPU programmability;Multi2Sim heterogeneous CPU-GPU processor simulator;advanced microarchitectural simulators;coalescing vector memory requests;exascale computing capabilities;general purpose computing;miss status holding registers;nonblocking GPU stores;Computational modeling;Computer architecture;Graphics processing units;Load modeling},
}

@Article{Candel2017,
  author       = {Candel, Francisco and Petit, Salvador and Sahuquillo, Julio and Duato},
  title        = {Accurately modeling the on-chip and off-chip {GPU} memory subsystem},
  journaltitle = {Future Generation Computer Systems},
  date         = {2017},
  issn         = {0167-739X},
  doi          = {10.1016/j.future.2017.02.012},
  abstract     = {Research on GPU architecture is becoming pervasive in both the academia and the industry because these architectures offer much more performance per watt than typical CPU architectures. This is the main reason why massive deployment of GPU multiprocessors is considered one of the most feasible solutions to attain exascale computing capabilities. \\The memory hierarchy of the GPU is a critical research topic, since its design goals widely differ from those of conventional CPU memory hierarchies. Researchers typically use detailed microarchitectural simulators to explore novel designs to better support GPGPU computing as well as to improve the performance of GPU and CPU-GPU systems. In this context, the memory hierarchy is a critical and continuously evolving subsystem. \\Unfortunately, the fast evolution of current memory subsystems deteriorates the accuracy of existing state-of-the-art simulators. This paper focuses on accurately modeling the entire (both on-chip and off-chip) GPU memory subsystem. For this purpose, we identify four main memory related components that impact on the overall performance accuracy. Three of them belong to the on-chip memory hierarchy: (i) memory request coalescing mechanisms, (ii) miss status holding registers, and (iii) cache coherence protocol; while the fourth component refers to the memory controller and GDDR memory working activity. \\To evaluate and quantify our claims, we accurately modeled the aforementioned memory components in an extended version of the state-of-the-art Multi2Sim heterogeneous CPU-GPU processor simulator. Experimental results show important deviations, which can vary the final system performance provided by the simulation framework up to a factor of three. The proposed GPU model has been compared and validated against the original framework and the results from a real AMD Southern-Islands 7870HD GPU.},
  keywords     = {Applied modeling and simulation},
}

@InProceedings{Carneiro2011,
  author     = {Carneiro, Tiago and Muritiba, Albert Einstein and Negreiros, Marcos and de Campos, Gustavo Augusto Lima},
  title      = {A New Parallel Schema for Branch-and-Bound Algorithms Using {GPGPU}},
  booktitle  = {{P}roceedings of the 23rd {I}nternational {S}ymposium on {C}omputer {A}rchitecture and {H}igh {P}erformance {C}omputing},
  date       = {2011},
  series     = {SBAC-PAD '11},
  location   = {Vitória, Espírito Santo, BR},
  month      = oct,
  pages      = {41--47},
  doi        = {10.1109/SBAC-PAD.2011.20},
  abstract   = {This work presents a new parallel procedure designed to process combinatorial B\&B algorithms using GPGPU. In our schema we dispatch a number of threads that treats intelligently the massively parallel processors of NVIDIA GeForce graphical units. The strategy is to build sequentially a series of initial searches that can map a subspace of the B\&B tree by starting a number of limited threads after achieving a specific level of the tree. The search is then processed massively by DFS. The whole subspace is optimized accordingly to memory and limits of threads and blocks available by the GPU. We compare our results with its OpenMP and Serial versions of the same search schema using explicitly enumeration (all possible solutions) to the Asymmetrical Travelling Salesman Problem's instances. We also show the great superiority of our GPGPU based method.},
  annotation = {{T}he implementation used {CUDA}. {I}t's the depth first search ({DFS}). {S}earch starts after 2 levels of branching. {E}ach thread solves a problem. {E}ach {DFS} generates only one node at the time and if this is pruned, the search goes back to the parent. {E}ach active node is processed by a {GPU} thread. {A}t the end of its processing, it is synchronized with the other threads. {W}hen a node doesn't find a solution, it returns its parent, else it returns the value of the solution. {T}hus, each node is a {DFS} root. {T}he work is tested on randomly generated instances of the asymmetric travelling salesman problem ($c_{ij} \neq c_{ij}$). {T}he solution is found by complete enumeration of all solutions $({N}-1)!$ where ${N}$ is the number of cities. {R}esults are compared against an {O}pen{MP} version of the algorithm. {T}he test environment is an {I}ntel {C}ore i5 750 (2.66 {GH}z, 3.2 {GH}z over demand, 4 {GB} {RAM}), an {U}buntu},
  file       = {:home/ap8213/Documents/PhD/Papers/Carneiro et al. - A new parallel schema for branch-and-bound algorithms using GPGPU.pdf:PDF},
  issn       = {1550-6533},
  owner      = {ap8213},
  timestamp  = {2014.10.09},
}

@InProceedings{Chakroun2012,
  author    = {Chakroun, I. and Melab, N.},
  title     = {An Adaptive Multi-{GPU} based Branch-and-Bound. A Case Study: the Flow-Shop Scheduling Problem},
  booktitle = {{P}roceedings of the 14th {IEEE} {I}nternational {C}onference on {H}igh {P}erformance {C}omputing and {C}ommunications},
  date      = {2012},
  series    = {HPCC '12},
  location  = {Liverpool, UK},
  month     = {6},
  pages     = {389--395},
  doi       = {10.1109/HPCC.2012.59},
  abstract  = {Solving exactly Combinatorial Optimization Problems (COPs) using a Branch-and-Bound (B\&B) algorithm requires a huge amount of computational resources. Therefore, we recently investigated designing B\&B algorithms on top of graphics processing units (GPUs) using a parallel bounding model. The proposed model assumes parallelizing the evaluation of the lower bounds on pools of sub-problems. The results demonstrated that the size of the evaluated pool has a significant impact on the performance of B\&B and that it depends strongly on the problem instance being solved. In this paper, we design an adaptative parallel B\&B algorithm for solving permutation-based combinatorial optimization problems such as FSP (Flow-shop Scheduling Problem) on GPU accelerators. To do so, we propose a dynamic heuristic for parameter auto-tuning at runtime. Another challenge of this pioneering work is to exploit larger degrees of parallelism by using the combined computational power of multiple GPU devices. The approach has been applied to the permutation flow-shop problem. Extensive experiments have been carried out on well-known FSP benchmarks using an Nvidia Tesla S1070 Computing System equipped with two Tesla T10 GPUs. Compared to a CPU-based execution, accelerations up to 105 are achieved for large problem instances.},
  owner     = {andrea},
  timestamp = {2014.12.11},
}

@Article{Chakroun2013,
  author       = {Chakroun, I. and Melab, N. and Mezmaz, M. and Tuyttens, D.},
  title        = {Combining multi-core and {GPU} computing for solving combinatorial optimization problems},
  journaltitle = {Journal of Parallel and Distributed computing},
  date         = {2013},
  volume       = {73},
  number       = {12},
  month        = dec,
  pages        = {1563--1577},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2013.07.023},
  abstract     = {In this paper, we revisit the design and implementation of Branch-and-Bound (B\&B) algorithms for solving large combinatorial optimization problems on GPU-enhanced multi-core machines. B\&B is a tree-based optimization method that uses four operators (selection, branching, bounding and pruning) to build and explore a highly irregular tree representing the solution space. In our previous works, we have proposed a GPU-accelerated approach in which only a single CPU core is used and only the bounding operator is performed on the GPU device. Here, we extend the approach (LL-GB\&B) in order to minimize the CPU-GPU communication latency and thread divergence. Such an objective is achieved through a GPU-based fine-grained parallelization of the branching and pruning operators in addition to the bounding one. The second contribution consists in investigating the combination of a GPU with multi-core processing. Two scenarios have been explored leading to two approaches: a concurrent (RLL-GB\&B) and a cooperative one (PLL-GB\&B). In the first one, the exploration process is performed concurrently by the GPU and the CPU cores. In the cooperative approach, the CPU cores prepare and off-load to GPU pools of tree nodes using data streaming while the GPU performs the exploration. The different approaches have been extensively experimented on the Flowshop scheduling problem. Compared to a single CPU-based execution, LL-GB\&B allows accelerations up to ($\times$160) for large problem instances. Moreover, when combining multi-core and GPU, we figure out that using RLL-GB\&B is not beneficial while PLL-GB\&B enables an improvement up to 36\% compared to LL-GB\&B.},
  acmid        = {2537492},
  issue_date   = {December, 2013},
  keywords     = {Flowshop scheduling problem, GPU accelerators, Multi-core computing, Parallel branch-and-bound},
  location     = {Orlando, FL, USA},
  numpages     = {15},
  owner        = {ap8213},
  publisher    = {Academic Press, Inc.},
  timestamp    = {2014.10.09},
}

@Article{Chakroun2012a,
  author       = {Chakroun, I. and Mezmaz, M. and Melab, N. and Bendjoudi, A.},
  title        = {Reducing thread divergence in a {GPU}-accelerated branch-and-bound algorithm},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2012},
  volume       = {25},
  number       = {8},
  month        = sep,
  pages        = {1121--1136},
  doi          = {10.1002/cpe.2931},
  abstract     = {In this paper, we address the design and implementation of graphical processing unit (GPU)-accelerated branch-and-bound algorithms (B\&B) for solving flow-shop scheduling optimization problems (FSP). Such applications are CPU-time consuming and highly irregular. On the other hand, GPUs are massively multithreaded accelerators using the single instruction multiple data model at execution. A major issue that arises when executing on GPU, a B\&B applied to FSP is thread or branch divergence. Such divergence is caused by the lower bound function of FSP that contains many irregular loops and conditional instructions. Our challenge is therefore to revisit the design and implementation of B\&B applied to FSP dealing with thread divergence. Extensive experiments of the proposed approach have been carried out on well-known FSP benchmarks using an Nvidia Tesla (C2050 GPU card). Compared with a CPU-based execution, accelerations up to $\times$ 77.46 are achieved for large problem instances.},
  owner        = {andrea},
  timestamp    = {2014.12.11},
}

@InProceedings{Che2013,
  author    = {Che, Shuai and Beckmann, Bradford M. and Reinhardt, Steven K. and Skadron, Kevin},
  title     = {Pannotia: Understanding Irregular {GPGPU} Graph Applications},
  booktitle = {{P}roceedings of the {IEEE} {I}nternational {S}ymposium on {W}orkload {C}haracterization},
  date      = {2013},
  series    = {IISWC '13},
  location  = {Portland, OR, USA},
  month     = sep,
  pages     = {185--195},
  doi       = {10.1109/IISWC.2013.6704684},
  abstract  = {GPUs have become popular recently to accelerate general-purpose data-parallel applications. However, most existing work has focused on GPU-friendly applications with regular data structures and access patterns. While a few prior studies have shown that some irregular workloads can also achieve speedups on GPUs, this domain has not been investigated thoroughly. Graph applications are one such set of irregular workloads, used in many commercial and scientific domains. In particular, graph mining -as well as web and social network analysis- are promising applications that GPUs could accelerate. However, implementing and optimizing these graph algorithms on SIMD architectures is challenging because their data-dependent behavior results in significant branch and memory divergence. To address these concerns and facilitate research in this area, this paper presents and characterizes a suite of GPGPU graph applications, Pannotia, which is implemented in OpenCL and contains problems from diverse and important graph application domains. We perform a first-step characterization and analysis of these benchmarks and study their behavior on real hardware. We also use clustering analysis to illustrate the similarities and differences of the applications in the suite. Finally, we make architectural and scheduling suggestions that will improve their execution efficiency on GPUs.},
  keywords  = {data mining;data structures;graph theory;graphics processing units;parallel processing;pattern clustering;scheduling;GPU-friendly applications;OpenCL;Pannotia;SIMD architectures;Web analysis;access patterns;branch and memory divergence;clustering analysis;commercial domains;data structures;data-dependent behavior;general-purpose data-parallel applications;graph algorithms;graph mining;irregular GPGPU graph applications;scientific domains;social network analysis;Kernel;Labeling;Radiation detectors},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Online{Chen2016,
  author    = {Chen, Zhangxin and Liu, Hui and Yang, Bo},
  title     = {Parallel Triangular Solvers on {GPU}},
  date      = {2016},
  abstract  = {In this paper, we investigate GPU based parallel triangular solvers systematically. The parallel triangular solvers are fundamental to incomplete LU factorization family preconditioners and algebraic multigrid solvers. We develop a new matrix format suitable for GPU devices. Parallel lower triangular solvers and upper triangular solvers are developed for this new data structure. With these solvers, ILU preconditioners and domain decomposition preconditioners are developed. Numerical results show that we can speed triangular solvers around seven times faster.},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp2.uni-trier.de/rec/bib/journals/corr/ChenLY16},
  eprint    = {1606.00541},
  eprinttype    = {arXiv},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  volume    = {abs/1606.00541},
}

@PhdThesis{Choi2006,
  author      = {Choi, S. C.},
  title       = {Iterative methods for singular linear equations and least-square problems},
  institution = {Stanford University},
  date        = {2006},
  url         = {https://web.stanford.edu/group/SOL/dissertations/sou-cheng-choi-thesis.pdf},
  abstract    = {CG, MINRES, and SYMMLQ are Krylov subspace methods for solving large symmetric systems of linear equations. CG (the conjugate-gradient method) is reliable on positive-definite systems, while MINRES and SYMMLQ are designed for indefinite systems. When these methods are applied to an inconsistent system (that is, a singular symmetric least-squares problem), CG could break down and SYMMLQ's solution could explode, while MINRES would give a least-squares solution but not necessarily the minimum-length solution (often called the pseudoinverse solution). This understanding motivates us to design a MINRES-like algorithm to compute minimum-length solutions to singular symmetric systems. \\MINRES uses QR factors of the tridiagonal matrix from the Lanczos process (where R is upper-tridiagonal). Our algorithm uses a QLP decomposition (where rotations on the right reduce R to lower-tridiagonal form), and so we call it MINRES-QLP. On singular or nonsingular systems, MINRES-QLP can give more accurate solutions than MINRES or SYMMLQ. We derive preconditioned MINRES-QLP, new stopping rules, and better estimates of the solution and residual norms, the matrix norm and condition number. \\For a singular matrix of arbitrary shape, we observe that null vectors can be obtained by solving least-squares problems involving the transpose of the matrix. For sparse rectangular matrices, this suggests an application of the iterative solver LSQR. In the square case, MINRES, MINRES-QLP, or LSQR are applicable. Results are given for solving homogeneous systems, computing the stationary probability vector for Markov Chain models, and finding null vectors for sparse systems arising in helioseismology.},
  owner       = {ap8213},
  timestamp   = {2014.03.31},
}

@InProceedings{Choo2014,
  Title                    = {Understanding and Optimizing {GPU} Cache Memory Performance for Compute Workloads},
  Author                   = {Choo, Kyoshin and Panlener, William and Jang, Byunghyun},
  Booktitle                = {{P}roceedings of the 13th {IEEE} {I}nternational {S}ymposium on {P}arallel and {D}istributed {C}omputing},

  Address                  = {Washington, DC, USA},
  Pages                    = {189--196},
  Publisher                = {IEEE Computer Society},
  Series                   = {ISPDC '14},

 abstract = {Processing elements such as CPUs and GPUs depend on cache technology to bridge the classic processor memory subsystem performance gap. As GPUs evolve into general purpose co-processors with CPUs sharing the load, good cache design and use becomes increasingly important. While both CPUs and GPUs must cooperate and perform well, their memory access patterns are very different. On CPUs only a few threads access memory simultaneously. On GPUs, there is significantly higher memory access contention among thousands of threads. Despite such different behavior, there is little research that investigates the behavior and performance of GPU caches in depth. In this paper, we present our extensive study on the characterization and improvement of GPU cache behavior and performance for general-purpose workloads using a cycle-accurate ISA level GPU architectural simulator that models one of the latest GPU architectures, Graphics Core Next (GCN) from AMD. Our study makes the following observations and improvements. First, we observe that L1 vector data cache hit rate is substantially lower when compared to CPU caches. The main culprit is compulsory misses caused by lack of data reuse among massively simultaneous threads. Second, there is significant memory access contention in shared L2 data cache, accounting for up to 19\% of total access for some benchmarks. This high contention remains a main performance barrier in L2 data cache even though its hit rate is high. Third, we demonstrate that memory access coalescing plays a critical role in reducing memory traffic. Finally we found that there exists inter-workgroup locality which can affect the cache behavior and performance. Our experimental results show memory performance can be improved by 1) shared L1 vector data cache where multiple compute units share a single cache to exploit inter-workgroup locality and increase data reusability, and 2) clustered workgroup scheduling where workgroups with consecutive IDs are assigned on the same compute unit.},
  Acmid                    = {2673102},
  Date                     = {2014},
  Doi                      = {10.1109/ISPDC.2014.29},
  ISBN                     = {978-1-4799-5919-8},
  Location                 = {Lymassol, CY},
  Numpages                 = {8},
  Owner                    = {andrea},
  Timestamp                = {2017.05.10}
}

@InProceedings{Christen2009,
  author    = {Christen, M. and Schenk, O. and Neufeld, E. and Messmer, P. and Burkhart, H.},
  title     = {Parallel data-locality aware stencil computations on modern micro-architectures},
  booktitle = {{P}roceedings of the 2009 {IEEE} {I}nternational {S}ymposium on {P}arallel {D}istributed {P}rocessing},
  date      = {2009},
  series    = {IPDPS '09},
  location  = {Rome, IT},
  month     = may,
  pages     = {1--10},
  doi       = {10.1109/IPDPS.2009.5161031},
  abstract  = {Novel micro-architectures including the Cell Broadband Engine Architecture and graphics processing units are attractive platforms for compute-intensive simulations. This paper focuses on stencil computations arising in the context of a biomedical simulation and presents performance benchmarks on both the Cell BE and GPUs and contrasts them with a benchmark on a traditional CPU system. Due to the low arithmetic intensity of stencil computations, typically only a fraction of the peak performance of the compute hardware is reached. An algorithm is presented, which reduces the bandwidth requirements and thereby improves performance by exploiting temporal locality of the data. We report on performance improvements over CPU implementations.},
  issn      = {1530-2075},
  keywords  = {coprocessors;digital simulation;medical computing;multiprocessing systems;parallel processing;performance evaluation;Cell Broadband Engine Architecture;biomedical simulation;graphics processing units;modern microarchitectures;parallel data-locality;performance benchmarks;stencil computations;Arithmetic;Biomedical computing;Central Processing Unit;Computational modeling;Computer architecture;Concurrent computing;Context modeling;Engines;Graphics;Hardware},
}

@InProceedings{Cong2008,
  author    = {Cong, G. and Kodali, S. and Krishnamoorthy, S. and Lea, D. and Saraswat, V. and Wen, T.},
  title     = {Solving Large, Irregular Graph Problems Using Adaptive Work-Stealing},
  booktitle = {{P}roceedings of the 37th {I}nternational {C}onference on {P}arallel {P}rocessing},
  date      = {2008},
  series    = {ICPP '08},
  location  = {Portland, OR, USA},
  month     = sep,
  pages     = {536--545},
  doi       = {10.1109/ICPP.2008.88},
  abstract  = {Solving large, irregular graph problems efficiently is challenging. Current software systems and commodity multiprocessors do not support fine-grained, irregular parallelism well. We present XWS, the X10 work stealing framework, an open-source runtime for the parallel programming language X10 and a library to be used directly by application writers. XWS extends the Cilk work-stealing framework with several features necessary to efficiently implement graph algorithms, viz., support for improperly nested procedures, global termination detection, and phased computation. We also present a strategy to adaptively control the granularity of parallel tasks in the work-stealing scheme, depending on the instantaneous size of the work queue. We compare the performance of the XWS implementations of spanning tree algorithms with that of the hand-written C and Cilk implementations using various graph inputs. We show that XWS programs (written in Java) scale and exhibit comparable or better performance.},
  issn      = {0190-3918},
}

@Article{Cope2010,
  author       = {Cope, Ben and Cheung, Peter Y. K. and Luk, Wayne and Howes, Lee},
  title        = {Performance Comparison of Graphics Processors to Reconfigurable Logic: A Case Study},
  journaltitle = {{IEEE} Transactions on Computers},
  date         = {2010},
  volume       = {59},
  number       = {4},
  month        = {4},
  pages        = {433--448},
  doi          = {10.1109/TC.2009.179},
  abstract     = {A systematic approach to the comparison of the graphics processor (GPU) and reconfigurable logic is defined in terms of three throughput drivers. The approach is applied to five case study algorithms, characterized by their arithmetic complexity, memory access requirements, and data dependence, and two target devices: the nVidia GeForce 7900 GTX GPU and a Xilinx Virtex-4 field programmable gate array (FPGA). Two orders of magnitude speedup, over a general-purpose processor, is observed for each device for arithmetic intensive algorithms. An FPGA is superior, over a GPU, for algorithms requiring large numbers of regular memory accesses, while the GPU is superior for algorithms with variable data reuse. In the presence of data dependence, the implementation of a customized data path in an FPGA exceeds GPU performance by up to eight times. The trends of the analysis to newer and future technologies are analyzed.},
  owner        = {andrea},
  timestamp    = {2014.12.16},
}

@InProceedings{Daga2015,
  author    = {Daga, Mayank and Greathouse, Joseph L.},
  title     = {Structural Agnostic {SpMV}: Adapting {CSR}-Adaptive for Irregular Matrices},
  booktitle = {{P}roceedings of the 22nd {IEEE} {I}nternational {C}onference on {H}igh {P}erformance {C}omputing},
  date      = {2015},
  series    = {HiPC '15},
  location  = {Bengaluru, IN},
  month     = dec,
  pages     = {64--74},
  doi       = {10.1109/HiPC.2015.55},
  abstract  = {Sparse matrix vector multiplication (SpMV) is an important linear algebra primitive. Recent research has focused on improving the performance of SpMV on GPUs when using compressed sparse row (CSR), the most frequently used matrix storage format on CPUs. Efficient CSR-based SpMV obviates the need for other GPU-specific storage formats, thereby saving runtime and storage overheads. However, existing CSR-based SpMV algorithms on GPUs perform poorly on irregular sparse matrices, limiting their usefulness. We propose a novel approach for SpMV on GPUs which works well for both regular and irregular matrices while keeping the CSR format intact. We start with CSR-Adaptive, which dynamically chooses between two SpMV algorithms depending on the length of each row. We then add a series of performance improvements, such as a more efficient reduction technique. Finally, we add a third algorithm which uses multiple parallel execution units when operating on irregular matrices with very long rows. Our implementation dynamically assigns the best algorithm to sets of rows in order to ensure that the GPU is efficiently utilized. We effectively double the performance of CSR-Adaptive, which had previously demonstrated better performance than algorithms that use other storage formats. In addition, our implementation is 36\% faster than CSR5, the current state of the art for SpMV on GPUs.},
  owner     = {andrea},
  timestamp = {2016.05.27},
}

@Article{Dantzig1949,
  author              = {Dantzig, G. B.},
  title               = {Programming of Interdependent Activities: {II} Mathematical Model},
  journaltitle        = {Econometrica},
  date                = {1949},
  language            = {English},
  volume              = {17},
  number              = {3/4},
  pages               = {200--211},
  issn                = {0012-9682},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {Jul. - Oct., 1949},
  owner               = {ap8213},
  publisher           = {The Econometric Society},
  timestamp           = {2014.10.09},
}

@Book{Davis2006,
  author    = {Davis, Timothy A.},
  title     = {Direct Methods for Sparse Linear Systems},
  date      = {2006},
  volume    = {2},
  series    = {Fundamentals of Algorithms},
  publisher = {Society for Industrial and Applied Mathematics},
  location  = {Philadelphia, PA, USA},
  isbn      = {0898716136},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@Article{Davis1997,
  author       = {Davis, Timothy A. and Duff, Iain S.},
  title        = {An Unsymmetric-Pattern Multifrontal Method for Sparse {LU} Factorization},
  journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
  date         = {1997},
  volume       = {18},
  number       = {1},
  pages        = {140--158},
  doi          = {10.1137/S0895479894246905},
  abstract     = {Sparse matrix factorization algorithms for general problems are typically characterized by irregular memory access patterns that limit their performance on parallel-vector supercomputers. For symmetric problems, methods such as the multifrontal method avoid indirect addressing in the innermost loops by using dense matrix kernels. However, no efficient LU factorization algorithm based primarily on dense matrix kernels exists for matrices whose pattern is very unsymmetric. We address this deficiency and present a new unsymmetric-pattern multifrontal method based on dense matrix kernels. As in the classical multifrontal method, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix, thus enabling the use of Level 2 and Level 3 BLAS. The performance is compared with the classical multifrontal method and other unsymmetric solvers on a CRAY C-98.},
  owner        = {andrea},
  timestamp    = {2014.12.13},
}

@Article{Davis1999,
  author       = {Davis, Timothy A. and Duff, Iain S.},
  title        = {A Combined Unifrontal/Multifrontal Method for Unsymmetric Sparse Matrices},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {1999},
  volume       = {25},
  number       = {1},
  month        = mar,
  pages        = {1--20},
  issn         = {0098-3500},
  doi          = {10.1145/305658.287640},
  abstract     = {We discuss the organization of frontal matrices in multifrontal methods for the solution of large sparse sets of unsymmetric linear equations. In the multifrontal method, work on a frontal matrix can be suspended, the frontal matrix can be stored for later reuse, and a new frontal matrix can be generated. There are thus several frontal matrices stored during the factorization, and one or more of these are assembled (summed) when creating a new frontal matrix. Although this means that arbitrary sparsity patterns can be handled efficiently, extra work is required to sum the frontal matrices together and can be costly because indirect addressing is requred. The (uni)frontal method avoids this extra work by factorizing the matrix with a single frontal matrix. Rows and columns are added to the frontal matrix, and pivot rows and columns are removed. Data movement is simpler, but higher fill-in can result if the matrix cannot be permuted into a variable-band form with small profile. We consider a combined unifrontal/multifrontal algorithm to enable general fill-in reduction orderings to be applied without the data movement of previous multifrontal approaches. We discuss this technique in the context of a code designed for the solution of sparse systems with unsymmetric pattern.},
  acmid        = {287640},
  issue_date   = {March 1999},
  keywords     = {frontal methods, linear equations, multifrontal methods, sparse unsymmetric matrices},
  location     = {New York, NY, USA},
  numpages     = {20},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2017.05.08},
}

@Article{Davis2011,
  author       = {Davis, Timothy A. and Hu, Yifan},
  title        = {The {U}niversity of {F}lorida Sparse Matrix Collection},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {2011},
  volume       = {38},
  number       = {1},
  month        = dec,
  pages        = {1--25},
  issn         = {0098-3500},
  doi          = {10.1145/2049662.2049663},
 abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB, Mathematica, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
  acmid        = {2049663},
  articleno    = {1},
  issue_date   = {November 2011},
  keywords     = {Graph drawing, multilevel algorithms, performance evaluation, sparse matrices},
  location     = {New York, NY, USA},
  numpages     = {25},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2015.09.07},
}

@Article{Davis2010,
  author       = {Davis, Timothy A. and Palamadai Natarajan, Ekanathan},
  title        = {Algorithm 907: {KLU}, A Direct Sparse Solver for Circuit Simulation Problems},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {2010},
  volume       = {37},
  number       = {3},
  month        = sep,
  pages        = {1--17},
  issn         = {0098-3500},
  doi          = {10.1145/1824801.1824814},
 abstract = {KLU is a software package for solving sparse unsymmetric linear systems of equations that arise in circuit simulation applications. It relies on a permutation to Block Triangular Form (BTF), several methods for finding a fill-reducing ordering (variants of approximate minimum degree and nested dissection), and Gilbert/Peierls' sparse left-looking LU factorization algorithm to factorize each block. The package is written in C and includes a MATLAB interface. Performance results comparing KLU with SuperLU, Sparse 1.3, and UMFPACK on circuit simulation matrices are presented. KLU is the default sparse direct solver in the XyceTMcircuit simulation package developed by Sandia National Laboratories.},
  acmid        = {1824814},
  articleno    = {36},
  issue_date   = {September 2010},
  keywords     = {LU factorization, circuit simulation, sparse matrices},
  location     = {New York, NY, USA},
  numpages     = {17},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2016.11.07},
}

@Article{Demidov2013,
  author       = {Demidov, Denis and Ahnert, Karsten and Rupp, Karl and Gottschling, Peter},
  title        = {Programming {CUDA} and {OpenCL}: A Case Study Using Modern {C++} Libraries},
  journaltitle = {{SIAM} {J}ournal on {S}cientific {C}omputing},
  date         = {2013},
  volume       = {35},
  number       = {5},
  pages        = {C453--C472},
  doi          = {10.1137/120903683},
  abstract     = {We present a comparison of several modern C++ libraries providing high-level interfaces for programming multi- and many-core architectures on top of CUDA or OpenCL. The comparison focuses on the solution of ordinary differential equations (ODEs) and is based on odeint, a framework for the solution of systems of ODEs. Odeint is designed in a very flexible way and may be easily adapted for effective use of libraries such as MTL4, VexCL, or ViennaCL, using CUDA or OpenCL technologies. We found that CUDA and OpenCL work equally well for problems of large sizes, while OpenCL has higher overhead for smaller problems. Furthermore, we show that modern high-level libraries allow us to effectively use the computational resources of many-core GPUs or multicore CPUs without much knowledge of the underlying technologies.},
}

@InProceedings{Dieguez2015,
  author    = {Diéguez, Adrián Pérez and Amor, Margarita and Doallo, Ramon},
  title     = {New Tridiagonal Systems Solvers on {GPU} Architectures},
  booktitle = {{P}roceedings of the 22nd {IEEE} {I}nternational {C}onference on {H}igh {P}erformance {C}omputing},
  date      = {2015},
  series    = {HiPC '15},
  location  = {Bengaluru, IN},
  month     = dec,
  pages     = {85--94},
  doi       = {10.1109/HiPC.2015.17},
  abstract  = {Modern GPUs (Graphics Processing Units) offer very high computing power at relatively low cost. Nevertheless, designing efficient algorithms for the GPUs usually requires additional time and effort, even for experienced programmers. On the other hand, tridiagonal systems solvers are an important building block for a wide range of applications. In this paper, we present a new tuning parallel proposal in order to generate new tridiagonal systems solvers. This proposal is based on the combination of a new reduction algorithm (Redundant Reduction-RR) with a tuning proposal to generate efficient parallel prefix algorithms on the GPU. Specifically, we present two new solvers combining RR with two GPU efficient parallel prefix patterns. The performance of the resulting proposals was analyzed using three different CUDA GPUs, obtaining an improvement of up to 20.5$\times$ over the CUSPARSE library and 28.9$\times$ over CUDPP.},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Ding1999,
  author       = {Ding, Chen and Kennedy, Ken},
  title        = {Improving Cache Performance in Dynamic Applications Through Data and Computation Reorganization at Run Time},
  journaltitle = {{ACM} {SIGPLAN} Notices},
  date         = {1999},
  volume       = {34},
  number       = {5},
  month        = may,
  pages        = {229--241},
  issn         = {0362-1340},
  doi          = {10.1145/301631.301670},
 abstract = {With the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been static---applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a dynamic approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated run-time overhead.},
  acmid        = {301670},
  issue_date   = {May 1999},
  location     = {New York, NY, USA},
  numpages     = {13},
  publisher    = {ACM},
}

@Article{Doggett2012,
  author       = {Doggett, M.},
  title        = {Texture Caches},
  journaltitle = {{IEEE} {M}icro},
  date         = {2012},
  volume       = {32},
  number       = {3},
  month        = may,
  pages        = {136--141},
  issn         = {0272-1732},
  doi          = {10.1109/MM.2012.44},
  abstract     = {This column examines the texture cache, an essential component of modern GPUs that plays an important role in achieving real-time performance when generating realistic images. GPUs have many components and the texture cache is only one of them. But it has a real impact on the performance of the GPU if rasterization and memory tiling are set up correctly.},
  keywords     = {cache storage;graphics processing units;image texture;GPU;memory tiling;real-time performance;realistic images;texture caches;Cache memory;Computer graphics;Games;Graphics processing unit;Image processing;Real time systems;GPU;gaming;graphics;rasterization;texture cache},
}

@InProceedings{Domahidi2012,
  author    = {Domahidi, Alexander and Zgraggen, Aldo U. and Zeilinger, Melanie Nicole and Morari, Manfred and Jones, Colin},
  title     = {Efficient Interior Point Methods for Multistage Problems Arising in Receding Horizon Control},
  booktitle = {{P}roceedings of the 51st {IEEE} {C}onference on {D}ecision and {C}ontrol},
  date      = {2012},
  series    = {CDC '12},
  location  = {Maui, HI, USA},
  month     = dec,
  pages     = {668--674},
  doi       = {10.1109/CDC.2012.6426855},
  abstract  = {Receding horizon control requires the solution of an optimization problem at every sampling instant. We present efficient interior point methods tailored to convex multistage problems, a problem class which most relevant MPC problems with linear dynamics can be cast in, and specify important algorithmic details required for a high speed implementation with superior numerical stability. In particular, the presented approach allows for quadratic constraints, which is not supported by existing fast MPC solvers. A categorization of widely used MPC problem formulations into classes of different complexity is given, and we show how the computational burden of certain quadratic or linear constraints can be decreased by a low rank matrix forward substitution scheme. Implementation details are provided that are crucial to obtain high speed solvers. We present extensive numerical studies for the proposed methods and compare our solver to three well-known solver packages, outperforming the fastest of these by a factor 2-5 in speed and 3-70 in code size. Moreover, our solver is shown to be very efficient for large problem sizes and for quadratically constrained QPs, extending the set of systems amenable to advanced MPC formulations on low-cost embedded hardware.},
  address   = {Maui, HI, USA},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Book{Dongarra1998,
  Title                    = {Numerical Linear Algebra for High-Performance Computers},
  Author                   = {Dongarra, J. and Duff, I. and Sorensen, D. and van der Vorst, H.},
  Publisher                = {Society for Industrial and Applied Mathematics},

  Date                     = {1998},
  Doi                      = {10.1137/1.9780898719611},
  Eprint                   = {http://epubs.siam.org/doi/pdf/10.1137/1.9780898719611},
  Owner                    = {andrea},
  Timestamp                = {2016.09.07}
}

@Manual{Eaton2015,
  author    = {Eaton, John W. and Bateman, David and Hauberg, Søren and Wehbring, Rik},
  title     = {{GNU} Octave version 4.0.0 manual: a high-level interactive language for numerical computations},
  date      = {2015},
  note      = {Accessed 20 July 2016},
  url       = {http://www.gnu.org/software/octave/doc/interpreter},
  owner     = {andrea},
  timestamp = {2016.11.07},
}

@Article{Ehrenberg2015,
  author       = {Ehrenberg, Nils},
  title        = {Heading for Urban Energy Internets},
  journaltitle = {Pictures of the Future},
  date         = {2015},
  month        = {6},
  note         = {Accessed online on September 28th, 2015},
  url          = {http://www.siemens.com/innovation/en/home/pictures-of-the-future/infrastructure-and-finance/smart-cities-smart-buildings.html},
  howpublished = {Online},
  owner        = {andrea},
  timestamp    = {2015.09.28},
}

@InProceedings{Elteir2011,
  author    = {Elteir, Marwa and Lin, Heshan and Feng, Wu-chun},
  title     = {Performance Characterization and Optimization of Atomic Operations on {AMD} {GPU}s},
  booktitle = {{P}roceedings of the 2011 {IEEE} {I}nternational {C}onference on {C}luster {C}omputing},
  date      = {2011},
  series    = {CLUSTER '11},
  location  = {Austin, TX, US},
  month     = sep,
  pages     = {234--243},
  doi       = {10.1109/CLUSTER.2011.34},
  abstract  = {Atomic operations are important building blocks in supporting general-purpose computing on graphics processing units (GPUs). For instance, they can be used to coordinate execution between concurrent threads, and in turn, assist in constructing complex data structures such as hash tables or implementing GPU-wide barrier synchronization. While the performance of atomic operations has improved substantially on the latest NVIDIA Fermi-based GPUs, system-provided atomic operations still incur significant performance penalties on AMD GPUs. A memory-bound kernel on an AMD GPU, for example, can suffer severe performance degradation when including an atomic operation, even if the atomic operation is never executed. In this paper, we first quantify the performance impact of atomic instructions to application kernels on AMD GPUs. We then propose a novel software-based implementation of atomic operations that can significantly improve the overall kernel performance. We evaluate its performance against the system-provided atomic using two micro-benchmarks and four real applications. The results show that using our software based atomic operations on an AMD GPU can speedup an application kernel by 67-fold over the same application kernel but with the (default) system-provided atomic operations.},
  keywords  = {coprocessors;data structures;synchronisation;AMD GPU;GPU-wide barrier synchronization;NVIDIA Fermi-based GPU;application kernels;atomic instructions;complex data structure construction;concurrent threads;four real applications;general-purpose computing;graphics processing units;hash tables;kernel performance;memory-bound kernel;micro-benchmarks;optimization;performance characterization;performance degradation;software based atomic operations;software-based implementation;system-provided atomic operations;Arrays;Graphics processing unit;High definition video;Instruction sets;Kernel;Synchronization;GPGPU;GPU;MapReduce;atomic operations;heterogeneous computing},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@TechReport{Feldman2015,
  author      = {Feldman, Michael and Snell, Addison},
  title       = {Accelerated computing: a tipping point for {HPC}},
  institution = {Intersect360 Research},
  date        = {2015},
  note        = {Accessed 21 July 2016},
  month       = nov,
  url         = {http://images.nvidia.com/content/pdf/tesla/accelerated-computing-at-a-tipping-point.pdf},
  owner       = {andrea},
  timestamp   = {2016.11.07},
}

@Article{Filippone2017,
  author       = {Filippone, Salvatore and Cardellini, Valeria and Barbieri, Davide and Fanfarillo, Alessandro},
  title        = {Sparse Matrix-Vector Multiplication on {GPGPUs}},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {2017},
  volume       = {43},
  number       = {4},
  month        = {1},
  pages        = {30:1--30:49},
  issn         = {0098-3500},
  doi          = {10.1145/3017994},
 abstract = {The multiplication of a sparse matrix by a dense vector (SpMV) is a centerpiece of scientific computing applications: it is the essential kernel for the solution of sparse linear systems and sparse eigenvalue problems by iterative methods. The efficient implementation of the sparse matrix-vector multiplication is therefore crucial and has been the subject of an immense amount of research, with interest renewed with every major new trend in high-performance computing architectures. The introduction of General-Purpose Graphics Processing Units (GPGPUs) is no exception, and many articles have been devoted to this problem. \\With this article, we provide a review of the techniques for implementing the SpMV kernel on GPGPUs that have appeared in the literature of the last few years. We discuss the issues and tradeoffs that have been encountered by the various researchers, and a list of solutions, organized in categories according to common features. We also provide a performance comparison across different GPGPU models and on a set of test matrices coming from various application domains.},
  acmid        = {3017994},
  articleno    = {30},
  issue_date   = {March 2017},
  keywords     = {GPU programming, Sparse matrices},
  location     = {New York, NY, USA},
  numpages     = {49},
  publisher    = {ACM},
}

@Article{Flynn1972,
  Title                    = {Some Computer Organizations and Their Effectiveness},
  Author                   = {Flynn, Michael J.},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {948--960},
  Volume                   = {C-21},

  Date                     = {1972},
  Doi                      = {10.1109/TC.1972.5009071},
  ISSN                     = {0018-9340},
  Journaltitle             = {{IEEE} Transactions on Computers},
  Keywords                 = {Automata;Computer aided instruction;Concurrent computing;Parallel processing;Performance evaluation;Time sharing computer systems;Transmission electron microscopy;Computer organization;instruction stream;overlapped;parallel processors;resource hierarchy},
  Owner                    = {andrea},
  Timestamp                = {2016.09.07}
}

@Manual{Forrest2004,
  author       = {Forrest, John and de la Nuez, David and Lougee-Heimer, Robin},
  title        = {{CLP} User Guide},
  date         = {2004},
  organization = {IBM Research},
  url          = {http://www.coin-or.org/Clp/userguide/index.html},
  owner        = {andrea},
  timestamp    = {2014.12.14},
}

@Article{Forrest1972,
  Title                    = {Updated triangular factors of the basis to maintain sparsity in the product form simplex method},
  Author                   = {Forrest, J. J. H. and Tomlin, J. A.},
  Number                   = {1},
  Pages                    = {263--278},
  Volume                   = {2},

  Date                     = {1972},
  Doi                      = {10.1007/BF01584548},
  ISSN                     = {1436-4646},
  Journaltitle             = {Mathematical Programming}
}

@InProceedings{Fraguela1998,
  author    = {Fraguela, Basilio B. and Doallo, Ramón and Zapata, Emilio L.},
  title     = {Modeling Set Associative Caches Behavior for Irregular Computations},
  booktitle = {{P}roceedings of the 1998 {ACM SIGMETRICS} {J}oint {I}nternational {C}onference on {M}easurement and {M}odeling of {C}omputer {S}ystems},
  date      = {1998},
  series    = {SIGMETRICS '98/PERFORMANCE '98},
  publisher = {ACM},
  location  = {Madison, Wisconsin, USA},
  isbn      = {0-89791-982-3},
  pages     = {192--201},
  doi       = {10.1145/277851.277910},
  abstract  = {While much work has been devoted to the study of cache behavior during the execution of codes with regular access patterns, little attention has been paid to irregular codes. An important portion of these codes are scientific applications that handle compressed sparse matrices. In this work a probabilistic model for the prediction of the number of misses on a K-way associative cache memory considering sparse matrices with a uniform or banded distribution is presented. Two different irregular kernels are considered: the sparse matrix-vector product and the transposition of a sparse matrix. The model was validated with simulations on synthetic uniform matrices and banded matrices from the Harwell-Boeing collection.},
  acmid     = {277910},
  address   = {New York, NY, USA},
  numpages  = {10},
  owner     = {andrea},
  timestamp = {2017.05.09},
}

@InProceedings{Gabriel2004,
  author    = {Gabriel, Edgar and Fagg, Graham E. and Bosilca, George and Angskun, Thara and Dongarra, Jack J. and Squyres, Jeffrey M. and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and Castain, Ralph H. and Daniel, David J. and Graham, Richard L. and Woodall, Timothy S.},
  title     = {Open {MPI}: Goals, Concept, and Design of a Next Generation {MPI} Implementation},
  booktitle = {{P}roceedings of the 11th {E}uropean {PVM/MPI} {U}sers' {G}roup {M}eeting},
  date      = {2004},
  location  = {Budapest, HUN},
  month     = sep,
  pages     = {97--104},
}

@Manual{Galassi2016,
  Title                    = {{GNU} Scientific Library},
  Author                   = {Galassi, Mark and Davies, Jim and Theiler, James and Gough, Brian and Jungman, Gerard and Alken, Patrick and Booth, Michael and Rossi, Fabrice and Ulerich, Rhys},
  Edition                  = {2.3 for {GSL} vesion 2.3},

  Date                     = {2016},
  Keywords                 = {dblp},
  Timestamp                = {2011-07-02T11:38:40.000+0200},
  Url                      = {https://www.gnu.org/software/gsl/manual/gsl-ref.pdf}
}

@Article{Gamrath2015,
  Title                    = {Progress in presolving for mixed integer programming},
  Author                   = {Gamrath, Gerald and Koch, Thorsten and Martin, Alexander and Miltenberger, Matthias and Weninger, Dieter},
  Number                   = {4},
  Pages                    = {367--398},
  Volume                   = {7},

 abstract = {This paper describes three presolving techniques for solving mixed integer programming problems (MIPs) that were implemented in the academic MIP solver SCIP. The task of presolving is to reduce the problem size and strengthen the formulation, mainly by eliminating redundant information and exploiting problem structures. The first method fixes continuous singleton columns and extends results known from duality fixing. The second analyzes and exploits pairwise dominance relations between variables, whereas the third detects isolated subproblems and solves them independently. The performance of the presented techniques is demonstrated on two MIP test sets. One contains all benchmark instances from the last three MIPLIB versions, while the other consists of real-world supply chain management problems. The computational results show that the combination of all three presolving techniques almost halves the solving time for the considered supply chain management problems. For the MIPLIB instances we obtain a speedup of 20\% on affected instances while not degrading the performance on the remaining problems.},
  Date                     = {2015},
  Doi                      = {10.1007/s12532-015-0083-5},
  ISSN                     = {1867--2957},
  Journaltitle             = {Mathematical Programming Computation},
  Owner                    = {andrea},
  Timestamp                = {2017.04.23}
}

@Article{Garland2008,
  author       = {Garland, Michael and Le Grand, Scott and Nickolls, John and Anderson, Joshua and Hardwick, Jim and Morton, Scott and Phillips, Everett and Zhang, Yao and Volkov, Vasily},
  title        = {Parallel Computing Experiences with {CUDA}},
  journaltitle = {{IEEE} Micro},
  date         = {2008},
  volume       = {28},
  number       = {4},
  month        = jul,
  pages        = {13--27},
  issn         = {0272-1732},
  doi          = {10.1109/MM.2008.57},
 abstract = {The CUDA programming model provides a straightforward means of describing inherently parallel computations, and NVIDIA's Tesla GPU architecture delivers high computational throughput on massively parallel problems. This article surveys experiences gained in applying CUDA to a diverse set of problems and the parallel speedups over sequential codes running on traditional CPU architectures attained by executing key computations on the GPU.},
  acmid        = {1442820},
  issue_date   = {July 2008},
  location     = {Los Alamitos, CA, USA},
  numpages     = {15},
  publisher    = {IEEE Computer Society Press},
}

@Article{Gay1985,
  author       = {Gay, D. M.},
  title        = {Electronic mail distribution of linear programming test problems},
  journaltitle = {Mathematical Programming Society {COAL} Newsletter},
  date         = {1985},
  volume       = {13},
  pages        = {10--12},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Online{Gay2005,
  author    = {Gay, David M.},
  title     = {Netlib {LP} benchmark documentation},
  date      = {2005},
  url       = {http://www.netlib.org/lp/data/readme},
  note      = {Accessed on 7 November 2014},
  month     = aug,
  owner     = {andrea},
  timestamp = {2014.11.07},
}

@InProceedings{Ghose2016,
  Title                    = {Divergence Aware Automated Partitioning of {OpenCL} Workloads},
  Author                   = {Ghose, Anirban and Dey, Soumyajit and Mitra, Pabitra and Chaudhuri, Mainak},
  Booktitle                = {{P}roceedings of the 9th {I}ndia {S}oftware {E}ngineering {C}onference},

  Address                  = {New York, NY, USA},
  Pages                    = {131--135},
  Publisher                = {ACM},
  Series                   = {ISEC '16},

 abstract = {Heterogeneous partitioning is a key step for efficient mapping and scheduling of data parallel applications on multi-core computing platforms involving both CPUs and GPUs. Over the last few years, several automated partitioning methodologies, both static as well as dynamic, have been proposed for this purpose. The present work provides an in-depth analysis of control flow divergence and its impact on the quality of such program partitions. We characterize the amount of divergence in a program as an important performance feature and train suitable Machine Learning (ML) based classifiers which statically decide the partitioning of an OpenCL workload for a heterogeneous platform involving a single CPU and a single GPU. Our approach reports improved partitioning results with respect to timing performance when compared with existing approaches for ML based static partitioning of data parallel workloads.},
  Acmid                    = {2856639},
  Date                     = {2016},
  Doi                      = {10.1145/2856636.2856639},
  ISBN                     = {978-1-4503-4018-2},
  Keywords                 = {Control Flow Divergence, Feature Extraction, OpenCL},
  Location                 = {Goa, India},
  Numpages                 = {5}
}

@InProceedings{Gilbert2006,
  Title                    = {High-performance Graph Algorithms from Parallel Sparse Matrices},
  Author                   = {Gilbert, John R. and Reinhardt, Steve and Shah, Viral B.},
  Booktitle                = {{P}roceedings of the 8th {I}nternational {C}onference on {A}pplied {P}arallel {C}omputing: {S}tate of the {A}rt in {S}cientific {C}omputing},

  Address                  = {Berlin, Heidelberg},
  Pages                    = {260--269},
  Publisher                = {Springer-Verlag},
  Series                   = {PARA '06},

 abstract = {Large-scale computation on graphs and other discrete structures is becoming increasingly important in many applications, including computational biology, web search, and knowledge discovery. High-performance combinatorial computing is an infant field, in sharp contrast with numerical scientific computing. \\We argue that many of the tools of high-performance numerical computing - in particular, parallel algorithms and data structures for computation with sparse matrices - can form the nucleus of a robust infrastructure for parallel computing on graphs. We demonstrate this with an implementation of a graph analysis benchmark using the sparse matrix infrastructure in Star-P, our parallel dialect of the MATLAB programming language.},
  Acmid                    = {1775097},
  Date                     = {2006},
  ISBN                     = {978-3-540-75754-2},
  Location                 = {Umeå, SE},
  Numpages                 = {10},
  Url                      = {http://dl.acm.org/citation.cfm?id=1775097}
}

@TechReport{Gleixner2012a,
  author      = {Gleixner, Ambros M.},
  title       = {Factorization and update of a reduced basis matrix for the revised simplex method},
  institution = {Konrad-Zuse-Zentrum für Informationstechnik Berlin},
  date        = {2012},
  language    = {eng},
  number      = {12--36},
  location    = {Takustr, 7, 14195 Berlin},
  url         = {https://opus4.kobv.de/opus4-zib/files/1634/ZR-12-36.pdf},
  abstract    = {In this paper, we describe a method to enhance the FTRAN and BTRAN operations in the revised simplex algorithm by using a reduced basis matrix defined by basic columns and nonbasic rows. This submatrix of the standard basis matrix is potentially much smaller, but may change its dimension dynamically from iteration to iteration. \\For the classical product form update (eta updates), the idea has been noted already by Zoutendijk, but only preliminarily tested by Powell in the early 1970s. We extend these ideas to Forrest-Tomlin type update formulas for an LU factorization of the reduced basis matrix, which are suited for efficient implementation within a state-of-the-art simplex solver. The computational advantages of the proposed method apply to pure LP solving as well as to LP-based branch-cut-and-price algorithms. It can easily be integrated into existing simplex codes.},
  owner       = {andrea},
  timestamp   = {2017.05.08},
  urn         = {urn:nbn:de:0297-zib-16349},
}

@InProceedings{Gleixner2012,
  author    = {Gleixner, Ambros M. and Steffy, Daniel E. and Wolter, Kati},
  title     = {Improving the Accuracy of Linear Programming Solvers with Iterative Refinement},
  booktitle = {{P}roceedings of the 37th {I}nternational {S}ymposium on {S}ymbolic and {A}lgebraic {C}omputation},
  date      = {2012},
  series    = {ISSAC '12},
  publisher = {ACM},
  location  = {Grenoble, FR},
  month     = jul,
  isbn      = {978-1-4503-1269-1},
  pages     = {187--194},
  doi       = {10.1145/2442829.2442858},
  abstract  = {We describe an iterative refinement procedure for computing extended precision or exact solutions to linear programming problems (LPs). Arbitrarily precise solutions can be computed by solving a sequence of closely related LPs with limited precision arithmetic. The LPs solved share the same constraint matrix as the original problem instance and are transformed only by modification of the objective function, right-hand side, and variable bounds. Exact computation is used to compute and store the exact representation of the transformed problems, while numeric computation is used for solving LPs. At all steps of the algorithm the LP bases encountered in the transformed problems correspond directly to LP bases in the original problem description. \\We demonstrate that this algorithm is effective in practice for computing extended precision solutions and that this leads to direct improvement of the best known methods for solving LPs exactly over the rational numbers.},
  acmid     = {2442858},
  address   = {Grenoble, FR},
  keywords  = {iterative refinement, linear programming, optimization},
  numpages  = {8},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Book{Golub1996,
  author    = {Golub, Gene H. and Van Loan, Charles F.},
  title     = {Matrix Computations},
  date      = {1996},
  edition   = {3},
  publisher = {Johns Hopkins University Press},
  location  = {Baltimore, MD, USA},
  isbn      = {0-8018-5414-8},
  owner     = {andrea},
  timestamp = {2015.09.27},
}

@InProceedings{Gonzalez2012,
  Title                    = {{PowerGraph}: Distributed Graph-parallel Computation on Natural Graphs},
  Author                   = {Gonzalez, Joseph E. and Low, Yucheng and Gu, Haijie and Bickson, Danny and Guestrin, Carlos},
  Booktitle                = {{P}roceedings of the 10th {USENIX} {C}onference on {O}perating {S}ystems {D}esign and {I}mplementation},

  Address                  = {Berkeley, CA, USA},
  Pages                    = {17--30},
  Publisher                = {USENIX Association},
  Series                   = {OSDI '12},

 abstract = {Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability. \\In this paper, we characterize the challenges of computation on natural graphs in the context of existing graph-parallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.},
  Acmid                    = {2387883},
  Date                     = {2012},
  ISBN                     = {978-1-931971-96-6},
  Location                 = {Hollywood, CA, USA},
  Numpages                 = {14},
  Url                      = {http://dl.acm.org/citation.cfm?id=2387880}
}

@Online{Gottschling2013,
  author       = {Gottschling, Peter},
  title        = {{CUDA-MTL4} manual},
  date         = {2013},
  url          = {http://www.simunova.com/node/300},
  note         = {Accessed on 10 May 2017},
  organization = {SimuNova UG},
  abstract     = {Many things can be realized on a computer very elegantly and efficiently today thanks to progress in software and programming languages. One thing that cannot be done elegantly on a computer is computing. At least not computing fast. \\In the Matrix Template Library 4 we aim for a natural mathematical notation without sacrifying performance. You can write an expression like $x = y * z$ and the library will perform the according operation: scaling a vector, multiplying a sparse matrix with a dense vector or two sparse matrices. Some operations like dense matrix product use tuned BLAS implementation. In parallel, all described operations in this manual are also realized in C++ so that the library can be used without BLAS and is not limited to types supported by BLAS. For short, general applicability is combined with maximal available performance. We developed new techniques to allow for (1) Unrolling of dynamicly sized data with user-define block and tile sizes; (2) Combining multiple vector assignments in a single statement (and more importingly perform them in one single loop); (3) Storing matrices recursively in a never-before realized generality; (4) Performing operations on recursive and non-recursive matrices recursively; (5) Filling compressed sparse matrices efficiently; and much more. \\The manual still not covers all features and techniques of the library. But it should give you enough information to get started.},
  owner        = {andrea},
  timestamp    = {2017.05.10},
}

@InProceedings{Gregor2005,
  author    = {Gregor, Douglas and Lumsdaine, Andrew},
  title     = {The parallel {BGL}: A generic library for distributed graph computations},
  booktitle = {{P}arallel {O}bject-{O}riented {S}cientific {C}omputing},
  date      = {2005},
  series    = {POOSC '05},
  month     = jul,
  url       = {http://www.osl.iu.edu/publications/prints/2005/Gregor:POOSC:2005.pdf},
  abstract  = {This paper presents the Parallel BGL, a generic C++ library for distributed graph computation. Like the sequential Boost Graph Library (BGL) upon which it is based, the Parallel BGL applies the paradigm of generic programming to the domain of graph computations. Emphasizing efficient generic algorithms and the use of concepts to specify the requirements on type parameters, the Parallel BGL also provides flexible supporting data structures such as distributed adjacency lists and external property maps. The generic programming approach simultaneously stresses flexibility and efficiency, resulting in a parallel graph library that can adapt to various data structures and communication models while retaining the efficiency of equivalent hand-coded programs. Performance data for selected algorithms are provided demonstrating the efficiency and scalability of the Parallel BGL},
  owner     = {andrea},
  timestamp = {2017.05.09},
}

@Article{Grigori2007,
  author       = {Grigori, L. and Demmel, J. W. and Li, X. S.},
  title        = {Parallel Symbolic Factorization for Sparse {LU} with Static Pivoting},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2007},
  volume       = {29},
  number       = {3},
  pages        = {1289--1314},
  doi          = {10.1137/050638102},
  abstract     = {This paper presents the design and implementation of a memory scalable parallel symbolic factorization algorithm for general sparse unsymmetric matrices. Our parallel algorithm uses a graph partitioning approach, applied to the graph of $|A|+|A|^T$, to partition the matrix in such a way that is good for sparsity preservation as well as for parallel factorization. The partitioning yields a so-called separator tree which represents the dependencies among the computations. We use the separator tree to distribute the input matrix over the processors using a block cyclic approach and a subtree to subprocessor mapping. The parallel algorithm performs a bottom-up traversal of the separator tree. With a combination of right-looking and left-looking partial factorizations, the algorithm obtains one column structure of L and one row structure of U at each step. The algorithm is implemented in C and MPI. From a performance study on large matrices, we show that the parallel algorithm significantly reduces the memory requirement of the symbolic factorization step, as well as the overall memory requirement of the parallel solver. It also often reduces the runtime of the sequential algorithm, which is already relatively small. In general, the parallel algorithm prevents the symbolic factorization step from being a time or memory bottleneck of the parallel solver.},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Hall2010,
  author       = {Hall, J. A. J.},
  title        = {Towards a practical parallelisation of the simplex method},
  journaltitle = {Computational Management Science},
  date         = {2010},
  language     = {English},
  volume       = {7},
  number       = {2},
  pages        = {139--170},
  issn         = {1619--697X},
  doi          = {10.1007/s10287-008-0080-5},
  abstract     = {The simplex method is frequently the most efficient method of solving linear programming (LP) problems. This paper reviews previous attempts to parallelise the simplex method in relation to efficient serial simplex techniques and the nature of practical LP problems. For the major challenge of solving general large sparse LP problems, there has been no parallelisation of the simplex method that offers significantly improved performance over a good serial implementation. However, there has been some success in developing parallel solvers for LPs that are dense or have particular structural properties. As an outcome of the review, this paper identifies scope for future work towards the goal of developing parallel implementations of the simplex method that are of practical value.},
  keywords     = {Linear programming; Simplex method; Sparse; Parallel computing; 90C05},
  owner        = {andrea},
  publisher    = {Springer-Verlag},
  timestamp    = {2017.05.08},
}

@InCollection{Hall2012,
  author    = {Hall, Julian and Huangfu, Qi},
  title     = {A High Performance Dual Revised Simplex Solver},
  booktitle = {{P}arallel {P}rocessing and {A}pplied {M}athematics},
  date      = {2012},
  editor    = {Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Waśniewski, Jerzy},
  language  = {English},
  volume    = {7203},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-642-31463-6},
  pages     = {143--151},
  doi       = {10.1007/978-3-642-31464-3_15},
  abstract  = {When solving families of related linear programming (LP) problems and many classes of single LP problems, the simplex method is the preferred computational technique. Hitherto there has been no efficient parallel implementation of the simplex method that gives good speed-up on general, large sparse LP problems. This paper presents a variant of the dual simplex method and a prototype parallelisation scheme. The resulting implementation, ParISS, is efficient when run in serial and offers modest speed-up for a range of LP test problems.},
  keywords  = {Linear programming; Dual revised simplex method; Parallel algorithms},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Hamzic2011,
  author    = {Hamzić, Adis and Huseinović, Alvin and Nosović, Novica},
  title     = {Implementation and performance analysis of the Simplex algorithm adapted to run on commodity {OpenCL} enabled graphics processors},
  booktitle = {{P}roceedings of the {XXIII} {I}nternational {S}ymposium on {I}nformation {C}ommunication and {A}utomation {T}echnologies},
  date      = {2011},
  series    = {ICAT '11},
  location  = {Sarajevo, BIH},
  month     = oct,
  pages     = {1--7},
  doi       = {10.1109/ICAT.2011.6102135},
  abstract  = {The Simplex algorithm is commonly used for solving Linear Optimization problems. Linear Optimization methods are used to solve problems in areas such as Economics, Business, Planning and Engineering. Developments of hardware platforms have allowed the use of Linear Optimization methods on problems that presented serious computational challenges in the past. However, solving large optimization problems can be time consuming, which has to be taken into consideration for time-critical applications. With the invention of the GPU assisted computing the situation in this field has progressed. In this paper, implementation and performance analysis of the Simplex algorithm adapted to take the advantage of modern graphics processors versus traditional CPU adapted implementation is presented.},
  keywords  = {computer graphics;graphics processing units;linear programming;CPU adapted implementation;GPU assisted computing;commodity OpenCL enabled graphics processors;hardware platforms;linear optimization methods;linear optimization problems;performance analysis;simplex algorithm;Aggregates;Graphics processing unit;Indexes;Instruction sets;Libraries;Optimization;GPU;Linear optimization;OpenCL;linear programming;simplex},
  owner     = {andrea},
  timestamp = {2014.03.05},
}

@Article{Han2006,
  author       = {Han, Hwansoo and Tseng, Chau-Wen},
  title        = {Exploiting locality for irregular scientific codes},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2006},
  volume       = {17},
  number       = {7},
  month        = jul,
  pages        = {606--618},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2006.88},
  abstract     = {Irregular scientific codes experience poor cache performance due to their irregular memory access patterns. In this paper, we present two new locality improving techniques for irregular scientific codes. Our techniques exploit geometric structures hidden in data access patterns and computation structures. Our new data reordering (GPART) finds the graph structure within data accesses and applies hierarchical clustering. Quality partitions are constructed quickly by clustering multiple neighbor nodes with priority on nodes with high degree and repeating a few passes. Overhead is kept low by clustering multiple nodes in each pass and considering only edges between partitions. Our new computation reordering (Z-SORT) treats the values of index arrays as coordinates and reorders corresponding computations in Z-curve order. Applied to dense inputs, Z-SORT achieves performance close to data reordering combined with other computation reordering but without the overhead involved in data reordering. Experiments on irregular scientific codes for a variety of meshes show locality optimization techniques are effective for both sequential and parallelized codes, improving performance by 60-87 percent. GPART achieved within 1-2 percent of the performance of more sophisticated partitioning algorithms, but with one third of the overhead. Z-SORT also yields the performance improvement of 64 percent for dense inputs, which is comparable with data reordering combined with computation reordering},
  keywords     = {cache storage;data structures;optimising compilers;Z-SORT computation reordering;Z-curve order;data access;data access patterns;data reordering;graph structure;hierarchical clustering;index arrays;irregular scientific codes;locality optimization techniques;Cache memory;Computational fluid dynamics;Computer architecture;Concurrent computing;Data mining;Data structures;Fluid dynamics;Microprocessors;Parallel processing;Partitioning algorithms;Compiler optimization;cache memories;computation reordering.;data reordering;inspector/executor},
}

@InProceedings{Harshvardhan2014,
  Title                    = {{KLA}: A New Algorithmic Paradigm for Parallel Graph Computations},
  Author                   = {Harshvardhan and Fidel, Adam and Amato, Nancy M. and Rauchwerger, Lawrence},
  Booktitle                = {{P}roceedings of the 23rd {I}nternational {C}onference on {P}arallel {A}rchitectures and {C}ompilation},

  Address                  = {New York, NY, USA},
  Pages                    = {27--38},
  Publisher                = {ACM},
  Series                   = {PACT '14},

 abstract = {This paper proposes a new algorithmic paradigm - k-level asynchronous (KLA) - that bridges level-synchronous and asynchronous paradigms for processing graphs. The KLA paradigm enables the level of asynchrony in parallel graph algorithms to be parametrically varied from none (level-synchronous) to full (asynchronous). The motivation is to improve execution times through an appropriate trade-off between the use of fewer, but more expensive global synchronizations, as in level-synchronous algorithms, and more, but less expensive local synchronizations (and perhaps also redundant work), as in asynchronous algorithms. We show how common patterns in graph algorithms can be expressed in the KLA pardigm and provide techniques for determining k, the number of asynchronous steps allowed between global synchronizations. Results of an implementation of KLA in the STAPL Graph Library show excellent scalability on up to 96K cores and improvements of 10$\times$ or more over level-synchronous and asynchronous versions for graph algorithms such as breadth-first search, PageRank, k-core decomposition and others on certain classes of real-world graphs.},
  Acmid                    = {2628091},
  Date                     = {2014},
  Doi                      = {10.1145/2628071.2628091},
  ISBN                     = {978-1-4503-2809-8},
  Keywords                 = {asynchronous graph algorithms, big data, distributed computing, graph analytics, parallel algorithms},
  Location                 = {Edmonton, AB, CA},
  Numpages                 = {12},
  Owner                    = {andrea},
  Timestamp                = {2016.09.07}
}

@Article{Hartley2014,
  author       = {Hartley, Edward N. and Jerez, Juan L. and Suardi, Andrea and Macjeowski, Jan M. and Kerrigan, Eric C. and Constantinides, George A.},
  title        = {Predictive control using an {FPGA} with application to aircraft control},
  journaltitle = {{IEEE} Transactions on Control Systems Technology},
  date         = {2014},
  volume       = {22},
  number       = {3},
  month        = may,
  pages        = {1006--1017},
  issn         = {1063-6536},
  doi          = {10.1109/TCST.2013.2271791},
  abstract     = {Alternative and more efficient computational methods can extend the applicability of model predictive control (MPC) to systems with tight real-time requirements. This paper presents a system-on-a-chip MPC system, implemented on a field-programmable gate array (FPGA), consisting of a sparse structure-exploiting primal dual interior point (PDIP) quadratic program (QP) solver for MPC reference tracking and a fast gradient QP solver for steady-state target calculation. A parallel reduced precision iterative solver is used to accelerate the solution of the set of linear equations forming the computational bottleneck of the PDIP algorithm. A numerical study of the effect of reducing the number of iterations highlights the effectiveness of the approach. The system is demonstrated with an FPGA-in-the-loop testbench controlling a nonlinear simulation of a large airliner. This paper considers many more manipulated inputs than any previous FPGA-based MPC implementation to date, yet the implementation comfortably fits into a midrange FPGA, and the controller compares well in terms of solution quality and latency to state-of-the-art QP solvers running on a standard PC.},
  keywords     = {Aerospace control;field-programmable gate arrays (FPGAs);optimization methods;predictive control.},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InBook{Hbeika2017,
  author    = {Hbeika, Jad and Kulkarni, Milind},
  title     = {Locality-Aware Task-Parallel Execution on {GPUs}},
  booktitle = {{P}roceedings of the 29th {I}nternational {W}orkshop on {L}anguages and {C}ompilers for {P}arallel {C}omputing},
  date      = {2017},
  editor    = {Ding, Chen and Criswell, John and Wu, Peng},
  series    = {LCPC '16},
  publisher = {Springer International Publishing},
  location  = {Rochester, NY, USA},
  isbn      = {978-3-319-52709-3},
  pages     = {250--264},
  doi       = {10.1007/978-3-319-52709-3_19},
  abstract  = {GPGPUs deliver high speedup for regular applications while remaining energy efficient. In recent years, there has been much focus on tuning irregular, task-parallel applications and/or the GPU architecture in order to achieve similar benefits for irregular applications running on GPUs. While most of the previous works have focused on minimizing the effect of control and memory divergence, which are prominent in irregular applications and which degrade the performance, there has been less attention paid to decreasing cache pressure and hence improving performance of applications given the small cache sizes on GPUs. \\In this paper we tackle two problems. First we extract data parallelism from irregular task parallel applications, which we do by subdividing each task into sub tasks at the CPU side and sending these sub tasks to the GPU for execution. By doing so we take advantage of the massive parallelism provided by the GPU. Second, to mitigate the memory demands of many tasks that access irregular data structures, we schedule these subtasks in a way to minimize the memory footprint of each warp running on the GPU. We use our framework with 3 task-parallel algorithms and show that we can achieve significant speedups over optimized GPU code.},
  owner     = {andrea},
  timestamp = {2017.02.17},
}

@Book{Hennessy2011,
  author    = {Hennessy, John L. and Patterson, David A.},
  title     = {Computer Architecture: A Quantitative Approach},
  date      = {2011},
  edition   = {5th},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location  = {San Francisco, CA, USA},
  isbn      = {012383872X},
  owner     = {andrea},
  timestamp = {2015.10.22},
}

@Article{Hogg2013a,
  author       = {Hogg, Jonathan and Scott, Jennifer},
  title        = {New Parallel Sparse Direct Solvers for Multicore Architectures},
  journaltitle = {{A}lgorithms},
  date         = {2013},
  volume       = {6},
  number       = {4},
  pages        = {702--725},
  issn         = {1999-4893},
  doi          = {10.3390/a6040702},
  url          = {http://www.mdpi.com/1999-4893/6/4/702},
  abstract     = {At the heart of many computations in science and engineering lies the need to efficiently and accurately solve large sparse linear systems of equations. Direct methods are frequently the method of choice because of their robustness, accuracy and potential for use as black-box solvers. In the last few years, there have been many new developments, and a number of new modern parallel general-purpose sparse solvers have been written for inclusion within the HSL mathematical software library. In this paper, we introduce and briefly review these solvers for symmetric sparse systems. We describe the algorithms used, highlight key features (including bit-compatibility and out-of-core working) and then, using problems arising from a range of practical applications, we illustrate and compare their performances. We demonstrate that modern direct solvers are able to accurately solve systems of order 106 in less than 3 minutes on a 16-core machine.},
}

@Article{Hogg2016,
  author       = {Hogg, Jonathan D. and Ovtchinnikov, Evgueni and Scott, Jennifer A.},
  title        = {A Sparse Symmetric Indefinite Direct Solver for {GPU} Architectures},
  journaltitle = {{ACM} Transactions on Mathematical Software},
  date         = {2016},
  volume       = {42},
  number       = {1},
  month        = {1},
  pages        = {1:1--1:25},
  issn         = {0098-3500},
  doi          = {10.1145/2756548},
  abstract     = {In recent years, there has been considerable interest in the potential for graphics processing units (GPUs) to speed up the performance of sparse direct linear solvers. Efforts have focused on symmetric positive-definite systems for which no pivoting is required, while little progress has been reported for the much harder indefinite case. We address this challenge by designing and developing a sparse symmetric indefinite solver SSIDS. This new library-quality $LDL^T$ factorization is designed for use on GPU architectures and incorporates threshold partial pivoting within a multifrontal approach. Both the factorize and the solve phases are performed using the GPU. Another important feature is that the solver produces bit-compatible results. Numerical results for indefinite problems arising from a range of practical applications demonstrate that, for large problems, SSIDS achieves performance improvements of up to a factor of 4.6$\times$ compared with a state-of-the-art multifrontal solver on a multicore CPU.},
  acmid        = {2756548},
  articleno    = {1},
  issue_date   = {February 2016},
  location     = {New York, NY, USA},
  numpages     = {25},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2017.01.05},
}

@InProceedings{Hong2012,
  Title                    = {{Green-Marl}: A {DSL} for Easy and Efficient Graph Analysis},
  Author                   = {Hong, Sungpack and Chafi, Hassan and Sedlar, Edic and Olukotun, Kunle},
  Booktitle                = {{P}roceedings of the {S}eventeenth {I}nternational {C}onference on {A}rchitectural {S}upport for {P}rogramming {L}anguages and {O}perating {S}ystems},

  Address                  = {New York, NY, USA},
  Pages                    = {349--362},
  Publisher                = {ACM},
  Series                   = {ASPLOS XVII},

 abstract = {The increasing importance of graph-data based applications is fueling the need for highly efficient and parallel implementations of graph analysis software. In this paper we describe Green-Marl, a domain-specific language (DSL) whose high level language constructs allow developers to describe their graph analysis algorithms intuitively, but expose the data-level parallelism inherent in the algorithms. We also present our Green-Marl compiler which translates high-level algorithmic description written in Green-Marl into an efficient C++ implementation by exploiting this exposed data-level parallelism. Furthermore, our Green-Marl compiler applies a set of optimizations that take advantage of the high-level semantic knowledge encoded in the Green-Marl DSL. We demonstrate that graph analysis algorithms can be written very intuitively with Green-Marl through some examples, and our experimental results show that the compiler-generated implementation out of such descriptions performs as well as or better than highly-tuned hand-coded implementations.},
  Acmid                    = {2151013},
  Date                     = {2012},
  Doi                      = {10.1145/2150976.2151013},
  ISBN                     = {978-1-4503-0759-8},
  Keywords                 = {domain-specific language, graph, parallel programming},
  Location                 = {London, UK},
  Numpages                 = {14}
}

@InProceedings{Hong2009,
  Title                    = {An Analytical Model for a {GPU} Architecture with Memory-level and Thread-level Parallelism Awareness},
  Author                   = {Hong, Sunpyo and Kim, Hyesoon},
  Booktitle                = {{P}roceedings of the 36th {A}nnual {I}nternational {S}ymposium on {C}omputer {A}rchitecture},

  Address                  = {New York, NY, USA},
  Pages                    = {152--163},
  Publisher                = {ACM},
  Series                   = {ISCA '09},

 abstract = {GPU architectures are increasingly important in the multi-core era due to their high number of parallel processors. Programming thousands of massively parallel threads is a big challenge for software engineers, but understanding the performance bottlenecks of those parallel programs on GPU architectures to improve application performance is even more difficult. Current approaches rely on programmers to tune their applications by exploiting the design space exhaustively without fully understanding the performance characteristics of their applications. \\To provide insights into the performance bottlenecks of parallel applications on GPU architectures, we propose a simple analytical model that estimates the execution time of massively parallel programs. The key component of our model is estimating the number of parallel memory requests (we call this the memory warp parallelism) by considering the number of running threads and memory bandwidth. Based on the degree of memory warp parallelism, the model estimates the cost of memory requests, thereby estimating the overall execution time of a program. Comparisons between the outcome of the model and the actual execution time in several GPUs show that the geometric mean of absolute error of our model on micro-benchmarks is 5.4\% and on GPU computing applications is 13.3\%. All the applications are written in the CUDA programming language.},
  Acmid                    = {1555775},
  Date                     = {2009},
  Doi                      = {10.1145/1555754.1555775},
  ISBN                     = {978-1-60558-526-0},
  Keywords                 = {GPU architecture, analytical model, cuda, memory level parallelism, performance estimation, warp level parallelism},
  Location                 = {Austin, TX, USA},
  Numpages                 = {12}
}

@InProceedings{Hong2010,
  Title                    = {An Integrated {GPU} Power and Performance Model},
  Author                   = {Hong, Sunpyo and Kim, Hyesoon},
  Booktitle                = {{P}roceedings of the 37th {A}nnual {I}nternational {S}ymposium on {C}omputer {A}rchitecture},

  Address                  = {New York, NY, USA},
  Pages                    = {280--289},
  Publisher                = {ACM},
  Series                   = {ISCA '10},

 abstract = {GPU architectures are increasingly important in the multi-core era due to their high number of parallel processors. Performance optimization for multi-core processors has been a challenge for programmers. Furthermore, optimizing for power consumption is even more difficult. Unfortunately, as a result of the high number of processors, the power consumption of many-core processors such as GPUs has increased significantly. \\Hence, in this paper, we propose an integrated power and performance (IPP) prediction model for a GPU architecture to predict the optimal number of active processors for a given application. The basic intuition is that when an application reaches the peak memory bandwidth, using more cores does not result in performance improvement. \\We develop an empirical power model for the GPU. Unlike most previous models, which require measured execution times, hardware performance counters, or architectural simulations, IPP predicts execution times to calculate dynamic power events. We then use the outcome of IPP to control the number of running cores. We also model the increases in power consumption that resulted from the increases in temperature. \\With the predicted optimal number of active cores, we show that we can save up to 22.09\%of runtime GPU energy consumption and on average 10.99\% of that for the five memory bandwidth-limited benchmarks.},
  Acmid                    = {1815998},
  Date                     = {2010},
  Doi                      = {10.1145/1815961.1815998},
  ISBN                     = {978-1-4503-0053-7},
  Keywords                 = {CUDA, GPU architecture, analytical model, energy, performance, power estimation},
  Location                 = {Saint-Malo, France},
  Numpages                 = {10}
}

@Article{Huangfu2015,
  Title                    = {Novel update techniques for the revised simplex method},
  Author                   = {Huangfu, Qi and Hall, Julian},
  Number                   = {3},
  Pages                    = {587--608},
  Volume                   = {60},

 abstract = {This paper introduces three novel techniques for updating the invertible representation of the basis matrix when solving practical sparse linear programming problems using a high performance implementation of the dual revised simplex method, being of particular value when suboptimization is used. Two are variants of the product form update and the other permits multiple Forrest-Tomlin updates to be performed. Computational results show that one of the product form variants is significantly more efficient than the traditional approach, with its performance approaching that of the Forrest-Tomlin update for some problems. The other is less efficient, but valuable in the context of the dual revised simplex method with suboptimization. Results show that the multiple Forrest-Tomlin updates are performed with no loss of serial efficiency.},
  Date                     = {2015},
  Doi                      = {10.1007/s10589-014-9689-1},
  Journaltitle             = {Computational Optimization and Applications},
  Owner                    = {andrea},
  Publisher                = {Springer},
  Timestamp                = {2017.04.08}
}

@InProceedings{Hugues2010,
  author    = {Hugues, Maxime R. and Petiton, Serge G.},
  title     = {Sparse Matrix Formats Evaluation and Optimization on a {GPU}},
  booktitle = {{P}roceedings of the 12th {IEEE} {I}nternational {C}onference on {H}igh {P}erformance {C}omputing and {C}ommunications},
  date      = {2010},
  series    = {HPCC '10},
  location  = {Melbourne, AUS},
  month     = sep,
  pages     = {122--129},
  doi       = {10.1109/HPCC.2010.85},
  abstract  = {The data parallel programming model comes back with massive multicore architectures. The GPU is one of these and offers important possibilities to accelerate linear algebra. However, the irregular structure of sparse matrix operations generates problems with this programming model to obtain efficient performance. This depends on the used format to store values and the matrix structure. The sparse matrix-vector product (SpMV) is one of the most used kernel in scientific computing and is the main performance source of iterative methods. We propose an evaluation and optimization of several sparse formats for the SpMV kernel which have succeeded at the time of data parallel computer. This study is realized by analyzing the performances following the distribution of the non zeros values in the matrix to determine the best and the worst reachable value. The results show that all sparse formats converge to the same efficiency and perform poorly with a strong distribution of elements.},
  keywords  = {computer graphic equipment;coprocessors;iterative methods;multiprocessing systems;parallel programming;sparse matrices;GPU;SpMV kernel;data parallel programming model;iterative method;linear algebra;massive multicore;sparse matrix format evaluation;sparse matrix format optimization;sparse matrix vector product;Data Parallel Programming;GPU;Many-Core;SpMV;Sparse Format},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Jacomy2014,
  author       = {Jacomy, Mathieu and Venturini, Tommaso and Heymann, Sebastien and Bastian, Mathieu},
  title        = {{ForceAtlas2}, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software},
  journaltitle = {{PL}oS {ONE}},
  date         = {2014},
  volume       = {9},
  number       = {6},
  month        = {6},
  doi          = {10.1371/journal.pone.0098679},
  abstract     = {Gephi is a network visualization software used in various disciplines (social network analysis, biology, genomics). One of its key features is the ability to display the spatialization process, aiming at transforming the network into a map, and ForceAtlas2 is its default layout algorithm. The latter is developed by the Gephi team as an all-around solution to Gephi users' typical networks (scale-free, 10 to 10,000 nodes). We present here for the first time its functioning and settings. ForceAtlas2 is a force-directed layout close to other algorithms used for network spatialization. We do not claim a theoretical advance but an attempt to integrate different techniques such as the Barnes Hut simulation, degree-dependent repulsive force, and local and global adaptive temperatures. It is designed for the Gephi user experience (it is a continuous algorithm), and we explain which constraints it implies. The algorithm benefits from much feedback and is developed in order to provide many possibilities through its settings. We lay out its complete functioning for the users who need a precise understanding of its behaviour, from the formulas to graphic illustration of the result. We propose a benchmark for our compromise between performance and quality. We also explain why we integrated its various features and discuss our design choices.},
  owner        = {andrea},
  timestamp    = {2015.01.18},
}

@Article{Jang2011,
  author       = {Jang, B. and Schaa, D. and Mistry, P. and Kaeli, D.},
  title        = {Exploiting Memory Access Patterns to Improve Memory Performance in Data-Parallel Architectures},
  journaltitle = {{IEEE} {T}ransactions on {P}arallel and {D}istributed {S}ystems},
  date         = {2011},
  volume       = {22},
  number       = {1},
  month        = {1},
  pages        = {105--118},
  issn         = {1045--9219},
  doi          = {10.1109/TPDS.2010.107},
  abstract     = {The introduction of General-Purpose computation on GPUs (GPGPUs) has changed the landscape for the future of parallel computing. At the core of this phenomenon are massively multithreaded, data-parallel architectures possessing impressive acceleration ratings, offering low-cost supercomputing together with attractive power budgets. Even given the numerous benefits provided by GPGPUs, there remain a number of barriers that delay wider adoption of these architectures. One major issue is the heterogeneous and distributed nature of the memory subsystem commonly found on data-parallel architectures. Application acceleration is highly dependent on being able to utilize the memory subsystem effectively so that all execution units remain busy. In this paper, we present techniques for enhancing the memory efficiency of applications on data-parallel architectures, based on the analysis and characterization of memory access patterns in loop bodies; we target vectorization via data transformation to benefit vector-based architectures (e.g., AMD GPUs) and algorithmic memory selection for scalar-based architectures (e.g., NVIDIA GPUs). We demonstrate the effectiveness of our proposed methods with kernels from a wide range of benchmark suites. For the benchmark kernels studied, we achieve consistent and significant performance improvements (up to 11.4$\times$ and 13.5$\times$ over baseline GPU implementations on each platform, respectively) by applying our proposed methodology.},
  keywords     = {computer graphic equipment;coprocessors;multi-threading;parallel architectures;GPU;algorithmic memory selection;low-cost supercomputing;massive multithreaded data-parallel architectures;memory access patterns;memory subsystem;parallel computing;power budgets;scalar-based architectures;vector-based architectures;GPU computing;General-purpose computation on GPUs (GPGPUs);data parallelism;data-parallel architectures.;memory access pattern;memory coalescing;memory optimization;memory selection;vectorization},
}

@Article{Jerez2012a,
  author       = {Jerez, J. L. and Ling, K.-V. and G.A. Constantinides, E. C. and Kerrigan},
  title        = {Model predictive control for deeply pipelined field-programmable gate array implementation: algorithms and circuitry},
  journaltitle = {Control Theory Applications, {IET}},
  date         = {2012},
  volume       = {6},
  number       = {8},
  month        = may,
  pages        = {1029--1041},
  issn         = {1751-8644},
  doi          = {10.1049/iet-cta.2010.0441},
  abstract     = {Model predictive control (MPC) is an optimisation-based scheme that imposes a real-time constraint on computing the solution of a quadratic programming (QP) problem. The implementation of MPC in fast embedded systems presents new technological challenges. In this paper we present a parameterised field-programmable gate array implementation of a customised QP solver for optimal control of linear processes with constraints, which can achieve substantial acceleration over a general purpose microprocessor, especially as the size of the optimisation problem grows. The focus is on exploiting the structure and accelerating the computational bottleneck in a primal-dual interior-point method. We then introduce a new MPC formulation that can take advantage of the novel computational opportunities, in the form of parallel computational channels, offered by the proposed pipelined architecture to improve performance even further. This highlights the importance of the interaction between the control theory and digital system design communities for the success of MPC in fast embedded systems.},
  keywords     = {field programmable gate arrays;pipeline processing;predictive control;quadratic programming;customised QP solver;deeply pipelined field-programmable gate array implementation;digital system design;fast embedded systems;general purpose microprocessor;linear processes;model predictive control;optimal control;optimisation-based scheme;parallel computational channels;pipelined architecture;primal-dual interior-point method;quadratic programming problem;real-time constraint},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InProceedings{Jerez2011a,
  author    = {Jerez, Juan L. and Constantinides, George A. and Kerrigan, Eric C.},
  title     = {An {FPGA} Implementation of a Sparse Quadratic Programming Solver for Constrained Predictive Control},
  booktitle = {{P}roceedings of the 19th {ACM}/{SIGDA} {I}nternational {S}ymposium on {F}ield {P}rogrammable {G}ate {A}rrays},
  date      = {2011},
  series    = {FPGA '11},
  publisher = {ACM},
  location  = {Monterey, CA, USA},
  isbn      = {978-1-4503-0554-9},
  pages     = {209--218},
  doi       = {10.1145/1950413.1950454},
  abstract  = {Model predictive control (MPC) is an advanced industrial control technique that relies on the solution of a quadratic programming (QP) problem at every sampling instant to determine the input action required to control the current and future behaviour of a physical system. Its ability in handling large multiple input multiple output (MIMO) systems with physical constraints has led to very successful applications in slow processes, where there is sufficient time for solving the optimization problem between sampling instants. The application of MPC to faster systems, which adds the requirement of greater sampling frequencies, relies on new ways of finding faster solutions to QP problems. Field-programmable gate arrays (FPGAs) are specially well suited for this application due to the large amount of computation for a small amount of I/O. In addition, unlike a software implementation, an FPGA can provide the precise timing guarantees required for interfacing the controller to the physical system. We present a high-throughput floating-point FPGA implementation that exploits the parallelism inherent in interior-point optimization methods. It is shown that by considering that the QPs come from a control formulation, it is possible to make heavy use of the sparsity in the problem to save computations and reduce memory requirements by 75\%. The implementation yields a 6.5$\times$ improvement in latency and a 51$\times$ improvement in throughput for large problems over a software implementation running on a general purpose microprocessor.},
  acmid     = {1950454},
  address   = {New York, NY, USA},
  keywords  = {interior-point, model predictive control, optimization},
  numpages  = {10},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Jerez2012,
  author    = {Jerez, Juan L. and Constantinides, George A. and Kerrigan, Eric C.},
  title     = {Towards a Fixed Point {QP} Solver for Predictive Control},
  booktitle = {{P}roceedings of the 51st {IEEE} {A}nnual {C}onference on {D}ecision and {C}ontrol},
  date      = {2012},
  series    = {CDC '12},
  location  = {Maui, HI, USA},
  month     = dec,
  pages     = {675--680},
  doi       = {10.1109/CDC.2012.6427015},
  abstract  = {There is a need for high speed, low cost and low energy solutions for convex quadratic programming to enable model predictive control (MPC) to be implemented in a wider set of applications than is currently possible. For most quadratic programming (QP) solvers the computational bottleneck is the solution of systems of linear equations, which we propose to solve using a fixed-point implementation of an iterative linear solver to allow for fast and efficient computation in parallel hardware. However, fixed point arithmetic presents additional challenges, such as having to bound peak values of variables and constrain their dynamic ranges. For these types of algorithms the problems cannot be automated by current tools. We employ a preconditioner in a novel manner to allow us to establish tight analytical bounds on all the variables of the Lanczos process, the heart of modern iterative linear solving algorithms. The proposed approach is evaluated through the implementation of a mixed precision interior-point controller for a Boeing 747 aircraft. The numerical results show that there does not have to be a loss of control quality by moving from floating-point to fixed-point.},
  issn      = {0743-1546},
  keywords  = {aircraft control;fixed point arithmetic;iterative methods;predictive control;quadratic programming;Boeing 747 aircraft;Lanczos process;MPC;control quality;convex quadratic programming;fixed point QP solver;fixed point arithmetic;fixed-point implementation;iterative linear solver;iterative linear solving algorithms;linear equations;mixed precision interior-point controller;model predictive control;parallel hardware;Dynamic range;Equations;Hardware;Heuristic algorithms;Mathematical model;Optimization;Symmetric matrices},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Jerez2012c,
  author    = {Jerez, Juan L. and Constantinides, George A. and Kerrigan, Eric C.},
  title     = {Fixed Point Lanczos: Sustaining {TFLOP}-equivalent Performance in {FPGAs} for Scientific Computing},
  booktitle = {{P}roceedings of the 20th {IEEE} {A}nnual {I}nternational {S}ymposium on {F}ield-{P}rogrammable {C}ustom {C}omputing {M}achines},
  date      = {2012},
  series    = {FCCM '12},
  location  = {Toronto, ON, CA},
  month     = {4},
  pages     = {53--60},
  doi       = {10.1109/FCCM.2012.19},
  abstract  = {We consider the problem of enabling fixed-point implementations of linear algebra kernels to match the strengths of the field-programmable gate array (FPGA). Algorithms for solving linear equations, finding eigen values or finding singular values are typically nonlinear and recursive making the problem of establishing analytical bounds on variable dynamic range non-trivial. Current approaches fail to provide tight bounds for this type of algorithms. We use as a case study one of the most important kernels in scientific computing, the Lanczos iteration, which lies at the heart of well known methods such as conjugate gradient and minimum residual, and we show how we can modify the algorithm to allow us to apply standard linear algebra analysis to prove tight analytical bounds on all variables of the process, regardless of the properties of the original matrix. It is shown that the numerical behaviour of fixed-point implementations of the modified problem can be chosen to be at least as good as a double precision floating point implementation. Using this approach it is possible to get sustained FPGA performance very close to the peak general-purpose graphics processing unit (GPGPU) performance in FPGAs of comparable size when solving a single problem. If there are several independent problems to solve simultaneously it is possible to exceed the peak floating-point performance of a GPGPU, obtaining approximately 1, 2 or 4 TFLOPs for error tolerances of $10^-7$, $10^-5$ and $10^-3$, respectively, in a large Virtex 7 FPGA.},
  keywords  = {conjugate gradient methods;eigenvalues and eigenfunctions;field programmable gate arrays;fixed point arithmetic;graphics processing units;iterative methods;FPGA;GPGPU;Lanczos iteration;TFLOP-equivalent performance;conjugate gradient;double precision floating point;eigenvalues;error tolerance;field-programmable gate array;fixed point Lanczos;general-purpose graphics processing unit;linear algebra kernel;linear equation;minimum residual;peak floating-point;scientific computing;Accuracy;Dynamic range;Eigenvalues and eigenfunctions;Field programmable gate arrays;Kernel;Linear algebra;Symmetric matrices;Field programmable gate arrays;Fixed-point arithmetic;High performance computing;Iterative algorithms;Scientific computing},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Jerez2011,
  author    = {Jerez, Juan L. and Constantinides, George A. and Kerrigan, Eric C. and Ling, Keck-Voon},
  title     = {Parallel {MPC} for Real-Time {FPGA}-based Implementation},
  booktitle = {{P}roceedings of the 18th {IFAC} {W}orld {C}ongress},
  date      = {2011},
  volume    = {18},
  location  = {Milan, IT},
  month     = sep,
  pages     = {1338--1345},
  doi       = {10.3182/20110828-6-IT-1002.01392},
  abstract  = {The succesful application of model predictive control (MPC) in fast embedded systems relies on faster and more energy efficient ways of solving complex optimization problems. A custom quadratic programming (QP) solver implementation on a field-programmable gate array (FPGA) can provide substantial acceleration by exploiting the parallelism inherent in some optimization algorithms, apart from providing novel computational opportunities arising from deep pipelining. This paper presents a new MPC algorithm based on multiplexed MPC that can take advantage of the full potential of an existing FPGA design by utilizing the provided free parallel computational channels arising from such pipelining. The result is greater acceleration over a conventional MPC implementation and reduced silicon usage. The FPGA implementation is shown to be approximately 200$\times$ more energy efficient than a high performance general purpose processor (GPP) for large control problems.},
  owner     = {andrea},
  timestamp = {2014.12.12},
}

@InProceedings{Jerez2013,
  author    = {Jerez, Juan L. and Goulart, Paul J. and Richter, Stefan and Constantinides, George A. and Kerrigan, Eric C. and Morari, Manfred},
  title     = {Embedded Predictive Control on an {FPGA} using the Fast Gradient Method},
  booktitle = {{P}roceedings of the 2013 {E}uropean {C}ontrol {C}onference},
  date      = {2013},
  series    = {ECC '13},
  location  = {Zurich, CH},
  month     = jul,
  isbn      = {978-3-033-03962-9},
  pages     = {3614--3620},
  url       = {http://ieeexplore.ieee.org/document/6669598/},
  abstract  = {Model predictive control (MPC) in resource-constrained embedded platforms requires faster, cheaper and more power-efficient solvers for convex programs than is currently offered by software-based solutions. In this paper we present the first field programmable gate array (FPGA) implementation of a fast gradient solver for linear-quadratic MPC problems with input constraints. We use fixed-point arithmetic to exploit the characteristics of the computing platform and provide analytical guarantees ensuring no overflow errors occur during operation. We further prove that the arithmetic errors due to round-off can lead only to reduced accuracy, but not instability, of the fast gradient method. The results are demonstrated on a model of an industrial atomic force microscope (AFM) where we show that, on a low-end FPGA, satisfactory control performance at a sample rate beyond 1 MHz is achievable, opening up new possibilities for the application of MPC.},
  keywords  = {control engineering computing;convex programming;embedded systems;field programmable gate arrays;gradient methods;linear quadratic control;predictive control;AFM;FPGA;arithmetic errors;convex programs;embedded predictive control;fast gradient method;fast gradient solver;field programmable gate array;fixed-point arithmetic;industrial atomic force microscope;linear-quadratic MPC problems;model predictive control;resource-constrained embedded platforms;satisfactory control performance;Computer architecture;Convergence;Field programmable gate arrays;Gradient methods;Hardware;Upper bound;Vectors},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Jerez2014,
  author       = {Jerez, Juan L. and Goulart, Paul J. and Richter, Stefan and Constantinides, George A. and Kerrigan, Eric C. and Morari, Manfred},
  title        = {Embedded Online Optimization for Model Predictive Control at Megahertz Rates},
  journaltitle = {{IEEE} Transactions on Automatic Control},
  date         = {2014},
  volume       = {59},
  number       = {12},
  month        = dec,
  pages        = {3238--3251},
  issn         = {0018-9286},
  doi          = {10.1109/TAC.2014.2351991},
  abstract     = {Faster, cheaper, and more power efficient optimization solvers than those currently possible using general-purpose techniques are required for extending the use of model predictive control (MPC) to resource-constrained embedded platforms. We propose several custom computational architectures for different first-order optimization methods that can handle linear-quadratic MPC problems with input, input-rate, and soft state constraints. We provide analysis ensuring the reliable operation of the resulting controller under reduced precision fixed-point arithmetic. Implementation of the proposed architectures in FPGAs shows that satisfactory control performance at a sample rate beyond 1 MHz is achievable even on low-end devices, opening up new possibilities for the application of MPC on embedded systems.},
  keywords     = {constraint theory;field programmable gate arrays;fixed point arithmetic;linear quadratic control;optimisation;predictive control;FPGA;custom computational architectures;embedded online optimization;first-order optimization method;general-purpose techniques;input-rate constraint;linear-quadratic MPC problem;megahertz rates;model predictive control;power efficient optimization solver;reduced precision fixed-point arithmetic;resource-constrained embedded platform;satisfactory control performance;soft state constraint;Computer architecture;Convergence;Gradient methods;Hardware;Indexes;Mathematical model;Embedded systems;optimization algorithms;predictive control of linear systems},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Jerez2012b,
  author       = {Jerez, Juan L. and Kerrigan, Eric C. and Constantinides, George A.},
  title        = {A sparse and condensed {QP} formulation for predictive control of {LTI} systems},
  journaltitle = {Automatica},
  date         = {2012},
  volume       = {48},
  number       = {5},
  pages        = {999--1002},
  issn         = {0005-1098},
  doi          = {10.1016/j.automatica.2012.03.010},
  abstract     = {The computational burden that model predictive control (MPC) imposes depends to a large extent on the way the optimal control problem is formulated as an optimization problem. We present a formulation where the input is expressed as an affine function of the state such that the closed-loop dynamics matrix becomes nilpotent. Using this approach and removing the equality constraints leads to a compact and sparse optimization problem to be solved at each sampling instant. The problem can be solved with a cost per interior-point iteration that is linear with respect to the horizon length, when this is bigger than the controllability index of the plant. The computational complexity of existing condensed approaches grow cubically with the horizon length, whereas existing non-condensed and sparse approaches also grow linearly, but with a greater proportionality constant than with the method presented here.},
  keywords     = {Predictive control},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InProceedings{Jones2010,
  author    = {Jones, David H. and Powell, Adam and Cheung, Christos-Savvas Bouganis Peter Y. K.},
  title     = {{GPU} Versus {FPGA} for High Productivity Computing},
  booktitle = {{P}roceedings of the 2010 {I}nternational {C}onference on {F}ield {P}rogrammable {L}ogic and {A}pplications},
  date      = {2010},
  series    = {FPL '10},
  publisher = {IEEE Computer Society},
  location  = {Milan, IT},
  isbn      = {978-0-7695-4179-2},
  pages     = {119--124},
  doi       = {10.1109/FPL.2010.32},
  abstract  = {Heterogeneous or co-processor architectures are becoming an important component of high productivity computing systems (HPCS). In this work the performance of a GPU based HPCS is compared with the performance of a commercially available FPGA based HPC. Contrary to previous approaches that focussed on specific examples, a broader analysis is performed by considering processes at an architectural level. A set of benchmarks is employed that use different process architectures in order to exploit the benefits of each technology. These include the asynchronous pipelines common to "map" tasks, a partially synchronous tree common to "reduce" tasks and a fully synchronous, fully connected mesh. We show that the GPU is more productive than the FPGA architecture for most of the benchmarks and conclude that FPGA-based HPCS is being marginalised by GPUs.},
  acmid     = {1933774},
  address   = {Washington, DC, USA},
  keywords  = {High Productivity Computing, GPU, FPGA},
  numpages  = {6},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Jung2008,
  author       = {Jung, Jin Hyuk and O'Leary, Dianne P.},
  title        = {Implementing an interior point method for linear programs on a {CPU}-{GPU} system.},
  journaltitle = {Electronic Transactions on Numerical Analysis},
  date         = {2008},
  language     = {eng},
  volume       = {28},
  pages        = {174--189},
  url          = {http://eudml.org/doc/117656},
  abstract     = {Graphics processing units (GPUs), present in every laptop and desktop computer, are potentially powerful computational engines for solving numerical problems. We present a mixed precision CPU-GPU algorithm for solving linear programming problems using interior point methods. This algorithm, based on the rectangular-packed matrix storage scheme of Gunnels and Gustavson, uses the GPU for computationally intensive tasks such as matrix assembly, Cholesky factorization, and forward and back substitution. Comparisons with a CPU implementation demonstrate that we can improve performance by using the GPU for sufficiently large problems. Since GPU architectures and programming languages are rapidly evolving, we expect that GPUs will be an increasingly attractive tool for matrix computation in the future},
  keywords     = {Cholesky factorization; matrix decomposition; forward and back substitution; rectangular packed format},
  owner        = {andrea},
  publisher    = {Kent State University, Department of Mathematics and Computer Science},
  timestamp    = {2014.12.12},
}

@Article{Kabir2015,
  author       = {Kabir, Khairul and Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack},
  title        = {Performance Analysis and Optimisation of Two-Sided Fractorization Algorithms for Heterogeneous Platform},
  journaltitle = {Procedia Computer Science},
  date         = {2015},
  volume       = {51},
  pages        = {180--190},
  doi          = {10.1016/j.procs.2015.05.222},
  abstract     = {Many applications, ranging from big data analytics to nanostructure designs, require the solution of large dense singular value decomposition (SVD) or eigenvalue problems. A first step in the solution methodology for these problems is the reduction of the matrix at hand to condensed form by two-sided orthogonal transformations. This step is standardly used to significantly accelerate the solution process. We present a performance analysis of the main two-sided factorizations used in these reductions: the bidiagonalization, tridiagonalization, and the upper Hessenberg factorizations on heterogeneous systems of multicore CPUs and Xeon Phi coprocessors. We derive a performance model and use it to guide the analysis and to evaluate performance. We develop optimized implementations for these methods that get up to 80\% of the optimal performance bounds. Finally, we describe the heterogeneous multicore and coprocessor development considerations and the techniques that enable us to achieve these high-performance results. The work here presents the first highly optimized implementation of these main factorizations for Xeon Phi coprocessors. Compared to the LAPACK versions optmized by Intel for Xeon Phi (in MKL), we achieve up to 50\% speedup.},
  owner        = {andrea},
  timestamp    = {2016.05.27},
}

@InProceedings{Kaleem2016,
  author    = {Kaleem, Rashid and Venkat, Anand and Pai, Sreepathi and Hall, Mary and Pingali, Keshav},
  title     = {Synchronization Trade-Offs in {GPU} Implementations of Graph Algorithms},
  booktitle = {{P}roceedings of the 30th {I}nternational {P}arallel and {D}istributed {P}rocessing {S}ymposium},
  date      = {2016},
  series    = {IPDPS '16},
  location  = {Chicago, IL, USA},
  month     = may,
  pages     = {514--523},
  doi       = {10.1109/IPDPS.2016.106},
  abstract  = {Although there is an extensive literature on GPU implementations of graph algorithms, we do not yet have a clear understanding of how implementation choices impact performance. As a step towards this goal, we studied how the choice of synchronization mechanism affects the end-to-end performance of complex graph algorithms, using stochastic gradient descent (SGD) as an exemplar. We implemented seven synchronization strategies for this application and evaluated them on two GPU platforms, using both road networks and social network graphs as inputs. Our experiments showed that although none of the seven strategies dominates the rest, it is possible to use properties of the platform and input graph to predict the best strategy.},
  issn      = {1530-2075},
  keywords  = {gradient methods;graph theory;graphics processing units;stochastic processes;synchronisation;GPU implementations;SGD;graph algorithms;graphics processing unit;road networks;social network graphs;stochastic gradient descent;synchronization trade-offs;Graphics processing units;Instruction sets;Motion pictures;Roads;Schedules;Sparse matrices;Synchronization;Edge-coloring;GPGPU;Scheduling;Stochastic Gradient Descent},
}

@TechReport{Kamvar2003,
  author      = {Kamvar, Sepandar and Haveliwala, Taher and Manning, Christopher and Golub, Gene},
  title       = {Exploiting the Block Structure of the Web for Computing PageRank},
  institution = {InfoLab, Stanford University},
  date        = {2003},
  number      = {2003--17},
  url         = {http://ilpubs.stanford.edu:8090/579/},
  abstract    = {The web link graph has a nested block structure: the vast majority of hyperlinks link pages on a host to other pages on the same host, and many of those that do not link pages within the same domain. We show how to exploit this structure to speed up the computation of PageRank by a 3-stage algorithm whereby (1)\textasciitilde{}the local PageRanks of pages for each host are computed independently using the link structure of that host, (2)\textasciitilde{}these local PageRanks are then weighted by the ``importance'' of the corresponding host, and (3)\textasciitilde{}the standard PageRank algorithm is then run using as its starting vector the weighted aggregate of the local PageRanks. Empirically, this algorithm speeds up the computation of PageRank by a factor of 2 in realistic scenarios. Further, we develop a variant of this algorithm that efficiently computes many different ``personalized'' PageRanks, and a variant that efficiently recomputes PageRank after node updates.},
  publisher   = {Stanford},
}

@Article{Karypis1999,
  author       = {Karypis, George and Kumar, Vipin},
  title        = {A fast and high quality multilevel scheme for partitioning irregular graphs},
  journaltitle = {{SIAM} Journal on scientific Computing},
  date         = {1999},
  volume       = {20},
  number       = {1},
  pages        = {359--392},
  doi          = {10.1137/S1064827595287997},
  abstract     = {Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [Bui and Jones, Proc. of the 6th SIAM Conference on Parallel Processing for Scientific Computing, 1993, 445--452; Hendrickson and Leland, A Multilevel Algorithm for Partitioning Graphs, Tech. report SAND 93-1301, Sandia National Laboratories, Albuquerque, NM, 1993]. From the early work it was clear that multilevel techniques held great promise; however, it was not knownif they can be made to consistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the effectiveness of many different choices for all three phases: coarsening, partition of the coarsest graph, and refinement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the final partition obtained after multilevel refinement. We also present a much faster variation of the Kernighan--Lin (KL) algorithm for refining during uncoarsening. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute fill-reducing orderings for sparse matrices, it produces orderings that have substantially smaller fill than the widely used multiple minimum degree algorithm.},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InProceedings{Kayiran2013,
  author    = {Kayıran, Onur and Jog, Adwait and Kandemir, Mahmut Taylan and Das, Chita Ranjan},
  title     = {Neither More nor Less: Optimizing Thread-level Parallelism for {GPGPU}s},
  booktitle = {{P}roceedings of the 22nd {I}nternational {C}onference on {P}arallel {A}rchitectures and {C}ompilation {T}echniques},
  date      = {2013},
  series    = {PACT '13},
  publisher = {IEEE Press},
  location  = {Edinburgh, UK},
  isbn      = {978-1-4799-1021-2},
  pages     = {157--166},
  url       = {http://dl.acm.org/citation.cfm?id=2523721.2523745},
  abstract  = {General-purpose graphics processing units (GPGPUs) are at their best in accelerating computation by exploiting abundant thread-level parallelism (TLP) offered by many classes of HPC applications. To facilitate such high TLP, emerging programming models like CUDA and OpenCL allow programmers to create work abstractions in terms of smaller work units, called cooperative thread arrays (CTAs). CTAs are groups of threads and can be executed in any order, thereby providing ample opportunities for TLP. The state-of-the-art GPGPU schedulers allocate maximum possible CTAs per-core (limited by available on-chip resources) to enhance performance by exploiting TLP. However, we demonstrate in this paper that executing the maximum possible number of CTAs on a core is not always the optimal choice from the performance perspective. High number of concurrently executing threads might cause more memory requests to be issued, and create contention in the caches, network and memory, leading to long stalls at the cores. To reduce resource contention, we propose a dynamic CTA scheduling mechanism, called DYNCTA, which modulates the TLP by allocating optimal number of CTAs, based on application characteristics. To minimize resource contention, DYNCTA allocates fewer CTAs for applications suffering from high contention in the memory sub-system, compared to applications demonstrating high throughput. Simulation results on a 30-core GPGPU platform with 31 applications show that the proposed CTA scheduler provides 28\% average improvement in performance compared to the existing CTA scheduler.},
  acmid     = {2523745},
  address   = {Piscataway, NJ, USA},
  keywords  = {GPGPUs, scheduling, thread-level parallelism},
  numpages  = {10},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Kelman2011,
  author    = {Kelman, A. and Borrelli, F.},
  title     = {Bilinear Model Predictive Control of a HVAC System Using Sequential Quadratic Programming},
  booktitle = {{P}roceedings of the 18th {IFAC} {W}orld {C}ongress},
  date      = {2011},
  series    = {IFAC '11},
  location  = {Milan, IT},
  doi       = {10.3182/20110828-6-IT-1002.03811},
  abstract  = {We study the problem of heating, ventilation, and air conditioning (HVAC) control in a typical commercial building. We propose a model predictive control (MPC) approach which minimizes energy use while satisfying occupant comfort constraints. A sequential quadratic programming algorithm is used to efficiently solve the resulting bilinear optimization problem. This paper presents the control design approach and the procedure for computing its solution. Extensive numerical simulations show the effectiveness of the proposed approach. In particular, the MPC is able to systematically reproduce a variety of well-known commercial solutions for energy savings, which include demand response, economizer mode and precooling/preheating.},
  owner     = {ap8213},
  timestamp = {2014.05.18},
}

@InProceedings{Kerrigan2015,
  author    = {Kerrigan, Eric C.},
  title     = {Feedback and Time are Important for the Optimal Control of Computing Systems},
  booktitle = {{P}roceedings of the 5th {IFAC} {C}onference on {N}onlinear {M}odel {P}redictive {C}ontrol},
  date      = {2015},
  series    = {NMPC '15},
  location  = {Seville, ES},
  month     = sep,
  doi       = {10.1016/j.ifacol.2015.11.309},
  abstract  = {The performance, reliability, cost, size and energy usage of computing systems can be improved by one or more orders of magnitude by the systematic use of modern control and optimization methods. Computing systems rely on the use of feedback algorithms to schedule tasks, data and resources, but the models that are used to design these algorithms are validated using open-loop metrics. By using closed-loop metrics instead, such as the gap metric developed in the control community, it should be possible to develop improved scheduling algorithms and computing systems that have not been over-engineered. Furthermore, scheduling problems are most naturally formulated as constraint satisfaction or mathematical optimization problems, but these are seldom implemented using state of the art numerical methods, nor do they explicitly take into account the fact that the scheduling problem itself takes time to solve. This paper makes the case that recent results in real-time model predictive control, where optimization problems are solved in order to control a process that evolves in time, are likely to form the basis of scheduling algorithms of the future. We therefore outline some of the research problems and opportunities that could arise by explicitly considering feedback and time when designing optimal scheduling algorithms for computing systems.},
  owner     = {andrea},
  timestamp = {2015.04.24},
}

@InProceedings{Khorasani2014,
  Title                    = {{CuSha}: Vertex-centric Graph Processing on {GPUs}},
  Author                   = {Khorasani, Farzad and Vora, Keval and Gupta, Rajiv and Bhuyan, Laxmi N.},
  Booktitle                = {{P}roceedings of the 23rd {I}nternational {S}ymposium on {H}igh-performance {P}arallel and {D}istributed {C}omputing},

  Address                  = {New York, NY, USA},
  Pages                    = {239--252},
  Publisher                = {ACM},
  Series                   = {HPDC '14},

 abstract = {Vertex-centric graph processing is employed by many popular algorithms (e.g., PageRank) due to its simplicity and efficient use of asynchronous parallelism. The high compute power provided by SIMT architecture presents an opportunity for accelerating these algorithms using GPUs. Prior works of graph processing on a GPU employ Compressed Sparse Row (CSR) form for its space-efficiency; however, CSR suffers from irregular memory accesses and GPU underutilization that limit its performance. In this paper, we present CuSha, a CUDA-based graph processing framework that overcomes the above obstacle via use of two novel graph representations: G-Shards and Concatenated Windows (CW). G-Shards uses a concept recently introduced for non-GPU systems that organizes a graph into autonomous sets of ordered edges called shards. CuSha's mapping of GPU hardware resources on to shards allows fully coalesced memory accesses. CW is a novel representation that enhances the use of shards to achieve higher GPU utilization for processing sparse graphs. Finally, CuSha fully utilizes the GPU power by processing multiple shards in parallel on GPU's streaming multiprocessors. For ease of programming, CuSha allows the user to define the vertex-centric computation and plug it into its framework for parallel processing of large graphs. Our experiments show that CuSha provides significant speedups over the state-of-the-art CSR-based virtual warp-centric method for processing graphs on GPUs.},
  Acmid                    = {2600227},
  Date                     = {2014},
  Doi                      = {10.1145/2600212.2600227},
  ISBN                     = {978-1-4503-2749-7},
  Keywords                 = {coalesced memory accesses, concatenated windows, g-shards, gpu, graph representation},
  Location                 = {Vancouver, BC, CA},
  Numpages                 = {14}
}

@TechReport{Knijnenburg1994,
  author      = {Knijnenburg, Peter M. W. and Wijshoff, Harry A. G.},
  title       = {On Improving Data Locality in Sparse Matrix Computations},
  institution = {High Performance Computing Division, Dept. of Computer Science, Leiden University},
  date        = {1994},
  url         = {http://liacs.leidenuniv.nl/assets/PDF/TechRep/tr94-15.pdf},
  abstract    = {Sparse matrix computations and irregular type computations show poor data locality behavior. Recently compiler optimizations techniques have been proposed to improve the data locality for regular type loop structures. Sparse matrix computations do not fall into this categorie of computations and the issue of compiler optimizations for sparse computations is merely understood. In this paper we describe how compiler optimizations based on pattern matching techniques can be used to improve the data locality behavior for sparse computations.},
}

@Article{Knyazev2007,
  author       = {Knyazev, Andrew V. and Lashuk, Ilya},
  title        = {Steepest descent and conjugate gradient methods with variable preconditioning},
  journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
  date         = {2007},
  volume       = {29},
  number       = {4},
  pages        = {1267--1280},
  doi          = {10.1137/060675290},
  abstract     = {We analyze the conjugate gradient (CG) method with variable preconditioning for solving a linear system with a real symmetric positive definite (SPD) matrix of coefficients A. We assume that the preconditioner is SPD on each step, and that the condition number of the preconditioned system matrix is bounded above by a constant independent of the step number. We show that the CG method with variable preconditioning under this assumption may not give improvement, compared to the steepest descent (SD) method. We describe the basic theory of CG methods with variable preconditioning with the emphasis on worst case scenarios, and provide complete proofs of all facts not available in the literature. We give a new elegant geometric proof of the SD convergence rate bound. Our numerical experiments, comparing the preconditioned SD and CG methods, not only support and illustrate our theoretical findings, but also reveal two surprising and potentially practically important effects. First, we analyze variable preconditioning in the form of inner-outer iterations. In previous such tests, the unpreconditioned CG inner iterations are applied to an artificial system with some fixed preconditioner as a matrix of coefficients. We test a different scenario, where the unpreconditioned CG inner iterations solve linear systems with the original system matrix A. We demonstrate that the CG-SD inner-outer iterations perform as well as the CG-CG inner-outer iterations in these tests. Second, we compare the CG methods using a two-grid preconditioning with fixed and randomly chosen coarse grids, and observe that the fixed preconditioner method is twice as slow as the method with random preconditioning.},
  owner        = {andrea},
  publisher    = {SIAM},
  timestamp    = {2017.05.08},
}

@Article{Koch2011,
  author       = {Koch, Thorsten and Achterberg, Tobias and Andersen, Erling and Bastert, Oliver and Berthold, Timo and Bixby, Robert E. and Danna, Emilie and Gamrath, Gerald and Gleixner, Ambros M. and Heinz, Stefan and Lodi, Andrea and Mittelmann, Hans and Ralphs, Ted and Salvagnin, Domenico and Steffy, Daniel E. and Wolter, Kati},
  title        = {{MIPLIB} 2010},
  journaltitle = {Mathematical Programming Computation},
  date         = {2011},
  volume       = {3},
  number       = {2},
  pages        = {103--163},
  doi          = {10.1007/s12532-011-0025-9},
  abstract     = {This paper reports on the fifth version of the Mixed Integer Programming Library. The miplib 2010 is the first miplib release that has been assembled by a large group from academia and from industry, all of whom work in integer programming. There was mutual consent that the concept of the library had to be expanded in order to fulfill the needs of the community. The new version comprises 361 instances sorted into several groups. This includes the main benchmark test set of 87 instances, which are all solvable by today's codes, and also the challenge test set with 164 instances, many of which are currently unsolved. For the first time, we include scripts to run automated tests in a predefined way. Further, there is a solution checker to test the accuracy of provided solutions using exact arithmetic.},
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@Article{Koch2012,
  Title                    = {Could we use a million cores to solve an integer program?},
  Author                   = {Koch, Thorsten and Ralphs, Ted and Shinano, Yuji},
  Number                   = {1},
  Pages                    = {67--93},
  Volume                   = {76},

 abstract = {Given the steady increase in cores per CPU, it is only a matter of time before supercomputers will have a million or more cores. In this article, we investigate the opportunities and challenges that will arise when trying to utilize this vast computing power to solve a single integer linear optimization problem. We also raise the question of whether best practices in sequential solution of ILPs will be effective in massively parallel environments.},
  Date                     = {2012},
  Doi                      = {10.1007/s00186-012-0390-9},
  ISSN                     = {1432-5217},
  Journaltitle             = {Mathematical Methods of Operations Research}
}

@Article{Kreutzer2014,
  author       = {Kreutzer, Moritz and Hazen, Georg and Wellein, Gerhard and Fehske, Holger and Bishop, Alan R.},
  title        = {A Unified Sparse Matrix Data Format for Efficient General Sparse Matrix-Vector Multiplication on Modern Processors with Wide {SIMD} Units},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2014},
  volume       = {36},
  number       = {5},
  pages        = {C401--C423},
  doi          = {10.1137/130930352},
  abstract     = {Sparse matrix-vector multiplication (spMVM) is the most time-consuming kernel in many numerical algorithms and has been studied extensively on all modern processor and accelerator architectures. However, the optimal sparse matrix data storage format is highly hardware-specific, which could become an obstacle when using heterogeneous systems. Also, it is as yet unclear how the wide single instruction multiple data (SIMD) units in current multi- and many-core processors should be used most efficiently if there is no structure in the sparsity pattern of the matrix. We suggest SELL-$C$-$\sigma$, a variant of Sliced ELLPACK, as a SIMD-friendly data format which combines long-standing ideas from general-purpose graphics processing units and vector computer programming. We discuss the advantages of SELL-$C$-$\sigma$ compared to established formats like Compressed Row Storage and ELLPACK and show its suitability on a variety of hardware platforms (Intel Sandy Bridge, Intel Xeon Phi, and Nvidia Tesla K20) for a wide range of test matrices from different application areas. Using appropriate performance models we develop deep insight into the data transfer properties of the SELL-$C$-$\sigma$ spMVM kernel. SELL-$C$-$\sigma$ comes with two tuning parameters whose performance impact across the range of test matrices is studied and for which reasonable choices are proposed. This leads to a hardware-independent (``catch-all'') sparse matrix format, which achieves very high efficiency for all test matrices across all hardware platforms.},
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@Article{Kreutzer2016,
  Title                    = {{GHOST}: Building Blocks for High Performance Sparse Linear Algebra on Heterogeneous Systems},
  Author                   = {Kreutzer, Moritz and Thies, Jonas and Röhrig-Zöllner, Melven and Pieper, Andreas and Shahzad, Faisal and Galgon, Martin and Basermann, Achim and Fehske, Holger and Hager, Georg and Wellein, Gerhard},
  Pages                    = {1--27},

 abstract = {While many of the architectural details of future exascale-class high performance computer systems are still a matter of intense research, there appears to be a general consensus that they will be strongly heterogeneous, featuring standard as well as accelerated resources. Today, such resources are available as multicore processors, graphics processing units (GPUs), and other accelerators such as the Intel Xeon Phi. Any software infrastructure that claims usefulness for such environments must be able to meet their inherent challenges: massive multi-level parallelism, topology, asynchronicity, and abstraction. The General, Hybrid, and Optimized Sparse Toolkit (GHOST) is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the MPI+X paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We describe the details of its design with respect to the challenges posed by modern heterogeneous supercomputers and recent algorithmic developments. Implementation details which are indispensable for achieving high efficiency are pointed out and their necessity is justified by performance measurements or predictions based on performance models. We also provide instructions on how to make use of GHOST in existing software packages, together with a case study which demonstrates the applicability and performance of GHOST as a component within a larger software stack. The library code and several applications are available as open source.},
  Date                     = {2016},
  Doi                      = {10.1007/s10766-016-0464-z},
  ISSN                     = {1573-7640},
  Journaltitle             = {International Journal of Parallel Programming},
  Owner                    = {andrea},
  Timestamp                = {2017.05.02}
}

@InProceedings{Kulkarni2009a,
  author    = {Kulkarni, M. and Burtscher, M. and Cascaval, C. and Pingali, K.},
  title     = {Lonestar: A suite of parallel irregular programs},
  booktitle = {{P}roceedings of the 2009 {IEEE} {I}nternational {S}ymposium on {P}erformance {A}nalysis of {S}ystems and {S}oftware},
  date      = {2009},
  series    = {ISPASS '09},
  location  = {Boston, MA, USA},
  month     = {4},
  pages     = {65--76},
  doi       = {10.1109/ISPASS.2009.4919639},
  abstract  = {Until recently, parallel programming has largely focused on the exploitation of data-parallelism in dense matrix programs. However, many important application domains, including meshing, clustering, simulation, and machine learning, have very different algorithmic foundations: they require building, computing with, and modifying large sparse graphs. In the parallel programming literature, these types of applications are usually classified as irregular applications, and relatively little attention has been paid to them. To study and understand the patterns of parallelism and locality in sparse graph computations better, we are in the process of building the Lonestar benchmark suite. In this paper, we characterize the first five programs from this suite, which target domains like data mining, survey propagation, and design automation. We show that even such irregular applications often expose large amounts of parallelism in the form of amorphous data-parallelism. Our speedup numbers demonstrate that this new type of parallelism can successfully be exploited on modern multi-core machines.},
  keywords  = {data mining;graph theory;parallel programming;Lonestar;amorphous data-parallelism;data mining;design automation;parallel irregular programming;sparse graph;survey propagation;Buildings;Clustering algorithms;Computational modeling;Concurrent computing;Data mining;Machine learning;Machine learning algorithms;Parallel processing;Parallel programming;Sparse matrices},
}

@InProceedings{Kulkarni2009,
  Title                    = {How Much Parallelism is There in Irregular Applications?},
  Author                   = {Kulkarni, Milind and Burtscher, Martin and Inkulu, Rajeshkar and Pingali, Keshav and Caşcaval, Călin},
  Booktitle                = {{P}roceedings of the 14th {ACM SIGPLAN} {S}ymposium on {P}rinciples and {P}ractice of {P}arallel {P}rogramming},

  Address                  = {New York, NY, USA},
  Pages                    = {3--14},
  Publisher                = {ACM},
  Series                   = {PPoPP '09},

 abstract = {Irregular programs are programs organized around pointer-based data structures such as trees and graphs. Recent investigations by the Galois project have shown that many irregular programs have a generalized form of data-parallelism called amorphous data-parallelism. However, in many programs, amorphous data-parallelism cannot be uncovered using static techniques, and its exploitation requires runtime strategies such as optimistic parallel execution. This raises a natural question: how much amorphous data-parallelism actually exists in irregular programs? \\In this paper, we describe the design and implementation of a tool called ParaMeter that produces parallelism profiles for irregular programs. Parallelism profiles are an abstract measure of the amount of amorphous data-parallelism at different points in the execution of an algorithm, independent of implementation-dependent details such as the number of cores, cache sizes, load-balancing, etc. ParaMeter can also generate constrained parallelism profiles for a fixed number of cores. We show parallelism profiles for seven irregular applications, and explain how these profiles provide insight into the behavior of these applications.},
  Acmid                    = {1504181},
  Date                     = {2009},
  Doi                      = {10.1145/1504176.1504181},
  ISBN                     = {978-1-60558-397-6},
  Keywords                 = {optimistic parallelism, parallelism profiles, profiling},
  Location                 = {Raleigh, NC, USA},
  Numpages                 = {12}
}

@Article{Kurzak2013,
  author       = {Kurzak, J. and Luszczek, P. and Faverge, M. and Dongarra, J.},
  title        = {{LU} Factorization with Partial Pivoting for a Multicore System with Accelerators},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2013},
  volume       = {24},
  number       = {8},
  month        = aug,
  pages        = {1613--1621},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2012.242},
  abstract     = {LU factorization with partial pivoting is a canonical numerical procedure and the main component of the high performance LINPACK benchmark. This paper presents an implementation of the algorithm for a hybrid, shared memory, system with standard CPU cores and GPU accelerators. The difficulty of implementing the algorithm for such a system lies in the disproportion between the computational power of the CPUs, compared to the GPUs, and in the meager bandwidth of the communication link between their memory systems. An additional challenge comes from the complexity of the memory-bound and synchronization-rich nature of the panel factorization component of the block LU algorithm, imposed by the use of partial pivoting. The challenges are tackled with the use of a data layout geared toward complex memory hierarchies, autotuning of GPU kernels, fine-grain parallelization of memory-bound CPU operations and dynamic scheduling of tasks to different devices. Performance in excess of one TeraFLOPS is achieved using four AMD Magny Cours CPUs and four NVIDIA Fermi GPUs.},
  keywords     = {graphics processing units;multiprocessing systems;parallel processing;scheduling;shared memory systems;AMD Magny Cours CPU;CPU core;GPU accelerator;LU factorization;NVIDIA Fermi GPU;block LU algorithm;canonical numerical procedure;data layout;dynamic task scheduling;fine-grain parallelization;graphics processing unit;high performance LINPACK benchmark;memory hierarchy;memory-bound CPU operation;multicore system;panel factorization component;partial pivoting;shared memory system;Dynamic scheduling;Graphics processing unit;Kernel;Layout;Libraries;Plasmas;Tiles;Dynamic scheduling;GPU;Gaussian elimination;Graphics processing unit;Kernel;LU factorization;Layout;Libraries;Plasmas;Tiles;accelerator;manycore;multicore;partial pivoting},
  owner        = {ap8213},
  timestamp    = {2014.10.09},
}

@InProceedings{Lain1995,
  Title                    = {Exploiting Spatial Regularity in Irregular Iterative Applications},
  Author                   = {Lain, Antonio and Banerjee, Prithviraj},
  Booktitle                = {{P}roceedings of the 9th {I}nternational {S}ymposium on {P}arallel {P}rocessing},

  Address                  = {Washington, DC, USA},
  Pages                    = {820--826},
  Publisher                = {{IEEE} Computer Society},
  Series                   = {IPPS '95},

 abstract = {The increasing gap between the speed of microprocessors and memory subsystems makes it imperative to exploit locality of reference in sequential irregular applications. The parallelization of such applications requires special considerations. Current RTS (Run-Time Support) for irregular computations fails to exploit the fine grain regularity present in these applications, producing unnecessary time and memory overheads. PILAR (Parallel Irregular Library with Application of Regularity) is a new RTS for irregular computations that provides a variety of internal representations of communication patterns based on their regularity; allowing for the efficient support of a wide spectrum of regularity under a common framework. Experimental results on the IBM SP-1 and Intel Paragon demonstrate the validity of our approach.},
  Acmid                    = {663225},
  Date                     = {1995},
  ISBN                     = {0-8186-7074-6},
  Keywords                 = {IBM SP-1, Intel Paragon, PILAR, Parallel Irregular Library, Run-Time Support, communication patterns, computational geometry, irregular iterative applications, parallel processing, performance evaluation, sequential irregular applications, spatial regularity},
  Location                 = {Santa Barbara, CA, USA},
  Numpages                 = {7},
  Url                      = {http://dl.acm.org/citation.cfm?id=645605.663225&preflayout=tabs}
}

@InProceedings{Lalami2011a,
  author     = {Lalami, Mohammed Esseghir and Boyer, Vincent and El-Baz, Didier},
  title      = {Efficient Implementation of the Simplex Method on a {CPU}--{GPU} System},
  booktitle  = {{P}roceedings of the 2011 {IEEE} {I}nternational {P}arallel and {D}istributed {P}rocessing {S}ymposium},
  date       = {2011},
  series     = {IPDPS '11},
  location   = {Anchorage, AL, USA},
  month      = may,
  isbn       = {978-1-61284-425-1},
  pages      = {1999--2006},
  doi        = {10.1109/IPDPS.2011.362},
  abstract   = {The Simplex algorithm is a well known method to solve linear programming (LP) problems. In this paper, we propose a parallel implementation of the Simplex on a CPU-GPU systems via CUDA. Double precision implementation is used in order to improve the quality of solutions. Computational tests have been carried out on randomly generated instances for non-sparse LP problems. The tests show a maximum speedup of 12:5 on a GTX 260 board.},
  owner      = {ap8213},
  timestamp  = {2014.10.09},
}

@InProceedings{Lalami2012,
  author    = {Lalami, Mohammed Esseghir and El-Baz, Didier},
  title     = {{GPU} Implementation of the Branch and Bound Method for Knapsack Problems},
  booktitle = {{P}roceedings of the 26th {IEEE} {I}nternational {P}arallel and {D}istributed {P}rocessing {S}ymposium {W}orkshops and {P}h{D} {F}orum},
  date      = {2012},
  series    = {IPDPSW '12},
  location  = {Shanghai, CN},
  pages     = {1769--1777},
  doi       = {10.1109/IPDPSW.2012.219},
  abstract  = {In this paper, we propose an efficient implementation of the branch and bound method for knapsack problems on a CPU-GPU system via CUDA. Branch and bound computations can be carried out either on the CPU or on a GPU according to the size of the branch and bound list. A better management of GPUs memories, less GPU-CPU communications and better synchronization between GPU threads are proposed in this new implementation in order to increase efficiency. Indeed, a series of computational results is displayed and analyzed showing a substantial speedup on a Tesla C2050 GPU.},
  keywords  = {graphics processing units;knapsack problems;parallel architectures;tree searching;CPU-GPU system;CUDA;GPU-CPU communications;Tesla C2050 GPU;branch and bound method;knapsack problems;Central Processing Unit;Computer architecture;Graphics processing unit;Instruction sets;Kernel;Optimization;Upper bound;CUDA;GPU computing;branch and bound method;combinatorial optimization;hybrid computing;knapsack problems},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@InProceedings{Lalami2011,
  author    = {Lalami, Mohammed Esseghir and El-Baz, Didier and Boyer, Vincent},
  title     = {Multi {GPU} Implementation of the Simplex Algorithm},
  booktitle = {{P}roceedings of the 13th {IEEE} {I}nternational {C}onference on {H}igh {P}erformance {C}omputing and {C}ommunications},
  date      = {2011},
  series    = {HPCC '11},
  location  = {Banff, AB, CA},
  month     = sep,
  pages     = {179--186},
  doi       = {10.1109/HPCC.2011.32},
  abstract  = {The Simplex algorithm is a well known method to solve linear programming (LP) problems. In this paper, we propose an implementation via CUDA of the Simplex method on a multi GPU architecture. Computational tests have been carried out on randomly generated instances for non-sparse LP problems. The tests show a maximum speedup of 24.5 with two Tesla C2050 boards.},
  keywords  = {computer graphic equipment;coprocessors;linear programming;multiprocessing systems;parallel architectures;CUDA;Tesla C2050 boards;computational tests;linear programming;multi GPU architecture;nonsparse LP problem;randomly generated instances;simplex method;Computer architecture;Graphics processing unit;Indexes;Instruction sets;Kernel;Linear programming;Synchronization;CUDA;GPU computing;Simplex method;hybrid computing;linear programming;parallel computing},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Langr2016,
  author       = {Langr, Daniel and Tvrdik, Pavel},
  title        = {Evaluation Criteria for Sparse Matrix Storage Formats},
  journaltitle = {{IEEE}Transactions on Parallel and Distributed Systems},
  date         = {2016},
  volume       = {27},
  number       = {2},
  month        = feb,
  pages        = {428--440},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2015.2401575},
 abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
  acmid        = {2914075},
  issue_date   = {February 2016},
  location     = {Piscataway, NJ, USA},
  numpages     = {13},
  publisher    = {IEEE Press},
}

@Book{Lawson1995,
  Title                    = {Solving Least Squares Problems},
  Author                   = {Lawson, C. and Hanson, R.},
  Publisher                = {Society for Industrial and Applied Mathematics},

  Date                     = {1995},
  Doi                      = {10.1137/1.9781611971217},
  Eprint                   = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611971217}
}

@InProceedings{Lee2014,
  Title                    = {{CAWS}: Criticality-aware Warp Scheduling for {GPGPU} Workloads},
  Author                   = {Lee, Shin-Ying and Wu, Carole-Jean},
  Booktitle                = {{P}roceedings of the 23rd {I}nternational {C}onference on {P}arallel {A}rchitectures and {C}ompilation},

  Address                  = {New York, NY, USA},
  Pages                    = {175--186},
  Publisher                = {ACM},
  Series                   = {PACT '14},

 abstract = {The ability to perform fast context-switching and massive multi-threading is the forte of modern GPU architectures, which have emerged as an efficient alternative to traditional chip-multiprocessors for parallel workloads. One of the main benefits of such architecture is its latency-hiding capability. However, the efficacy of GPU's latency-hiding varies significantly across GPGPU applications. \\To investigate this, this paper first proposes a new algorithm that profiles execution behavior of GPGPU applications. We characterize latencies caused by various pipeline hazards, memory accesses, synchronization primitives, and the warp scheduler. Our results show that the current round-robin warp scheduler works well in overlapping various latency stalls with the execution of other available warps for only a few GPGPU applications. For other applications, there is an excessive latency stall that cannot be hidden by the scheduler effectively. With the latency characterization insight, we observe a significant execution time disparity for warps within the same thread block, which causes sub-optimal performance, called the warp criticality problem. \\To tackle the warp criticality problem, we design a family of criticality-aware warp scheduling (CAWS) policies by scheduling the critical warp(s) more frequently than other warps. Our results on the breadth-first-search, B+tree search, two point angular correlation function, and K-means clustering show that, with oracle knowledge of warp criticality, our best-performing scheduling policy can improve GPGPU applications' performance by 17\% on average. With our designed criticality predictor, the various scheduling policies can improve performance by 10-21\% on breadth-first-search. To our knowledge, this is the first paper to characterize warp criticality and explore different criticality-aware warp scheduling policies for GPGPU workloads.},
  Acmid                    = {2628107},
  Date                     = {2014},
  Doi                      = {10.1145/2628071.2628107},
  ISBN                     = {978-1-4503-2809-8},
  Keywords                 = {gpgpu, gpu performance characterization, warp/wavefront scheduling},
  Location                 = {Edmonton, AB, CA},
  Numpages                 = {12},
  Owner                    = {andrea},
  Timestamp                = {2016.09.07}
}

@InProceedings{Lee2010,
  Title                    = {Debunking the 100X {GPU} vs. {CPU} Myth: An Evaluation of Throughput Computing on {CPU} and {GPU}},
  Author                   = {Lee, Victor W. and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
  Booktitle                = {{P}roceedings of the 37th {A}nnual {I}nternational {S}ymposium on {C}omputer {A}rchitecture},

  Address                  = {New York, NY, USA},
  Pages                    = {451--460},
  Publisher                = {ACM},
  Series                   = {ISCA '10},

 abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10$\times$ and 1000$\times$) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5$\times$ on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
  Acmid                    = {1816021},
  Date                     = {2010},
  Doi                      = {10.1145/1815961.1816021},
  ISBN                     = {978-1-4503-0053-7},
  Keywords                 = {cpu architecture, gpu architecture, performance analysis, performance measurement, software optimization, throughput computing},
  Location                 = {Saint-Malo, FR},
  Numpages                 = {10},
  Owner                    = {andrea},
  Timestamp                = {2016.09.07}
}

@InProceedings{LiC2015,
  Title                    = {Locality-Driven Dynamic {GPU} Cache Bypassing},
  Author                   = {Li, Chao and Song, Shuaiwen Leon and Dai, Hongwen and Sidelnik, Albert and Hari, Siva Kumar Sastry and Zhou, Huiyang},
  Booktitle                = {{P}roceedings of the 29th {ACM} {I}nternational {C}onference on {S}upercomputing},

  Address                  = {New York, NY, USA},
  Pages                    = {67--77},
  Publisher                = {ACM},
  Series                   = {ICS '15},

 abstract = {This paper presents novel cache optimizations for massively parallel, throughput-oriented architectures like GPUs. L1 data caches (L1 D-caches) are critical resources for providing high-bandwidth and low-latency data accesses. However, the high number of simultaneous requests from single-instruction multiple-thread (SIMT) cores makes the limited capacity of L1 D-caches a performance and energy bottleneck, especially for memory-intensive applications. We observe that the memory access streams to L1 D-caches for many applications contain a significant amount of requests with low reuse, which greatly reduce the cache efficacy. Existing GPU cache management schemes are either based on conditional/reactive solutions or hit-rate based designs specifically developed for CPU last level caches, which can limit overall performance. \\To overcome these challenges, we propose an efficient locality monitoring mechanism to dynamically filter the access stream on cache insertion such that only the data with high reuse and short reuse distances are stored in the L1 D-cache. Specifically, we present a design that integrates locality filtering based on reuse characteristics of GPU workloads into the decoupled tag store of the existing L1 D-cache through simple and cost-effective hardware extensions. Results show that our proposed design can dramatically reduce cache contention and achieve up to 56.8\% and an average of 30.3\% performance improvement over the baseline architecture, for a range of highly-optimized cache-unfriendly applications with minor area overhead and better energy efficiency. Our design also significantly outperforms the state-of-the-art CPU and GPU bypassing schemes (especially for irregular applications), without generating extra L2 and DRAM level contention.},
  Acmid                    = {2751237},
  Date                     = {2015},
  Doi                      = {10.1145/2751205.2751237},
  ISBN                     = {978-1-4503-3559-1},
  Location                 = {Newport Beach, CA, USA},
  Numpages                 = {11}
}

@InProceedings{LiD2015,
  author    = {Li, D. and Wu, H. and Becchi, M.},
  title     = {Nested Parallelism on {GPU}: Exploring Parallelization Templates for Irregular Loops and Recursive Computations},
  booktitle = {{P}roceedings of the 44th {I}nternational {C}onference on {P}arallel {P}rocessing},
  date      = {2015},
  series    = {ICPP '15},
  location  = {Beijing, CN},
  month     = sep,
  pages     = {979--988},
  doi       = {10.1109/ICPP.2015.107},
  abstract  = {The effective deployment of applications exhibiting irregular nested parallelism on GPUs is still an open problem. A naive mapping of irregular code onto the GPU hardware often leads to resource underutilization and, thereby, limited performance. In this work, we focus on two computational patterns exhibiting nested parallelism: irregular nested loops and parallel recursive computations. In particular, we focus on recursive algorithms operating on trees and graphs. We propose different parallelization templates aimed to increase the GPU utilization of these codes. Specifically, we investigate mechanisms to effectively distribute irregular work to streaming multiprocessors and GPU cores. Some of our parallelization templates rely on dynamic parallelism, a feature recently introduced by Nvidia in their Kepler GPUs and announced as part of the Open CL 2.0 standard. We propose mechanisms to maximize the work performed by nested kernels and minimize the overhead due to their invocation. Our results show that the use of our parallelization templates on applications with irregular nested loops can lead to a 2-6$\times$ speedup over baseline GPU codes that do not include load balancing mechanisms. The use of nested parallelism-based parallelization templates on recursive tree traversal algorithms can lead to substantial speedups (up to 15-24$\times$) over optimized CPU implementations. However, the benefits of nested parallelism are still unclear in the presence of recursive applications operating on graphs, especially when recursive code variants require expensive synchronization. In these cases, a flat parallelization of iterative versions of the considered algorithms may be preferable.},
  issn      = {0190-3918},
  keywords  = {graphics processing units;iterative methods;multiprocessing systems;parallel processing;program control structures;resource allocation;synchronisation;trees (mathematics);GPU cores;GPU hardware;GPU utilization;Kepler GPUs;Nvidia;OpenCL 2.0 standard;baseline GPU codes;computational patterns;dynamic parallelism;graphs;irregular code;irregular nested loops;load balancing mechanisms;multiprocessors cores;naive mapping;nested parallelism-based parallelization templates;optimized CPU implementation;parallel recursive computations;recursive algorithms;recursive code variants;recursive tree traversal algorithms;resource underutilization;trees;Graphics processing units;Hardware;Heuristic algorithms;Instruction sets;Kernel;Load management;Parallel processing},
}

@InCollection{Li2011,
  author    = {Li, Jianming and Lv, Renping and Hu, Xiangpei and Jiang, Zhongqiang},
  title     = {A {GPU}-Based Parallel Algorithm for Large Scale Linear Programming Problem},
  booktitle = {{I}ntelligent {D}ecision {T}echnologies},
  date      = {2011},
  editor    = {Watada, J. and Phillips-Wren, G. and Jain, L. and Howlett, R. J.},
  volume    = {10},
  series    = {Smart Innovation, Systems and Technologies},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-642-22193-4},
  pages     = {37--46},
  doi       = {10.1007/978-3-642-22194-1_4},
  abstract  = {A GPU-based parallel algorithm to solve large scale linear programming problem is proposed in this research. It aims to improve the computing efficiency when the linear programming problem becomes sufficiently large scale or more complicated. This parallel algorithm, based on Gaussian elimination, uses the GPU (Graphics Processing Unit) for computationally intensive tasks such as basis matrix operation, canonical form transformation and entering variable selection. At the same time, CPU is used to control the iteration. Experimental results show that the algorithm is competitive with CPU algorithm and can greatly reduce the computing time, so the GPU-based parallel algorithm is an effective way to solve large scale linear programming problem.},
  keywords  = {Linear Programming; Parallel Algorithm; GPU; CUDA (Compute Unified Device Architecture)},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@InProceedings{Li2016,
  author    = {Li, Sicheng and Wang, Y. and Wen, Wujie and Wang, Y. and Chen, Yiran and Li, Hai},
  title     = {A data locality-aware design framework for reconfigurable sparse matrix-vector multiplication kernel},
  booktitle = {2016 {IEEE}/{ACM} {I}nternational {C}onference on {C}omputer-{A}ided {D}esign ({ICCAD})},
  date      = {2016},
  location  = {Austin, TX, US},
  month     = nov,
  pages     = {1--6},
  doi       = {10.1145/2966986.2966987},
  abstract  = {Sparse matrix-vector multiplication (SpMV) is an important computational kernel in many applications. For performance improvement, software libraries designated for SpMV computation have been introduced, e.g., MKL library for CPUs and cuSPARSE library for GPUs. However, the computational throughput of these libraries is far below the peak floating-point performance offered by hardware platforms, because the efficiency of SpMV kernel is greatly constrained by the limited memory bandwidth and irregular data access patterns. In this work, we propose a data locality-aware design framework for FPGA-based SpMV acceleration. We first include the hardware constraints in sparse matrix compression at software level to regularize the memory allocation and accesses. Moreover, a distributed architecture composed of processing elements is developed to improve the computation parallelism. We implement the reconfigurable SpMV kernel on Convey HC-2ex and conduct the evaluation by using the University of Florida sparse matrix collection. The experiments demonstrate an average computational efficiency of 48.2\%, which is a lot better than those of CPU and GPU implementations. Our FPGA-based kernel has a comparable runtime as GPU, and achieves 2.1$\times$ reduction than CPU. Moreover, our design obtains substantial saving in energy consumption, say, 9.3$\times$ and 5.6$\times$ better than the implementations on CPU and GPU, respectively.},
  keywords  = {energy consumption;field programmable gate arrays;floating point arithmetic;graphics processing units;mathematics computing;matrix multiplication;power aware computing;reconfigurable architectures;software libraries;sparse matrices;storage allocation;vectors;CPU implementations;CPU library;Convey HC-2ex;FPGA-based SpMV acceleration;FPGA-based kernel;GPU implementations;MKL library;SpMV kernel;University of Florida sparse matrix collection;computation parallelism;computational kernel;cuSPARSE library;data access patterns;data locality-aware design;distributed architecture;energy consumption;floating-point performance;hardware constraints;memory accesses;memory allocation;reconfigurable SpMV kernel;reconfigurable sparse matrix-vector multiplication kernel;software libraries;sparse matrix compression;Bandwidth;Hardware;Kernel;Libraries;Memory management;Runtime;Sparse matrices},
}

@InProceedings{Liu2014,
  author    = {Liu, Junyi and Peyril, Helfried and Burg, Andreas and Constantinides, George A.},
  title     = {{FPGA}Implementation of An Interior Point Method For High-speed Model Predictive Control},
  booktitle = {{P}roceedings of the 24th {I}nternational {C}onference on {F}ield {P}rogrammable {L}ogic and {A}pplications},
  date      = {2014},
  series    = {FPL '14},
  location  = {Munich, GER},
  month     = sep,
  pages     = {1--8},
  doi       = {10.1109/FPL.2014.6927473},
  abstract  = {In this paper, we present a hardware architecture for implementing an interior point method for model predictive control (MPC) on field programmable gate arrays (FPGA). The FPGA implementation allows the solution of quadratic programs occurring in MPC at very high speed. Experiments show that our hardware implementation is able to outperform an software implementation running on a high-end CPU while consuming significantly less power making it well-suited for embedded industrial control applications. In contrast to existing FPGA implementations, the proposed solution exploits the MPC-specific problem structure with the direct linear equation solver and uses an efficient predictor-corrector algorithm. Moreover, the modular design of the architecture simplifies customization or extension to special control problem classes. The proposed FPGA solution can broaden the applicability of solving complex or large MPC problems in embedded computing platforms that were so far considered out of reach.},
  keywords  = {control system synthesis;embedded systems;field programmable gate arrays;predictive control;predictor-corrector methods;quadratic programming;velocity control;FPGA implementation;MPC-specific problem structure;direct linear equation solver;embedded computing platforms;embedded industrial control applications;hardware architecture;high-end CPU;high-speed model predictive control;interior point method;modular design;predictor-corrector algorithm;quadratic programs;Computer architecture;Equations;Field programmable gate arrays;Hardware;Optimization;Pipelines;Vectors},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Liu2012,
  author       = {Liu, Li and Liu, Li and Yang, Guangwen},
  title        = {A Higly Efficient {CPU-GPU} Hybrid Parallel Implementation of Sparse {LU} Factorization},
  journaltitle = {Chinese Journal of Electronics},
  date         = {2012},
  volume       = {21},
  number       = {1},
  month        = {1},
  pages        = {7--12},
  url          = {http://www.ejournal.org.cn/Jweb_cje/CN/abstract/abstract916.shtml},
  abstract     = {In this paper, we try to accelerate sparse LU factorization on GPU. We present a tiled storage format and a parallel algorithm to improve the memory access pattern, and a register blocking method to compress the on-chip working set. The OPENMP implementation of our algorithm gives more stable performance over different matrices, and outperforms SuperLU and KLU by 1.88\textasciitilde{}6 times on an Intel 8-core CPU (Central processing unit) for matrices from the Florida matrix collection. Based on this algorithm, we further propose a GPU-CPU hybrid pipelined scheme to overlap computations on CPU with computations on GPU. Compared to the better of SuperLU and KLU on an Intel 8-core CPU, our algorithm achieves 1.1\textasciitilde{}19.7-fold speedup on GPU for double precision. Compared to the OPENMP implementation of our algorithm on an Intel 8-core CPU, our GPU implementation gets a 2-fold speedup for the best cases.},
  annotation   = {{T}he authors present two implementations of the right-looking {LU} decomposition on {O}pen{MP} and {C}+{CUDA}. {F}irst, the matrix is permuted into diagonal blocks, then it is factorised rowwise and columnwise starting from the diagonal element. {F}inally, on all the obtained blocks a right-looking update is performed. {T}he data format used is a modified version of the blocked compressed sparse column ({BCSC}) format, but for each block also a {CSR} version is stored in memory since it is convenient for the columnwise factorisation. {E}ach block is composed of 16$\times$16 entries, thus it can be associated to a {CUDA} thread block while each column or row can be processed by a half warp. {F}urther memory optimisations reduce the amount of used shared memory. {E}xperiments show that the right-looking update is the most consuming algorithmic step, so in order to obtain the best speedup two solutions are proposed. {I}n the first one every step is parallelised on multicore systems by using {O}pen{MP}, while in the second solution only the right-looking update is performed on the {GPU} and everything else is done on the {CPU} host with maximum overlap between the computations. {E}xperimental results show that performance is very sensitive with respect to the matrix in hand, but in general parallelism the performance is far from the theoretical peak flops of the machine.},
  file         = {:home/ap8213/Documents/PhD/Papers/Li et al. - A Highly Efficient GPU-CPU Hybrid Parallel Implementation of Sparse LU Factorization.pdf:PDF},
  owner        = {andrea},
  timestamp    = {2014.11.13},
}

@Article{Low2012,
  author       = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
  title        = {Distributed {GraphLab}: A Framework for Machine Learning and Data Mining in the Cloud},
  journaltitle = {{P}roc. {VLDB} {E}ndow.},
  date         = {2012},
  volume       = {5},
  number       = {8},
  month        = {4},
  pages        = {716--727},
  issn         = {2150-8097},
  doi          = {10.14778/2212351.2212354},
 abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. \\We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
  acmid        = {2212354},
  issue_date   = {April 2012},
  numpages     = {12},
  publisher    = {VLDB Endowment},
}

@Article{Ma2012,
  author       = {Ma, Y. and Borrelli, F. and Hencey, B. and Coffey, B. and Bengea, S. and Haves, P.},
  title        = {Model Predictive Control for the Operation of Building Cooling Systems},
  journaltitle = {{IEEE} Transactions on Control Systems Technology},
  date         = {2012},
  volume       = {20},
  number       = {3},
  month        = may,
  pages        = {796--803},
  doi          = {10.1109/TCST.2011.2124461},
  abstract     = {This brief presents a model-based predictive control (MPC) approach to building cooling systems with thermal energy storage. We focus on buildings equipped with a water tank used for actively storing cold water produced by a series of chillers. First, simplified models of chillers, cooling towers, thermal storage tanks, and buildings are developed and validated for the purpose of model-based control design. Then an MPC for the chilling system operation is proposed to optimally store the thermal energy in the tank by using predictive knowledge of building loads and weather conditions. This brief addresses real-time implementation and feasibility issues of the MPC scheme by using a simplified hybrid model of the system, a periodic robust invariant set as terminal constraints, and a moving window blocking strategy. The controller is experimentally validated at the University of California, Merced. The experiments show a reduction in the central plant electricity cost and an improvement of its efficiency.},
  owner        = {ap8213},
  publisher    = {Institute of Electrical and Electronics Engineers},
  timestamp    = {2014.10.09},
}

@Article{Muller2009,
  author       = {Müller, C. and Frey, S. and Strengert, M. and Dachsbacher, C. and Ertl, T.},
  title        = {A Compute Unified System Architecture for Graphics Clusters Incorporating Data Locality},
  journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
  date         = {2009},
  volume       = {15},
  number       = {4},
  month        = jul,
  pages        = {605--617},
  issn         = {1077--2626},
  doi          = {10.1109/TVCG.2008.188},
  abstract     = {We present a development environment for distributed GPU computing targeted for multi-GPU systems, as well as graphics clusters. Our system is based on CUDA and logically extends its parallel programming model for graphics processors to higher levels of parallelism, namely, the PCI bus and network interconnects. While the extended API mimics the full function set of current graphics hardware-including the concept of global memory-on all distribution layers, the underlying communication mechanisms are handled transparently for the application developer. To allow for high scalability, in particular for network-interconnected environments, we introduce an automatic GPU-accelerated scheduling mechanism that is aware of data locality. This way, the overall amount of transmitted data can be heavily reduced, which leads to better GPU utilization and faster execution. We evaluate the performance and scalability of our system for bus and especially network-level parallelism on typical multi-GPU systems and graphics clusters.},
  comment      = {Scheduling mechanism that is aware of data-locality. Very verbose, little maths.},
}

@Article{Munz2015,
  author       = {Münz, Ulrich and Metzger, Michael and Szabo, Andrei and Reischböck, Markus and Steinke, Florian and Wolfrum, Philipp and Sollacher, Rudolf and Obradovic, Dragan and Buhl, Michael and Lehmann, Thomas and Duckheim, Mathias and Langemeyer, Stefan},
  title        = {Overview of recent control technologies for future power systems: An industry perspective},
  journaltitle = {Automatisierungstechnik},
  date         = {2015},
  issue        = {11},
  month        = nov,
  pages        = {869--882},
  doi          = {10.1515/auto-2015-0047},
  owner        = {andrea},
  timestamp    = {2015.09.28},
}

@InProceedings{Malewicz2010,
  author    = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J. C. and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
  title     = {Pregel: A System for Large-scale Graph Processing},
  booktitle = {{P}roceedings of the 2010 {ACM SIGMOD} {I}nternational {C}onference on {M}anagement of {D}ata},
  date      = {2010},
  series    = {SIGMOD '10},
  publisher = {ACM},
  location  = {Indianapolis, IN, USA},
  isbn      = {978-1-4503-0032-2},
  pages     = {135--146},
  doi       = {10.1145/1807167.1807184},
  abstract  = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
  acmid     = {1807184},
  address   = {New York, NY, USA},
  keywords  = {distributed computing, graph algorigthms},
  numpages  = {12},
}

@Article{Markowitz1957,
  author       = {Markowitz, Harry M.},
  title        = {The Elimination Form of the Inverse and Its Application to Linear Programming},
  journaltitle = {Management Science},
  date         = {1957},
  volume       = {3},
  number       = {3},
  month        = {4},
  pages        = {255--269},
  issn         = {0025-1909},
  doi          = {10.1287/mnsc.3.3.255},
  acmid        = {2769307},
  issue_date   = {April 1957},
  location     = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
  numpages     = {15},
  publisher    = {INFORMS},
}

@Book{Maros2003,
  author    = {Maros, István},
  title     = {Computational Techniques of the Simplex Method},
  date      = {2003},
  editor    = {Springer},
  volume    = {61},
  series    = {International Series in Operations Research \& Management Science},
  publisher = {Kluwer Academic},
  owner     = {andrea},
  timestamp = {2014.03.05},
}

@Misc{Matlab2017a,
  author    = {MATLAB},
  title     = {version 9.2.0.538062 64-bit (R2017a)},
  date      = {2017},
  location  = {Natick, Massachusetts},
  publisher = {The MathWorks Inc.},
}

@Article{Mattingley2012,
  author       = {Mattingley, Jacob and Boyd, Stephen},
  title        = {{CVXGEN}: a code generator for embedded convex optimization},
  journaltitle = {Optimization Engineering},
  date         = {2012},
  language     = {English},
  volume       = {13},
  number       = {1},
  pages        = {1--27},
  issn         = {1389-4420},
  doi          = {10.1007/s11081-011-9176-9},
  abstract     = {CVXGEN is a software tool that takes a high level description of a convex optimization problem family, and automatically generates custom C code that compiles into a reliable, high speed solver for the problem family. The current implementation targets problem families that can be transformed, using disciplined convex programming techniques, to convex quadratic programs of modest size. CVXGEN generates simple, flat, library-free code suitable for embedding in real-time applications. The generated code is almost branch free, and so has highly predictable run-time behavior. The combination of regularization (both static and dynamic) and iterative refinement in the search direction computation yields reliable performance, even with poor quality data. In this paper we describe how CVXGEN is implemented, and give some results on the speed and reliability of the automatically generated solvers.},
  keywords     = {Convex optimization; Code generation; Embedded optimization},
  owner        = {ap8213},
  publisher    = {Springer US},
  timestamp    = {2014.10.09},
}

@Article{Mayer2009,
  author       = {Mayer, Jan},
  title        = {Parallel algorithms for solving linear systems with sparse triangular matrices},
  journaltitle = {Computing},
  date         = {2009},
  volume       = {86},
  number       = {4},
  month        = sep,
  pages        = {291--312},
  issn         = {0010-485X},
  doi          = {10.1007/s00607-009-0066-3},
 abstract = {In this article, we present two new algorithms for solving given triangular systems in parallel on a shared memory architecture. Multilevel incomplete LU factorization based preconditioners, which have been very successful for solving linear systems iteratively, require these triangular solves. Hence, the algorithms presented here can be seen as parallelizing the application of these preconditioners. The first algorithm solves the triangular matrix by block anti-diagonals. The drawback of this approach is that it can be difficult to choose an appropriate block structure. On the other hand, if a good block partition can be found, this algorithm can be quite effective. The second algorithm takes a hybrid approach by solving the triangular system by block columns and anti-diagonals. It is usually as effective as the first algorithm, but the block structure can be chosen in a nearly optimal manner. Although numerical results indicate that the speed-up can be fairly good, systems with matrices having a strong diagonal structure or narrow bandwidth cannot be solved effectively in parallel. Hence, for these matrices, the results are disappointing. On the other hand, the results are better for matrices having a more uniform distribution of non-zero elements. Although not discussed in this article, these algorithms can possibly be adapted for distributed memory architectures.},
  keywords     = {Preconditioning; Iterative methods; Sparse linear systems; Parallelization; 65F10; 65F50; 65Y05},
  owner        = {ap8213},
  publisher    = {Springer Vienna},
  timestamp    = {2015.07.22},
}

@Article{Mehrotra1992,
  author       = {Mehrotra, Sanjay},
  title        = {On the Implementation of a Primal-Dual Interior Point Method},
  journaltitle = {{SIAM} Journal on Optimization},
  date         = {1992},
  volume       = {2},
  number       = {4},
  pages        = {575--601},
  doi          = {10.1137/0802028},
  abstract     = {This paper gives an approach to implementing a second-order primal-dual interior point method. It uses a Taylor polynomial of second order to approximate a primal-dual trajectory. The computations for the second derivative are combined with the computations for the centering direction. Computations in this approach do not require that primal and dual solutions be feasible. Expressions are given to compute all the higher-order derivatives of the trajectory of interest. The implementation ensures that a suitable potential function is reduced by a constant amount at each iteration. \\There are several salient features of this approach. An adaptive heuristic for estimating the centering parameter is given. The approach used to compute the step length is also adaptive. A new practical approach to compute the starting point is given. This approach treats primal and dual problems symmetrically. \\Computational results on a subset of problems available from netlib are given. On mutually tested problems the results show that the proposed method requires approximately 40 percent fewer iterations than the implementation proposed in Lustig, Marsten, and Shanno [Tech. Rep. TR J-89-11, Georgia Inst. of Technology, Atlanta, 1989]. It requires approximately 50 percent fewer iterations than the dual affine scaling method in Adler, Karmarkar, Resende, and Veiga [Math. Programming, 44 (1989), pp. 297-336], and 35 percent fewer iterations than the second-order dual affine scaling method in the same paper. The new approach for estimating the centering parameter and finding the step length and the starting point have contributed to the reduction in the number of iterations. However, the contribution due to the use of second derivative is most significant. \\On the tested problems, on the average the implementation shown was found to be approximately two times faster than OBl (version 02/90) described in Lustig, Marsten, and Shanno and 2.5 times faster than MINOS 5.3 described in Murtagh and Saunders [Tech. Rep. SOL 83-20, Dept. of Operations Research, Stanford Univ., Stanford, CA, 1983].},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@InProceedings{Melab2012,
  author    = {Melab, N. and Chakroun, I. and Mezmaz, M. and Tuyttens, D.},
  title     = {A {GPU}-accelerated Branch-and-Bound Algorithm for the Flow-Shop Scheduling Problem},
  booktitle = {{IEEE} {I}nt. {C}onf. {C}luster {C}omputing},
  date      = {2012},
  series    = {CLUSTER '12},
  location  = {Beijing, CN},
  month     = sep,
  pages     = {10--17},
  doi       = {10.1109/CLUSTER.2012.18},
  abstract  = {Branch-and-Bound (B\&B) algorithms are time-intensive tree-based exploration methods for solving to optimality combinatorial optimization problems. In this paper, we investigate the use of GPU computing as a major complementary way to speed up those methods. The focus is put on the bounding mechanism of B\&B algorithms, which is the most time consuming part of their exploration process. We propose a parallel B\&B algorithm based on a GPU-accelerated bounding model. The proposed approach concentrate on optimizing data access management to further improve the performance of the bounding mechanism which uses large and intermediate data sets that do not completely fit in GPU memory. Extensive experiments of the contribution have been carried out on well-known FSP benchmarks using an Nvidia Tesla C2050 GPU card. We compared the obtained performances to a single and a multithreaded CPU-based execution. Accelerations up to $\times$100 are achieved for large problem instances.},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@Article{Mellor-Crummey2001,
  author       = {Mellor-Crummey, John and Whalley, David and Kennedy, Ken},
  title        = {Improving Memory Hierarchy Performance for Irregular Applications Using Data and Computation Reorderings},
  journaltitle = {International Journal of Parallel Programming},
  date         = {2001},
  volume       = {29},
  number       = {3},
  month        = {6},
  pages        = {217--247},
  issn         = {0885-7458},
  doi          = {10.1023/A:1011119519789},
 abstract = {The performance of irregular applications on modern computer systems is hurt by the wide gap between CPU and memory speeds because these applications typically under-utilize multi-level memory hierarchies, which help hide this gap. This paper investigates using data and computation reorderings to improve memory hierarchy utilization for irregular applications. We evaluate the impact of reordering on data reuse at different levels in the memory hierarchy. We focus on coordinated data and computation reordering based on space-filling curves and we introduce a new architecture-independent multi-level blocking strategy for irregular applications. For two particle codes we studied, the most effective reorderings reduced overall execution time by a factor of two and four, respectively. Preliminary experience with a scatter benchmark derived from a large unstructured mesh application showed that careful data and computation ordering reduced primary cache misses by a factor of two compared to a random ordering.},
  acmid        = {608774},
  issue_date   = {June 2001},
  keywords     = {computation reordering, data reordering, memory hierarchy optimization, multi-level blocking, space-filling curves},
  location     = {Norwell, MA, USA},
  numpages     = {31},
  publisher    = {Kluwer Academic Publishers},
}

@Article{Merrill2012,
  author       = {Merrill, Duane and Garland, Michael and Grimshaw, Andrew},
  title        = {Scalable {GPU} Graph Traversal},
  journaltitle = {{ACM SIGPLAN} Notices},
  date         = {2012},
  volume       = {47},
  number       = {8},
  month        = feb,
  pages        = {117--128},
  issn         = {0362-1340},
  doi          = {10.1145/2370036.2145832},
 abstract = {Breadth-first search (BFS) is a core primitive for graph traversal and a basis for many higher-level graph analysis algorithms. It is also representative of a class of parallel computations whose memory accesses and work distribution are both irregular and data-dependent. Recent work has demonstrated the plausibility of GPU sparse graph traversal, but has tended to focus on asymptotically inefficient algorithms that perform poorly on graphs with non-trivial diameter. \\We present a BFS parallelization focused on fine-grained task management constructed from efficient prefix sum that achieves an asymptotically optimal $O(|V|+|E|)$ work complexity. Our implementation delivers excellent performance on diverse graphs, achieving traversal rates in excess of 3.3 billion and 8.3 billion traversed edges per second using single and quad-GPU configurations, respectively. This level of performance is several times faster than state-of-the-art implementations both CPU and GPU platforms.},
  acmid        = {2145832},
  issue_date   = {August 2012},
  keywords     = {GPU, breadth-first search, cooperative allocation, graph algorithms, graph traversal, parallel algorithms, prefix sum, sparse graph},
  location     = {New York, NY, USA},
  numpages     = {12},
  publisher    = {ACM},
}

@InProceedings{Meyer2011,
  author    = {Meyer, Xavier and Albuquerque, Paul and Chopard, Bastien},
  title     = {A multi-{GPU} implementation and performance model for the standard simplex method},
  booktitle = {{P}roceedings of the 1st {I}nternational {S}ymposium and 10th {B}alkan {C}onference on {O}perational {R}esearch},
  date      = {2011},
  series    = {BALCOR '11},
  location  = {Thessaloniki, GR},
  month     = sep,
  pages     = {312--319},
  url       = {http://spc.unige.ch/lib/exe/fetch.php?media=pub:sgpu_europar2011.pdf},
  abstract  = {The standard simplex method is a well-known optimization algorithm for solving linear programming models in the field of operational research. It is part of software that is often employed by businesses for solving scheduling or assignment problems. But their always increasing complexity and size drives the demand for more computational power.In the past few years, GPUs have gained a lot of popularity as they offer an opportunity to accelerate many algorithms. In this paper we present a mono and a multi-GPU implementation of the standard simplex method, which is based on CUDA. Measurements show that it outperforms the CLP solver provided the problem size is large enough. We also derive a performance model and establish its accurateness. To our knowledge, only the revised simplex method has so far been implemented on a GPU.},
  file      = {:home/andrea/Dropbox/PhD/Papers/Meyer et al. - A multi-GPU implementation and performance model for the standard simplex method.pdf:PDF},
  owner     = {ap8213},
  timestamp = {2014.03.27},
}

@InBook{Meyer2013,
  author    = {Meyer, Xavier and Chopard, Bastien and Albuquerque, Paul},
  title     = {Linear programming on a {GPU}: a case study},
  booktitle = {{D}esigning {S}cientific {A}pplications on {GPU}s},
  date      = {2013},
  editor    = {Raphaël Couturier},
  series    = {Numerical Analysis and Scientific Computing},
  publisher = {Chapman \& Hall/CRC Press},
  isbn      = {9781466571624},
  chapter   = {10},
  pages     = {215--249},
  file      = {:home/andrea/Dropbox/PhD/Papers/Meyer et al. - Linear programming on GPUs A case of study - 2013.pdf:PDF},
  owner     = {ap8213},
  timestamp = {2014.03.27},
}

@Book{Mitchell1998,
  author    = {Mitchell, M.},
  title     = {An introduction to genetic algorithms},
  date      = {1998},
  publisher = {The {MIT} Press},
  owner     = {ap8213},
  timestamp = {2014.03.25},
}

@InProceedings{Mocanu2013,
  author    = {Mocanu, Adrian and Ţăpuş, Nicolae},
  title     = {Sparse matrix permutations to a Block Triangular Form in a distributed environment},
  booktitle = {{IEEE} {I}nternational {C}onference on {I}ntelligent {C}omputer {C}ommunication and {P}rocessing},
  date      = {2013},
  series    = {ICCP '13},
  location  = {Lyon, FR},
  month     = sep,
  pages     = {331--338},
  doi       = {10.1109/ICCP.2013.6646131},
  abstract  = {Arranging the sparse circuit matrix into a diagonal block upper triangular form is the first step of the KLU algorithm. This paper presents the two steps of the parallel algorithm, running in a distributed environment, that performs unsymmetric and symmetric permutations of the matrix's rows. First, using the [Duff] maximum transversal algorithm and performing asymmetrical permutations, the matrix is shaped to achieve a zero free diagonal. Then, searching the strongly connected components of the associated matrix's graph, and performing symmetric permutation, the sparse matrix is shaped in a diagonal block upper triangular form. Both algorithm and architecture are presented.},
  keywords  = {graph theory;parallel algorithms;sparse matrices;Duff maximum transversal algorithm;KLU algorithm;associated matrix graph;asymmetrical permutation;diagonal block upper triangular form;distributed environment;parallel algorithm;sparse circuit matrix permutation;symmetric permutation;unsymmetric permutation;Algorithm design and analysis;Bipartite graph;Computer architecture;Mathematical model;Matrix converters;Sparse matrices;Symmetric matrices},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Monakov2010,
  author       = {Monakov, Alexander and Lokhmotov, Anton and Avetisyan, Arutyun},
  title        = {Automatically Tuning Sparse Matrix-Vector Multiplication for {GPU} Architectures},
  booktitle    = {{H}igh {P}erformance {E}mbedded {A}rchitectures and {C}ompilers},
  date         = {2010},
  editor       = {Patt, Yale N. and Foglia, Pierfrancesco and Duesterwald, Evelyn and Faraboschi, Paolo and Martorell, Xavier},
  booksubtitle = {HiPEAC '10},
  series       = {Lecture Notes in Computer Science},
  publisher    = {Springer Berlin Heidelberg},
  location     = {Pisa, IT},
  isbn         = {978-3-642-11515-8},
  pages        = {111--125},
  doi          = {10.1007/978-3-642-11515-8_10},
  abstract     = {Graphics processors are increasingly used in scientific applications due to their high computational power, which comes from hardware with multiple-level parallelism and memory hierarchy. Sparse matrix computations frequently arise in scientific applications, for example, when solving PDEs on unstructured grids. However, traditional sparse matrix algorithms are difficult to efficiently parallelize for GPUs due to irregular patterns of memory references. In this paper we present a new storage format for sparse matrices that better employs locality, has low memory footprint and enables automatic specialization for various matrices and future devices via parameter tuning. Experimental evaluation demonstrates significant speedups compared to previously published results.},
  address      = {Berlin, Heidelberg},
}

@Online{Mujtaba2015,
  author       = {Mujtaba, Hassan},
  title        = {{NVIDIA} Pascal {GPU}'s Double Precision Performance Rated at Over 4 {TFLOPs}, 16nm {FinFET} Architecture Confirmed -- Volta {GPU} Peaks at Over 7 {TFLOPs}, 1.2 {TB/s} {HBM2}},
  date         = {2015},
  url          = {http://wccftech.com/nvidia-pascal-volta-gpus-sc15/},
  note         = {Accessed 20 July 2016},
  month        = nov,
  journaltitle = {WCCFtech.com},
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@Online{Mujtaba2016,
  author       = {Mujtaba, Hassan},
  title        = {{NVIDIA} Pascal {GP100} {GPU} Expected To Feature 12 {TFLOPs} of Single Precision Compute, 4 {TFLOPs} of Double Precision Compute Performance},
  date         = {2016},
  url          = {http://wccftech.com/nvidia-pascal-gp100-gpu-compute-performance/},
  note         = {Accessed 20 July 2016},
  month        = feb,
  journaltitle = {WCCFtech.com},
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@InProceedings{Mukherjee1995,
  Title                    = {Efficient Support for Irregular Applications on Distributed-memory Machines},
  Author                   = {Mukherjee, Shubhendu S. and Sharma, Shamik D. and Hill, Mark D. and Larus, James R. and Rogers, Anne and Saltz, Joel},
  Booktitle                = {{P}roceedings of the {F}ifth {ACM SIGPLAN} {S}ymposium on {P}rinciples and {P}ractice of {P}arallel {P}rogramming},

  Address                  = {New York, NY, USA},
  Pages                    = {68--79},
  Publisher                = {ACM},
  Series                   = {PPoPP '95},

 abstract = {Irregular computation problems underlie many important scientific applications. Although these problems are computationally expensive, and so would seem appropriate for parallel machines, their irregular and unpredictable run-time behavior makes this type of parallel program difficult to write and adversely affects run-time performance.This paper explores three issues partitioning, mutual exclusion, and data transfer crucial to the efficient execution of irregular problems on distributed-memory machines. Unlike previous work, we studied the same programs running in three alternative systems on the same hardware base (a Thinking Machines CM-5): the CHAOS irregular application library, Transparent Shared Memory (TSM), and eXtensible Shared Memory (XSM). CHAOS and XSM performed equivalently for all three applications. Both systems were somewhat (13\%) to significantly faster (991\%) than TSM.},
  Acmid                    = {209945},
  Date                     = {1995},
  Doi                      = {10.1145/209936.209945},
  ISBN                     = {0-89791-700-6},
  Location                 = {Santa Barbara, CA, USA},
  Numpages                 = {12}
}

@InProceedings{Munawar2011,
  author    = {Munawar, Asim and Wahib, Mohamed and Munetomo, Mahasaru and Akama, Kiyoshi},
  title     = {Advanced Genetic Algorithm to solve {MINLP} problems over {GPU}},
  booktitle = {{P}roceedings of the {IEEE} {C}ongress on {E}volutionary {C}omputation},
  date      = {2011},
  series    = {CEC '11},
  location  = {New Orleans, LA, USA},
  month     = {6},
  pages     = {318--325},
  doi       = {10.1109/CEC.2011.5949635},
  abstract  = {In this paper we propose a many-core implementation of evolutionary computation for GPGPU (General-Purpose Graphic Processing Unit) to solve non-convex Mixed Integer Non-Linear Programming (MINLP) and non-convex Non Linear Programming (NLP) problems using a stochastic algorithm. Stochastic algorithms being random in their behavior are difficult to implement over GPU like architectures. In this paper we not only succeed in implementation of a stochastic algorithm over GPU but show considerable speedups over CPU implementations. The stochastic algorithm considered for this paper is an adaptive resolution approach to genetic algorithm (arGA), developed by the authors of this paper. The technique uses the entropy measure of each variable to adjust the intensity of the genetic search around promising individuals. Performance is further improved by hybridization with adaptive resolution local search (arLS) operator. In this paper, we describe the challenges and design choices involved in parallelization of this algorithm to solve complex MINLPs over a commodity GPU using Compute Unified Device Architecture (CUDA) programming model. Results section shows several numerical tests and performance measurements obtained by running the algorithm over an nVidia Fermi GPU. We show that for difficult problems we can obtain a speedup of up to 20$\times$ with double precision and up to 42$\times$ with single precision.},
  keywords  = {computer graphic equipment;coprocessors;genetic algorithms;integer programming;nonlinear programming;parallel algorithms;parallel architectures;stochastic processes;CPU implementations;GPGPU;GPU like architectures;MINLP problems;adaptive resolution local search operator;advanced genetic algorithm;compute unified device architecture programming model;evolutionary computation;general-purpose graphic processing unit;many-core implementation;nVidia Fermi GPU;nonconvex mixed integer nonlinear programming;parallelization;stochastic algorithm;Algorithm design and analysis;Entropy;Genetic algorithms;Genetics;Graphics processing unit;Kernel;Stochastic processes;Adaptive Resolution Genetic Algorithm;Compute Unified Device Architecture (CUDA);General-Purpose computation on Graphics Processing Units (GPGPU);Parallel Genetic Algorithms},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@InProceedings{Nasre2013,
  author    = {Nasre, Rupesh and Burtscher, Martin and Pingali, Keshav},
  title     = {Atomic-free Irregular Computations on {GPUs}},
  booktitle = {{P}roceedings of the 6th {W}orkshop on {G}eneral {P}urpose {P}rocessor {U}sing {G}raphics {P}rocessing {U}nits},
  date      = {2013},
  series    = {GPGPU-6},
  publisher = {ACM},
  location  = {Houston, Texas, USA},
  isbn      = {978-1-4503-2017-7},
  pages     = {96--107},
  doi       = {10.1145/2458523.2458533},
 abstract = {Atomic instructions are a key ingredient of codes that operate on irregular data structures like trees and graphs. It is well known that atomics can be expensive, especially on massively parallel GPUs, and are often on the critical path of a program. In this paper, we present two high-level methods to eliminate atomics in irregular programs. The first method advocates synchronous processing using barriers. We illustrate how to exploit synchronous processing to avoid atomics even when the threads' memory accesses conflict with each other. The second method is based on exploiting algebraic properties of algorithms to elide atomics. Specifically, we focus on three key properties: monotonicity, idempotency and associativity, and show how each of them enables an atomic-free implementation. We illustrate the generality of the two methods by applying them to five irregular graph applications: breadth-first search, single-source shortest paths computation, Delaunay mesh refinement, pointer analysis and survey propagation, and show that both methods provide substantial speedup in each case on different GPUs.},
  acmid     = {2458533},
  address   = {New York, NY, USA},
  comment   = {Comparison between the use of barriers and atomics on GPUs. For atomics, the use of "algebraic properties" is discussed.},
  keywords  = {GPGPU, atomic-free, graph algorithms, irregular algorithms},
  numpages  = {12},
}

@InProceedings{Nasre2013a,
  author    = {Nasre, Rupesh and Burtscher, Martin and Pingali, Keshav},
  title     = {Data-Driven Versus Topology-driven Irregular Computations on {GPUs}},
  booktitle = {{P}roceedings of the 27th {IEEE} {I}nternational {S}ymposium on {P}arallel and {D}istributed {P}rocessing},
  date      = {2013},
  series    = {IPDPS '13},
  publisher = {{IEEE} Computer Society},
  location  = {Boston, MA, US},
  isbn      = {978-0-7695-4971-2},
  pages     = {463--474},
  doi       = {10.1109/IPDPS.2013.28},
 abstract = {Irregular algorithms are algorithms with complex main data structures such as directed and undirected graphs, trees, etc. A useful abstraction for many irregular algorithms is its operator formulation in which the algorithm is viewed as the iterated application of an operator to certain nodes, called active nodes, in the graph. Each operator application, called an activity, usually touches only a small part of the overall graph, so nonoverlapping activities can be performed in parallel. In topology-driven implementations, all nodes are assumed to be active so the operator is applied everywhere in the graph even if there is no work to do at some nodes. In contrast, in data-driven implementations the operator is applied only to nodes at which there might be work to do. Multicore implementations of irregular algorithms are usually data-driven because current multicores only support small numbers of threads and work-efficiency is important. Conversely, many irregular GPU implementations use a topology-driven approach because work inefficiency can be counterbalanced by the large number of GPU threads. In this paper, we study data-driven and topology-driven implementations of six important graph algorithms on GPUs. Our goal is to understand the tradeoffs between these implementations and how to optimize them. We find that data-driven versions are generally faster and scale better despite the cost of maintaining a worklist. However, topology-driven versions can be superior when certain algorithmic properties are exploited to optimize the implementation. These results led us to devise hybrid approaches that combine the two techniques and outperform both of them.},
  acmid     = {2511363},
  address   = {Washington, DC, USA},
  comment   = {Using Pingali's work on "The Tao of Parallelism in Algorithms", shows that there can be topolgy-driven and data-driven implementations.},
  keywords  = {irregular algorithms, data-driven, topology-driven, algorithmic properties, GPGPU},
  numpages  = {12},
}

@TechReport{Naumov2011,
  author       = {Naumov, Maxim},
  title        = {Parallel Solution of Sparse Triangular Linear Systems in the Preconditioned Iterative Methods on the {GPU}},
  institution  = {NVIDIA Corporation},
  date         = {2011},
  type         = {Technical report},
  number       = {2011--001},
  month        = {6},
  url          = {http://research.nvidia.com/sites/default/files/publications/nvr-2011-001.pdf},
  abstract     = {A novel algorithm for solving in parallel a sparse triangular linear system on a graphical processing unit is proposed. It implements the solution of the triangular system in two phases. First, the analysis phase builds a dependency graph based on the matrix sparsity pattern and groups the independent rows into levels. Second, the solve phase obtains the full solution by iterating sequentially across the constructed levels. The solution elements corresponding to each single level are obtained at once in parallel. The numerical experiments are also presented and it is shown that the incomplete-LU and Cholesky preconditioned iterative methods, using the parallel sparse triangular solve algorithm, can achieve on average more than 2$\times$ speedup on graphical processing units (GPUs) over their CPU implementation.},
  organization = {NVIDIA},
  owner        = {andrea},
  timestamp    = {2014.07.27},
}

@InProceedings{Nelson2014,
  author    = {Nelson, Jacob and Holt, Brandon and Myers, Brandon and Briggs, Preston and Ceze, Luis and Kahan, Simon and Oskin, Mark},
  title     = {Grappa: A Latency-Tolerant Runtime for Large-Scale Irregular Applications},
  booktitle = {{P}roceedings of the {I}nternational {W}orkshop on {R}ack-{S}cale {C}omputing},
  date      = {2014},
  series    = {WRSC '14},
  location  = {Amsterdam, NL},
  month     = {4},
  url       = {ftp://trout.cs.washington.edu/tr/2014/02/UW-CSE-14-02-01.PDF},
  abstract  = {Grappa is a runtime system for commodity clusters of multicore computers that presents a massively parallel, single address space abstraction to applications. Grappa's purpose is to enable scalable performance of irregular parallel applications, such as branch and bound optimization, SPICE circuit simulation, and graph processing. Poor data locality, imbalanced parallel work and complex communication patterns make scaling these applications difficult. \\Grappa serves both as a C++ user library and as a foundation for higher level languages. Grappa tolerates delays to remote memory by multiplexing thousands of lightweight workers to each processor core, balances load via fine-grained distributed work-stealing, increases communication throughput by aggregating smaller data requests into large ones, and provides efficient synchronization and remote operations. We present a detailed description of the Grappa system and performance comparisons on several irregular benchmarks to hand-optimized MPI code and to the Cray XMT, a custom system used to target the real-time graph-analytics market. We find Grappa to be 9$\times$ faster than MPI on a random access microbenchmark, between 3.5$\times$ and 5.4$\times$ slower than MPI on applications, and between 2.6$\times$ faster and 4.4$\times$ slower than the XMT.},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Neves2017,
  author       = {Neves, Diogo Telmo},
  title        = {{EPIC}: A framework to exploit parallelism in irregular codes},
  journaltitle = {Concurrency and Computation - Practice \& Experience},
  date         = {2017},
  language     = {{English}},
  volume       = {29},
  number       = {2},
  month        = {1},
  issn         = {{1532-0626}},
  doi          = {10.1002/cpe.3842},
  abstract     = {To harness the performance potential of current multicore processors, a multitude of algorithms, frameworks and libraries have been developed. Nevertheless, it is still extremely difficult to take advantage of the full potential of multicore processors. Moreover, when using third-party tools and/or in the presence of asymmetric sets of tasks, this problem would only aggravate. The EPIC framework was developed to ease the exploitation of task parallelism in irregular applications that use third-party tools and/or generate asymmetric sets of tasks. It is based on a software design and implements two algorithms that, together, allow, in a seamlessly way, the efficient exploitation of coarse-grained parallelism, fine-grained parallelism, and the combination of both of these types. Thus, it becomes possible to make a better and transparent usage of the performance potential of current multicore processors on shared-memory systems. In this paper, we present two refinements to the EPIC framework: one that refines the software design of the EPIC framework and another that refines the scheduling algorithm of the EPIC framework. Together, these refinements allow to cope with a special class of sets of tasks: sets of tasks where asymmetry is insignificant or can be neglected. Thus, these refinements broaden the applicability of the EPIC framework to a large class of irregular applications where task parallelism can be exploited. To assess the feasibility and the benefit of using this new version of the EPIC framework to exploit task parallelism, we used four real-world irregular applications three from phylogenetics and another from astrophysics and several input data sets with different characteristics. Our studies show groundbreaking results in terms of the achieved speedups and that scalability is not impaired, even when using third-party tools and/or in the presence of (a)symmetric sets of tasks.},
  location     = {{111 RIVER ST, HOBOKEN 07031-5774, NJ USA}},
  owner        = {andrea},
  publisher    = {{WILEY-BLACKWELL}},
  timestamp    = {2017.05.08},
  type         = {{Article}},
}

@InProceedings{Nguyen2013,
  Title                    = {A Lightweight Infrastructure for Graph Analytics},
  Author                   = {Nguyen, Donald and Lenharth, Andrew and Pingali, Keshav},
  Booktitle                = {{P}roceedings of the 24th {ACM} {S}ymposium on {O}perating {S}ystems {P}rinciples},

  Address                  = {New York, NY, USA},
  Pages                    = {456--471},
  Publisher                = {ACM},
  Series                   = {SOSP '13},

 abstract = {Several domain-specific languages (DSLs) for parallel graph analytics have been proposed recently. In this paper, we argue that existing DSLs can be implemented on top of a general-purpose infrastructure that (i) supports very fine-grain tasks, (ii) implements autonomous, speculative execution of these tasks, and (iii) allows application-specific control of task scheduling policies. To support this claim, we describe such an implementation called the Galois system. \\We demonstrate the capabilities of this infrastructure in three ways. First, we implement more sophisticated algorithms for some of the graph analytics problems tackled by previous DSLs and show that end-to-end performance can be improved by orders of magnitude even on power-law graphs, thanks to the better algorithms facilitated by a more general programming model. Second, we show that, even when an algorithm can be expressed in existing DSLs, the implementation of that algorithm in the more general system can be orders of magnitude faster when the input graphs are road networks and similar graphs with high diameter, thanks to more sophisticated scheduling. Third, we implement the APIs of three existing graph DSLs on top of the common infrastructure in a few hundred lines of code and show that even for power-law graphs, the performance of the resulting implementations often exceeds that of the original DSL systems, thanks to the lightweight infrastructure.},
  Acmid                    = {2522739},
  Date                     = {2013},
  Doi                      = {10.1145/2517349.2522739},
  ISBN                     = {978-1-4503-2388-8},
  Location                 = {Farminton, PA, USA},
  Numpages                 = {16}
}

@Book{Nocedal2006,
  author      = {Nocedal, J. and Wright, S. J.},
  title       = {Numerical Optimization},
  date        = {2006},
  edition     = {2nd},
  publisher   = {Springer},
  location    = {New York},
  ad_theotech = {General},
  owner       = {ap8213},
  remark      = {Section 8.2 gives a short introduction to automatic differentiation},
  timestamp   = {2014.10.09},
}

@Article{Notay2000,
  author       = {Notay, Yvan},
  title        = {Flexible Conjugate Gradients},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2000},
  volume       = {22},
  number       = {4},
  pages        = {1444--1460},
  doi          = {10.1137/S1064827599362314},
  abstract     = {We analyze the conjugate gradient (CG) method with preconditioning slightly variable from one iteration to the next. To maintain the optimal convergence properties, we consider a variant proposed by Axelsson that performs an explicit orthogonalization of the search directions vectors. For this method, which we refer to as flexible CG, we develop a theoretical analysis that shows that the convergence rate is essentially independent of the variations in the preconditioner as long as the latter are kept sufficiently small. We further discuss the real convergence rate on the basis of some heuristic arguments supported by numerical experiments. Depending on the eigenvalue distribution corresponding to the fixed reference preconditioner, several situations have to be distinguished. In some cases, the convergence is as fast with truncated versions of the algorithm or even with the standard CG method, whereas quite large variations are allowed without too much penalty. In other cases, the flexible variant effectively outperforms the standard method, while the need for truncation limits the size of the variations that can be reasonably allowed.},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Manual{Osier1993,
  Title                    = {{GNU} gprof},
  Author                   = {Osier, J.},
  Month                    = {1},
  Organization             = {Free Software Foundation, Inc.},

  Date                     = {1993},
  Owner                    = {ap8213},
  Timestamp                = {2014.03.04},
  Url                      = {https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html_chapter/gprof_1.html}
}

@Book{Papadimitriou1982,
  author    = {Papadimitriou, Christos H. and Steiglitz, Kenneth},
  title     = {Combinatorial Optimization: Algorithms and Complexity},
  date      = {1982},
  publisher = {Prentice-Hall, Inc.},
  location  = {Upper Saddle River, NJ, USA},
  isbn      = {0-13-152462-3},
  owner     = {ap8213},
  timestamp = {2015.10.06},
}

@Article{Pedram2014,
  author       = {Pedram, Ardavan and Gerstlauer, Andreas and van de Geijn, Robert A.},
  title        = {Algorithm, Architecture, and Floating-Point Unit Codesign of a Matrix Factorization Accelerator},
  journaltitle = {{IEEE} Transactions on Computers},
  date         = {2014},
  volume       = {63},
  number       = {8},
  issn         = {0018-9340},
  doi          = {10.1109/TC.2014.2315627},
  abstract     = {This paper examines the mapping of algorithms encountered when solving dense linear systems and linear least-squares problems to a custom Linear Algebra Processor. Specifically, the focus is on Cholesky, LU (with partial pivoting), and QR factorizations and their blocked algorithms. As part of the study, we expose the benefits of redesigning floating point units and their surrounding data-paths to support these complicated operations. We show how adding moderate complexity to the architecture greatly alleviates complexities in the algorithm. We study design tradeoffs and the effectiveness of architectural modifications to demonstrate that we can improve power and performance efficiency to a level that can otherwise only be expected of full-custom ASIC designs. A feasibility study of inner kernels is extended to blocked level and shows that, at block level, the Linear Algebra Core (LAC) can achieve high efficiencies with up to 45 GFLOPS/W for both Cholesky and LU factorization, and over 35 GFLOPS/W for QR factorization. While maintaining such efficiencies, our extensions to the MAC units can achieve up to 10, 12, and 20 percent speedup for the blocked algorithms of Cholesky, LU, and QR factorization, respectively.},
  location     = {Los Alamitos, CA, USA},
  owner        = {andrea},
  publisher    = {IEEE Computer Society},
  timestamp    = {2017.05.08},
}

@Online{Picciau2016a,
  author = {Picciau, Andrea},
  title  = {Smart building optimisation problems},
  date   = {2016},
  url    = {https://github.com/andpic/smart-building-optimisation},
  note   = {Git repository, Accessed 10 May 2017},
}

@Article{Pingali2011,
  author       = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and Méndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
  title        = {The Tao of Parallelism in Algorithms},
  journaltitle = {{ACM} {SIGPLAN} Notices},
  date         = {2011},
  volume       = {46},
  number       = {6},
  month        = {6},
  pages        = {12--25},
  issn         = {0362-1340},
  doi          = {10.1145/1993316.1993501},
 abstract = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets. \\To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context. \\These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
  acmid        = {1993501},
  issue_date   = {June 2011},
  keywords     = {amorphous data-parallelism, galois system, irregular programs, operator formulation, tao-analysis},
  location     = {New York, NY, USA},
  numpages     = {14},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2017.05.02},
}

@Article{Ploskas2013,
  author       = {Ploskas, Nikolaos and Samaras, Nikolaos},
  title        = {A Computational Comparison of Basis Updating Schemes for the Simplex Algorithm on a {CPU}-{GPU} System},
  journaltitle = {American Journal of Operations Research},
  date         = {2013},
  volume       = {3},
  number       = {6},
  month        = nov,
  pages        = {497--505},
  doi          = {10.4236/ajor.2013.36048},
  abstract     = {The computation of the basis inverse is the most time-consuming step in simplex type algorithms. This inverse does not have to be computed from scratch at any iteration, but updating schemes can be applied to accelerate this calculation. In this paper, we perform a computational comparison in which the basis inverse is computed with five different updating schemes. Then, we propose a parallel implementation of two updating schemes on a CPU-GPU System using MATLAB and CUDA environment. Finally, a computational study on randomly generated full dense linear programs is presented to establish the practical value of GPU-based implementation.},
  owner        = {andrea},
  timestamp    = {2014.12.13},
}

@Manual{Poulson2015,
  author    = {Poulson, Jack},
  title     = {Elemental Manual},
  date      = {2015},
  version   = {0.84},
  url       = {http://libelemental.org/documentation/elem-0.85.pdf},
  owner     = {andrea},
  timestamp = {2017.05.10},
}

@InProceedings{Prountzos2015,
  Title                    = {Synthesizing Parallel Graph Programs via Automated Planning},
  Author                   = {Prountzos, Dimitrios and Manevich, Roman and Pingali, Keshav},
  Booktitle                = {{P}roceedings of the 36th {ACM SIGPLAN} {C}onference on {P}rogramming {L}anguage {D}esign and {I}mplementation},

  Address                  = {New York, NY, USA},
  Pages                    = {533--544},
  Publisher                = {ACM},
  Series                   = {PLDI '15},

 abstract = {We describe a system that uses automated planning to synthesize correct and efficient parallel graph programs from high-level algorithmic specifications. Automated planning allows us to use constraints to declaratively encode program transformations such as scheduling, implementation selection, and insertion of synchronization. Each plan emitted by the planner satisfies all constraints simultaneously, and corresponds to a composition of these transformations. In this way, we obtain an integrated compilation approach for a very challenging problem domain. We have used this system to synthesize parallel programs for four graph problems: triangle counting, maximal independent set computation, preflow-push maxflow, and connected components. Experiments on a variety of inputs show that the synthesized implementations perform competitively with hand-written, highly-tuned code.},
  Acmid                    = {2737953},
  Date                     = {2015},
  Doi                      = {10.1145/2737924.2737953},
  ISBN                     = {978-1-4503-3468-6},
  Keywords                 = {Amorphous Data-parallelism, Compiler Optimization, Concurrency, Irregular Programs, Parallelism, Synthesis},
  Location                 = {Portland, OR, USA},
  Numpages                 = {12}
}

@InProceedings{Reguly2012,
  author    = {Reguly, I. and Giles, M.},
  title     = {Efficient sparse matrix-vector multiplication on cache-based {GPUs}},
  booktitle = {{P}roceedings of {I}nnovative {P}arallel {C}omputing},
  date      = {2012},
  series    = {InPar '12},
  location  = {San Jose, CA, USA},
  month     = may,
  pages     = {1--12},
  doi       = {10.1109/InPar.2012.6339602},
  abstract  = {Sparse matrix-vector multiplication is an integral part of many scientific algorithms. Several studies have shown that it is a bandwidth-limited operation on current hardware. On cache-based architectures the main factors that influence performance are spatial locality in accessing the matrix, and temporal locality in re-using the elements of the vector. This paper discusses efficient implementations of sparse matrix-vector multiplication on NVIDIA's Fermi architecture, the first to introduce conventional L1 caches to GPUs. We focus on the compressed sparse row (CSR) format for developing general purpose code. We present a parametrised algorithm, show the effects of parameter tuning on performance and introduce a method for determining the nearoptimal set of parameters that incurs virtually no overhead. On a set of sparse matrices from the University of Florida Sparse Matrix Collection we show an average speed-up of 2.1 times over NVIDIA's CUSPARSE 4.0 library in single precision and 1.4 times in double precision. Many algorithms require repeated evaluation of sparse matrix-vector products with the same matrix, so we introduce a dynamic run-time auto-tuning system which improves performance by 10-15\% in seven iterations. The CSR format is compared to alternative ELLPACK and HYB formats and the cost of conversion is assessed using CUSPARSE. Sparse matrix-vector multiplication performance is also analysed when solving a finite element problem with the conjugate gradient method. We show how problemspecific knowledge can be used to improve performance by up to a factor of two.},
  keywords  = {cache storage;conjugate gradient methods;finite element analysis;graphics processing units;mathematics computing;matrix multiplication;sparse matrices;vectors;CSR format;CUSPARSE;ELLPACK formats;HYB formats;L1 caches;NVIDIA CUSPARSE 4.0 library;NVIDIA Fermi architecture;University of Florida sparse matrix collection;bandwidth-limited operation;cache-based GPU;cache-based architectures;compressed sparse row format;conjugate gradient method;dynamic run-time autotuning system;finite element problem;general purpose code;sparse matrix-vector multiplication;spatial locality;temporal locality;Algorithm design and analysis;Graphics processing unit;Heuristic algorithms;Instruction sets;Memory management;Sparse matrices;Vectors;autotuning;cache performance;conjugate gradient method;finite element method;sparse matrix-vector multiplication},
}

@InProceedings{Ren2012,
  author    = {Ren, Ling and Chen, Xiaoming and Wang, Yu and Zhang, Chenxi and Yang, Huazhong},
  title     = {Sparse {LU} Factorization for Parallel Circuit Simulation on {GPU}},
  booktitle = {{P}roceedings of the 49th {ACM}/{EDAC}/{IEEE} {D}esign {A}utomation {C}onference},
  date      = {2012},
  series    = {DAC '12},
  location  = {San Francisco, CA, US},
  month     = {6},
  isbn      = {978-1-4503-1199-1},
  pages     = {1125--1130},
  url       = {http://ieeexplore.ieee.org/document/6241646/},
  abstract  = {Sparse solver has become the bottleneck of SPICE simulators. There has been few work on GPU-based sparse solver because of the high data-dependency. The strong data-dependency determines that parallel sparse LU factorization runs efficiently on shared-memory computing devices. But the number of CPU cores sharing the same memory is often limited. The state of the art Graphic Processing Units (GPU) naturally have numerous cores sharing the device memory, and provide a possible solution to the problem. In this paper, we propose a GPU-based sparse LU solver for circuit simulation. We optimize the work partitioning, the number of active thread groups, and the memory access pattern, based on GPU architecture. On matrices whose factorization involves many floating-point operations, our GPU-based sparse LU factorization achieves 7.90$\times$ speedup over 1-core CPU and 1.49$\times$ speedup over 8-core CPU. We also analyze the scalability of parallel sparse LU factorization and investigate the specifications on CPUs and GPUs that most influence the performance.},
  issn      = {0738-100X},
  keywords  = {SPICE;circuit simulation;graphics processing units;shared memory systems;CPU cores;GPU;SPICE simulators;data-dependency;graphic processing units;parallel circuit simulation;parallel sparse LU factorization;shared-memory computing devices;sparse solver;Bandwidth;Graphics processing unit;Instruction sets;Parallel processing;Sorting;Sparse matrices;Vectors;Circuit Simulation;GPU;Parallel Sparse LU Factorization},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Book{Rodriguez2008,
  author    = {Rodriguez, Giuseppe},
  title     = {Algoritmi numerici},
  date      = {2008},
  publisher = {Pitagora Editrice},
  isbn      = {88-371-1714-0},
  pages     = {117},
  owner     = {andrea},
  timestamp = {2016.10.13},
}

@Online{Rupp2014,
  author    = {Rupp, Karl},
  title     = {{CPU}, {GPU} and {MIC} Hardware Characteristics over Time},
  date      = {2014},
  url       = {https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/},
  note      = {Accessed 21 July 2016},
  month     = mar,
  owner     = {andrea},
  timestamp = {2016.11.07},
}

@Article{Rupp2016,
  author       = {Rupp, Karl and Tillet, Philippe and Rudolf, Florian and Weinbub, Josef and Morhammer, Andreas and Grasser, Tibor and Jüngel, Ansgar and Selberherr, Siegfried},
  title        = {{ViennaCL} - Linear Algebra Library for Multi- and Many-Core Architectures},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2016},
  volume       = {38},
  number       = {5},
  pages        = {S412--S439},
  doi          = {10.1137/15M1026419},
  abstract     = {CUDA, OpenCL, and OpenMP are popular programming models for the multicore architectures of CPUs and many-core architectures of GPUs or Xeon Phis. At the same time, computational scientists face the question of which programming model to use to obtain their scientific results. We present the linear algebra library ViennaCL, which is built on top of all three programming models, thus enabling computational scientists to interface to a single library, yet obtain high performance for all three hardware types. Since the respective compute back end can be selected at runtime, one can seamlessly switch between different hardware types without the need for error-prone and time-consuming recompilation steps. We present new benchmark results for sparse linear algebra operations in ViennaCL, complementing results for the dense linear algebra operations in ViennaCL reported in earlier work. Comparisons with vendor libraries show that ViennaCL provides better overall performance for sparse matrix-vector and sparse matrix-matrix products. Additional benchmark results for pipelined iterative solvers with kernel fusion and preconditioners identify the respective sweet spots for CPUs, Xeon Phis, and GPUs.},
}

@InProceedings{Ryoo2008,
  Title                    = {Program Optimization Space Pruning for a Multithreaded {GPU}},
  Author                   = {Ryoo, Shane and Rodrigues, Christopher I. and Stone, Sam S. and Baghsorkhi, Sara S. and Ueng, Sain-Zee and Stratton, John A. and Hwu, Wen-mei W.},
  Booktitle                = {{P}roceedings of the 6th {A}nnual {IEEE/ACM} {I}nternational {S}ymposium on {C}ode {G}eneration and {O}ptimization},

  Address                  = {New York, NY, USA},
  Pages                    = {195--204},
  Publisher                = {ACM},
  Series                   = {CGO '08},

 abstract = {Program optimization for highly-parallel systems has historically been considered an art, with experts doing much of the performance tuning by hand. With the introduction of inexpensive, single-chip, massively parallel platforms, more developers will be creating highly-parallel applications for these platforms, who lack the substantial experience and knowledge needed to maximize their performance. This creates a need for more structured optimization methods with means to estimate their performance effects. Furthermore these methods need to be understandable by most programmers. This paper shows the complexity involved in optimizing applications for one such system and one relatively simple methodology for reducing the workload involved in the optimization process. \\This work is based on one such highly-parallel system, the GeForce 8800 GTX using CUDA. Its flexible allocation of resources to threads allows it to extract performance from a range of applications with varying resource requirements, but places new demands on developers who seek to maximize an application's performance. We show how optimizations interact with the architecture in complex ways, initially prompting an inspection of the entire configuration space to find the optimal configuration. Even for a seemingly simple application such as matrix multiplication, the optimal configuration can be unexpected. We then present metrics derived from static code that capture the first-order factors of performance. We demonstrate how these metrics can be used to prune many optimization configurations, down to those that lie on a Pareto-optimal curve. This reduces the optimization space by as much as 98\% and still finds the optimal configuration for each of the studied applications.},
  Acmid                    = {1356084},
  Date                     = {2008},
  Doi                      = {10.1145/1356058.1356084},
  ISBN                     = {978-1-59593-978-4},
  Keywords                 = {gpgpu, optimization, parallel computing},
  Location                 = {Boston, MA, USA},
  Numpages                 = {10}
}

@Book{Saad2003,
  author    = {Saad, Yousef},
  title     = {Iterative Methods for Sparse Linear Systems},
  date      = {2003},
  edition   = {2},
  publisher = {Society for Industrial and Applied Mathematics},
  doi       = {10.1137/1.9780898718003},
  owner     = {ap8213},
  timestamp = {2015.09.23},
}

@InProceedings{Sadrieh2011,
  author    = {Sadrieh, Arash and Bahri, Parisa A.},
  title     = {Optimal Control of the Process Systems Using Graphic Processing Units},
  booktitle = {{P}roceedings of the 18th {IFAC} {W}orld {C}ongress},
  date      = {2011},
  series    = {IFAC '11},
  location  = {Milan, IT},
  doi       = {10.3182/20110828-6-IT-1002.01804},
  abstract  = {In this paper the Graphic Processing Unit (GPU) is applied in order to improve the computational performance of process systems optimal control calculations. To apply GPU massive parallel architecture, a simplified version of interior point optimisation algorithm was selected and modified to fulfil special hardware requirements of GPU architecture. In this algorithm, a damped nonlinear Newton with preconditioned iterative linear solver was used to solve Dual-Primal equations. The comparison of results between this implementation and standard CPU-based implementation shows considerable improvement in computational performance.},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@InProceedings{Sager2008,
  author    = {Sager, Sebastian and Kirches, Christian and Bock, Hans Georg},
  title     = {Fast solution of periodic optimal control problems in automobile test-driving with gear shifts},
  booktitle = {{P}roceedings of the 47th {IEEE} {C}onference on {D}ecision and {C}ontrol},
  date      = {2008},
  series    = {CDC '08},
  location  = {Cancun, MEX},
  month     = dec,
  pages     = {1563--1568},
  doi       = {10.1109/CDC.2008.4739014},
  abstract  = {Optimal control problems involving time-dependent decisions from a finite set have gained much interest lately, as they occur in practical applications with a high potential for optimization. A typical application is automobile driving with gear shifts. Recent work [7], [8], [9] lead to a tremendous speedup in computational times to obtain optimal solutions, allowing for more complex scenarios. In this paper we extend a benchmark mixed-integer optimal control problem to a more complicated case in which a periodic solution on a closed track is considered. Our generic solution approach is based on a convexification and relaxation of the integer control constraint. It may also be used for other objectives, such as energy minimization. Using the direct multiple shooting method we solve the new benchmark problem and present numerical results.},
  issn      = {0191-2216},
  keywords  = {automobiles;automotive components;gears;optimal control;testing;vehicle dynamics;automobile driving;automobile test-driving;direct multiple shooting method;energy minimization;finite set;gear shifts;integer control constraint;mixed-integer optimal control problem;optimization;periodic optimal control problems;time-dependent decisions;Automobiles;Chemical engineering;Computational efficiency;Differential equations;Gears;Optimal control;Optimization methods;Scientific computing;Testing;Valves},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@InProceedings{Salihoglu2014,
  Title                    = {{HelP}: High-level Primitives For Large-Scale Graph Processing},
  Author                   = {Salihoglu, Semih and Widom, Jennifer},
  Booktitle                = {{P}roceedings of {W}orkshop on {GRA}ph {D}ata {M}anagement {E}xperiences and {S}ystems},

  Address                  = {New York, NY, USA},
  Pages                    = {3:1--3:6},
  Publisher                = {ACM},
  Series                   = {GRADES '14},

 abstract = {Large-scale graph processing systems typically expose a small set of functions, such as the compute() function of Pregel, or the gather(), apply(), and scatter() functions of PowerGraph. For some computations, these APIs are too low-level, yielding long and complex programs, but with shared coding patterns. Similar issues with the MapReduce framework have led to widely-used languages such as Pig Latin and Hive, which introduce higher-level primitives. We take an analogous approach for graph processing: we propose HelP, a set of high-level primitives that capture commonly appearing operations in large-scale graph computations. Using our primitives we have implemented a large suite of algorithms, some of which we previously implemented with the APIs of existing systems. Our experience has been that implementing algorithms using our primitives is more intuitive and much faster than using the APIs of existing distributed systems. All of our primitives and algorithms are fully implemented as a library on top of the open-source GraphX system.},
  Acmid                    = {2621938},
  Articleno                = {3},
  Date                     = {2014},
  Doi                      = {10.1145/2621934.2621938},
  ISBN                     = {978-1-4503-2982-8},
  Location                 = {Snowbird, UT, USA},
  Numpages                 = {6}
}

@Book{Scarpino2011,
  author    = {Scarpino, M.},
  title     = {Open{CL} in action},
  date      = {2011},
  publisher = {Manning Publications Co.},
  owner     = {andrea},
  timestamp = {2014.03.05},
}

@Article{Scholtes2005,
  author       = {Scholtes, Carsten},
  title        = {A Method to Derive the Cache Performance of Irregular Applications on Machines with Direct Mapped Caches},
  journaltitle = {International Journal of Computational Science and Engineering},
  date         = {2005},
  volume       = {1},
  number       = {2-4},
  month        = may,
  pages        = {157--174},
  issn         = {1742-7185},
  doi          = {10.1504/IJCSE.2005.009700},
 abstract = {A probabilistic method is presented to derive the cache performance of irregular applications on machines with direct mapped caches from inspection of the source code. The method has been applied to analyse both a program to multiply a sparse matrix with a dense matrix and a program for the Cholesky-factorisation of a sparse matrix. The resulting predictions are compared with measurements of the respective programs.},
  acmid        = {1360421},
  issue_date   = {May 2005},
  keywords     = {cache memories, cache performance, direct mapped caches, irregular applications, irregularity, parallel computing, prediction methods},
  location     = {Inderscience Publishers, Geneva, SWITZERLAND},
  numpages     = {18},
  publisher    = {Inderscience Publishers},
}

@InProceedings{Shahzad2010,
  author    = {Shahzad, Amir and Kerrigan, Eric C. and Constantinides, George A.},
  title     = {A Warm-start Interior-point Method for Predictive Control},
  booktitle = {{P}roceedings of the 6th {UKACC} {I}nternational {C}onference on {C}ontrol},
  date      = {2010},
  series    = {UKACC '10},
  location  = {Coventry, UK},
  doi       = {10.1137/S1052623400369235},
  abstract  = {We study the situation in which, having solved a linear program with an interior-point method, we are presented with a new problem instance whose data is slightly perturbed from the original. We describe strategies for recovering a "warm-start" point for the perturbed problem instance from the iterates of the original problem instance. We obtain worst-case estimates of the number of iterations required to converge to a solution of the perturbed instance from the warm-start points, showing that these estimates depend on the size of the perturbation and on the conditioning and other properties of the problem instances.},
  owner     = {andrea},
  timestamp = {2014.12.15},
}

@TechReport{Shewchuk1994,
  author      = {Shewchuk, Jonathan Richard},
  title       = {An Introduction to the Conjugate Gradient Method Without the Agonizing Pain},
  institution = {Carnegie Mellon University},
  date        = {1994},
  location    = {Pittsburgh, PA, USA},
  url         = {http://www.cs.cmu.edu/~./quake-papers/painless-conjugate-gradient.pdf},
  abstract    = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written so that even their own authors would be mystified, if they bothered to read their own writing. For this reason, an understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.},
  owner       = {andrea},
  timestamp   = {2016.11.07},
}

@InProceedings{Shimai2009,
  author    = {Shimai, Yusuke and Tani, Junichi and Noguchi, Hiroki and Kawaguchi, Hiroshi and Yoshimoto, Masahiko},
  title     = {{FPGA} Implementation of Mixed Integer Quadratic Programming Solver for Mobile Robot Control},
  booktitle = {{P}roceedings of the 2009 {I}nternational {C}onference on {F}ield-{P}rogrammable {T}echnology},
  date      = {2009},
  series    = {FPT '09},
  location  = {Sidney, AUS},
  month     = dec,
  pages     = {447--450},
  doi       = {10.1109/FPT.2009.5377635},
  abstract  = {We propose a high-speed mixed integer quadratic programming (MIQP) solver on an FPGA. The MIQP solver can be applied to various optimizing applications including real-time robot control. In order to rapidly solve the MIQP problem, we implement reusing a first solution (first point), pipeline architecture, and multi-core architecture on the single FPGA. By making use of them, we confirmed that 79.5\% of the cycle times are reduced, compared with straightforward sequential processing. The operating frequency is 67 MHz, although a core 2 duo PC requires 3.16 GHz in processing the same size problem. The power consumption of the MIQP solver is 4.2 W.},
  keywords  = {control engineering computing;field programmable gate arrays;integer programming;mobile robots;pipeline processing;quadratic programming;FPGA implementation;MIQP solver;high-speed mixed integer quadratic programming solver;mobile robot control;multicore architecture;pipeline architecture;power consumption;real-time robot control;straightforward sequential processing;Artificial intelligence;Control systems;Field programmable gate arrays;Linear matrix inequalities;Mobile robots;NP-hard problem;Quadratic programming;Robot control;Robot programming;Symmetric matrices},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Shin1995,
  author       = {Shin, K. G. and Cui, X.},
  title        = {Computing time delay and its effects on real-time control systems},
  journaltitle = {{IEEE} Transactions on Control Systems Technology},
  date         = {1995},
  volume       = {3},
  number       = {2},
  pages        = {218--224},
  issn         = {1063-6536},
  doi          = {10.1109/87.388130},
  abstract     = {The reliability of a real-time digital control computer depends not only on the reliability of the hardware and software used, but also on the time delay in computing the control output, because of the negative effects of computing time delay on control system performance. For a given fixed sampling interval, the effects of computing time delay are classified into the delay and loss problems. The delay problem occurs when the computing time delay is nonzero but smaller than the sampling interval, while the loss problem occurs when the computing time delay is greater than, or equal to, the sampling interval, i.e., loss of the control output. These two problems are analyzed as a means of evaluating real-time control systems. First, a generic analysis of the effects of computing time delay is presented along with necessary conditions for system stability. Then, we present both qualitative and quantitative analyses of the computing time delay effects on a robot control system, deriving upper bounds of the computing time delay with respect to system stability and system performance.},
  keywords     = {computerised control;control system analysis;control systems;delays;performance evaluation;stability;computing time delay;digital control computer;loss problem;real-time control systems;robot control;sampling interval;system performance;system stability;upper bounds;Control systems;Delay effects;Digital control;Hardware;Performance analysis;Real time systems;Sampling methods;Software performance;Stability analysis;System performance},
  owner        = {ap8213},
  timestamp    = {2014.10.09},
}

@InProceedings{Shun2013,
  Title                    = {Ligra: A Lightweight Graph Processing Framework for Shared Memory},
  Author                   = {Shun, Julian and Blelloch, Guy E.},
  Booktitle                = {{P}roceedings of the 18th {ACM SIGPLAN} {S}ymposium on {P}rinciples and {P}ractice of {P}arallel {P}rogramming},

  Address                  = {New York, NY, USA},
  Pages                    = {135--146},
  Publisher                = {ACM},
  Series                   = {PPoPP '13},

 abstract = {There has been significant recent interest in parallel frameworks for processing graphs due to their applicability in studying social networks, the Web graph, networks in biology, and unstructured meshes in scientific simulation. Due to the desire to process large graphs, these systems have emphasized the ability to run on distributed memory machines. Today, however, a single multicore server can support more than a terabyte of memory, which can fit graphs with tens or even hundreds of billions of edges. Furthermore, for graph algorithms, shared-memory multicores are generally significantly more efficient on a per core, per dollar, and per joule basis than distributed memory systems, and shared-memory algorithms tend to be simpler than their distributed counterparts. \\In this paper, we present a lightweight graph processing framework that is specific for shared-memory parallel/multicore machines, which makes graph traversal algorithms easy to write. The framework has two very simple routines, one for mapping over edges and one for mapping over vertices. Our routines can be applied to any subset of the vertices, which makes the framework useful for many graph traversal algorithms that operate on subsets of the vertices. Based on recent ideas used in a very fast algorithm for breadth-first search (BFS), our routines automatically adapt to the density of vertex sets. We implement several algorithms in this framework, including BFS, graph radii estimation, graph connectivity, betweenness centrality, PageRank and single-source shortest paths. Our algorithms expressed using this framework are very simple and concise, and perform almost as well as highly optimized code. Furthermore, they get good speedups on a 40-core machine and are significantly more efficient than previously reported results using graph frameworks on machines with many more cores.},
  Acmid                    = {2442530},
  Date                     = {2013},
  Doi                      = {10.1145/2442516.2442530},
  ISBN                     = {978-1-4503-1922-5},
  Keywords                 = {graph algorithms, parallel programming, shared memory},
  Location                 = {Shenzhen, CN},
  Numpages                 = {12}
}

@Article{Smith2011,
  author       = {Smith, Barry and Zhang, Hong},
  title        = {Sparse triangular solves for {ILU} revisited: data layout crucial to better performance},
  journaltitle = {International Journal of High Performance Computing Applications},
  date         = {2011},
  volume       = {25},
  number       = {4},
  month        = nov,
  pages        = {386--391},
  doi          = {10.1177/1094342010389857},
  abstract     = {A key to good processor utilization for sparse matrix computations is storing the data in the format that is most conducive to fast access by the memory system. In particular, for sparse matrix triangular solves the traditional compressed sparse matrix format is poor, and minor adjustments to the data structure can increase the processor utilization dramatically. Such adjustments involve storing the L and U factors separately and storing the U rows backwards so that they are accessed in a simple streaming fashion during the triangular solves. Changes to the PETSc libraries to use this modified storage format resulted in over twice the floating-point rate for some matrices. This improvement can be accounted for by a decrease in the cache misses and TLB (transaction lookaside buffer) misses in the modified code.},
  owner        = {andrea},
  timestamp    = {2015.09.28},
}

@PhdThesis{Smith2013,
  author      = {Smith, E.},
  title       = {Parallel solution of linear programs},
  institution = {University of Edinburgh},
  date        = {2013},
  url         = {http://hdl.handle.net/1842/8833},
  abstract    = {The factors limiting the performance of computer software periodically undergo sudden shifts, resulting from technological progress, and these shifts can have profound implications for the design of high performance codes. At the present time, the speed with which hardware can execute a single stream of instructions has reached a plateau. It is now the number of instruction streams that may be executed concurrently which underpins estimates of compute power, and with this change, a critical limitation on the performance of software has come to be the degree to which it can be parallelised. The research in this thesis is concerned with the means by which codes for linear programming may be adapted to this new hardware. For the most part, it is codes implementing the simplex method which will be discussed, though these have typically lower performance for single solves than those implementing interior point methods. However, the ability of the simplex method to rapidly re-solve a problem makes it at present indispensable as a subroutine for mixed integer programming. The long history of the simplex method as a practical technique, with applications in many industries and government, has led to such codes reaching a great level of sophistication. It would be unexpected in a research project such as this one to match the performance of top commercial codes with many years of development behind them. The simplex codes described in this thesis are, however, able to solve real problems of small to moderate size, rather than being confined to random or otherwise artificially generated instances. The remainder of this thesis is structured as follows. The rest of this chapter gives a brief overview of the essential elements of modern parallel hardware and of the linear programming problem. Both the simplex method and interior point methods are discussed, along with some of the key algorithmic enhancements required for such systems to solve real-world problems. Some background on the parallelisation of both types of code is given. The next chapter describes two standard simplex codes designed to exploit the current generation of hardware. i6 is a parallel standard simplex solver capable of being applied to a range of real problems, and showing exceptional performance for dense, square programs. i8 is also a parallel, standard simplex solver, but now implemented for graphics processing units (GPUs).},
  owner       = {andrea},
  timestamp   = {2014.03.05},
}

@InCollection{Smith2012,
  author    = {Smith, E. and Gondzio, J. and Hall, J. A. J.},
  title     = {{GPU} acceleration of the matrix-free interior point method},
  booktitle = {{P}arallel {P}rocessing and {A}pplied {M}athematics},
  date      = {2012},
  editor    = {Wyrzykowski, R. and Dongarra, J. and Karczewski, K. and Waśniewski, J.},
  volume    = {7203},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-642-31463-6},
  pages     = {681--689},
  doi       = {10.1007/978-3-642-31464-3_69},
  abstract  = {The matrix-free technique is an iterative approach to interior point methods (IPM), so named because both the solution procedure and the computation of an appropriate preconditioner require only the results of the operations Ax and ATy, where A is the matrix of constraint coefficients. This paper demonstrates its overwhelmingly superior performance on two classes of linear programming (LP) problems relative to both the simplex method and to IPM with equations solved directly. It is shown that the reliance of this technique on sparse matrix-vector operations enables further, significant performance gains from the use of a GPU, and from multi-core processors.},
  keywords  = {interior point methods; linear programming; matrix-free methods; parallel sparse linear algebra},
  owner     = {ap8213},
  timestamp = {2014.03.21},
}

@Book{Smith1993,
  author    = {Smith, Justin R.},
  title     = {The Design and Analysis of Parallel Algorithms},
  date      = {1993},
  publisher = {Oxford University Press},
  isbn      = {0195078810},
  abstract  = {This text for students and professionals in computer science provides a valuable overview of current knowledge concerning parallel algorithms. These computer operations have recently acquired increased importance due to their ability to enhance the power of computers by permitting multiple processors to work on different parts of a problem independently and simultaneously. This approach has led to solutions of difficult problems in a number of vital fields, including artificial intelligence, image processing, and differential equations. As the first up-to-date summary of the topic, this book will be sought after by researchers, computer science professionals, and advanced students involved in parallel computing and parallel algorithms.},
  owner     = {andrea},
  timestamp = {2017.05.10},
}

@InProceedings{Soman2010,
  author    = {Soman, J. and Kishore, K. and Narayanan, P. J.},
  title     = {A fast {GPU} algorithm for graph connectivity},
  booktitle = {{P}roceedings of the 2010 {IEEE} {I}nternational {S}ymposium on {P}arallel {D}istributed {P}rocessing, {W}orkshops and {P}hd {F}orum},
  date      = {2010},
  series    = {IPDPSW '10},
  location  = {Atlanta, GA, US},
  month     = {4},
  pages     = {1--8},
  doi       = {10.1109/IPDPSW.2010.5470817},
  abstract  = {Graphics processing units provide a large computational power at a very low price which position them as an ubiquitous accelerator. General purpose programming on the graphics processing units (GPGPU) is best suited for regular data parallel algorithms. They are not directly amenable for algorithms which have irregular data access patterns such as list ranking, and finding the connected components of a graph, and the like. In this work, we present a GPU-optimized implementation for finding the connected components of a given graph. Our implementation tries to minimize the impact of irregularity, both at the data level and functional level. Our implementation achieves a speed up of 9 to 12 times over the best sequential CPU implementation. For instance, our implementation finds connected components of a graph of 10 million nodes and 60 million edges in about 500 milliseconds on a GPU, given a random edge list. We also draw interesting observations on why PRAM algorithms, such as the Shiloach-Vishkin algorithm may not be a good fit for the GPU and how they should be modified.},
  keywords  = {coprocessors;parallel processing;GPU-optimized implementation;Shiloach-Vishkin algorithm;craphics processing units;fast GPU algorithm;general purpose programming;graph connectivity;large computational power;regular data parallel algorithms;ubiquitous accelerator;Arithmetic;Central Processing Unit;Graphics;Inference algorithms;Parallel algorithms;Parallel processing;Parallel programming;Partitioning algorithms;Pervasive computing;Phase change random access memory;Connected Components;GPGPU;GPU;Irregular algorithms},
}

@InProceedings{Spampinato2009,
  author    = {Spampinato, Daniele D. and Elster, Anne C.},
  title     = {Linear optimization on modern {GPU}s},
  booktitle = {{P}roceedings of the {IEEE} {I}nternational {S}ymposium on {P}arallel and {D}istributed {P}rocessing},
  date      = {2009},
  series    = {IPDPS '09},
  location  = {Rome, IT},
  month     = may,
  pages     = {1--8},
  doi       = {10.1109/IPDPS.2009.5161106},
  abstract  = {Optimization algorithms are becoming increasingly more important in many areas, such as finance and engineering. Typically, real problems involve several hundreds of variables, and are subject to as many constraints. Several methods have been developed trying to reduce the theoretical time complexity. Nevertheless, when problems exceed reasonable sizes they end up being very computationally intensive. Heterogeneous systems composed by coupling commodity CPUs and GPUs are becoming relatively cheap, highly performing systems. Recent developments of GPGPU technologies give even more powerful control over them. In this paper, we show how we use a revised simplex algorithm for solving linear programming problems originally described by Dantzig for both our CPU and GPU implementations. Previously, this approach has showed not to scale beyond around 200 variables. However, by taking advantage of modern libraries such as ATLAS for matrix-matrix multiplication, and the NVIDIA CUDA programming library on recent GPUs, we show that we can scale to problem sizes up to at least 2000 variables in our experiments for both architectures. On the GPU, we also achieve an appreciable precision on large problems with thousands of variables and constraints while achieving between 2$\times$ and 2.5$\times$ speed-ups over the serial ATLAS-based CPU version. With further tuning of both the algorithm and its implementations, even better results should be achievable for both the CPU and GPU versions.},
  file      = {:home/andrea/Dropbox/PhD/Papers/Spampinato - Linear Optimization on Modern GPU.pdf:PDF},
  issn      = {1530-2075},
  keywords  = {coprocessors;linear programming;matrix multiplication;parallel architectures;ATLAS-based CPU version;GPGPU technologies;NVIDIA CUDA programming library;graphics processing unit;linear optimization;linear programming problems;matrix-matrix multiplication;modern GPUs;theoretical time complexity;Computer graphics;Design automation;Finance;Hardware;High performance computing;Information science;Libraries;Linear programming;Optimization methods;Power engineering computing},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@Article{Spielman2004,
  author       = {Spielman, Daniel A. and Teng, Shang-Hua},
  title        = {Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes Polynomial Time},
  journaltitle = {Journal of the {ACM}},
  date         = {2004},
  volume       = {51},
  number       = {3},
  month        = may,
  pages        = {385--463},
  issn         = {0004-5411},
  doi          = {10.1145/990308.990310},
  abstract     = {We introduce the smoothed analysis of algorithms, which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has smoothed complexity polynomial in the input size and the standard deviation of Gaussian perturbations.},
  acmid        = {990310},
  issue_date   = {May 2004},
  keywords     = {Simplex method, complexity, perturbation, smoothed analysis},
  location     = {New York, NY, USA},
  numpages     = {79},
  owner        = {andrea},
  publisher    = {ACM},
  timestamp    = {2017.05.08},
}

@Book{Stoer2002,
  author    = {Stoer, J. and Bulirsch, R.},
  title     = {Introduction to numerical analysis},
  date      = {2002},
  series    = {Texts in applied mathematics},
  publisher = {Springer},
  location  = {New York},
  isbn      = {0-387-95452-X},
  owner     = {ap8213},
  timestamp = {2014.10.09},
}

@Article{Stone2010,
  author       = {Stone, J. E. and Gohara, D. and Shi, G.},
  title        = {{OpenCL}: A Parallel Programming Standard for Heterogeneous Computing Systems},
  journaltitle = {Computing in Science \& Engineering},
  date         = {2010},
  volume       = {12},
  number       = {3},
  month        = may,
  pages        = {66--73},
  issn         = {0740-7475},
  doi          = {10.1109/MCSE.2010.69},
 abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
  acmid        = {1803953},
  issue_date   = {May 2010},
  location     = {Los Alamitos, CA, USA},
  numpages     = {8},
  owner        = {ap8213},
  publisher    = {IEEE Computer Society Press},
  timestamp    = {2014.10.09},
}

@InBook{Strout2001,
  Title                    = {Rescheduling for Locality in Sparse Matrix Computations},
  Author                   = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne},
  Editor                   = {Alexandrov, Vassil N. and Dongarra, Jack J. and Juliano, Benjoe A. and Renner, R. S. and Tan, C. J. Kenneth},
  Pages                    = {137--146},
  Publisher                = {Springer Berlin Heidelberg},

  Address                  = {Berlin, Heidelberg},
  Series                   = {ICCS '01},

 abstract = {In modern computer architecture the use of memory hierarchies causes a program's data locality to directly affect performance. Data locality occurs when a piece of data is still in a cache upon reuse. For dense matrix computations, loop transformations can be used to improve data locality. However, sparse matrix computations have non-affine loop bounds and indirect memory references which prohibit the use of compile time loop transformations. This paper describes an algorithm to tile at runtime called serial sparse tiling. We test a runtime tiled version of sparse Gauss-Seidel on 4 different architectures where it exhibits speedups of up to 2.7. The paper also gives a static model for determining tile size and outlines how overhead affects the overall speedup.},
  Booktitle                = {{P}roceedings of the 2001 {I}nternational {C}onference on {C}omputational {S}cience},
  Date                     = {2001},
  Doi                      = {10.1007/3-540-45545-0_23},
  ISBN                     = {978-3-540-45545-5},
  Location                 = {San Francisco, CA, USA},
  Owner                    = {andrea},
  Part                     = {I},
  Timestamp                = {2017.05.08}
}

@InProceedings{Suchoski2012,
  author    = {Suchoski, Brad and Severn, Caleb and Shantharam, Manu and Raghavan, Padma},
  title     = {Adapting Sparse Triangular Solution to {GPUs}},
  booktitle = {{P}roceeding of the 41st {I}nternational {C}onference on {P}arallel {P}rocessing {W}orkshops},
  date      = {2012},
  series    = {ICPPW '12},
  location  = {Los Alamitos, CA, USA},
  month     = sep,
  pages     = {140--148},
  doi       = {10.1109/ICPPW.2012.23},
  abstract  = {High performance computing systems are increasingly incorporating hybrid CPU/GPU nodes to accelerate the rate at which floating point calculations can be performed for scientific applications. Currently, a key challenge is adapting scientific applications to such systems when the underlying computations are sparse, such as sparse linear solvers for the simulation of partial differential equation models using semi-implicit methods. Now, a key bottleneck is sparse triangular solution for solvers such as preconditioned conjugate gradients (PCG). We show that sparse triangular solution can be effectively mapped to GPUs by extracting very large degrees of fine-grained parallelism using graph coloring. We develop simple performance models to predict these effects at intersection of the data and hardware attributes and we evaluate our scheme on a Nvidia Tesla M2090 GPU relative to the level set scheme developed at NVIDIA. Our results indicate that our approach significantly enhances the available fine-grained parallelism to speed-up PCG iteration time compared to the NVIDIA scheme, by a factor with a geometric mean of 5.41 on a single GPU, with speedups as high as 63 in some cases.},
  issn      = {1530-2016},
  keywords  = {floating point arithmetic;graphics processing units;multiprocessing systems;natural sciences computing;NVIDIA Tesla M2090 GPU;fine-grained parallelism;floating point calculations;graph coloring;high performance computing systems;partial differential equation models;preconditioned conjugate gradients;scientific applications;semiimplicit methods;sparse linear solvers;sparse triangular solution;Color;Concurrent computing;Graphics processing unit;Image color analysis;Level set;Parallel processing;Sparse matrices},
  owner     = {andrea},
  timestamp = {2015.09.28},
}

@Article{Suhl1993,
  author       = {Suhl, Leena M. and Suhl, Uwe H.},
  title        = {A fast {LU} update for linear programming},
  journaltitle = {Annals of Operations Research},
  date         = {1993},
  volume       = {43},
  number       = {1},
  pages        = {33--47},
  issn         = {0254-5330},
  doi          = {10.1007/BF02025534},
  abstract     = {This paper discusses sparse matrix kernels of simplex-based linear programming software. State-of-the-art implementations of the simplex method maintain an LU factorization of the basis matrix which is updated at each iteration. The LU factorization is used to solve two sparse sets of linear equations at each iteration. We present new implementation techniques for a modified Forrest-Tomlin LU update which reduce the time complexity of the update and the solution of the associated sparse linear systems. We present numerical results on Netlib and other real-life LP models.},
  file         = {:home/ap8213/Dropbox/Documents/Papers/Suhl, Suhl - A fast LU update for linear programming.pdf:PDF},
  owner        = {ap8213},
  publisher    = {Baltzer Science Publishers, Baarn/Kluwer Academic Publishers},
  timestamp    = {2014.10.09},
}

@Online{Sutton2016,
  author     = {Sutton, Michael and Ben{-}Nun, Tal and Barak, Amnon and Pai, Sreepathi and Pingali, Keshav},
  title      = {Adaptive Work-Efficient Connected Components on the {GPU}},
  date       = {2016},
  abstract   = {This report presents an adaptive work-efficient approach for implementing the Connected Components algorithm on GPUs. The results show a considerable increase in performance (up to 6.8$\times$) over current state-of-the-art solutions.},
  eprint     = {1612.01178},
  eprinttype = {arXiv},
  timestamp  = {Mon, 02 Jan 2017 11:09:15 +0100},
}

@InProceedings{Tang2013,
  Title                    = {Accelerating Sparse Matrix-vector Multiplication on {GPUs} Using Bit-representation-optimized Schemes},
  Author                   = {Tang, Wai Teng and Tan, Wen Jun and Ray, Rajarshi and Wong, Yi Wen and Chen, Weiguang and Kuo, Shyh-hao and Goh, Rick Siow Mong and Turner, Stephen John and Wong, Weng-Fai},
  Booktitle                = {{P}roceedings of the {I}nternational {C}onference on {H}igh {P}erformance {C}omputing, {N}etworking, {S}torage and {A}nalysis},

  Address                  = {New York, NY, USA},
  Pages                    = {26:1--26:12},
  Publisher                = {ACM},
  Series                   = {SC '13},

 abstract = {The sparse matrix-vector (SpMV) multiplication routine is an important building block used in many iterative algorithms for solving scientific and engineering problems. One of the main challenges of SpMV is its memory-boundedness. Although compression has been proposed previously to improve SpMV performance on CPUs, its use has not been demonstrated on the GPU because of the serial nature of many compression and decompression schemes. In this paper, we introduce a family of bit-representation-optimized (BRO) compression schemes for representing sparse matrices on GPUs. The proposed schemes, BRO-ELL, BRO-COO, and BRO-HYB, perform compression on index data and help to speed up SpMV on GPUs through reduction of memory traffic. Furthermore, we formulate a BRO-aware matrix reordering scheme as a data clustering problem and use it to increase compression ratios. With the proposed schemes, experiments show that average speedups of 1.5$\times$ compared to ELLPACK and HYB can be achieved for SpMV on GPUs.},
  Acmid                    = {2503234},
  Articleno                = {26},
  Date                     = {2013},
  Doi                      = {10.1145/2503210.2503234},
  ISBN                     = {978-1-4503-2378-9},
  Keywords                 = {GPU, compression, data compression, matrix-vector multiplication, memory bandwidth, parallelism, sparse matrix format},
  Location                 = {Denver, CO, USA},
  Numpages                 = {12}
}

@Article{Tarjan1972,
  author       = {Tarjan, Robert},
  title        = {Depth-first search and linear graph algorithms},
  journaltitle = {{SIAM} Journal on Computing},
  date         = {1972},
  volume       = {1},
  number       = {2},
  month        = {6},
  pages        = {146--160},
  owner        = {andrea},
  timestamp    = {2015.04.24},
}

@InProceedings{Temam1992,
  author    = {Temam, O. and Jalby, W.},
  title     = {Characterizing the Behaviour of Sparse Algorithms on Caches},
  booktitle = {{P}roceedings of the {I}nternational {C}onference on {H}igh {P}erformance {C}omputing, {N}etworking, {S}torage and {A}nalysis},
  date      = {1992},
  series    = {SC '92},
  location  = {Minneapolis, MN, USA},
  pages     = {578--587},
  abstract  = {A methodology is presented for modeling the irregular references of sparse codes using probabilistic methods. The behavior on cache of one of the most frequent primitives, SpMxV sparse matrix vector multiply, is analyzed. A model of its references is built, and performance bottlenecks of SpMxV are analyzed using the model and simulations. The main parameters are identified and their role is explained and quantified. This analysis is then used to discuss optimizations of SpMxV. A blocking technique which takes into account the specifics of sparse codes is proposed.},
  owner     = {andrea},
  timestamp = {2015.09.27},
}

@InProceedings{Thoman2011,
  author    = {Thoman, Peter and Kofler, Klaus and Studt, Heiko and Thomson, John and Fahringer, Thomas},
  title     = {Automatic {OpenCL} Device Characterization: Guiding Optimized Kernel Design},
  booktitle = {{P}roceedings of the 17th {I}nternational {C}onference on {P}arallel {P}rocessing},
  date      = {2011},
  series    = {Euro-Par '11},
  publisher = {Springer-Verlag},
  location  = {Bordeaux, FR},
  isbn      = {978-3-642-23396-8},
  pages     = {438--452},
  doi       = {10.1007/978-3-642-23397-5_43},
  abstract  = {The OpenCL standard allows targeting a large variety of CPU, GPU and accelerator architectures using a single unified programming interface and language. While the standard guarantees portability of functionality for complying applications and platforms, performance portability on such a diverse set of hardware is limited. Devices may vary significantly in memory architecture as well as type, number and complexity of computational units. To characterize and compare the OpenCL performance of existing and future devices we propose a suite of microbenchmarks, uCLbench. \\We present measurements for eight hardware architectures four GPUs, three CPUs and one accelerator and illustrate how the results accurately reflect unique characteristics of the respective platform. In addition to measuring quantities traditionally benchmarked on CPUs like arithmetic throughput or the bandwidth and latency of various address spaces, the suite also includes code designed to determine parameters unique to OpenCL like the dynamic branching penalties prevalent on GPUs. We demonstrate how our results can be used to guide algorithm design and optimization for any given platform on an example kernel that represents the key computation of a linear multigrid solver. Guided manual optimization of this kernel results in an average improvement of 61\% across the eight platforms tested.},
  acmid     = {2033459},
  address   = {Berlin, Heidelberg},
  numpages  = {15},
}

@Article{Totoni2014,
  author       = {Totoni, Ehsan and Health, Michael T. and Kale, Laxmikant V.},
  title        = {Structure-adaptive parallel solution of sparse triangular linear systems},
  journaltitle = {Parallel Computing},
  date         = {2014},
  volume       = {40},
  number       = {9},
  month        = oct,
  pages        = {454--470},
  doi          = {10.1016/j.parco.2014.06.006},
  abstract     = {Solving sparse triangular systems of linear equations is a performance bottleneck in many methods for solving more general sparse systems. Both for direct methods and for many iterative preconditioners, it is used to solve the system or improve an approximate solution, often across many iterations. Solving triangular systems is notoriously resistant to parallelism, however, and existing parallel linear algebra packages appear to be ineffective in exploiting significant parallelism for this problem. \\We develop a novel parallel algorithm based on various heuristics that adapt to the structure of the matrix and extract parallelism that is unexploited by conventional methods. By analyzing and reordering operations, our algorithm can often extract parallelism even for cases where most of the nonzero matrix entries are near the diagonal. Our main parallelism strategies are: (1) identify independent rows, (2) send data earlier to achieve greater overlap, and (3) process dense off-diagonal regions in parallel. We describe the implementation of our algorithm in Charm++ and MPI and present promising experimental results on up to 512 cores of BlueGene/P, using numerous sparse matrices from real applications.},
  owner        = {ap8213},
  timestamp    = {2015.08.15},
}

@InProceedings{Ubal2012,
  author    = {Ubal, R. and Jang, B. and Mistry, P. and Schaa, D. and Kaeli, D.},
  title     = {Multi2Sim: A simulation framework for {CPU}-{GPU} computing},
  booktitle = {21st {I}nternational {C}onference on {P}arallel {A}rchitectures and {C}ompilation {T}echniques},
  date      = {2012},
  series    = {PACT '12},
  month     = sep,
  pages     = {335--344},
  abstract  = {Accurate simulation is essential for the proper design and evaluation of any computing platform. Upon the current move toward the CPU-GPU heterogeneous computing era, researchers need a simulation framework that can model both kinds of computing devices and their interaction. In this paper, we present Multi2Sim, an open-source, modular, and fully configurable toolset that enables ISA-level simulation of an x86 CPU and an AMD Evergreen GPU. Focusing on a model of the AMD Radeon 5870 GPU, we address program emulation correctness, as well as architectural simulation accuracy, using AMD's OpenCL benchmark suite. Simulation capabilities are demonstrated with a preliminary architectural exploration study, and workload characterization examples. The project source code, benchmark packages, and a detailed user's guide are publicly available at www.multi2sim.org.},
  keywords  = {graphics processing units;microprocessor chips;simulation;AMD Evergreen GPU;AMD OpenCL benchmark suite;AMD Radeon 5870 GPU;CPU-GPU computing;ISA-level simulation;Multi2Sim;architectural exploration study;open-source modular fully configurable toolset;simulation framework;workload characterization examples;Benchmark testing;Computational modeling;Computer architecture;Graphics processing units;Hardware;Kernel;VLIW;AMD;Evergreen ISA;GPU;Multi2Sim},
}

@InProceedings{Unkule2012,
  Title                    = {Automatic Restructuring of {GPU} Kernels for Exploiting Inter-thread Data Locality},
  Author                   = {Unkule, Swapneela and Shaltz, Christopher and Qasem, Apan},
  Booktitle                = {{P}roceedings of the 21st {I}nternational {C}onference on {C}ompiler {C}onstruction},

  Address                  = {Berlin, Heidelberg},
  Pages                    = {21--40},
  Publisher                = {Springer-Verlag},
  Series                   = {CC '12},

 abstract = {Hundreds of cores per chip and support for fine-grain multithreading have made GPUs a central player in today's HPC world. For many applications, however, achieving a high fraction of peak on current GPUs, still requires significant programmer effort. A key consideration for optimizing GPU code is determining a suitable amount of work to be performed by each thread. Thread granularity not only has a direct impact on occupancy but can also influence data locality at the register and shared-memory levels. This paper describes a software framework to analyze dependencies in parallel GPU threads and perform source-level restructuring to obtain GPU kernels with varying thread granularity. The framework supports specification of coarsening factors through source-code annotation and also implements a heuristic based on estimated register pressure that automatically recommends coarsening factors for improved memory performance. We present preliminary experimental results on a select set of CUDA kernels. The results show that the proposed strategy is generally able to select profitable coarsening factors. More importantly, the results demonstrate a clear need for automatic control of thread granularity at the software level for achieving higher performance.},
  Acmid                    = {2259233},
  Date                     = {2012},
  Doi                      = {10.1007/978-3-642-28652-0_2},
  ISBN                     = {978-3-642-28651-3},
  Location                 = {Tallinn, EE},
  Numpages                 = {20}
}

@Article{Vielma2015,
  author       = {Vielma, Juan Pablo},
  title        = {Mixed Integer Linear Programming Formulation Techniques},
  journaltitle = {{SIAM} Review},
  date         = {2015},
  volume       = {57},
  number       = {1},
  month        = feb,
  pages        = {3--57},
  doi          = {10.1137/130915303},
  abstract     = {A wide range of problems can be modeled as Mixed Integer Linear Programming (MIP) problems using standard formulation techniques. However, in some cases the resulting MIP can be either too weak or too large to be effectively solved by state of the art solvers. In this survey we review advanced MIP formulation techniques that result in stronger and/or smaller formulations for a wide class of problems.},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Vilches2015,
  author       = {Vilches, Antonio and Asenjo, Rafael and navarro, Angeles and Corbera, Francisco and Gran, Rub́en and Garzarán, María},
  title        = {Adaptive Partitioning for irregular Applications on Heterogeneous {CPU-GPU} Chips},
  journaltitle = {Procedia Computer Science},
  date         = {2015},
  volume       = {51},
  pages        = {140--149},
  doi          = {10.1016/j.procs.2015.05.213},
  abstract     = {Commodity processors are comprised of several CPU cores and one integrated GPU. To fully exploit this type of architectures, one needs to automatically determine how to partition the workload between both devices. This is specially challenging for irregular workloads, where each iteration's work is data dependent and shows control and memory divergence. In this paper, we present a novel adaptive partitioning strategy specially designed for irregular applications running on heterogeneous CPU-GPU chips. The main novelty of this work is that the size of the workload assigned to the GPU and CPU adapts dynamically to maximize the GPU and CPU utilization while balancing the workload among the devices. Our experimental results on an Intel Haswell architecture using a set of irregular benchmarks show that our approach outperforms exhaustive static and adaptive state-of-the-art approaches in terms of performance and energy consumption.},
  owner        = {andrea},
  timestamp    = {2016.05.27},
}

@InProceedings{Vuduc2010,
  Title                    = {On the Limits of {GPU} Acceleration},
  Author                   = {Vuduc, Richard and Chandramowlishwaran, Aparna and Choi, Jee and Guney, Murat and Shringarpure, Aashay},
  Booktitle                = {{P}roceedings of the 2nd {USENIX} {C}onference on {H}ot {T}opics in {P}arallelism},

  Address                  = {Berkeley, CA, USA},
  Pages                    = {13--13},
  Publisher                = {USENIX Association},
  Series                   = {HotPar'10},

 abstract = {This paper throws a small "wet blanket" on the hot topic of GPGPU acceleration, based on experience analyzing and tuning both multithreaded CPU and GPU implementations of three computations in scientific computing. These computations--(a) iterative sparse linear solvers; (b) sparse Cholesky factorization; and (c) the fast multipole method--exhibit complex behavior and vary in computational intensity and memory reference irregularity. In each case, algorithmic analysis and prior work might lead us to conclude that an idealized GPU can deliver better performance, but we find that for at least equal-effort CPU tuning and consideration of realistic workloads and calling-contexts, we can with two modern quad-core CPU sockets roughly match one or two GPUs in performance. \\Our conclusions are not intended to dampen interest in GPU acceleration; on the contrary, they should do the opposite: they partially illuminate the boundary between CPU and GPU performance, and ask architects to consider application contexts in the design of future coupled on-die CPU/GPU processors.},
  Acmid                    = {1863099},
  Date                     = {2010},
  Location                 = {Berkeley, CA, USA},
  Numpages                 = {1},
  Owner                    = {ap8213},
  Timestamp                = {2015.10.06},
  Url                      = {http://dl.acm.org/citation.cfm?id=1863086.1863099}
}

@InProceedings{Vuduc2002,
  author        = {Vuduc, Richard and Kamil, Shoaib and Hsu, Jen and Nishtala, Rajesh and Demmel, James W. and Yelick, Katherine A.},
  title         = {Automatic Performance Tuning and Analysis of Sparse Triangular Solve},
  booktitle     = {{P}roceeding of the {W}orkshop on {P}erformance {O}ptimization of {H}igh-level {L}anguages and {L}ibraries ({POHLL}) at the {ACM} {I}nternational {C}onference on {S}upercomputing},
  date          = {2002},
  series        = {ICS '02},
  location      = {Venice, IT},
  month         = {6},
  url           = {http://bebop.cs.berkeley.edu/pubs/vuduc2002-sts-bounds.pdf},
  address       = {New York, USA},
  bdsk-url-2    = {http://www.ece.lsu.edu/jxr/pohll-02/papers/vuduc.pdf},
  cvsidenote    = {Winner, Best Presentation; Winner, Best Student Paper},
  date-added    = {2009-08-27 10:11:50 -0400},
  date-modified = {2010-03-16 08:52:47 -0400},
  owner         = {andrea},
  timestamp     = {2016.09.07},
  topic         = {sparse linear algebra; performance modeling; autotuning},
}

@Article{Waechter2006,
  author       = {Waechter, A. and Biegler, L. T.},
  title        = {On the Implementation of an Interior-point Filter Line-search Algorithm for Large-scale Nonlinear Programming},
  journaltitle = {Mathematical Programming},
  date         = {2006},
  volume       = {106},
  number       = {1},
  month        = may,
  pages        = {25--57},
  issn         = {0025-5610},
  doi          = {10.1007/s10107-004-0559-y},
  abstract     = {We present a primal-dual interior-point algorithm with a filter line-search method for nonlinear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the feasibility restoration phase for the filter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlinear programming.},
  acmid        = {1107695},
  issue_date   = {May 2006},
  keywords     = {65K05, 90C30, 90C51},
  location     = {Secaucus, NJ, USA},
  numpages     = {33},
  owner        = {ap8213},
  publisher    = {Springer-Verlag New York, Inc.},
  timestamp    = {2014.10.09},
}

@InProceedings{Wang2016,
  Title                    = {Gunrock: A High-performance Graph Processing Library on the {GPU}},
  Author                   = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
  Booktitle                = {{P}roceedings of the 21st {ACM SIGPLAN} {S}ymposium on {P}rinciples and {P}ractice of {P}arallel {P}rogramming},

  Address                  = {New York, NY, USA},
  Pages                    = {11:1--11:12},
  Publisher                = {ACM},
  Series                   = {PPoPP '16},

 abstract = {For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.},
  Acmid                    = {2851145},
  Articleno                = {11},
  Date                     = {2016},
  Doi                      = {10.1145/2851141.2851145},
  ISBN                     = {978-1-4503-4092-2},
  Location                 = {Barcelona, ES},
  Numpages                 = {12}
}

@TechReport{Whitehead2011,
  author      = {Whitehead, Nathan and Fit-Florea, Alex},
  title       = {Precision \& performance: Floating point and {IEEE} 754 compliance for NVIDIA {GPU}s},
  institution = {NVIDIA Corporation},
  date        = {2011},
  number      = {1},
  url         = {http://docs.nvidia.com/cuda/floating-point/#axzz4gbha7lKQ},
  owner       = {andrea},
  timestamp   = {2017.05.08},
  volume      = {21},
}

@Article{Wilson2010,
  author       = {Wilson, G. and Banzhaf, W.},
  title        = {Deployment of parallel linear genetic programming using {GPU}s on {PC} and video game console platforms},
  journaltitle = {Genetic Programming and Evolvable Machines},
  date         = {2010},
  language     = {English},
  volume       = {11},
  number       = {2},
  pages        = {147--184},
  issn         = {1389--2576},
  doi          = {10.1007/s10710-010-9102-5},
  abstract     = {We present a general method for deploying parallel linear genetic programming (LGP) to the PC and Xbox 360 video game console by using a publicly available common framework for the devices called XNA (for XNA's Not Acronymed). By constructing the LGP within this framework, we effectively produce an LGP game for PC and XBox 360 that displays results as they evolve. We use the GPU of each device to parallelize fitness evaluation and the mutation operator of the LGP algorithm, thus providing a general LGP implementation suitable for parallel computation on heterogeneous devices. While parallel GP implementations on PCs are now common, both the implementation of GP on a video game console using GPU and the construction of a GP around a framework for heterogeneous devices are novel contributions. The objective of this work is to describe how to implement the parallel execution of LGP in order to use the underlying hardware (especially GPU) on the different platforms while still maintaining loyalty to the general methodology of the LGP algorithm built for the common framework. We discuss the implementation of texture-based data structures and the sequential and parallel algorithms built for their use on both CPU and GPU. Following the description of the general algorithm, the particular tailoring of the implementations for each hardware platform is described. Sequential (CPU) and parallel (GPU-based) algorithm performance is compared on both PC and video game platforms using the metrics of GP operations per second, actual time elapsed, speedup of parallel over sequential implementation, and percentage of execution time used by the GPU versus CPU.},
  keywords     = {Genetic programming; Parallel processing; SIMD; Graphics processing unit (GPU); GPGPU; Xbox 360; Heterogeneous devices},
  owner        = {ap8213},
  publisher    = {Springer US},
  timestamp    = {2014.10.09},
}

@InProceedings{Wolf2011,
  author    = {Wolf, Michael M. and Heroux, Michael A. and Boman, Erik G.},
  title     = {Factors Impacting Performance of Multithreaded Sparse Triangular Solve},
  booktitle = {{P}roceedings of the 9th {I}nternational {C}onference on {H}igh {P}erformance {C}omputing for {C}omputational {S}cience},
  date      = {2011},
  series    = {VECPAR '10},
  publisher = {Springer-Verlag},
  location  = {Berkeley, CA, USA},
  isbn      = {978-3-642-19327-9},
  pages     = {32--44},
  url       = {http://dl.acm.org/citation.cfm?id=1964246},
  abstract  = {As computational science applications grow more parallel with multi-core supercomputers having hundreds of thousands of computational cores, it will become increasingly difficult for solvers to scale. Our approach is to use hybrid MPI/threaded numerical algorithms to solve these systems in order to reduce the number of MPI tasks and increase the parallel efficiency of the algorithm. However, we need efficient threaded numerical kernels to run on the multi-core nodes in order to achieve good parallel efficiency. In this paper, we focus on improving the performance of a multithreaded triangular solver, an important kernel for preconditioning. We analyze three factors that affect the parallel performance of this threaded kernel and obtain good scalability on the multi-core nodes for a range of matrix sizes.},
  acmid     = {1964246},
  address   = {Berlin, Heidelberg},
  numpages  = {13},
}

@PhdThesis{Wolter2006,
  author      = {Wolter, Kati},
  title       = {Implementation of Cutting Plane Separators for Mixed Integer Programs},
  institution = {Technische Universität Berlin},
  date        = {2006},
  owner       = {andrea},
  timestamp   = {2015.09.28},
}

@InProceedings{Wu2009,
  author    = {Wu, Chih-Hung and Memik, Seda Ogrenci and Merothra, Sanjay},
  title     = {{FPGA} implementation of the interior-point algorithm with applications to collision detection},
  booktitle = {{P}roceedings of the 17th {IEEE} {S}ymposium on {F}ield {P}rogrammable {C}ustom {C}omputing {M}achines},
  date      = {2009},
  series    = {FCCM '09},
  location  = {Napa Valley, CA, USA},
  month     = {4},
  pages     = {295--298},
  doi       = {10.1109/FCCM.2009.38},
  abstract  = {The interior-point algorithm is a powerful method for solving a Linear Program (LP). A variety of optimization problems can be formulated as LPs. Often times the limiting factor of deploying an algorithm to solve LPs in a high performance system is the run-time efficiency. In this paper, we present the FPGA implementation of an affine interior-point algorithm that is designed to solve LPs. Specifically, we present the application of this algorithm to solving the LP for the real-time collision detection. The most important feature that distinguishes this particular algorithm from other collision detection methods is its superior ability to perform detection between pairs of objects undergoing fast rotational and translational motions.},
  keywords  = {field programmable gate arrays;linear programming;FPGA;collision detection;interior-point algorithm;linear program;optimization problems;Acceleration;Application software;Field programmable gate arrays;Hardware;Linear programming;Mathematical programming;Object detection;Parallel processing;Solids;Sparse matrices;inter-frame collision;interior point;linear programming},
  owner     = {andrea},
  timestamp = {2017.05.08},
}

@Article{Wu2013,
  Title                    = {A preconditioned conjugate gradient algorithm for {GeneRank} with application to microarray data mining},
  Author                   = {Wu, Gang and Xu, Wei and Zhang, Ying and Wei, Yimin},
  Number                   = {1},
  Pages                    = {27--56},
  Volume                   = {26},

 abstract = {The problem of identifying key genes is of fundamental importance in biology and medicine. The GeneRank model explores connectivity data to produce a prioritization of the genes in a microarray experiment that is less susceptible to variation caused by experimental noise than the one based on expression levels alone. The GeneRank algorithm amounts to solving an unsymmetric linear system. However, when the matrix in question is very large, the GeneRank algorithm is inefficient and even can be infeasible. On the other hand, the adjacency matrix is symmetric in the GeneRank model, while the original GeneRank algorithm fails to exploit the symmetric structure of the problem in question. In this paper, we discover that the GeneRank problem can be rewritten as a symmetric positive definite linear system, and propose a preconditioned conjugate gradient algorithm to solve it. Numerical experiments support our theoretical results, and show superiority of the novel algorithm.},
  Date                     = {2013},
  Doi                      = {10.1007/s10618-011-0245-7},
  ISSN                     = {1573-756X},
  Journaltitle             = {Data Mining and Knowledge Discovery},
  Owner                    = {andrea},
  Timestamp                = {2016.06.14}
}

@PhdThesis{Wunderling1996,
  author      = {Wunderling, Roland},
  title       = {Paralleler und objektorientierter {S}implex-{A}lgorithmus},
  institution = {Technische Universität Berlin},
  date        = {1996},
  language    = {German},
}

@InProceedings{Yoo2013,
  author    = {Yoo, Richard M. and Hughes, Christopher J. and Kim, Changkyu and Chen, Yen-Kuang and Kozyrakis, Christos},
  title     = {Locality-aware Task Management for Unstructured Parallelism: A Quantitative Limit Study},
  booktitle = {{P}roceedings of the 25th {A}nnual {ACM} {S}ymposium on {P}arallelism in {A}lgorithms and {A}rchitectures},
  date      = {2013},
  series    = {SPAA '13},
  publisher = {ACM},
  location  = {Montréal, QB, Canada},
  isbn      = {978-1-4503-1572-2},
  pages     = {315--325},
  doi       = {10.1145/2486159.2486175},
 abstract = {As we increase the number of cores on a processor die, the on-chip cache hierarchies that support these cores are getting larger, deeper, and more complex. As a result, non-uniform memory access effects are now prevalent even on a single chip. To reduce execution time and energy consumption, data access locality should be exploited. This is especially important for task-based programming systems, where a scheduler decides when and where on the chip the code segments, i.e., tasks, should execute. Capturing locality for structured task parallelism has been done effectively, but the more difficult case, unstructured parallelism, remains largely unsolved - little quantitative analysis exists to demonstrate the potential of locality-aware scheduling, and to guide future scheduler implementations in the most fruitful direction. \\ This paper quantifies the potential of locality-aware scheduling for unstructured parallelism on three different many-core processors. Our simulation results of 32-core systems show that locality-aware scheduling can bring up to 2.39$\times$ speedup over a randomized schedule, and 2.05$\times$ speedup over a state-of-the-art baseline scheduling scheme. At the same time, a locality-aware schedule reduces average energy consumption by 55\% and 47\%, relative to the random and the baseline schedule, respectively. In addition, our 1024-core simulation results project that these benefits will only increase: Compared to 32-core executions, we see up to 1.83$\times$ additional locality benefits. To capture such potentials in a practical setting, we also perform a detailed scheduler design space exploration to quantify the impact of different scheduling decisions. We also highlight the importance of locality-aware stealing, and demonstrate that a stealing scheme can exploit significant locality while performing load balancing. Over randomized stealing, our proposed scheme shows up to 2.0$\times$ speedup for stolen tasks.},
  acmid     = {2486175},
  address   = {New York, NY, USA},
  keywords  = {energy, locality, performance, task scheduling, task stealing},
  numpages  = {11},
}

@InProceedings{Zhang2006,
  Title                    = {A Hierarchical Model of Data Locality},
  Author                   = {Zhang, Chengliang and Ding, Chen and Ogihara, Mitsunori and Zhong, Yutao and Wu, Youfeng},
  Booktitle                = {{C}onference {R}ecord of the 33rd {ACM SIGPLAN-SIGACT} {S}ymposium on {P}rinciples of {P}rogramming {L}anguages},

  Address                  = {New York, NY, USA},
  Pages                    = {16--29},
  Publisher                = {ACM},
  Series                   = {POPL '06},

 abstract = {In POPL 2002, Petrank and Rawitz showed a universal result---finding optimal data placement is not only NP-hard but also impossible to approximate within a constant factor if P $\neq$ NP. Here we study a recently published concept called reference affinity, which characterizes a group of data that are always accessed together in computation. On the theoretical side, we give the complexity for finding reference affinity in program traces, using a novel reduction that converts the notion of distance into satisfiability. We also prove that reference affinity automatically captures the hierarchical locality in divide-and-conquer computations including matrix solvers and N-body simulation. The proof establishes formal links between computation patterns in time and locality relations in space.On the practical side, we show that efficient heuristics exist. In particular, we present a sampling method and show that it is more effective than the previously published technique, especially for data that are often but not always accessed together. We show the effect on generated and real traces. These theoretical and empirical results demonstrate that effective data placement is still attainable in general-purpose programs because common (albeit not all) locality patterns can be precisely modeled and efficiently analyzed.},
  Acmid                    = {1111040},
  Date                     = {2006},
  Doi                      = {10.1145/1111037.1111040},
  ISBN                     = {1-59593-027-2},
  Keywords                 = {N-body simulation, NP-complete, hierarchical data placement, program locality, reference affinity, volume distance},
  Location                 = {Charleston, SC, USA},
  Numpages                 = {14}
}

@InProceedings{Zhang2009,
  author    = {Zhang, Y. and Shalabi, Y. H. and Jain, R. and Nagar, K. K. and Bakos, J. D.},
  title     = {{FPGA} vs. {GPU} for sparse matrix vector multiply},
  booktitle = {Proceedings of the 2009 International Conference on Field-Programmable Technology},
  date      = {2009},
  series    = {FPT '09},
  location  = {Sydney, NSW, AUS},
  month     = dec,
  pages     = {255--262},
  doi       = {10.1109/FPT.2009.5377620},
  abstract  = {Sparse matrix-vector multiplication (SpMV) is a common operation in numerical linear algebra and is the computational kernel of many scientific applications. It is one of the original and perhaps most studied targets for FPGA acceleration. Despite this, GPUs, which have only recently gained both general-purpose programmability and native support for double precision floating-point arithmetic, are viewed by some as a more effective platform for SpMV and similar linear algebra computations. In this paper, we present an analysis comparing an existing GPU SpMV implementation to our own, novel FPGA implementation. In this analysis, we describe the challenges faced by any SpMV implementation, the unique approaches to these challenges taken by both FPGA and GPU implementations, and their relative performance for SpMV.},
  keywords  = {coprocessors;field programmable gate arrays;sparse matrices;FPGA acceleration;FPGA implementation;GPU implementation;SpMV implementation;computational kernel;field programmable gate arrays;graphics processing unit;linear algebra computation;numerical linear algebra;precision floating-point arithmetic;programmability;sparse matrix vector multiplication;Acceleration;Biological system modeling;Biology computing;Computer architecture;Field programmable gate arrays;Kernel;Linear algebra;Scientific computing;Sparse matrices;Vectors},
}

@Article{Zhao2015,
  author       = {Zhao, C. and Dong, S. and Li, F. and Song, Y.},
  title        = {Optimal home energy management system with mixed types of loads},
  journaltitle = {{CSEE} Journal of Power and Energy Systems},
  date         = {2015},
  series       = {PP},
  volume       = {1},
  number       = {4},
  month        = dec,
  pages        = {29--37},
  doi          = {10.17775/CSEEJPES.2015.00045},
  abstract     = {This paper presents a novel home area energy management system (HEMS) for smart homes with different load profiles installed with photovoltaic generation, energy storage, and DC demand. The developed HEMS is shown to optimize the utilization of local renewables while minimizing energy waste between AC and DC conversions and between storage charging and discharging. Previous studies on HEMS have not considered the impact of load types. In this study, the performance of the proposed HEMS is demonstrated on different smart homes with and without electric heating. A comparative study is carried out to investigate battery behavior, characteristics of AC and DC conversion, and benefits that customers receive. A sensitivity analysis is also conducted to discuss the effects from varied energy storage capacities, AC/DC conversion efficiencies, and PV output. The results show that cost reduction in energy bills can be greatly impacted by load profiles, and customers with electric heating load coupled with sufficiently large energy storage would receive the most reduction in their energy bills.},
}

@Article{Zhong2014,
  author       = {Zhong, Jianlong and He, Bingsheng},
  title        = {Medusa: Simplified Graph Processing on {GPUs}},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2014},
  volume       = {25},
  number       = {6},
  month        = {6},
  pages        = {1543--1552},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2013.111},
 abstract = {Graphs are common data structures for many applications, and efficient graph processing is a must for application performance. Recently, the graphics processing unit (GPU) has been adopted to accelerate various graph processing algorithms such as BFS and shortest paths. However, it is difficult to write correct and efficient GPU programs and even more difficult for graph processing due to the irregularities of graph structures. To simplify graph processing on GPUs, we propose a programming framework called Medusa which enables developers to leverage the capabilities of GPUs by writing sequential C/C++ code. Medusa offers a small set of user-defined APIs and embraces a runtime system to automatically execute those APIs in parallel on the GPU. We develop a series of graph-centric optimizations based on the architecture features of GPUs for efficiency. Additionally, Medusa is extended to execute on multiple GPUs within a machine. Our experiments show that 1) Medusa greatly simplifies implementation of GPGPU programs for graph processing, with many fewer lines of source code written by developers and 2) the optimization techniques significantly improve the performance of the runtime system, making its performance comparable with or better than manually tuned GPU graph operations.},
  acmid        = {2707627},
  issue_date   = {June 2014},
  location     = {Piscataway, NJ, USA},
  numpages     = {10},
  publisher    = {IEEE Press},
}

@Proceedings{Feo2011,
  title     = {Proceedings of the First Workshop on Irregular Applications: Architectures and Algorithms},
  year      = {2011},
  editor    = {John Feo and Oreste Villa and Antonino Tumeo and Simone Secchi},
  series    = {IAAA '11},
  publisher = {ACM},
  location  = {Seattle, WA, USA},
  isbn      = {978-1-4503-1121-2},
  pages     = {1--2},
  doi       = {10.1145/2089142.2089144},
  abstract  = {Irregular applications are characterized by irregular data structures, control and communication patterns. Novel irregular high performance applications which deal with large data sets and require have recently appeared. Unfortunately, current high performance systems and software infrastructures executes irregular algorithms poorly. Only coordinated efforts by end user, area specialists and computer scientists that consider both the architecture and the software stack may be able to provide solutions to the challenges of modern irregular applications.},
  acmid     = {2089144},
  address   = {New York, NY, USA},
  booktitle = {{P}roceedings of the {F}irst {W}orkshop on {I}rregular {A}pplications: {A}rchitectures and {A}lgorithms},
  keywords  = {algorithms, architectures, irregular applications},
  numpages  = {2},
  owner     = {andrea},
  timestamp = {2016.09.07},
}

@Manual{Altera2013,
  Title                    = {Implementing {FPGA} Design with the {OpenCL} Standard},
  Month                    = nov,

  Date                     = {2013},
  Institution              = {Altera Corporation},
  Owner                    = {andrea},
  Timestamp                = {2015.01.30},
  Type                     = {Whitepaper},
  Url                      = {http://www.altera.co.uk/literature/wp/wp-01173-opencl.pdf}
}

@TechReport{Amd2012,
  Title                    = {{AMD} Graphics Cores Next ({GCN}) Architecture},
  Institution              = {{AMD} Corporation},
  Note                     = {Accessed on 21 June 2017},
  Type                     = {Online},

  Date                     = {2012},
  Url                      = {https://www.amd.com/Documents/GCN_Architecture_whitepaper.pdf}
}

@Misc{Amd2014,
  Title                    = {{AMD} {F}irePro {W}5000},
  HowPublished             = {Online},

  Date                     = {2014},
  Organization             = {AMD Corporation},
  Owner                    = {andrea},
  Timestamp                = {2016.03.17},
  Url                      = {http://www.amd.com/documents/2795_W5000_DataSheet_R3.pdf}
}

@Online{Clsparse2016,
  Title                    = {{clSPARSE} Documentation},
  Date                     = {2016},
  Url                      = {http://clmathlibraries.github.io/clSPARSE/},

  Note                     = {Accessed on 10 May 2017},

  Owner                    = {andrea},
  Timestamp                = {2017.05.10},
  Version                  = {0.10.0.0}
}

@Manual{CPLEX2011,
  title        = {{CPLEX} User's manual},
  date         = {2011},
  edition      = {12.4},
  note         = {Accessed 30 March 2014},
  organization = {IBM corporation},
  url          = {http://pic.dhe.ibm.com/infocenter/cosinfoc/v12r4/topic/ilog.odms.studio.help/pdf/usrcplex.pdf},
  owner        = {andrea},
  timestamp    = {2014.03.30},
}

@Manual{Cublas2015,
  Title                    = {{CUBLAS} Library User Guide},
  Edition                  = {7.5},
  Month                    = oct,
  Organization             = {NVIDIA},

  Date                     = {2015},
  Owner                    = {andrea},
  Timestamp                = {2016.11.07},
  Url                      = {http://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf}
}

@Manual{CUDA2013,
  title        = {{CUDA} Toolkit Documentation},
  date         = {2013},
  language     = {English},
  edition      = {5.5},
  note         = {Accessed 1 March 2014},
  organization = {{Nvidia corporation}},
  url          = {http://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html},
  month        = oct,
  owner        = {andrea},
  timestamp    = {2014.03.05},
}

@TechReport{Cuda65P,
  Title                    = {{CUDA} 6.5 performance report},
  Institution              = {NVIDIA Corporation},
  Month                    = sep,
  Note                     = {Accessed 20 July 2016},

  Date                     = {2014},
  Owner                    = {andrea},
  Timestamp                = {2016.11.07},
  Url                      = {http://developer.download.nvidia.com/compute/cuda/6_5/rel/docs/CUDA_6.5_Performance_Report.pdf}
}

@TechReport{Cuda70P,
  Title                    = {{CUDA} 7.0 performance report},
  Institution              = {NVIDIA Corporation},
  Month                    = may,
  Note                     = {Accessed 20 July 2016},

  Date                     = {2015},
  Owner                    = {andrea},
  Timestamp                = {2016.11.07},
  Url                      = {http://developer.download.nvidia.com/compute/cuda/compute-docs/cuda-performance-report.pdf}
}

@TechReport{CudaK20B,
  Title                    = {{NVIDIA} Tesla {K20}-{K20X} {GPU} Accelerators -- Benchmarks},
  Institution              = {NVIDIA Corporation},
  Note                     = {Accessed 20 July 2016},

  Date                     = {2012},
  Owner                    = {andrea},
  Timestamp                = {2016.11.07},
  Url                      = {http://www.nvidia.com/docs/IO/122874/K20-and-K20X-application-performance-technical-brief.pdf}
}

@Manual{Cusparse2015,
  Title                    = {{CUSPARSE} library},
  Edition                  = {7.0},
  Month                    = mar,
  Note                     = {Accessed 1 September 2015.},
  Organization             = {NVIDIA corporation},

  Date                     = {2015},
  Owner                    = {ap8213},
  Timestamp                = {2015.09.28},
  Url                      = {https://developer.nvidia.com/cuda-toolkit-70}
}

@Manual{GLPK2010,
  title        = {{GNU} Linear Programming Kit Reference Manual},
  date         = {2010},
  organization = {Free Software Foundation},
  url          = {http://kam.mff.cuni.cz/~elias/glpk.pdf},
  owner        = {andrea},
  timestamp    = {2014.12.14},
}

@Misc{ImperialHpc,
  title = {Imperial College High Performance Computing Service},
  date  = {2017},
  url   = {http://www.imperial.ac.uk/admin-services/ict/self-service/research-support/hpc/},
}

@TechReport{Intel2015,
  title       = {The Compute Architecture of Intel Processor Graphics {Gen9}},
  institution = {Intel Corporation},
  date        = {2015},
  url         = {https://software.intel.com/sites/default/files/managed/c5/9a/The-Compute-Architecture-of-Intel-Processor-Graphics-Gen9-v1d0.pdf},
  owner       = {andrea},
  timestamp   = {2017.04.27},
}

@Misc{IntelI3,
  title        = {Intel Core i3-2130 Processor},
  date         = {2011},
  howpublished = {Online},
  note         = {Accessed 13 October 2016},
  organization = {Intel Corporation},
  url          = {http://ark.intel.com/products/53428/Intel-Core-i3-2130-Processor-3M-Cache-3_40-GHz},
  owner        = {andrea},
  timestamp    = {2016.03.17},
}

@Misc{IntelXeonE5,
  Title                    = {{Intel} {Xeon} Processor {E5-2640 v3}},
  HowPublished             = {Online},
  Note                     = {Accessed 13 October 2016},

  Date                     = {2014},
  Organization             = {Intel Corporation},
  Owner                    = {andrea},
  Timestamp                = {2016.03.17},
  Url                      = {http://ark.intel.com/products/83359/Intel-Xeon-Processor-E5-2640-v3-20M-Cache-2_60-GHz}
}

@Misc{Nvidia2013,
  Title                    = {{NVIDIA} professional graphics solution},
  HowPublished             = {Online},
  Month                    = jul,

  Date                     = {2013},
  Organization             = {NVIDIA Corporation},
  Owner                    = {andrea},
  Timestamp                = {2016.03.17},
  Url                      = {http://www.nvidia.co.uk/content/PDF/line_card/6660-nv-prographicssolutions-linecard-july13-final-lr.pdf}
}

@TechReport{NvidiaFermi2009,
  Title                    = {{NVIDIA}'s Next Generation {CUDA} Compute Architecture: Fermi},
  Institution              = {NVIDIA Corporation},

  Date                     = {2009},
  Url                      = {http://www.nvidia.it/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf}
}

@Misc{NvidiaGt430,
  Title                    = {{NVIDIA} {GeForce} {GT} 430},
  HowPublished             = {Online},
  Note                     = {Accessed 13 October 2016},

  Date                     = {2010},
  Organization             = {NVIDIA corporation},
  Owner                    = {andrea},
  Timestamp                = {2016.03.17},
  Url                      = {http://www.nvidia.com/object/product-geforce-gt-430-oem-us.html}
}

@Misc{NvidiaK80,
  Title                    = {{NVIDIA} Tesla {GPU} Accelerators},
  HowPublished             = {Online},
  Note                     = {Accessed 13 October 2016},

  Date                     = {2014},
  Organization             = {NVIDIA corporation},
  Owner                    = {andrea},
  Timestamp                = {2016.03.17},
  Url                      = {http://international.download.nvidia.com/pdf/kepler/TeslaK80-datasheet.pdf}
}

@Manual{OpenMP2008,
  title        = {{OpenMP} application program interface},
  date         = {2008},
  edition      = {3},
  organization = {{OpenMP Architecture Review Board}},
  url          = {http://www.openmp.org/mp-documents/spec30.pdf},
  month        = may,
  owner        = {andrea},
  timestamp    = {2014.03.28},
}

@Misc{Oracle2014,
  Title                    = {x86 Assembly Language Reference Manual},
  HowPublished             = {Online},
  Note                     = {Accessed 4 June 2017},

  Date                     = {2014},
  Organization             = {Oracle},
  Url                      = {http://docs.oracle.com/cd/E36784_01/html/E36859/docinfo.html#scrolltoc}
}

@Manual{Paralution2016,
  title     = {Paralution - User Manual},
  date      = {2016},
  version   = {1.1.0},
  url       = {http://www.paralution.com/downloads/paralution-um.pdf},
  owner     = {andrea},
  timestamp = {2017.05.10},
}

@Online{Phist2016,
  Title                    = {{PHIST} - a Pipelined Hybrid Parallel Iterative Solver Toolkit},
  Date                     = {2016},
  Url                      = {https://bitbucket.org/essex/phist/wiki/Getting_Started/Overview},

  Note                     = {Git repository, Accessed 10 May 2017},

  Owner                    = {andrea},
  Timestamp                = {2017.05.10}
}

@Manual{SDAccel2014,
  Title                    = {The {Xilinx} {SDAccel} Development Environment},

  Date                     = {2014},
  Institution              = {Xilinx Inc.},
  Owner                    = {andrea},
  Timestamp                = {2015.01.30},
  Url                      = {http://www.xilinx.com/publications/prod_mktg/sdnet/sdaccel-backgrounder.pdf}
}

@Online{Spral2014,
  Title                    = {The Sparse Parallel Robust Algorithms Library ({SPRAL})},
  Date                     = {2014},
  Url                      = {http://www.numerical.rl.ac.uk/spral/},
  Organization             = {Numerical Analysis Group, Rutherford Appleton Laboratory}
}

@Manual{Virtex7,
  title       = {7 Series {FPGA} Overview},
  date        = {2014},
  type        = {Product Specification},
  number      = {1.16.1},
  url         = {https://www.xilinx.com/support/documentation/data_sheets/ds180_7Series_Overview.pdf},
  institution = {Xilinx Inc.},
  month       = dec,
  owner       = {andrea},
  timestamp   = {2015.01.29},
}

@Manual{Kincaid1989,
  author = {Kincaid, D. R. and Oppe, T. C. and Young, D. M.},
  title  = {{ITPACKV} {2D} User's Guide},
  date   = {1989},
  url    = {http://www.netlib.no/netlib/itpack/index.html},
  month  = may,
}

@InProceedings{Liu2015,
  author    = {Liu, Weifeng and Vinter, Brian},
  title     = {{CSR5}: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication},
  booktitle = {Proceedings of the 29th {ACM} International Conference on Supercomputing},
  date      = {2015},
  series    = {ICS '15},
  publisher = {ACM},
  location  = {Newport Beach, CA, USA},
  isbn      = {978-1-4503-3559-1},
  pages     = {339--350},
  doi       = {10.1145/2751205.2751209},
 abstract = {Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations.\\We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6\%, 28.5\%, 173.0\% and 293.3\% (up to 213.3\%, 153.6\%, 405.1\% and 943.3\%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.},
  acmid     = {2751209},
  address   = {New York, NY, USA},
  keywords  = {cpu, csr, csr5, gpu, sparse matrices, spmv, storage formats, xeon phi},
  numpages  = {12},
}

@Article{Fong2011,
  author       = {Fong, David Chin-Lung and Saunders, Michael},
  title        = {{LSMR}: An Iterative Algorithm for Sparse Least-Squares Problems},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2011},
  volume       = {33},
  number       = {5},
  pages        = {2950--2971},
  doi          = {10.1137/10079687X},
  abstract     = {An iterative method LSMR is presented for solving linear systems $Ax=b$ and least-squares problems $\min \|Ax-b\|_2$, with A being sparse or a fast linear operator. LSMR is based on the Golub--Kahan bidiagonalization process. It is analytically equivalent to the MINRES method applied to the normal equation $A^T\! Ax = A^T\! b$, so that the quantities $\|A^T\! r_k\|$ are monotonically decreasing (where $r_k = b - Ax_k$ is the residual for the current iterate $x_k$). We observe in practice that $\|r_k\|$ also decreases monotonically, so that compared to LSQR (for which only $\|r_k\|$ is monotonic) it is safer to terminate LSMR early. We also report some experiments with reorthogonalization.},
}

@Article{Agullo2009,
  author       = {Agullo, Emmanuel and Demmel, Jim and Dongarra, Jack and Hadri, Bilel and Kurzak, Jakub and Langou, Julien and Ltaief, Hatem and Luszczek, Piotr and Tomov, Stanimire},
  title        = {Numerical linear algebra on emerging architectures: The {PLASMA} and {MAGMA} projects},
  journaltitle = {Journal of Physics: Conference Series},
  date         = {2009},
  volume       = {180},
  number       = {1},
  pages        = {012037},
  url          = {http://stacks.iop.org/1742-6596/180/i=1/a=012037},
  abstract     = {The emergence and continuing use of multi-core architectures and graphics processing units require changes in the existing software and sometimes even a redesign of the established algorithms in order to take advantage of now prevailing parallelism. Parallel Linear Algebra for Scalable Multi-core Architectures (PLASMA) and Matrix Algebra on GPU and Multics Architectures (MAGMA) are two projects that aims to achieve high performance and portability across a wide range of multi-core architectures and hybrid systems respectively. We present in this document a comparative study of PLASMA's performance against established linear algebra packages and some preliminary results of MAGMA on hybrid multi-core and GPU systems.},
}

@InProceedings{Shinano2012,
  author    = {Shinano, Yuji and Achterberg, Tobias and Berthold, Timo and Heinz, Stefan and Koch, Thorsten},
  title     = {{ParaSCIP}: a parallel extension of {SCIP}},
  booktitle = {Competence in High Performance Computing},
  date      = {2012},
  editor    = {Christian Bischof and Heinz-Gerd Hegering and Wolfgang Nagel and Gabriel Wittum},
  pages     = {135--148},
  doi       = {10.1007/978-3-642-24025-6_12},
  abstract  = {Mixed integer programming (MIP) has become one of the most important techniques in Operations Research and Discrete Optimization. SCIP (Solving Constraint Integer Programs) is currently one of the fastest non-commercial MIP solvers. It is based on the branch-and-bound procedure in which the problem is recursively split into smaller subproblems, thereby creating a so-called branching tree. We present ParaSCIP, an extension of SCIP, which realizes a parallelization on a distributed memory computing environment. ParaSCIP uses SCIP solvers as independently running processes to solve subproblems (nodes of the branching tree) locally. This makes the parallelization development independent of the SCIP development. Thus, ParaSCIP directly profits from any algorithmic progress in future versions of SCIP. Using a first implementation of ParaSCIP, we were able to solve two previously unsolved instances from MIPLIB2003, a standard test set library for MIP solvers. For these computations, we used up to 2048 cores of the HLRN~II supercomputer.},
}

@TechReport{Shinano2013,
  author      = {Shinano, Yuji and Heinz, Stefan and Vigerske, Stefan and Winkler, Michael},
  title       = {{FiberSCIP}: A shared memory parallelization of {SCIP}},
  institution = {ZIB},
  date        = {2013},
  number      = {13--55},
  location    = {Takustr.7, 14195 Berlin},
  abstract    = {Recently, parallel computing environments have become significantly popular. In order to obtain the benefit of using parallel computing environments, we have to deploy our programs for these effectively. This paper focuses on a parallelization of SCIP (Solving Constraint Integer Programs), which is a MIP solver and constraint integer programming framework available in source code. There is a parallel extension of SCIP named ParaSCIP, which parallelizes SCIP on massively parallel distributed memory computing environments. This paper describes FiberSCIP, which is yet another parallel extension of SCIP to utilize multi-threaded parallel computation on shared memory computing environments, and has the following contributions: First, the basic concept of having two parallel extensions and the relationship between them and the parallelization framework provided by UG (Ubiquity Generator) is presented, including an implementation of deterministic parallelization. Second, the difficulties to achieve a good performance that utilizes all resources on an actual computing environment and the difficulties of performance evaluation of the parallel solvers are discussed. Third, a way to evaluate the performance of new algorithms and parameter settings of the parallel extensions is presented. Finally, current performance of FiberSCIP for solving mixed-integer linear programs (MIPs) and mixed-integer non-linear programs (MINLPs) in parallel is demonstrated.},
  urn         = {urn:nbn:de:0297-zib-42595},
}

@InProceedings{Bell2009,
  author    = {Bell, Nathan and Garland, Michael},
  title     = {Implementing Sparse Matrix-vector Multiplication on Throughput-oriented Processors},
  booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
  date      = {2009},
  series    = {SC '09},
  publisher = {ACM},
  location  = {Portland, Oregon},
  isbn      = {978-1-60558-744-8},
  pages     = {1--11},
  doi       = {10.1145/1654059.1654078},
 abstract = {Sparse matrix-vector multiplication (SpMV) is of singular importance in sparse linear algebra. In contrast to the uniform regularity of dense linear algebra, sparse operations encounter a broad spectrum of matrices ranging from the regular to the highly irregular. Harnessing the tremendous potential of throughput-oriented processors for sparse operations requires that we expose substantial fine-grained parallelism and impose sufficient regularity on execution paths and memory access patterns. We explore SpMV methods that are well-suited to throughput-oriented architectures like the GPU and which exploit several common sparsity classes. The techniques we propose are efficient, successfully utilizing large percentages of peak bandwidth. Furthermore, they deliver excellent total throughput, averaging 16 GFLOP/s and 10 GFLOP/s in double precision for structured grid and unstructured mesh matrices, respectively, on a GeForce GTX 285. This is roughly 2.8 times the throughput previously achieved on Cell BE and more than 10 times that of a quad-core Intel Clovertown system.},
  acmid     = {1654078},
  address   = {New York, NY, USA},
  articleno = {18},
  numpages  = {11},
}

@Article{Rafique2015,
  author       = {Rafique, A. and Constantinides, G. A. and Kapre, N.},
  title        = {Communication Optimization of Iterative Sparse Matrix-Vector Multiply on {GPUs} and {FPGAs}},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2015},
  volume       = {26},
  number       = {1},
  month        = {1},
  pages        = {24--34},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2014.6},
  abstract     = {Trading communication with redundant computation can increase the silicon efficiency of FPGAs and GPUs in accelerating communication-bound sparse iterative solvers. While k iterations of the iterative solver can be unrolled to provide O(k) reduction in communication cost, the extent of this unrolling depends on the underlying architecture, its memory model, and the growth in redundant computation. This paper presents a systematic procedure to select this algorithmic parameter k, which provides communication-computation tradeoff on hardware accelerators like FPGA and GPU. We provide predictive models to understand this tradeoff and show how careful selection of k can lead to performance improvement that otherwise demands significant increase in memory bandwidth. On an Nvidia C2050 GPU, we demonstrate a 1.9$\times$-42.6$\times$ speedup over standard iterative solvers for a range of benchmarks and that this speedup is limited by the growth in redundant computation. In contrast, for FPGAs, we present an architecture-aware algorithm that limits off-chip communication but allows communication between the processing cores. This reduces redundant computation and allows large k and hence higher speedups. Our approach for FPGA provides a 0.3$\times$-4.4$\times$ speedup over same-generation GPU devices where k is picked carefully for both architectures for a range of benchmark.},
  keywords     = {field programmable gate arrays;graphics processing units;iterative methods;mathematics computing;matrix multiplication;FPGA;Nvidia C2050 GPU;algorithmic parameter;architecture-aware algorithm;communication cost reduction;communication optimization;communication-bound sparse iterative solvers;communication-computation tradeoff;field programmable gate array;graphics processing unit;hardware accelerators;iterative sparse matrix-vector multiplication;off-chip communication;processing core communication;Field programmable gate arrays;Graphics processing units;Instruction sets;Kernel;Registers;Sparse matrices;Vectors;Iterative numerical methods;field programmable gate arrays (FPGAs);graphics processing units (GPUs);matrix powers kernel;spare matrix-vector multiply},
}

@Article{Li2013,
  author       = {Li, Ruipeng and Saad, Yousef},
  title        = {{GPU}-accelerated Preconditioned Iterative Linear Solvers},
  journaltitle = {Journal of Supercomputing},
  date         = {2013},
  volume       = {63},
  number       = {2},
  month        = feb,
  pages        = {443--466},
  issn         = {0920-8542},
  doi          = {10.1007/s11227-012-0825-3},
 abstract = {This work is an overview of our preliminary experience in developing a high-performance iterative linear solver accelerated by GPU coprocessors. Our goal is to illustrate the advantages and difficulties encountered when deploying GPU technology to perform sparse linear algebra computations. Techniques for speeding up sparse matrix-vector product (SpMV) kernels and finding suitable preconditioning methods are discussed. Our experiments with an NVIDIA TESLA M2070 show that for unstructured matrices SpMV kernels can be up to 8 times faster on the GPU than the Intel MKL on the host Intel Xeon X5675 Processor. Overall performance of the GPU-accelerated Incomplete Cholesky (IC) factorization preconditioned CG method can outperform its CPU counterpart by a smaller factor, up to 3, and GPU-accelerated The incomplete LU (ILU) factorization preconditioned GMRES method can achieve a speed-up nearing 4. However, with better suited preconditioning techniques for GPUs, this performance can be further improved.},
  acmid        = {2434703},
  issue_date   = {February 2013},
  keywords     = {GPU computing, Preconditioned iterative methods, Sparse matrix computations},
  location     = {Hingham, MA, USA},
  numpages     = {24},
  publisher    = {Kluwer Academic Publishers},
}

@InProceedings{Bolz2003,
  author    = {Bolz, Jeff and Farmer, Ian and Grinspun, Eitan and Schröder, Peter},
  title     = {Sparse Matrix Solvers on the {GPU}: Conjugate Gradients and Multigrid},
  booktitle = {ACM SIGGRAPH 2003 Papers},
  date      = {2003},
  series    = {SIGGRAPH '03},
  publisher = {ACM},
  location  = {San Diego, California},
  isbn      = {1-58113-709-5},
  pages     = {917--924},
  doi       = {10.1145/1201775.882364},
 abstract = {Many computer graphics applications require high-intensity numerical simulation. We show that such computations can be performed efficiently on the GPU, which we regard as a full function streaming processor with high floating-point performance. We implemented two basic, broadly useful, computational kernels: a sparse matrix conjugate gradient solver and a regular-grid multigrid solver. Real time applications ranging from mesh smoothing and parameterization to fluid solvers and solid mechanics can greatly benefit from these, evidence our example applications of geometric flow and fluid simulation running on NVIDIA's GeForce FX.},
  acmid     = {882364},
  address   = {New York, NY, USA},
  keywords  = {GPU computing, Navier-Stokes, conjugate gradient, fluid simulation, mesh smoothing, multigrid, numerical simulation},
  numpages  = {8},
}

@Article{Buatois2009,
  author       = {Buatois, Luc and Caumon, Guillaume and Lévy, Bruno},
  title        = {Concurrent number cruncher: a {GPU} implementation of a general sparse linear solver},
  journaltitle = {International Journal of Parallel, Emergent and Distributed Systems},
  date         = {2009},
  volume       = {24},
  number       = {3},
  pages        = {205--223},
  doi          = {10.1080/17445760802337010},
  abstract     = {A wide class of numerical methods needs to solve a linear system, where the matrix pattern of non-zero coefficients can be arbitrary. These problems can greatly benefit from highly multithreaded computational power and large memory bandwidth available on graphics processor units (GPUs), especially since dedicated general purpose APIs such as close-to-metal (CTM) (AMD--ATI) and compute unified device architecture (CUDA) (NVIDIA) have appeared. CUDA even provides a BLAS implementation, but only for dense matrices (CuBLAS). Other existing linear solvers for the GPU are also limited by their internal matrix representation. This paper describes how to combine recent GPU programming techniques and new GPU dedicated APIs with high performance computing strategies (namely block compressed row storage (BCRS), register blocking and vectorization), to implement a sparse general-purpose linear solver. Our implementation of the Jacobi-preconditioned conjugate gradient algorithm outperforms by up to a factor of 6.0$\times$ leading-edge CPU counterparts, making it attractive for applications which are content with single precision.},
}

@Manual{Gurobi2017,
  title        = {Gurobi Optimizer Reference Manual},
  date         = {2017},
  version      = {7.0},
  organization = {Gurobi Optimization},
  url          = {http://www.gurobi.com/documentation/7.0/refman.pdf},
  institution  = {Xilinx Inc.},
  month        = dec,
  owner        = {andrea},
  timestamp    = {2015.01.29},
}

@Article{Saad2010,
  author       = {Saad, Y. and Yeung, M. and Erhel, J. and Guyomarc'h, F.},
  title        = {A Deflated Version of the Conjugate Gradient Algorithm},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2000},
  volume       = {21},
  number       = {5},
  pages        = {1909--1926},
  doi          = {10.1137/S1064829598339761},
  abstract     = {We present a deflated version of the conjugate gradient algorithm for solving linear systems. The new algorithm can be useful in cases when a small number of eigenvalues of the iteration matrix are very close to the origin. It can also be useful when solving linear systems with multiple right-hand sides, since the eigenvalue information gathered from solving one linear system can be recycled for solving the next systems and then updated.},
}

@InProceedings{Wong2010,
  author    = {Wong, H. and Papadopoulou, M. M. and Sadooghi-Alvandi, M. and Moshovos, A.},
  title     = {Demystifying {GPU} microarchitecture through microbenchmarking},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems Software},
  date      = {2010},
  series    = {ISPASS '10},
  month     = mar,
  pages     = {235--246},
  doi       = {10.1109/ISPASS.2010.5452013},
  abstract  = {Graphics processors (GPU) offer the promise of more than an order of magnitude speedup over conventional processors for certain non-graphics computations. Because the GPU is often presented as a C-like abstraction (e.g., Nvidia's CUDA), little is known about the characteristics of the GPU's architecture beyond what the manufacturer has documented. This work develops a microbechmark suite and measures the CUDA-visible architectural characteristics of the Nvidia GT200 (GTX280) GPU. Various undisclosed characteristics of the processing elements and the memory hierarchies are measured. This analysis exposes undocumented features that impact program performance and correctness. These measurements can be useful for improving performance optimization, analysis, and modeling on this architecture and offer additional insight on the decisions made in developing this GPU.},
  keywords  = {computer graphics;coprocessors;GPU microarchitecture;Nvidia GT200 GPU;graphics processors;microbenchmarking;Clocks;Computer architecture;Delay;Hardware;Kernel;Microarchitecture;Performance analysis;Registers;Samarium;Yarn},
}

@Manual{Nvprof2015,
  title        = {Profiler User's Guide},
  date         = {2015},
  edition      = {7.0},
  note         = {Accessed 12 July 2017.},
  organization = {NVIDIA corporation},
  url          = {http://docs.nvidia.com/cuda/profiler-users-guide/index.html#axzz4mitW5BwQ},
  month        = mar,
  owner        = {ap8213},
  timestamp    = {2015.09.28},
}

@Manual{CudaProgramming2015,
  title        = {{CUDA} {C} Programming Guide},
  date         = {2015},
  edition      = {7.5},
  organization = {NVIDIA},
  url          = {http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#axzz4mitW5BwQ},
  month        = oct,
  owner        = {andrea},
  timestamp    = {2016.11.07},
}

@Manual{OpenCl2010,
  editor       = {Aaftab Munshi},
  title        = {The {OpenCL} Specification},
  date         = {2010},
  version      = {1.1},
  organization = {Khronos {OpenCL} Working Group},
  url          = {https://www.khronos.org/registry/OpenCL/specs/opencl-1.1.pdf},
}

@Article{Byrd2000,
  author       = {Byrd, Richard H. and Gilbert, Jean Charles and Nocedal, Jorge},
  title        = {A trust region method based on interior point techniques for nonlinear programming},
  journaltitle = {Mathematical Programming},
  date         = {2000-11-01},
  volume       = {89},
  number       = {1},
  pages        = {149--185},
  issn         = {1436-4646},
  doi          = {10.1007/PL00011391},
 abstract = {An algorithm for minimizing a nonlinear function subject to nonlinear inequality constraints is described. It applies sequential quadratic programming techniques to a sequence of barrier problems, and uses trust regions to ensure the robustness of the iteration and to allow the direct use of second order derivatives. This framework permits primal and primal-dual steps, but the paper focuses on the primal version of the new algorithm. An analysis of the convergence properties of this method is presented.},
  day          = {01},
}

@Misc{IntelI7,
  title        = {Intel Core i7-3770 Processor},
  date         = {2012},
  howpublished = {Online},
  note         = {Accessed 13 October 2016},
  organization = {Intel Corporation},
  url          = {http://ark.intel.com/products/65719/Intel-Core-i7-3770-Processor-8M-Cache-up-to-3_90-GHz},
  owner        = {andrea},
  timestamp    = {2016.03.17},
}

@InProceedings{Picciau2016,
  author    = {Picciau, A. and Inggs, G. E. and Wickerson, J. and Kerrigan, E. C. and Constantinides, G. A.},
  title     = {Balancing Locality and Concurrency: Solving Sparse Triangular Systems on {GPUs}},
  booktitle = {Proceedings of the 23rd {IEEE} International Conference on High Performance Computing},
  date      = {2016-12},
  series    = {HiPC '16},
  location  = {Hyderabad, IN},
  pages     = {183--192},
  doi       = {10.1109/HiPC.2016.030},
  abstract  = {Many numerical optimisation problems rely on fast algorithms for solving sparse triangular systems of linear equations (STLs). To accelerate the solution of such equations, two types of approaches have been used: on GPUs, concurrency has been prioritised to the disadvantage of data locality, while on multi-core CPUs, data locality has been prioritised to the disadvantage of concurrency. In this paper, we discuss the interaction between data locality and concurrency in the solution of STLs on GPUs, and we present a new algorithm that balances both. We demonstrate empirically that, subject to there being enough concurrency available in the input matrix, our algorithm outperforms Nvidia's concurrency-prioritising CUSPARSE algorithm for GPUs. Experimental results show a maximum speedup of 5.8-fold. Our solution algorithm, which we have implemented in OpenCL, requires a pre-processing phase that partitions the graph associated with the input matrix into sub-graphs, whose data can be stored in low-latency local memories. This preliminary analysis phase is expensive, but because it depends only on the input matrix, its cost can be amortised when solving for many different right-hand sides.},
  keywords  = {concurrency control;data handling;graph theory;graphics processing units;matrix algebra;parallel algorithms;storage management;GPU;Nvidia concurrency-prioritising CUSPARSE algorithm;OpenCL;data locality;graph partitioning;input matrix;low-latency local memories;multicore CPU;numerical optimisation problem;sparse triangular system of linear equations;Algorithm design and analysis;Concurrent computing;Context;Data structures;Graphics processing units;Partitioning algorithms;Sparse matrices;CUSPARSE;GPU;OpenCL;concurrency;data locality;linear algebra;sparse;systems of equations},
}

@Article{Davis2020,
  author       = {Davis, Timothy A. and Hager, William W. and Kolodziej, Scott P. and Yearalan, S. Nuri},
  journaltitle = {ACM Transactions on Mathematical Software},
  title        = {Algorithm 1003: {Mongoose}, A Graph Coarsening and Partitioning Library},
  doi          = {10.1145/3387915},
  issue        = {1},
  number       = {7},
  volume       = {46},
  abstract     = {Partitioning  graphs  is  a  common  and  useful  operation  in  many  areas,  from  parallel  computing  to  VLSI design to sparse matrix algorithms. In this paper, we introduce Mongoose, a multilevel hybrid graph partitioning algorithm and library. Building on previous work in multilevel partitioning frameworks and combinatoric approaches, we introduce novel stall-reducing and stall-free coarsening strategies, as well as an efficient hybrid algorithm leveraging 1) traditional combinatoric methods and 2) continuous quadratic programming  formulations. We  demonstrate how  this  new hybrid  algorithm outperforms  either strategy in isolation, and we also compare Mongoose to METIS and demonstrate its effectiveness on large and social networking (power law) graphs.},
  year         = {2020},
}

@InProceedings{Hong2018,
  author    = {Hong, Changwan and Sukumaran-Rajam, Aravind and Bandyopadhyay, Bortik and Kim, Jinsung and Kurt, S\"{u}reyya Emre and Nisa, Israt and Sabhlok, Shivani and \c{C}ataly\"{u}rek, \"{U}mit V. and Parthasarathy, Srinivasan and Sadayappan, P.},
  title     = {Efficient Sparse-matrix Multi-vector Product on {GPU}s},
  booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing},
  date      = {2018},
  series    = {HPDC '18},
  publisher = {ACM},
  location  = {Tempe, Arizona},
  isbn      = {978-1-4503-5785-2},
  pages     = {66--79},
  doi       = {10.1145/3208040.3208062},
  abstract  = {Sparse Matrix-Vector (SpMV) and Sparse Matrix-Multivector (SpMM) products are key kernels for computational science and data science. While GPUs offer significantly higher peak performance and memory bandwidth than multicore CPUs, achieving high performance on sparse computations on GPUs is very challenging. A tremendous amount of recent research has focused on various GPU implementations of the SpMV kernel. But the multi-vector SpMM kernel has received much less attention. In this paper, we present an in-depth analysis to contrast SpMV and SpMM, and develop a new sparse-matrix representation and computation approach suited to achieving high data-movement efficiency and effective GPU parallelization of SpMM. Experimental evaluation using the entire SuiteSparse matrix suite demonstrates significant performance improvement over existing SpMM implementations from vendor libraries.},
  keywords  = {GPU, sparse matrix multi-vector multiplication, sparse matrix-matrix multiplication, sparse matrix-vector multiplication},
  numpages  = {14},
}

@Article{Saad1993,
  author       = {Saad, Youcef},
  title        = {A Flexible Inner-Outer Preconditioned GMRES Algorithm},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {1993},
  volume       = {14},
  number       = {2},
  pages        = {461--469},
  doi          = {10.1137/0914028},
  abstract     = {A variant of the GMRES algorithm is presented that allows changes in the preconditioning at every step. There are many possible applications of the new algorithm, some of which are briefly discussed. In particular, a result of the flexibility of the new variant is that any iterative method can be used as a preconditioner. For example, the standard GMRES algorithm itself can be used as a preconditioner, as can CGNR (or CGNE), the conjugate gradient method applied to the normal equations. However, the more appealing utilization of the method is in conjunction with relaxation techniques, possibly multilevel techniques. The possibility of changing preconditioners may be exploited to develop efficient iterative methods and to enhance robustness. A few numerical experiments are reported to illustrate this fact.},
}

@Article{Ekstrom2018,
  author       = {Ekstr{\"o}m, Sven-Erik and Garoni, Carlo},
  title        = {A matrix-less and parallel interpolation--extrapolation algorithm for computing the eigenvalues of preconditioned banded symmetric Toeplitz matrices},
  journaltitle = {Numerical Algorithms},
  date         = {2018},
  doi          = {10.1007/s11075-018-0508-0},
  abstract     = {In  the  past  few  years,  Bogoya,  Bottcher,  Grudsky,  and  Maximenko obtained the precise asymptotic expansion for the eigenvalues of a Toeplitz matrix $T_n(f)$, under suitable assumptions on the generating function $f$, as the matrix size $n$ goes to infinity. On the basis of  several numerical experiments, it was conjectured by Serra-Capizzano that a completely analogous expansion also holds for the eigenvalues of the preconditioned Toeplitz matrix $T_n(u)^{-1}T_n(v)$, provided $f = v/u$ is monotone and further conditions on $u$ and $v$ are satisfied. Based on this expansion, we here propose and analyze an interpolation--extrapolation algorithm for computing the eigenvalues of $T_n(u)^{-1}T_n(v)$. The algorithm is suited for parallel implementation and it may be called “matrix-less” as it does not need to store the entries of the matrix. We illustrate the performance of the algorithm through numerical experiments and we also present its generalization to the case where $f = v/u$ is non-monotone.},
  institution  = {Uppsala University, Division of Scientific Computing},
}

@InProceedings{Zhong2017,
  author    = {Zhong, Wenyong and Cao, Yanxin and Li, Jiawen and Sun, Jianhua and Chen, Hao},
  title     = {Specialization or generalization: A study on breadth-first graph traversal on {GPUs}},
  booktitle = {Proceedings of the International Conference on Progress in Informatics and Computing},
  date      = {2017-12},
  series    = {{PIC}},
  pages     = {294--301},
}

@Book{Diestel2005,
  author       = {Diestel, Reinhard},
  title        = {Graph Theory},
  date         = {2005},
  volume       = {173},
  edition      = {3},
  series       = {Graduate Texts in Mathematics},
  publisher    = {Springer--Verlag Heidelberg},
  isbn         = {3540261826},
  howpublished = {Hardcover},
  month        = aug,
  owner        = {andrea},
  timestamp    = {2016.07.23},
}

@Article{Frick2015,
  author       = {Frick, Damian and Domahidi, Alexander and Morari, Manfred},
  title        = {Embedded optimization for mixed logical dynamical systems},
  journaltitle = {Computers and Chemical Engineering},
  date         = {2015},
  volume       = {72},
  number       = {0},
  month        = {1},
  pages        = {21--33},
  note         = {A Tribute to Ignacio E. Grossmann},
  issn         = {0098-1354},
  doi          = {10.1016/j.compchemeng.2014.06.005},
  abstract     = {Predictive control of hybrid systems is currently considered prohibitive using embedded computing platforms. To overcome this limitation for mixed logical dynamical systems of small to medium size, we propose to use (1) a standard branch-and-bound approach combined with a fast embedded interior point solver, (2) pre-processing heuristics, run once and offline, to significantly reduce the number of subproblems to be solved, and (3) relaxations of the original MPC problem that allow a trade off between computation time and closed-loop performance. A problem-specific ANSI C implementation of the proposed method can be automatically generated, and has a fixed memory footprint and a code size that is insignificantly larger than that of the subproblem solver. Two extensive numerical studies are presented, where problems with up to 60 binary variables are solved in less than 0.2 s with a performance deterioration of below 2\% when compared to an optimal MPC scheme.},
  keywords     = {Mixed logical dynamical systems},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Peyton1993,
  author       = {Peyton, Barry W. and Pothen, Alex and Yuan, Xiaoqing},
  title        = {Partitioning a chordal graph into transitive subgraphs for parallel sparse triangular solution},
  journaltitle = {Linear Algebra and its Applications},
  date         = {1993},
  volume       = {192},
  number       = {0},
  pages        = {329--353},
  issn         = {0024-3795},
  doi          = {10.1016/0024-3795(93)90248-M},
  url          = {http://www.sciencedirect.com/science/article/pii/002437959390248M},
  abstract     = {A recent approach for solving sparse triangular systems of equations on massively parallel computers employs a factorization of the triangular coefficient matrix to obtain a representation of its inverse in product form. The number of general communication steps required by this approach is proportional to the number of factors in the factorization. The triangular matrix can be symmetrically permuted to minimize the number of factors over suitable classes of permutations, and thereby the complexity of the parallel algorithm can be minimized. Algorithms for minimizing the number of factors over several classes of permutations have been considered in earlier work. Let $F = L + L^T$ denote the symmetric filled matrix corresponding to a Cholesky factor L, and let $G_F$ denote the adjacency graph of F. We consider the problem of minimizing the number of factors over all permutations which preserve the structure of $G_F$. The graph model of this problem is to partition the vertices $G_F$ into the fewest transitively closed subgraphs over all perfect elimination orderings while satisfying a certain precedence relationship. The solution to this chordal-graph partitioning problem can be described by a greedy scheme which eliminates a largest permissible subgraph at each step. Further, the subgraph eliminated at each step can be characterized in terms of lengths of chordless paths in the current elimination graph. This solution relies on several results concerning transitive perfect elimination orderings introduced in this paper. We describe a partitioning algorithm with $O(|V| + |E|)$ time and space complexity.},
}

@Article{Ploskas2014,
  author       = {Ploskas, Nikolaos and Samaras, Nikolaos},
  title        = {{GPU} accelerated pivoting rules for the simplex algorithm},
  journaltitle = {Journal of Systems and Software},
  date         = {2014},
  volume       = {96},
  pages        = {1--9},
  issn         = {0164-1212},
  doi          = {10.1016/j.jss.2014.04.047},
  url          = {http://www.sciencedirect.com/science/article/pii/S0164121214001174},
  abstract     = {Simplex type algorithms perform successive pivoting operations (or iterations) in order to reach the optimal solution. The choice of the pivot element at each iteration is one of the most critical step in simplex type algorithms. The flexibility of the entering and leaving variable selection allows to develop various pivoting rules. In this paper, we have proposed some of the most well-known pivoting rules for the revised simplex algorithm on a CPU-GPU computing environment. All pivoting rules have been implemented in MATLAB and CUDA. Computational results on randomly generated optimal dense linear programs and on a set of benchmark problems (Netlib-optimal, Kennington, Netlib-infeasible, Maros) are also presented. These results showed that the proposed GPU implementations of the pivoting rules outperform the corresponding CPU implementations.},
  keywords     = {Linear programming},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Ploskas2015,
  author       = {Ploskas, Nikolaos and Samaras, Nikolaos},
  title        = {Efficient {GPU}-based implementations of simplex type algorithms},
  journaltitle = {Applied Mathematics and Computation},
  date         = {2015},
  volume       = {250},
  pages        = {552--570},
  issn         = {0096-3003},
  doi          = {10.1016/j.amc.2014.10.096},
  abstract     = {Recent hardware advances have made it possible to solve large scale Linear Programming problems in a short amount of time. Graphical Processing Units (GPUs) have gained a lot of popularity and have been applied to linear programming algorithms. In this paper, we propose two efficient GPU-based implementations of the Revised Simplex Algorithm and a Primal-Dual Exterior Point Simplex Algorithm. Both parallel algorithms have been implemented in MATLAB using MATLAB's Parallel Computing Toolbox. Computational results on randomly generated optimal sparse and dense linear programming problems and on a set of benchmark problems (netlib, kennington, Maros) are also presented. The results show that the proposed GPU implementations outperform MATLAB's interior point method.},
  keywords     = {Linear Programming},
  owner        = {andrea},
  timestamp    = {2017.05.08},
}

@Article{Tomov2010,
  author       = {Tomov, Stanimire and Dongarra, Jack and Baboulin, Marc},
  title        = {Towards dense linear algebra for hybrid {GPU} accelerated manycore systems},
  journaltitle = {Parallel Computing},
  date         = {2010},
  volume       = {36},
  number       = {5--6},
  month        = {6},
  pages        = {232--240},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2009.12.005},
  abstract     = {We highlight the trends leading to the increased appeal of using hybrid multicore + GPU systems for high performance computing. We present a set of techniques that can be used to develop efficient dense linear algebra algorithms for these systems. We illustrate the main ideas with the development of a hybrid LU factorization algorithm where we split the computation over a multicore and a graphics processor, and use particular techniques to reduce the amount of pivoting and communication between the hybrid components. This results in an efficient algorithm with balanced use of a multicore processor and a graphics processor.},
  owner        = {andrea},
  posted-at    = {2010-12-17 09:48:58},
  timestamp    = {2016.11.07},
}

@Article{Abdelfattah2018,
  author       = {Abdelfattah, A. and Haidar, A. and Tomov, S. and Dongarra, J.},
  title        = {Analysis and Design Techniques towards High-Performance and Energy-Efficient Dense Linear Solvers on {GPU}s},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2018},
  pages        = {1--14},
  issn         = {1045--9219},
  doi          = {10.1109/TPDS.2018.2842785},
  abstract     = {Graphics Processing Units (GPUs) are widely used in accelerating dense linear solvers. The matrix factorizations, which dominate the runtime for these solvers, are often designed using a hybrid scheme, where GPUs perform trailing matrix updates, while the CPUs perform the panel factorizations. Consequently, hybrid solutions require high-end CPUs and optimized CPU software in order to deliver high performance. Furthermore, they lack the energy efficiency inherent for GPUs due to the use of less energy-efficient CPUs, as well as CPU-GPU communications.},
  keywords     = {Graphics processing units;Libraries;Task analysis;Layout;Kernel;Multicore processing;Dense Linear Solvers;GPU Computing;Energy Efficiency},
}

@InProceedings{Jaiganesh2018,
  author    = {Jaiganesh, Jayadharini and Burtscher, Martin},
  title     = {A High-performance Connected Components Implementation for {GPUs}},
  booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing},
  date      = {2018},
  series    = {HPDC '18},
  publisher = {ACM},
  location  = {Tempe, Arizona},
  isbn      = {978-1-4503-5785-2},
  pages     = {92--104},
  doi       = {10.1145/3208040.3208041},
  abstract  = {Computing connected components is an important graph algorithm that is used, for example, in medicine, image processing, and biochemistry. This paper presents a fast connected-components implementation for GPUs called ECL-CC. It builds upon the best features of prior algorithms and augments them with GPU-specific optimizations. For example, it incorporates a parallelism-friendly version of pointer jumping to speed up union-find operations and uses several compute kernels to exploit the multiple levels of hardware parallelism. The resulting CUDA code is asynchronous and lock free, employs load balancing, visits each edge exactly once, and only processes edges in one direction. It is 1.8 times faster on average than the fastest prior GPU implementation running on a Titan X and faster on most of the eighteen real-world and synthetic graphs we tested.},
  acmid     = {3208041},
  address   = {New York, NY, USA},
  keywords  = {GPU implementation, connected components, graph algorithm, parallelism, union-find},
  numpages  = {13},
}

@Online{Li2018,
  author     = {Li, Ruipeng and Xi, Yuanzhe and Erlandson, Lucas and Saad, Yousef},
  title      = {The Eigenvalues Slicing Library (EVSL): Algorithms, Implementation, and Software},
  date       = {2018},
  abstract   = {This paper describes a software package called EVSL (for EigenValues Slicing Library) for solving large sparse real symmetric standard and generalized eigenvalue problems. As its name indicates, the package exploits spectrum slicing, a strategy that consists of dividing the spectrum into a number of subintervals and extracting eigenpairs from each subinterval independently. In order to enable such a strategy, the methods implemented in EVSL rely on a quick calculation of the spectral density of a given matrix, or a matrix pair. What distinguishes EVSL from other currently available packages is that EVSL relies entirely on filtering techniques. Polynomial and rational filtering are both implemented and are coupled with Krylov subspace methods and the subspace iteration algorithm. On the implementation side, the package offers interfaces for various scenarios including matrix-free modes, whereby the user can supply his/her own functions to perform matrix-vector operations or to solve sparse linear systems. The paper describes the algorithms in EVSL, provides details on their implementations, and discusses performance issues for the various methods.},
  eprint     = {1802.05215},
  eprinttype = {arXiv},
}

@Online{Coutinho2018,
  author     = {Coutinho, Demetrios and Xavier-de-Souza, Samuel and Aloise, Daniel},
  title      = {A Scalable Shared-Memory Parallel Simplex for Large-Scale Linear Programming},
  date       = {2018},
  abstract   = {We present a shared-memory parallel implementation of the Simplex tableau algorithm for dense large-scale Linear Programming (LP) problems. We present the general scheme and explain each parallelization step of the standard simplex algorithm, emphasizing important solutions for solving performance bottlenecks. We analyzed the speedup and the parallel efficiency for the proposed implementation relative to the standard Simplex algorithm using a shared-memory system with 64 processing cores. The experiments were performed for several different problems, with up to 8192 variables and constraints, in their primal and dual formulations. The results show that the performance is mostly much better when we use the formulation with more variables than inequality constraints. Also, they show that the parallelization strategies applied to avoid bottlenecks caused the implementation to scale well with the problem size and the core count up to a certain limit of problem size. Further analysis showed that this was an effect of resource limitation. Even though, our implementation was able to reach speedups in the order of 19$\times$.},
  eprint     = {1804.04737},
  eprinttype = {arXiv},
}

@InProceedings{Simpson2018,
  author    = {Simpson, Toby and Dimosthenis, Pasadakis ad Kourounis, Drosos and Fujita, Kohei and Yamaguchi, Takuma and Tsuyoshi, Ichimura and Schenk, Olaf},
  title     = {Balanced Graph Partition Refinement using the Graph p-Laplacian},
  booktitle = {Proceedings of the {ACM} Platform for Advanced Scientific Computing Conference},
  date      = {2018-06},
  series    = {PASC '18},
  doi       = {10.1145/3218176.3218232},
  abstract  = {A continuous formulation of the optimal 2-way graph partitioning based on the p-norm minimization of the graph Laplacian Rayleigh quotient is presented, which provides a sharp approximation to the balanced graph partitioning problem, the optimality of which is known to be NP-hard. The minimization is initialized from a cut provided by a state-of-the-art multilevel recursive bisection algorithm, and then a continuation approach reduces the p-norm from a 2-norm towards a 1-norm, employing for each value of p a feasibility-preserving steepest-descent method that converges on the p-Laplacian eigenvector. A filter favors iterates advancing towards minimum edgecut and partition load imbalance. The complexity of the suggested approach is linear in graph edges. The simplicity of the steepest-descent algorithm renders the overall approach highly scalable and efficient in parallel distributed architectures. Parallel implementation of recursive bisection on multi-core CPUs and GPUs are presented for large-scale graphs with up to 1.9 billion tetrahedra. The suggested approach exhibits improvements of up to 52.8\% over METIS for graphs originating from triangular Delaunay meshes, 34.7\% over METIS and 21.9\% over KaHIP for power network graphs, 40.8\% over METIS and 20.6\% over KaHIP for sparse matrix graphs, and finally 93.2\% over METIS for graphs emerging from social networks.},
  timestamp = {2018.08.26},
}

@InProceedings{Dahiya2018,
  author    = {Dahiya, Yogesh and Konomis, Dimitris and Woodruff, David P.},
  title     = {An Empirical Evaluation of Sketching for Numerical Linear Algebra},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \&\#38; Data Mining},
  date      = {2018},
  series    = {KDD '18},
  publisher = {ACM},
  location  = {London, United Kingdom},
  isbn      = {978-1-4503-5552-0},
  pages     = {1292--1300},
  doi       = {10.1145/3219819.3220098},
  abstract  = {Over the last ten years, tremendous speedups for problems in randomized numerical linear algebra such as low rank approximation and regression have been made possible via the technique of randomized data dimensionality reduction, also known as sketching. In theory, such algorithms have led to optimal input sparsity time algorithms for a wide array of problems. While several scattered implementations of such methods exist, the goal of this work is to provide a comprehensive comparison of such methods to alternative approaches. We investigate least squares regression, iteratively reweighted least squares, logistic regression, robust regression with Huber and Bisquare loss functions, leverage score computation, Frobenius norm low rank approximation, and entrywise $\ell_1$-low rank approximation. We give various implementation techniques to speed up several of these algorithms, and the resulting implementations demonstrate the tradeoffs of such techniques in practice.},
  acmid     = {3220098},
  address   = {New York, NY, USA},
  keywords  = {logistic regression, low rank approximation, regression, robust methods, sketching},
  numpages  = {9},
}

@Article{Anzt2018a,
  author       = {Anzt, Hartwig and Huckle, Thomas K. and Bräckle, Jürgen and Dongarra, Jack},
  title        = {Incomplete Sparse Approximate Inverses for Parallel Preconditioning},
  journaltitle = {Parallel Computing},
  date         = {2018},
  volume       = {71},
  pages        = {1--22},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2017.10.003},
  url          = {http://www.sciencedirect.com/science/article/pii/S016781911730176X},
  abstract     = {In this paper, we propose a new preconditioning method that can be seen as a generalization of block-Jacobi methods, or as a simplification of the sparse approximate inverse (SAI) preconditioners. The Incomplete Sparse Approximate Inverses (ISAI) is in particular efficient in the solution of sparse triangular linear systems of equations. Those arise, for example, in the context of incomplete factorization preconditioning. ISAI preconditioners can be generated via an algorithm providing fine-grained parallelism, which makes them attractive for hardware with a high concurrency level. In a study covering a large number of matrices, we identify the ISAI preconditioner as an attractive alternative to exact triangular solves in the context of incomplete factorization preconditioning.},
  keywords     = {Preconditioning, Incomplete Sparse Approximate Inverse, Incomplete LU factorization, Approximate sparse triangular solves, Parallel computing},
  timestamp    = {2018.08.26},
}

@Online{Tavernier2018,
  author     = {Tavernier, Joris and Simm, Jaak and Meerbergen, Karl and Moreau, Yves},
  title      = {Multilevel preconditioning for Ridge Regression},
  date       = {2018},
  abstract   = {Solving linear systems is often the computational bottleneck in real-life problems. Iterative solvers are the only option due to the complexity of direct algorithms or because the system matrix is not explicitly known. Here, we develop a multilevel preconditioner for regularized least squares linear systems involving a feature or data matrix. Variants of this linear system may appear in machine learning applications, such as ridge regression, logistic regression, support vector machines and matrix factorization with side information. We use clustering algorithms to create coarser levels that preserve the principal components of the covariance or Gram matrix. These coarser levels approximate the dominant eigenvectors and are used to build a multilevel preconditioner accelerating the Conjugate Gradient method. We observed speed-ups for artificial and real-life data. For a specific data set, we achieved speed-up up to a factor 100.},
  eprint     = {1806.05826},
  eprinttype = {arXiv},
}

@InProceedings{Wang2018,
  author    = {Wang, Xinliang and Liu, Weifeng and Xue, Wei and Wu, Li},
  title     = {swSpTRSV: A Fast Sparse Triangular Solve with Sparse Level Tile Layout on Sunway Architectures},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  date      = {2018},
  series    = {PPoPP '18},
  publisher = {ACM},
  location  = {Vienna, Austria},
  isbn      = {978-1-4503-4982-6},
  pages     = {338--353},
  doi       = {10.1145/3178487.3178513},
  abstract  = {Sparse triangular solve (SpTRSV) is one of the most important kernels in many real-world applications. Currently, much research on parallel SpTRSV focuses on level-set construction for reducing the number of inter-level synchronizations. However, the out-of-control data reuse and high cost for global memory or shared cache access in inter-level synchronization have been largely neglected in existing work.\\In this paper, we propose a novel data layout called Sparse Level Tile to make all data reuse under control, and design a Producer-Consumer pairing method to make any inter-level synchronization only happen in very fast register communication. We implement our data layout and algorithms on an SW26010 many-core processor, which is the main building-block of the current world fastest supercomputer Sunway Taihulight. The experimental results of testing all 2057 square matrices from the Florida Matrix Collection show that our method achieves an average speedup of 6.9 and the best speedup of 38.5 over parallel level-set method. Our method also outperforms the latest methods on a KNC many-core processor in 1856 matrices and the latest methods on a K80 GPU in 1672 matrices, respectively.},
  acmid     = {3178513},
  address   = {New York, NY, USA},
  keywords  = {sparse level tile, sparse matrix, sparse triangular solve, sunway architecture},
  numpages  = {16},
}

@TechReport{Biswas2003,
  author      = {Biswas, Rupak and Oliker, Leonid and Shan, Hongzhang},
  title       = {Parallel computing strategies for irregular algorithms},
  institution = {LBNL},
  date        = {2003},
  number      = {53115},
  abstract    = {Parallel computing promises several orders of magnitude increase in our ability to solve realistic computationally intensive problems, but relies on their efficient mapping and execution on large-scale multiprocessor architectures. Unfortunately, many important applications are irregular and dynamic in nature, making their effective parallel implementation a daunting task. Moreover, with the proliferation of parallel architectures and programming paradigms, the typical scientist is faced with a plethora of questions that must be answered in order to obtain an acceptable parallel implementation of the solution algorithm. In this paper, we consider three representative irregular applications: unstructured remeshing, sparse matrix computations, and N-body problems, and parallelize them using various popular programming paradigms on a wide spectrum of computing platforms ranging from state-of-the-art supercomputers to PC clusters. We present the underlying problems, the solution algorithms, and the parallel implementation strategies. Smart load-balancing, partitioning, and ordering techniques are used to enhance parallel performance. Overall results demonstrate the complexity of efficiently parallelizing irregular algorithms.},
  timestamp   = {2018.08.26},
}

@Article{Liu2017,
  author       = {Liu, Weifeng and Li, Ang and Hogg, Jonathan D. and Duff, Iain S. and Vinter, Brian},
  title        = {Fast synchronization-free algorithms for parallel sparse triangular solves with multiple right-hand sides},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2017},
  volume       = {29},
  number       = {21},
  pages        = {e4244},
  note         = {e4244 cpe.4244},
  doi          = {10.1002/cpe.4244},
  eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4244},
  abstract     = {The sparse triangular solve kernels, SpTRSV and SpTRSM, are important building blocks for a number of numerical linear algebra routines. Parallelizing SpTRSV and SpTRSM on today's manycore platforms, such as GPUs, is not an easy task since computing a component of the solution may depend on previously computed components, enforcing a degree of sequential processing. As a consequence, most existing work introduces a preprocessing stage to partition the components into a group of level-sets or colour-sets so that components within a set are independent and can be processed simultaneously during the subsequent solution stage. However, this class of methods requires a long preprocessing time as well as significant runtime synchronization overheads between the sets. To address this, we propose in this paper novel approaches for SpTRSV and SpTRSM in which the ordering between components is naturally enforced within the solution stage. In this way, the cost for preprocessing can be greatly reduced, and the synchronizations between sets are completely eliminated. To further exploit the data-parallelism, we also develop an adaptive scheme for efficiently processing multiple right-hand sides in SpTRSM. A comparison with a state-of-the-art library supplied by the GPU vendor, using 20 sparse matrices on the latest GPU device, shows that the proposed approach obtains an average speedup of over two for SpTRSV and up to an order of magnitude speedup for SpTRSM. In addition, our method is up to two orders of magnitude faster for the preprocessing stage than existing SpTRSV and SpTRSM methods.},
  keywords     = {GPU, manycore processor, sparse triangular solve, synchronization-free algorithm},
}

@Article{Chow2018,
  author       = {Chow, Edmond and Anzt, Hartwig and Scott, Jennifer and Dongarra, Jack},
  title        = {Using Jacobi iterations and blocking for solving sparse triangular systems in incomplete factorization preconditioning},
  journaltitle = {Journal of Parallel and Distributed Computing},
  date         = {2018},
  volume       = {119},
  pages        = {219--230},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2018.04.017},
  url          = {http://www.sciencedirect.com/science/article/pii/S0743731518303034},
  abstract     = {When using incomplete factorization preconditioners with an iterative method to solve large sparse linear systems, each application of the preconditioner involves solving two sparse triangular systems. These triangular systems are challenging to solve efficiently on computers with high levels of concurrency. On such computers, it has recently been proposed to use Jacobi iterations, which are highly parallel, to approximately solve the triangular systems from incomplete factorizations. The effectiveness of this approach, however, is problem-dependent: the Jacobi iterations may not always converge quickly enough for all problems. Thus, as a necessary and important step to evaluate this approach, we experimentally test the approach on a large number of realistic symmetric positive definite problems. We also show that by using block Jacobi iterations, we can extend the range of problems for which such an approach can be effective. For block Jacobi iterations, it is essential for the blocking to be cognizant of the matrix structure.},
  keywords     = {Sparse linear systems, Triangular solves, Iterative solvers, Preconditioning},
}

@InProceedings{Nagasaka2018,
  author     = {Nagasaka, Yusuke and Matsuoka, Satoshi and Azad, Ariful and Buluç, Aydın},
  title      = {High-performance sparse matrix-matrix products on Intel KNL and multicore architectures},
  booktitle  = {Proceedings of the 47th International Conference on Parallel Processing (Workshops)},
  date       = {2018},
  series     = {ICPPW '18},
  eprint     = {1804.01698},
  eprinttype = {arXiv},
  abstract   = {Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional numerical applications to recent big data analysis and machine learning. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi- and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with memory management and thread scheduling on Intel Xeon Phi (Knights Landing or KNL). Specifically targeting multi- and many-core processors, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real graph algorithms, such as multi-source breadth-first search or triangle counting. Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases while different algorithms dominate the other scenarios with different matrix size, sparsity, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix.},
}

@InProceedings{Bromberger2018,
  author    = {Bromberger, Michael and Hoffmann, Markus and Rehrmann, Robin},
  title     = {Do Iterative Solvers Benefit from Approximate Computing? An Evaluation Study Considering Orthogonal Approximation Methods},
  booktitle = {Architecture of Computing Systems},
  date      = {2018},
  editor    = {Berekovic, Mladen and Buchty, Rainer and Hamann, Heiko and Koch, Dirk and Pionteck, Thilo},
  series    = {ARCS 2018},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-319-77610-1},
  pages     = {297--310},
  abstract  = {Employing algorithms of scientific computing often comes in hand with finding a trade-off between accuracy and performance. Novel parallel hardware and algorithms only slightly improve these issues due to the increasing size of the problems. While high accuracy is inevitable for most problems, there are parts in scientific computing that allow us to introduce approximation. Therefore, in this paper we give answers to the following questions: (1) Can we exploit different approximate computing strategies in scientific computing? (2) Is there a strategy to combine approaches? To answer these questions, we apply different approximation strategies to a widely used iterative solver for linear systems of equations. We show the advantages and the limits of each strategy and a way to configure a combination of strategies according to a given relative error. Combining orthogonal strategies as an overall concept gives us significant opportunities to increase the performance.},
}

@Online{Yang2018b,
  author     = {Yang, Carl and Buluç, Aydın and Owens, John D.},
  title      = {Design Principles for Sparse Matrix Multiplication on the {GPU}},
  date       = {2018},
  abstract   = {We implement two novel algorithms for sparse-matrix dense-matrix multiplication (SpMM) on the GPU. Our algorithms expect the sparse input in the popular compressed-sparse-row (CSR) format and thus do not require expensive format conversion. While previous SpMM work concentrates on thread-level parallelism, we additionally focus on latency hiding with instruction-level parallelism and load-balancing. We show, both theoretically and experimentally, that the proposed SpMM is a better fit for the GPU than previous approaches. We identify a key memory access pattern that allows efficient access into both input and output matrices that is crucial to getting excellent performance on SpMM. By combining these two ingredients -- (i) merge-based load-balancing and (ii) row-major coalesced memory access -- we demonstrate a 3.6x peak speedup and a 23.5\% geomean speedup over state-of-the-art SpMM implementations on real-world datasets.},
  eprint     = {1803.08601},
  eprinttype = {arXiv},
}

@InBook{Jacquelin2018,
  author    = {Jacquelin, Mathias and Ng, Esmond G. and Peyton, Barry W.},
  title     = {Fast and effective reordering of columns within supernodes using partition refinement},
  booktitle = {Proceedings of the Seventh SIAM Workshop on Combinatorial Scientific Computing},
  date      = {2018},
  pages     = {76--86},
  doi       = {10.1137/1.9781611975215.8},
  abstract  = {In this paper, we consider the problem of computing a triangular factorization of a sparse symmetric matrix using Gaussian elimination. We assume that the sparse matrix has been permuted using a fill-reducing ordering algorithm. When the matrix is symmetric positive definite, the sparsity structure of the triangular factor can be determined once the fill-reducing ordering has been computed. Thus, an efficient numerical factorization scheme can be designed so that only the nonzero entries are stored and operated on. On modern architectures, the positions of the nonzero entries in the triangular factor play a big role in determining the efficiency. It is desirable to have dense blocks in the factor so that the computation can be cast in terms of level-3 BLAS as much as possible. On architectures with GPUs, for example, it is also desirable for these dense blocks to be as large as possible in order to reduce the times to transfer data between the main CPU and the GPUs. We address the problem of locally refining the ordering so that the number of dense blocks is reduced and the sizes of these dense blocks are increased in the triangular factor.},
}

@Online{Mohammadi2018,
  author        = {Mohammadi, Mahdi Soltan and Cheshmi, Kazem and Gopalakrishnan, Ganesh and Hall, Mary W. and Dehnavi, Maryam Mehri and Venkat, Anand and Yuki, Tomofumi and Strout, Michelle Mills},
  title         = {Sparse Matrix Code Dependence Analysis Simplification at Compile Time},
  date          = {2018},
  abstract      = {Analyzing array-based computations to determine data dependences is useful for many applications including automatic parallelization, race detection, computation and communication overlap, verification, and shape analysis. For sparse matrix codes, array data dependence analysis is made more difficult by the use of index arrays that make it possible to store only the nonzero entries of the matrix (e.g., in A[B[i]], B is an index array). Here, dependence analysis is often stymied by such indirect array accesses due to the values of the index array not being available at compile time. Consequently, many dependences cannot be proven unsatisfiable or determined until runtime. Nonetheless, index arrays in sparse matrix codes often have properties such as monotonicity of index array elements that can be exploited to reduce the amount of runtime analysis needed. In this paper, we contribute a formulation of array data dependence analysis that includes encoding index array properties as universally quantified constraints. This makes it possible to leverage existing SMT solvers to determine whether such dependences are unsatisfiable and significantly reduces the number of dependences that require runtime analysis in a set of eight sparse matrix kernels. Another contribution is an algorithm for simplifying the remaining satisfiable data dependences by discovering equalities and/or subset relationships. These simplifications are essential to make a runtime-inspection-based approach feasible.},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1807-10852},
  eprint        = {1807.10852},
  eprinttype    = {arXiv},
  timestamp     = {Mon, 13 Aug 2018 16:46:36 +0200},
}

@InProceedings{Dufrechou2018a,
  author    = {Dufrechou, E. and Ezzatti, P.},
  title     = {A New {GPU} Algorithm to Compute a Level Set-Based Analysis for the Parallel Solution of Sparse Triangular Systems},
  booktitle = {Proceedings of the IEEE International Parallel and Distributed Processing Symposium},
  date      = {2018-05},
  series    = {IPDPS '18},
  pages     = {920--929},
  doi       = {10.1109/IPDPS.2018.00101},
  abstract  = {A myriad of problems in science and engineering, involve the solution of sparse triangular linear systems. They arise frequently as part of direct and iterative solvers for linear systems and eigenvalue problems, and hence can be considered as a key building block of sparse numerical linear algebra. This is why, since the early days, their parallel solution has been exhaustively studied, and efficient implementations of this kernel can be found for almost every hardware platform. In the GPU context, the most widespread implementation of this kernel is the one distributed in NVIDIA CUSPARSE library, which relies on a preprocessing stage to aggregate the unknowns of the triangular system into level sets. This determines an execution schedule for the solution of the system, where the level sets have to be processed sequentially while the unknowns that belong to one level set can be solved in parallel. One of the disadvantages of the CUSPARSE implementation is that this preprocessing stage is often extremely slow in comparison to the runtime of the solving phase. In this work, we present a parallel GPU algorithm that is able to compute the same level sets as CU S PARSE but takes significantly less runtime. Our experiments on a set of matrices from the SuiteSparse collection show acceleration factors of up to 44$\times$. Additionally, we provide a routine capable of solving a triangular linear system on the same pass used to calculate the level sets, yielding important performance benefits.},
  issn      = {1530-2075},
  keywords  = {eigenvalues and eigenfunctions;graphics processing units;iterative methods;linear systems;parallel algorithms;sparse matrices;parallel GPU algorithm;triangular linear system;parallel solution;sparse triangular linear systems;sparse numerical linear algebra;level set-based analysis;iterative solvers;direct solvers;eigenvalue problems;NVIDIA CUSPARSE library;SuiteSparse collection;Sparse matrices;Level set;Graphics processing units;Linear systems;Kernel;Runtime;Libraries;Sparse triangular linear systems;Level sets;cusparse;Graphics processors (GPUs)},
}

@Article{Tan2018,
  author       = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
  title        = {Design and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture},
  journaltitle = {ACM Trans. Math. Softw.},
  date         = {2018-08},
  volume       = {44},
  number       = {4},
  pages        = {46:1--46:25},
  issn         = {0098-3500},
  doi          = {10.1145/3218823},
  abstract     = {Sparse matrix vector multiplication (SpMV) is an important computational kernel in traditional high-performance computing and emerging data-intensive applications. Previous SpMV libraries are optimized by either application-specific or architecture-specific approaches but present difficulties for use in real applications. In this work, we develop an auto-tuning system (SMATER) to bridge the gap between specific optimizations and general-purpose use. SMATER provides programmers a unified interface based on the compressed sparse row (CSR) sparse matrix format by implicitly choosing the best format and fastest implementation for any input sparse matrix during runtime. SMATER leverages a machine-learning model and retargetable back-end library to quickly predict the optimal combination. Performance parameters are extracted from 2,386 matrices in the SuiteSparse matrix collection. The experiments show that SMATER achieves good performance (up to 10 times that of the Intel Math Kernel Library (MKL) on Intel E5-2680 v3) while being portable on state-of-the-art x86 multicore processors, NVIDIA GPUs, and Intel Xeon Phi accelerators. Compared with the Intel MKL library, SMATER runs faster by more than 2.5 times on average. We further demonstrate its adaptivity in an algebraic multigrid solver from the Hypre library and report greater than 20\% performance improvement.},
  acmid        = {3218823},
  articleno    = {46},
  issue_date   = {August 2018},
  keywords     = {Sparse matrix vector multiplication, auto-tuning, machine learning, multicore},
  location     = {New York, NY, USA},
  numpages     = {25},
  publisher    = {ACM},
}

@InProceedings{Wang2018a,
  author    = {Wang, Xinliang and Xu, Ping and Xue, Wei and Ao, Yulong and Yang, Chao and Fu, Haohuan and Gan, Lin and Yang, Guangwen and Zheng, Weimin},
  title     = {A Fast Sparse Triangular Solver for Structured-grid Problems on Sunway Many-core Processor SW26010},
  booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
  date      = {2018},
  series    = {ICPP 2018},
  publisher = {ACM},
  location  = {Eugene, OR, USA},
  isbn      = {978-1-4503-6510-9},
  pages     = {53:1--53:11},
  doi       = {10.1145/3225058.3225071},
  abstract  = {The sparse triangular solver (SpTRSV) is one of the most essential kernels in many scientific and engineering applications. Efficiently parallelizing the SpTRSV on modern many-core architectures is considerably difficult due to inherent dependency of computation and discontinuous memory accesses. Achieving high performance of SpTRSV is even more challenging for SW26010, the new-generation customized heterogeneous many-core processor equipped in the top-rank Sunway TaihuLight supercomputer. Owing to regular sparse pattern, structured-grid triangular problems show much different computing characteristics with general ones as well as new opportunities to algorithm design on many-core architectures, which ever lacks attention. In this work, we focus on how to design and implement fast SpTRSV for structured-grid problems on SW26010. A generalized algorithm framework of parallel SpTRSV is proposed for best utilization of the features and flexibilities of SW26010 many-core architecture according to the fine-grained Producer-Consumer model. Moreover, a novel parallel structured-grid SpTRSV is presented by using direct data transfers across registers of the computing elements of SW26010. Experiments on four typical structured-grid triangular problems with different problem sizes demonstrate that our SpTRSV can achieve an average momory bandwidth utilization of 79.7\% according to the stream benchmark, which leads to a speedup of 17.7 over serial version on SW26010. Furthermore, experiments with real world sparse linear problems show that our proposed SpTRSV can achieve superior preconditioning performance over the Intel Xeon E5-2670 v3 CPU and Intel Xeon Phi 7210 KNL over DDR4 memory.},
  acmid     = {3225071},
  address   = {New York, NY, USA},
  articleno = {53},
  keywords  = {Structured Grid, Sunway TaihuLight, Triangular Solver},
  numpages  = {11},
}

@Article{Abubaker2018,
  author       = {Abubaker, N. F. T. and Akbudak, K. and Aykanat, C.},
  title        = {Spatiotemporal Graph and Hypergraph Partitioning Models for Sparse Matrix-Vector Multiplication on Many-Core Architectures},
  journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
  date         = {2018},
  pages        = {1--1},
  issn         = {1045-9219},
  doi          = {10.1109/TPDS.2018.2864729},
  abstract     = {There exist graph/hypergraph partitioning-based row/column reordering methods for encoding either spatial or temporal locality separately for sparse matrix-vector multiplication (SpMV) operations. Spatial and temporal hypergraph models in these methods are extended to encapsulate both spatial and temporal localities based on cut/uncut net categorization obtained from vertex partitioning. These extensions of spatial and temporal hypergraph models encode the spatial locality primarily and the temporal locality secondarily, and vice-versa, respectively. However, the literature lacks models that simultaneously encode both spatial and temporal localities utilizing only vertex partitioning for further improving the performance of SpMV on shared-memory architectures. In order to fill this gap, we propose a novel spatiotemporal hypergraph model that leads to a one-phase spatiotemporal reordering method which encodes both types of locality simultaneously. We also propose a framework for spatiotemporal methods which encodes both types of locality in two dependent phases and two separate phases. The validity of the proposed spatiotemporal models and methods are tested on a wide range of sparse matrices and the experiments are performed on both a 60-core Intel Xeon Phi processor and a Xeon processor. Results show the validity of the methods via almost doubling the Gflop/s performance through enhancing data locality in parallel SpMV operations.},
  keywords     = {Spatiotemporal phenomena;Sparse matrices;Data models;Task analysis;Bipartite graph;Encoding;Taxonomy;Sparse matrix;sparse matrix-vector multiplication;data locality;spatial locality;temporal locality;hypergraph model;bipartite graph model;graph model;hypergraph partitioning;graph partitioning;Intel Many Integrated Core Architecture;Intel Xeon Phi},
}

@Article{Acer2018,
  author       = {Acer, Seher and Selvitopi, Oguz and Aykanat, Cevdet},
  title        = {Optimizing nonzero-based sparse matrix partitioning models via reducing latency},
  journaltitle = {Journal of Parallel and Distributed Computing},
  date         = {2018},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2018.08.005},
  url          = {http://www.sciencedirect.com/science/article/pii/S0743731518305860},
  abstract     = {For the parallelization of sparse matrix--vector multiplication (SpMV) on distributed memory systems, nonzero-based fine-grain and medium-grain partitioning models attain the lowest communication volume and computational imbalance among all partitioning models. This usually comes, however, at the expense of high message count, i.e., high latency overhead. This work addresses this shortcoming by proposing new fine-grain and medium-grain models that are able to minimize communication volume and message count in a single partitioning phase. The new models utilize message nets in order to encapsulate the minimization of total message count. We further fine-tune these models by proposing delayed addition and thresholding for message nets in order to establish a trade-off between the conflicting objectives of minimizing communication volume and message count. The experiments on an extensive dataset of nearly one thousand matrices show that the proposed models improve the total message count of the original nonzero-based models by up to 27\% on the average, which is reflected on the parallel runtime of SpMV as an average reduction of 15\% on 512 processors.},
  keywords     = {Sparse matrix, Sparse matrix--vector multiplication, Row-column-parallel SpMV, Load balancing, Communication overhead, Hypergraph, Fine-grain partitioning, Medium-grain partitioning, Recursive bipartitioning},
}

@Online{Schlag2018,
  author   = {Schlag, Sebastian and Schulz, Christian and Seemaier, Daniel and Strash, Darren},
  title    = {Scalable Edge Partitioning},
  date     = {2018},
  abstract = {Edge-centric distributed computations have appeared as a recent technique to improve the shortcomings of think-like-a-vertex algorithms on large scale-free networks. In order to increase parallelism on this model, edge partitioning - partitioning edges into roughly equally sized blocks - has emerged as an alternative to traditional (node-based) graph partitioning. In this work, we give a distributed memory parallel algorithm to compute high-quality edge partitions in a scalable way. Our algorithm scales to networks with billions of edges, and runs efficiently on thousands of PEs. Our technique is based on a fast parallelization of split graph construction and a use of advanced node partitioning algorithms. Our extensive experiments show that our algorithm has high quality on large real-world networks and large hyperbolic random graphs, which have a power law degree distribution and are therefore specifically targeted by edge partitioning},
  eprint   = {1808.06411},
  eprinttype   = {arXiv},
}

@InProceedings{Davis2018a,
  author    = {Davis, Timothy A.},
  title     = {Graph algorithms via {SuiteSparse}:{GraphBLAS}:triangle counting and K-truss},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2018},
  series    = {HPEC '18},
  abstract  = {SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which defines a set of sparse matrix operations on an extended algebra of semirings using an almost unlimited variety of operators and types. When applied to sparse adjacency matrices, these algebraic operations are equivalent tocomputations on graphs. GraphBLAS provides a powerful and expressive framework for creating graph algorithms based on the elegant mathematics of sparse matrix operations on a semiring. To illustrate GraphBLAS, two graph algorithms are constructed in GraphBLAS and compared with efficient implementations without GraphBLAS: triangle counting and constructing the k-truss of a graph.},
  timestamp = {2018.08.27},
}

@InProceedings{Haidar2018,
  author    = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
  title     = {Harnessing a {GPU}’s Tensor Cores for Fast {FP16} Arithmetic to Speed up Mixed-Precision Iterative Refinement Solvers},
  booktitle = {Proceedings of {NVIDIA}'s {GPU} Technology Conference},
  date      = {2018},
  series    = {GTC '18},
  abstract  = {Recent in-hardware GPU acceleration of half precision arithmetic (FP16) -- motivated by various machine learning (ML) and artificial intelligence (AI) applications -- has reinvigorated a great interest in the mixed-precision iterative refinement technique. The technique is based on use of low precision arithmetic to accelerate the general HPC problem of solving Ax = b, where A is a large dense matrix, and the solution is needed in FP64 accuracy. While being a well known technique, its successful modification, software development, and adjustment to match architecture specifics, is challenging. For current manycore GPUs the challenges range from efficient parallelization to scaling, and using the FP16 arithmetic. Here, we address these challenges by showing how to algorithmically modify, develop high-performance implementations, and in general, how to use the FP16 arithmetic to significantly accelerate, as well as make more energy efficient, FP64-precision Ax = b solvers. One can reproduce our results as the developments will be made available through the MAGMA library. We quantify in
practice the performance, and limitations of the approach stressing on the use of the Volta V100 Tensor Cores that provide additional FP16 performance boost},
  timestamp = {2018.08.27},
}

@Article{Hogg2013,
  author       = {Hogg, J.},
  title        = {A Fast Dense Triangular Solve in CUDA},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2013},
  volume       = {35},
  number       = {3},
  pages        = {C303-C322},
  doi          = {10.1137/12088358X},
  abstract     = {The level 2 BLAS operation _trsv performs a dense triangular solve and is often used in the solve phase of a direct solver following a matrix factorization. With the advent of manycore architectures reducing the cost of compute-bound parts of the computation, memory-bound operations such as this kernel become increasingly important. This is particularly noticeable in sparse direct solvers used for optimization applications where multiple memory-bound solves follow each (traditionally expensive) compute-bound factorization. In this paper, a high performance implementation of the triangular solve is developed through an analysis of theoretical and practical bounds on its run time. This implementation outperforms the CUBLAS by a factor of 5--15.},
}

@Conference{Abdelfattah2018a,
  author       = {Abdelfattah, Ahmad and Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack},
  title        = {Optimizing {GPU} Kernels for Irregular Batch Workloads: A Case Study for Cholesky Factorization},
  booktitle    = {IEEE High Performance Extreme Computing Conference},
  date         = {2018-09},
  series       = {HPEC'18},
  organization = {IEEE},
  publisher    = {IEEE},
  location     = {Waltham, MA},
  abstract     = {This paper introduces several frameworks for the design and implementation of high performance GPU kernels that target batch workloads with irregular sizes. Such workloads are ubiquitous in many scientific applications, including sparse direct solvers, astrophysics, and quantum chemistry. The paper addresses two main categories of frameworks, taking the Cholesky factorization as a case study. The first uses host-side kernel launches, and the second uses device-side launches. Within each category, different design options are introduced, with an emphasis on the advantages and the disadvantages of each approach. Our best performing design outperforms the state-of-the-art CPU implementation, scoring up to 4.7$\times$ speedup in double precision on a Pascal P100 GPU.},
}

@TechReport{Kaya2018,
  author      = {Kaya, Oguz and Kannan, Ramakrishnan and Ballard, Grey},
  title       = {Partitioning and Communication Strategies for Sparse Non-negative Matrix Factorization},
  institution = {INRIA Bordeaux Sud-Ouest},
  date        = {2018},
  abstract    = {Non-negative matrix factorization (NMF), the problem of finding two non-negative low-rank factors whose product approximates an input matrix, is a useful tool for many data mining and scientific applications such as topic modeling in text mining and blind source separation in microscopy. In this paper, we focus on scaling algorithms for NMF to very large sparse datasets and massively parallel machines by employing effective algorithms, communication patterns, and partitioning schemes that leverage the sparsity of the input matrix. In the case of machine learning workflow, the computations after SpMM must deal with dense matrices, as Sparse-Dense matrix multiplication will result in a dense matrix. Hence, the partitioning strategy considering only SpMM will result in a huge imbalance in the overall workflow especially on computations after SpMM and in this specific case of NMF on non-negative least squares computations. Towards this, we consider two previous works developed for related problems, one that uses a fine-grained partitioning strategy using a point-to-point communication pattern and on that uses a checkerboard partitioning strategy using a collective-based communication pattern. We show that a combination of the previous approaches balances the demands of the various computations within NMF algorithms and achieves high efficiency and scalability. From the experiments, we could see that our proposed algorithm communicates atleast 4x less than the collective and achieves upto 100$\times$ speed up over the baseline FAUN on real world datasets. Our algorithm was experimented in two different super computing platforms and we could scale up to 32000 processors on Bluegene/Q.},
  timestamp   = {2018.08.27},
}

@InProceedings{Cheshmi2018,
  author    = {Cheshmi, Kazem and Kamil, Shoaib and Strout, Michelle Mills and Dehnavi, Maryam Mehri},
  title     = {ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism},
  booktitle = {{P}roceedings of The International Conference for High Performance Computing, Networking, Storage, and Analysis},
  date      = {2018},
  series    = {SC '18},
  location  = {Dallas, TX, US},
  abstract  = {ParSy is a framework that generates parallel code for sparse matrix computations. It uses a novel inspection strategy along with code transformations to generate parallel code for shared memory processors that is optimized for locality and load balance. Code generated by existing automatic parallelism approaches for sparse algorithms can suffer from load imbalance and excessive synchronization, resulting in performance that does not scale well on multi-core systems. We propose a novel task coarsening strategy that creates well-balanced tasks that can execute in parallel. ParSy-generated code outperforms existing highly-optimized sparse matrix codes such as the Cholesky factorization on multi-core processors with speed-ups of 2.8$\times$ and 3.1$\times$ over the MKL Pardiso and PaStiX libraries respectively.},
}

@Article{Benatia2018,
  author       = {Benatia, Akrem and Ji, Weixing and Wang, Yizhuo and Shi, Feng},
  title        = {{BestSF}: A Sparse Meta-Format for Optimizing {SpMV} on {GPU}},
  journaltitle = {ACM Transactions on Architecture and Code Optimization ({TACO})},
  date         = {2018-09},
  volume       = {15},
  number       = {3},
  pages        = {29:1--29:27},
  issn         = {1544-3566},
  doi          = {10.1145/3226228},
  abstract     = {The Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed to improve this kernel on the recent GPU architectures. However, it has been widely observed that there is no “best-for-all” sparse format for the SpMV kernel on GPU. Indeed, serious performance degradation of an order of magnitude can be observed without a careful selection of the sparse format to use. To address this problem, we propose in this article BestSF (Best Sparse Format), a new learning-based sparse meta-format that automatically selects the most appropriate sparse format for a given input matrix. To do so, BestSF relies on a cost-sensitive classification system trained using Weighted Support Vector Machines (WSVMs) to predict the best sparse format for each input sparse matrix. Our experimental results on two different NVIDIA GPU architectures using a large number of real-world sparse matrices show that BestSF achieved a noticeable overall performance improvement over using a single sparse format. While BestSF is trained to select the best sparse format in terms of performance (GFLOPS), our further experimental investigations revealed that using BestSF also led, in most of the test cases, to the best energy efficiency (MFLOPS/W). To prove its practical effectiveness, we also evaluate the performance and energy efficiency improvement achieved when using BestSF as a building block in a GPU-based Preconditioned Conjugate Gradient (PCG) iterative solver.},
  acmid        = {3226228},
  articleno    = {29},
  issue_date   = {August 2018},
  keywords     = {GPU computing, Sparse matrix-vector multiplication (SpMV), energy efficiency, iterative solvers, performance modeling},
  location     = {New York, NY, USA},
  numpages     = {27},
  publisher    = {ACM},
}

@InProceedings{Nakamura2015,
  author    = {Nakamura, Takatoshi and Nodera, Takashi},
  title     = {The Flexible {ILU} Preconditioning for Solving Large Nonsymmetric Linear Systems of Equations},
  booktitle = {Proceedings of the International Workshop on Eigenvalue Problems: Algorithms, Software and Applications in Petascale Computing},
  date      = {2015},
  editor    = {Sakurai, Tetsuya and Zhang, Shao-Liang and Imamura, Toshiyuki and Yamamoto, Yusaku and Kuramashi, Yoshinobu and Hoshi, Takeo},
  series    = {EPASA 2015},
  publisher = {Springer International Publishing},
  isbn      = {978-3-319-62426-6},
  pages     = {51--61},
  abstract  = {The ILU factorization is one of the most popular preconditioners for the Krylov subspace method, alongside the GMRES. Properties of the preconditioner derived from the ILU factorization are relayed onto the dropping rules. Recently, Zhang et al. (Numer Linear Algebra Appl 19:555--569, 2011) proposed a Flexible incomplete Cholesky (IC) factorization for symmetric linear systems. This paper is a study of the extension of the IC factorization to the nonsymmetric case. The new algorithm is called the Crout version of the flexible ILU factorization, and attempts to reduce the number of nonzero elements in the preconditioner and computation time during the GMRES iterations. Numerical results show that our approach is effective and useful.},
}

@Article{Anzt2018,
  author       = {Anzt, Hartwig and Chow, Edmond and Dongarra, Jack},
  title        = {ParILUT -- A New Parallel Threshold ILU Factorization},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2018},
  volume       = {40},
  number       = {4},
  pages        = {C503-C519},
  doi          = {10.1137/16M1079506},
  abstract     = {We propose a parallel algorithm for computing a threshold incomplete LU (ILU) factorization. The main idea is to interleave a parallel fixed-point iteration that approximates an incomplete factorization for a given sparsity pattern with a procedure that adjusts the pattern. We describe and test a strategy for identifying nonzeros to be added and nonzeros to be removed from the sparsity pattern. The resulting pattern may be different and more effective than that of existing threshold ILU algorithms. Also in contrast to other parallel threshold ILU algorithms, much of the new algorithm has fine-grained parallelism.},
}

@Article{Dziekonski2018,
  author       = {Dziekonski, A. and Mrozowski, M.},
  title        = {A {GPU} Solver for Sparse Generalized Eigenvalue Problems with Symmetric Complex-Valued Matrices Obtained Using Higher-Order FEM},
  journaltitle = {IEEE Access},
  year         = {2018},
  date         = {2018},
  doi          = {10.1109/access.2018.2871219},
  abstract     = {The paper discusses a fast implementation of the stabilized locally optimal block preconditioned conjugate gradient (sLOBPCG) method, using a hierarchical multilevel preconditioner to solve non-Hermitian sparse generalized eigenvalue problems with large symmetric complex-valued matrices obtained using the higher-order finite-element method (FEM), applied to the analysis of a microwave resonator. The resonant  frequencies  of  the  low-order  modes  are  the  eigenvalues  of  the  smallest  real  part  of  a  complex symmetric  (though  non-Hermitian)  matrix  pencil.  These  type  of  pencils  arise  in  the  FEM analysis  of resonant cavities loaded with a lossy material. To accelerate the computations, graphics processing units (GPU, NVIDIA Pascal P100) were used. Single and dual-GPU variants are considered and a GPU-memory-saving implementation is proposed. An efficient sliced ELLR-T sparse matrix storage format was used and operations were performed on blocks of vectors for best performance on a GPU. As a result, significant speedups (exceeding a factor of six in some computational scenarios) were achieved over the reference parallel implementation using a multicore central processing unit (CPU, Intel Xeon E5-2680 v3, twelve cores). These results indicate that the solution of generalized eigenproblems needs much more GPU memory than iterative techniques when solving a sparse system of equations, and also requires a second GPU to store some data structures in order to reduce the footprint, even for a moderately large systems.},
  timestamp    = {2018.10.10},
}

@Article{Goddard2018,
  author       = {Goddard, Anthony and Wathen, Andy},
  title        = {A note on parallel preconditioning for all-at-once evolutionary {PDE}s},
  journaltitle = {Electronic Transactions on Numerical Analysis},
  date         = {2018},
  volume       = {XX},
  abstract     = {McDonald, Pestana and Wathen (SIAM J. Sci. Comput. 40 (2), pp. A2012--A1033, 2018) present a method for preconditioning of time-dependent PDEs via approximation by a nearby time-periodic problem, that is, they employ circulant-related matrices as preconditioners for the non-symmetric block Toeplitz matrices which arise from an all-at-once formulation. They suggest that such an approach might be efficiently implemented in parallel. In this short article, we present parallel numerical results for their preconditioner which exhibit strong scaling. We also extend their preconditioner via a Neumann series approach, which also allows for efficient parallel execution. Our simple implementation (in C++ and MPI) is available at the Git repository PARALAAOMPI.},
  timestamp    = {2018.10.10},
}

@Article{Miyata2018,
  author       = {Miyata, Takafumi},
  title        = {On Correction-Based Iterative Methods for Eigenvalue Problems},
  journaltitle = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  date         = {2018},
  volume       = {E101.A},
  number       = {10},
  pages        = {1668--1675},
  doi          = {10.1587/transfun.E101.A.1668},
  abstract     = {The Jacobi-Davidson method and the Riccati method for eigenvalue problems are studied. In the methods, one has to solve a nonlinear equation called the correction equation per iteration, and the difference between the methods comes from how to solve the equation. In the Jacobi-Davidson/Riccati method the correction equation is solved with/without linearization. In the literature, avoiding the linearization is known as an improvement to get a better solution of the equation and bring the faster convergence. In fact, the Riccati method showed superior convergence behavior for some problems. Nevertheless the advantage of the Riccati method is still unclear, because the correction equation is solved not exactly but with low accuracy. In this paper, we analyzed the approximate solution of the correction equation and clarified the point that the Riccati method is specialized for computing particular solutions of eigenvalue problems. The result suggests that the two methods should be selectively used depending on target solutions. Our analysis was verified by numerical experiments.},
}

@Online{Fukaya2018,
  author     = {Fukaya, Takeshi and Kannan, Ramaseshan and Nakatsukasa, Yuji and Yamamoto, Yusaku and Yanagisawa, Yuka},
  title      = {Shifted {CholeskyQR} for computing the {QR} factorization of ill-conditioned matrices},
  date       = {2018},
  abstract   = {The Cholesky QR algorithm is an efficient communication-minimizing algorithm for computing the QR factorization of a tall-skinny matrix. Unfortunately it has the inherent numerical instability and breakdown when the matrix is ill-conditioned. A recent work establishes that the instability can be cured by repeating the algorithm twice (called CholeskyQR2). However, the applicability of CholeskyQR2 is still limited by the requirement that the Cholesky factorization of the Gram matrix runs to completion, which means it does not always work for matrices X with $\kappa_2(X) \ge u^{-\frac{1}{2}}$ where $u$ is the unit roundoff. In this work we extend the applicability to $\kappa_2(X)=\mathcal{O}(u^{-1})$ by introducing a shift to the computed Gram matrix so as to guarantee the Cholesky factorization $R^T R = A^T A+sI$ succeeds numerically. We show that the computed $A R^{-1}$ has reduced condition number ${} \le u^{-\frac{1}{2*}}$, for which CholeskyQR2 safely computes the QR factorization, yielding a computed Q of orthogonality $\mid Q^T Q - I\mid_2$ and residual $\mid A-QR\mid _F/\mid A\mid_F$ both $\mathcal{O}(u)$. Thus we obtain the required QR factorization by essentially running Cholesky QR thrice. We extensively analyze the resulting algorithm shiftedCholeskyQR to reveal its excellent numerical stability. shiftedCholeskyQR is also highly parallelizable, and applicable and effective also when working in an oblique inner product space. We illustrate our findings through experiments, in which we achieve significant (up to $\times$40) speedup over alternative methods.},
  eprint     = {1809.11085},
  eprinttype = {arXiv},
}

@TechReport{Kurzak2018,
  author      = {Kurzak, Jakub and Gates, Mark and Yamazaki, Ichitaro and Charara, Ali and YarKhan, Asim and Finney, Jamie and Ragghianti, Gerald and Luszczek, Piotr and Dongarra, Jack},
  title       = {{SLATE} Working Note 8: Linear Systems Performance Report},
  institution = {Innovative Computing Laboratory, University of Tennessee},
  date        = {2018-09},
  number      = {ICL-UT-XX-XX},
  note        = {revision 09-2018},
  abstract    = {Software for Linear Algebra Targeting Exascale (SLATE) is being developed as part of the Exascale Computing Project (ECP), which is a collaborative effort between two US Department of Energy (DOE) organizations, the Office of Science and the National Nuclear Security Administration (NNSA). The purpose of SLATE is to serve as a replacement for ScaLAPACK for the upcoming pre-exascale and exascale DOE machines. SLATE will accomplish this objective by leveraging recent progress in parallel programming models and by strongly focusing on
supporting hardware accelerators.

This report focuses on the set of SLATE routines that solve linear systems of equations. Specifically, initial performance numbers are reported, alongside ScaLAPACK performance numbers, on the SummitDev machine at the Oak Ridge Leadership Computing Facility (OLCF). More
details about the design of the SLATE software infrastructure can be found in the report by Kurzak et al.},
}

@Article{Su2018,
  author       = {Su, Xing and Liao, Xiangke and Jiang, Hao and Yang, Canqun and Xue, Jingling},
  title        = {{SCP}: Shared Cache Partitioning for High-Performance {GEMM}},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  date         = {2018-10},
  series       = {TACO},
  volume       = {15},
  number       = {4},
  pages        = {43:1--43:21},
  issn         = {1544-3566},
  doi          = {10.1145/3274654},
  abstract     = {GEneral Matrix Multiply (GEMM) is the most fundamental computational kernel routine in the BLAS library. To achieve high performance, in-memory data must be prefetched into fast on-chip caches before they are used. Two techniques, software prefetching and data packing, have been used to effectively exploit the capability of on-chip least recent used (LRU) caches, which are popular in traditional high-performance processors used in high-end servers and supercomputers. However, the market has recently witnessed a new diversity in processor design, resulting in high-performance processors equipped with shared caches with non-LRU replacement policies. This poses a challenge to the development of high-performance GEMM in a multithreaded context. As several threads try to load data into a shared cache simultaneously, interthread cache conflicts will increase significantly. We present a Shared Cache Partitioning (SCP) method to eliminate interthread cache conflicts in the GEMM routines, by partitioning a shared cache into physically disjoint sets and assigning different sets to different threads. We have implemented SCP in the OpenBLAS library and evaluated it on Phytium 2000+, a 64-core AArch64 processor with private LRU L1 caches and shared pseudo-random L2 caches (per four-core cluster). Our evaluation shows that SCP has effectively reduced the conflict misses in both L1 and L2 caches in a highly optimized GEMM implementation, resulting in an improvement of its performance by 2.75\% to 6.91\%.},
  acmid        = {3274654},
  articleno    = {43},
  issue_date   = {October 2018},
  keywords     = {BLAS, GEMM, high-performance computing, linear algebra, optimization},
  location     = {New York, NY, USA},
  numpages     = {21},
  publisher    = {ACM},
}

@Article{Bernaschi2018,
  author       = {Bernaschi, Massimo and D'Ambra, Pasqua and Pasquini, Dario},
  title        = {AMG based on compatible weighted matching},
  journaltitle = {Parallel Computing},
  year         = {2018},
  date         = {2018},
  abstract     = {We describe main issues and design principles of an efficient implementation, tailored to recent generations of Nvidia Graphics Processing Units (GPUs), of an Algebraic MultiGrid (AMG) preconditioner previously proposed by one of the authors and already available in the open-source package BootCMatch: Bootstrap algebraic multigrid based on Compatible weighted Matching for standard CPU. The AMG method relies on a new approach for coarsening sparse symmetric positive definite (s.p.d.) matrices, named coarsening based on compatible weighted matching. It exploits maximum weight matching in the adjacency graph of the sparse matrix, driven by the principle of compatible relaxation, providing a suitable aggregation of unknowns which goes beyond the limits of the usual heuristics applied in the current methods. We adopt an approximate solution of the maximum weight matching problem, based on a recently proposed parallel algorithm, referred as the Suitor algorithm, and show that it allow us to obtain good quality coarse matrices for our AMG on GPUs. We exploit inherent parallelism of modern GPUs in all the kernels involving sparse matrix computations both for the setup of the preconditioner and for its application in a Krylov solver, outperforming preconditioners available in Nvidia AmgX library. We report results about a large set of linear systems arising from discretization of scalar and vector partial differential equations (PDEs).},
}

@Article{Kirmani2018,
  author   = {Kirmani, Shad and Sun, Hongyang and Raghavan, Padma},
  title    = {A Scalability and Sensitivity Study of Parallel Geometric Algorithms for Graph Partitioning},
  date     = {2018},
  doi      = {10.13140/RG.2.2.32020.96644},
  abstract = {Graph partitioning arises in many computational simulation workloads, including those that involve finite difference or finite element methods, where partitioning enables efficient parallel processing of the entire simulation. We focus on parallel geometric algorithms for partitioning large graphs whose vertices are associated with coordinates in two-or three-dimensional space on multi-core processors. Compared with other types of partitioning algorithms, geometric schemes generally show better scalability on a large number of processors or cores. This paper studies the scalability and sensitivity of two parallel algorithms, namely, recursive coordinate bisection (denoted by pRCB) and geometric mesh partitioning (denoted by pGMP), in terms of their robustness to several key factors that affect the partition quality, including coordinate perturbation, approximate embedding, mesh quality and graph planarity. Our results indicate that the quality of a partition as measured by the size of the edge separator (or cutsize) remains consistently better for pGMP compared to pRCB. On average for our test suite, relative to pRCB, pGMP yields 25\% smaller cutsizes on the original embedding, and across all perturbations cutsizes that are smaller by at least 8\% and by as much as 50\%. Not surprisingly, higher quality cuts are obtained at the expense of longer execution times; on a single core, pGMP has an average execution time that is almost 10 times slower than that of pRCB, but it scales better and catches up at 32-cores to be slower by less than 20\%. With the current trends in core counts that continue to increase per chip, these results suggest that pGMP presents an attractive solution if a modest number of cores can be deployed to reduce execution times while providing high quality partitions.},
}

@InProceedings{Abe2018,
  author    = {Abe, Kuniyoshi},
  title     = {On Convergence Speed of Parallel Variants of {BiCGSTAB} for Solving Linear Equations},
  booktitle = {Methods and Applications for Modeling and Simulation of Complex Systems},
  date      = {2018},
  editor    = {Li, Liang and Hasegawa, Kyoko and Tanaka, Satoshi},
  publisher = {Springer Singapore},
  location  = {Singapore},
  pages     = {401--413},
  doi       = {10.1007/978-981-13-2853-4_31},
  abstract  = {A number of hybrid Bi-Conjugate Gradient (Bi-CG) methods such as the Bi-CG STABilized (BiCGSTAB) method have been developed for solving linear equations. BiCGSTAB has been most often used for efficiently solving the linear equations, but we have sometimes seen the convergence behavior with a long stagnation phase. In such cases, it is important to have Bi-CG coefficients that are as accurate as possible, and the stabilization strategy for improving the accuracy of the Bi-CG coefficients has been proposed. In present petascale high-performance computing hardware, the main bottleneck of Krylov subspace methods for efficient parallelization is the inner products which require a global reduction. The parallel variants of BiCGSTAB such as communication avoiding and pipelined BiCGSTAB reducing the number of global communication phases and hiding the communication latency have been proposed. However, the numerical stability, specifically, the convergence speed of the parallel variants of BiCGSTAB has not previously been clarified on problems with situations where the convergence is slow (strongly affected by rounding errors). In this paper, therefore, we examine the convergence speed between the standard BiCGSTAB and the parallel variants, and the effectiveness of the stabilization strategy by numerical experiments on the problems where the convergence has a long stagnation phase.},
}

@Online{Rong2018,
  author     = {Rong, Hongbo},
  title      = {Expressing Sparse Matrix Computations for Productive Performance on Spatial Architectures},
  date       = {2018},
  abstract   = {This paper addresses spatial programming of sparse matrix computations for productive performance. The challenge is how to express an irregular computation and its optimizations in a regular way.

A sparse matrix has (non-zero) values and a structure. In this paper, we propose to classify the implementations of a computation on a sparse matrix into two categories: (1) structure-driven, or top-down, approach, which traverses the structure with given row and column indices and locates the corresponding values, and (2) values-driven, or bottom-up, approach, which loads and processes the values in parallel streams, and decodes the structure for the values’ corresponding row and column indices. On a spatial architecture like FPGAs, the values-driven approach is the norm. We show how to express a sparse matrix computation and its optimizations for a values-driven implementation. A compiler automatically synthesizes a code to decode the structure. In this way, programmers focus on optimizing the processing of the values, using familiar optimizations for dense matrices, while leaving the complex, irregular structure traversal to an automatic compiler. We also attempt to regularize the optimizations of the reduction for a dynamic number of values, which is common in a sparse matrix computation.},
  eprint     = {1810.07517},
  eprinttype = {arXiv},
}

@Article{Carson2018,
  author       = {Carson, E. and Rozložník, M. and Strakoš, Z. and Tichý, P. and Tůma, M.},
  title        = {The Numerical Stability Analysis of Pipelined Conjugate Gradient Methods: Historical Context and Methodology},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2018},
  volume       = {40},
  number       = {5},
  pages        = {A3549-A3580},
  doi          = {10.1137/16M1103361},
  abstract     = {Algebraic solvers based on preconditioned Krylov subspace methods are among the most powerful tools for large-scale numerical computations in applied mathematics, sciences, technology, as well as in emerging applications in social sciences. As the name suggests, Krylov subspace methods can be viewed as a sequence of projections onto nested subspaces of increasing dimension. They are therefore by their nature implemented as synchronized recurrences. This is the fundamental obstacle to efficient parallel implementation. Standard approaches to overcoming this obstacle described in the literature involve reducing the number of global synchronization points and increasing parallelism in performing arithmetic operations within individual iterations. One such approach, employed by the so-called pipelined Krylov subspace methods, involves overlapping the global communication needed for computing inner products with local arithmetic computations. Inexact computations in Krylov subspace methods, due to either floating point roundoff error or intentional action motivated by savings in computing time or energy consumption, have two basic effects, namely, slowing down convergence and limiting attainable accuracy. Although the methodologies for their investigation are different, these phenomena are closely related and cannot be separated from one another. The study of mathematical properties of Krylov subspace methods, in both the cases of exact and inexact computations, is a very active area of research and many issues in the analytic theory of Krylov subspace methods remain open. Numerical stability issues have been studied since the formulation of the conjugate gradient method in the middle of the last century, with many remarkable results achieved since then. Recently, the issues of attainable accuracy and delayed convergence caused by inexact computations became of interest in relation to pipelined conjugate gradient methods and their generalizations. In this contribution we recall the related early results and developments in synchronization-reducing conjugate gradient methods, identify the main factors determining possible numerical instabilities, and present a methodology for the analysis and understanding of pipelined conjugate gradient methods. We derive an expression for the residual gap that applies to any conjugate gradient method variant that uses a particular auxiliary vector in updating the residual, including pipelined conjugate gradient methods, and show how this result can be used to perform a full-scale analysis for a particular implementation. The paper concludes with a brief perspective on Krylov subspace methods in the forthcoming exascale era.},
}

@Article{Mohammad2018,
  author       = {Mohammad, Hassan and Waziri, Mohammed Yusuf and Santos, Sandra Augusta},
  title        = {A brief survey of methods for solving nonlinear least-squares problems},
  journaltitle = {Numerical Algebra, Control \& Optimization},
  date         = {2018},
  volume       = {9},
  number       = {2155-3289_2019_1_1},
  pages        = {1},
  issn         = {2155-3289},
  doi          = {10.3934/naco.2019001},
  url          = {http://aimsciences.org//article/id/7e25fb2d-b50c-46dd-9be5-23deee2b4242},
  abstract     = {In this paper, we present a brief survey of methods for solving nonlinear least-squares problems. We pay specific attention to methods that take into account the special structure of the problems. Most of the methods discussed belong to the quasi-Newton family (i.e. the structured quasi-Newton methods (SQN)). Our survey comprises some of the traditional and modern developed methods for nonlinear least-squares problems. At the end, we suggest a few topics for further research.},
}

@Article{Aliaga2018,
  author       = {Aliaga, José I. and Dufrechou, Ernesto and Ezzatti, Pablo and Quintana-Ortíz, Enrique S.},
  title        = {An efficient {GPU} version of the preconditioned {GMRES} method},
  journaltitle = {The Journal of Supercomputing},
  date         = {2018-10-25},
  issn         = {1573-0484},
  doi          = {10.1007/s11227-018-2658-1},
  abstract     = {In a large number of scientific applications, the solution of sparse linear systems is the stage that concentrates most of the computational effort. This situation has motivated the study and development of several iterative solvers, among which preconditioned Krylov subspace methods occupy a place of privilege. In a previous effort, we developed a GPU-aware version of the GMRES method included in ILUPACK, a package of solvers distinguished by its inverse-based multilevel ILU preconditioner. In this work, we study the performance of our previous proposal and integrate several enhancements in order to mitigate its principal bottlenecks. The numerical evaluation shows that our novel proposal can reach important run-time reductions.},
  day          = {25},
}

@Article{Franchetti2018,
  author       = {Franchetti, Franz and Moura, José M. F. and Padua, David A. and Dongarra, Jack},
  title        = {From High-Level Specification to High-Performance Code},
  journaltitle = {Proceedings of the IEEE},
  date         = {2018},
  volume       = {106},
  issue        = {11},
  pages        = {1875--1878},
  abstract     = {Computer architectures and systems are becoming ever more powerful but increasingly more complex. With the end of frequency scaling (about 2004) and the era of multicores/manycores/accelerators, it is exceedingly hard to extract the promised performance, in particular, at a reasonable energy budget. Only highly trained and educated experts can hope to conquer this barrier that, if not appropriately dealt with, can translate into multiple orders of magnitude of underutilization of computer systems when programmed by less specialized programmers or domain scientists. To overcome this challenge, the last ten years have seen a flurry of activity to automate the design and generation of highly efficient implementations for these multicore/ manycore architectures, and to translate high level descriptions of programs into high performance and power efficiency},
}

@Report{Cayrols2018,
  author      = {Cayrols, Sébastien and Duff, Iain and Lopez, Florent},
  title       = {Parallelization of the solve phase in a task-based Cholesky solver using a sequential task flow model},
  type        = {Technical Report},
  institution = {Science \& Technology Facilities Council, UK},
  date        = {2018},
  abstract    = {We describe the parallelization of the solve phase in the sparse Cholesky solver SpLLT [Duff, Hogg, and Lopez. Numerical Algebra, Control and Optimization. Volume 8, 235-237, 2018] when using a sequential task flow (STF) model. In the context of direct methods, the solution of a sparse linear system is achieved through three main phases: the analyse, the factorization and the solve phases. In the last two phases which involve numerical computation, the factorization corresponds to the most computationally costly phase, and it is therefore crucial to parallelize this phase in order to reduce the time-to-solution on modern architectures. As a consequence, the solve phase is often not as optimized as the factorization in state-of-the-art solvers and opportunities for parallelism are often not exploited in this phase. However, in some applications, the time spent in the solve phase is comparable or even greater than the time for the factorization and the user could dramatically benefit from a faster solve routine. This is the case, for example, for a CG solver using a block Jacobi preconditioner. The diagonal blocks are factorized once only but their factors are used to solve subsystems at each CG iteration. In this study we design and implement a parallel version of a task-based solve routine for an OpenMP version of the SpLLT solver. We show that we can obtain good scalability on a multicore architecture enabling a dramatic reduction of the overall time-to-solution in some applications.},
}

@Online{Knigge2018,
  author     = {Knigge, Timon E. and Bisseling, Rob H.},
  title      = {An improved exact algorithm and an NP-completeness proof for sparse matrix bipartitioning},
  date       = {2018},
  abstract   = {We formulate the sparse matrix bipartitioning problem of minimizing the communication volume in parallel sparse matrix-vector multiplication. We prove its NP-completeness in the perfectly balanced case, where both parts of the partitioned matrix must have an equal number of nonzeros, by reduction from the graph bisection problem. We present an improved exact branch-and-bound algorithm which finds the minimum communication volume for a given maximum allowed imbalance. The algorithm is based on a maximum-flow bound and a pack- ing bound, which extend previous matching and packing bounds. We implemented the algorithm in a new program called MP (Matrix Partitioner), which solved 839 matrices from the SuiteSparse collection to optimality, each within 24 hours of CPU-time. Furthermore, MP solved the difficult problem of the matrix cage6 in about 3 days. The new program is about 13.8 times faster than the previous program MondriaanOpt.},
  eprint     = {1811.02043},
  eprinttype = {arXiv},
}

@Article{Cartis2018,
  author     = {Cartis, Coralia and Gould, Nick I. M. and Toint, Philippe L.},
  title      = {Sharp worst-case evaluation complexity bounds for arbitrary-order nonconvex optimization with inexpensive constraints},
  date       = {2018},
  eprint     = {1811.01220},
  eprinttype = {arXiv},
  abstract   = {We provide sharp worst-case evaluation complexity bounds for non convex minimization problems with general inexpensive constraints, i.e. problems where the cost of evaluating/enforcing of the (possibly nonconvex or even disconnected) constraints, if any, is negligible compared to that of evaluating the objective function.  These bounds unify, extend or improve all known upper and lower complexity bounds for unconstrained and convexly-constrained problems. It is shown that, given an accuracy level $\epsilon$, a degree of highest available Lipschitz continuous derivatives $p$ and a desired optimality order $q$ between one and $p$, a conceptual regularization algorithm requires no more than $O(\epsilon^{-\frac{p+1}{p-q+1}}$ evaluations of the objective function and its derivatives to compute a suitably approximate $q$-th order minimizer. With an appropriate choice of the regularization, a similar result also holds if the $p$-th derivative is merely Holder rather than Lipschitz continuous. We provide an example that shows that the above complexity bound is sharp for unconstrained and a wide class of constrained problems; we also give reasons for the optimality of regularization methods from a worst-case complexity point of view, within a large class of algorithms that use the same derivative information.},
}

@TechReport{Buttari2018,
  author      = {Buttari, Alfredo},
  date        = {2018-09},
  institution = {IRIT, Institut de recherche en informatique de Toulouse},
  title       = {Scalability of parallel sparse direct solvers: methods, memory and performance},
  eprint      = {01913033},
  eprinttype  = {HAL},
  type        = {Habilitation {\`a} diriger des recherches},
  url         = {https://hal.archives-ouvertes.fr/tel-01913033},
  abstract    = {The fast and accurate solution of large size sparse systems of linear equations is at the heart of numerical applications from a very broad range of domains including structural mechanics, fluid dynamics, geophysics, medical imaging, chemistry. Among the most commonly used techniques, direct methods, based on the factorization of the system matrix, are generally appreciated for their numerical robustness and ease of use. These advantages, however, come at the price of a considerable operations count and memory footprint. The work presented in this thesis is concerned with improving the scalability of sparse direct solvers, intended as the ability to solve problems of larger and larger size. More precisely, our work aims at developing solvers which are scalable in performance, memory consumption and complexity. We address performance scalability, that is the ability to reduce the execution time as more computational resources are available, introducing algorithms that improve parallelism by reducing communications and synchronizations. We discuss the use of novel parallel programming paradigms and tools to achieve their implementation in an efficient and portable way on modern, heterogeneous supercomputers. We present methods that make sparse direct solvers memory-scalable, that is, capable of taking advantage of parallelism without increasing the overall memory footprint. Finally we show how it is possible to use data sparsity to achieve an asymptotic reduction of the cost of such methods. The presented algorithms have been implemented in the freely distributed MUMPS and qr_mumps solver packages and their effectiveness assessed on real life problems from academic and industrial applications.},
  file        = {hdr_manuscript.pdf:https\://hal.archives-ouvertes.fr/tel-01913033/file/hdr_manuscript.pdf:PDF},
  hal_id      = {tel-01913033},
  hal_version = {v1},
  keywords    = {Sparse linear system ; linear algebra ; sparse direct methods ; parallel high performance computing ; Syst{\`e}mes lin{\'e}aires creux ; alg{\`e}bre lin{\'e}aire ; m{\'e}thodes directes ; calcul parall{\`e}le {\`a} haute performance},
  school      = {{Toulouse INP}},
}

@InProceedings{Stoltzfus2018,
  author    = {Stoltzfus, Larisa and Emani, Murali and Lin, Pei-Hung and Liao, Chunhua},
  title     = {Data Placement Optimization in {GPU} Memory Hierarchy Using Predictive Modeling},
  booktitle = {Proceedings of the Workshop on Memory Centric High Performance Computing},
  date      = {2018},
  series    = {MCHPC'18},
  publisher = {ACM},
  location  = {Dallas, TX, USA},
  isbn      = {978-1-4503-6113-2},
  pages     = {45--49},
  doi       = {10.1145/3286475.3286482},
  abstract  = {Modern supercomputers often use Graphic Processing Units (or GPUs) to meet the ever-growing demands for high performance computing. GPUs typically have a complex memory architecture with various types of memories and caches, such as global memory, shared memory, constant memory, and texture memory. The placement of data on these memories has a tremendous impact on the performance of the HPC applications and identifying the optimal placement location is non-trivial.\\
In this paper, we propose a machine learning-based approach to build a classifier to determine the best class of GPU memory that will minimize GPU kernel execution time. This approach utilizes a set of performance counters obtained from profiling runs along with hardware features to generate the trained model. We evaluate our approach on several generations of NVIDIA GPUs, including Kepler, Maxwell, Pascal, and Volta on a set of benchmarks. The results show that the trained model achieves prediction accuracy over 90\% and given a global version, the classifier can accurately determine which data placement variant would yield the best performance.},
  acmid     = {3286482},
  address   = {New York, NY, USA},
  keywords  = {Data placement, GPU, Machine Learning, Memory},
  numpages  = {5},
}

@InProceedings{Yang2018,
  author    = {Yang, Carl},
  title     = {Linear Algebra is the Right Way to Think About Graphs},
  booktitle = {Proceedings of the 2018 ACM/IEEE Supercomputing Conference},
  date      = {2018},
  series    = {SC'18},
  abstract  = {Graph algorithms are challenging to implement on new accelerators such as GPUs. To address this problem, GraphBLAS is an innovative on-going effort by the graph analytics community to formulate graph algorithms as sparse linear algebra, so that they can be expressed in a performant, succinct and in a backend-agnostic manner. Initial research efforts in implementing GraphBLAS on GPUs for graph processing and analytics have been promising, but challenges such as feature-incompleteness and poor performance still exist compared to their vertex-centric (“think like a vertex”) graph framework counterparts. For our thesis, we propose a multi-language graph framework aiming to simplify the development of graph algorithms, which 1) provides a multi-language GraphBLAS interface for the end-users to express, develop, and refine graph algorithms more succinctly than existing distributed graph frameworks; 2) abstracts away from the end-users performance tuning decisions; 3) utilizes the advantages of existing low-level GPU computing primitives to maintain high performance.},
}

@Article{Aliaga2018a,
  author       = {Aliaga, José I. and Barreda, María and Castaño, Asunción},
  title        = {Energy-aware Strategies for Task-parallel Sparse Linear System Solvers},
  journaltitle = {Concurrency and Computation Practice and Experience},
  date         = {2018},
  doi          = {10.1002/cpe.4633},
  abstract     = {We present some energy-aware strategies to improve the energy efficiency of a task-parallel preconditioned Conjugate Gradient (PCG) iterative solver on a Haswell-EP Intel Xeon. These techniques leverage the power-saving states of the processor, promoting the hardware into a more energy-efficient C-state and modifying the CPU frequency (P-states of the processors) of some operations of the PCG. We demonstrate that the application of these strategies during the main operations of the iterative solver can reduce its energy consumption considerably.},
}

@Article{Zhang2018,
  author       = {Zhang, Yunming and Yang, Mengjiao and Baghdadi, Riyadh and Kamil, Shoaib and Shun, Julian and Amarasinghe, Saman},
  title        = {GraphIt: A High-performance Graph DSL},
  journaltitle = {Proceedings of the ACM Conference on Programming Languages},
  date         = {2018-10},
  series       = {OOPSLA'18},
  volume       = {2},
  pages        = {121:1--121:30},
  issn         = {2475-1421},
  doi          = {10.1145/3276491},
  abstract     = {The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. As a result, programmers must try different combinations of a large set of techniques, which make tradeoffs among locality, work-efficiency, and parallelism, to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks and domain specific languages (DSLs) lack flexibility, supporting only a limited set of optimizations.\\
This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The algorithm language simplifies expressing the algorithms, while exposing opportunities for optimizations. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of edge traversal, vertex data layout, and program structure optimizations. The separation of algorithm and schedule also enables us to build an autotuner on top of GraphIt to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. We evaluate GraphIt’s performance with seven algorithms on graphs with different structures and sizes. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\times$, and is never more than 43\% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.},
  acmid        = {3276491},
  articleno    = {121},
  issue_date   = {November 2018},
  keywords     = {Big Data, Code Generation, Compiler Optimizations, Domain Specific Languages, Graph Algorithms, Parallel Programming Languages},
  location     = {New York, NY, USA},
  numpages     = {30},
  publisher    = {ACM},
}

@Article{Neumann2018,
  author       = {Neumann, Christoph and Stein, Oliver},
  title        = {Generating feasible points for mixed-integer convex optimization problems by inner parallel cuts},
  journaltitle = {Optimization Online},
  date         = {2018},
  note         = {Preprint ID 2018-11-6947},
  abstract     = {In this article we introduce an inner parallel cutting plane method (IPCP) to compute good feasible points for mixed-integer convex optimization problems. The method iteratively generates polyhedral outer approximations of an enlarged inner parallel set (EIPS) of the continuously relaxed feasible set. This EIPS possesses the crucial property that any rounding of any of its elements is feasible for the original problem. The outer approximations are refined in each iteration by using modified Kelley cutting planes, which are defined via rounded optimal points of linear opti- mization problems (LPs).\\ We show that the method either computes a feasible point or certifies that the EIPS is empty. Moreover, we provide bounds on the objective value of the generated feasible point. As there exist consistent problems which possess an empty EIPS, the IPCP is not guaranteed to find a feasible point for the latter. Yet, the crucial advantage of the method lies in the complexity of each iteration: While other approaches need to solve a mixed-integer linear optimization problem, the IPCP only needs to solve an LP, which can be carried out efficiently. Our computational study indicates that the IPCP is able to quickly find feasible points for many practical applications. It further demonstrates that the objective values of the computed feasible points are generally of good quality and sometimes not easily obtainable by other methods.},
  timestamp    = {2018.12.20},
}

@InProceedings{Franceschini2018,
  author    = {Franceschini, Andrea and Ferronato, Massimiliano and Janna, Carlo and Magri, Victor A. P.},
  title     = {Recent advancements in preconditioning techniques for large size linear systems suited for High Performance Computing},
  booktitle = {Seminari Padovani di Analisi Numerica 2018},
  date      = {2018},
  volume    = {11},
  series    = {SPAN2018},
  pages     = {11--22},
  abstract  = {The numerical simulations of real-world engineering problems create models with several millions or even billions of degrees of freedom. Most of these simulations are centered on the solution of systems of non-linear equations, that, once linearized, become a sequence of linear systems, whose solution is often the most time-demanding task. Thus, in order to increase the capability of modeling larger cases, it is of paramount importance to exploit the resources of High Performance Computing architectures. In this framework, the development of new algorithms to accelerate the solution of linear systems for many-core architectures is a really active research field.  Our main focus is algebraic preconditioning and, among the various options, we elect to develop approximate inverses for symmetric and positive definite (SPD) linear systems [22], both as stand-alone preconditioner or smoother for AMG techniques. This choice is mainly supported by the almost perfect parallelism that intrinsically characterizes these algorithms. As basic kernel, the Factorized Sparse Approximate Inverse (FSAI) developed in its adaptive form by Janna and Ferronato [18] is selected. Recent developments are i) a robust multilevel approach for SPD problems based on FSAI preconditioning, which eliminates the chance of algorithmic breakdowns independently of the preconditioner sparsity [14] and ii) a novel AMG approach featuring the adaptive FSAI method as a flexible smoother as well as new approaches to adaptively compute the prolongation operator. In this latter work, a new technique to build the prolongation is also presented.},
  timestamp = {2019.01.01},
}

@InProceedings{Sun2018,
  author    = {Sun, X. and Wei, K. and Lai, L. and Tsai, S. and Wu, C.},
  title     = {Optimizing Sparse Matrix-Vector Multiplication on {GPUs} via Index Compression},
  booktitle = {Proceedings of the 3rd IEEE Advanced Information Technology, Electronic and Automation Control Conference},
  date      = {2018-10},
  series    = {IAEAC},
  pages     = {598--602},
  doi       = {10.1109/IAEAC.2018.8577693},
  abstract  = {Sparse matrix-vector multiplication (SpMV) as one of the most significant scientific kernels has been widely used in many scientific disciplines. In practical applications, large-scale spare matrices are usually used for calculation. During these years, Graphic Processing Unit (GPU) has become a powerful platform for high-performance computing, and optimizing$S$pMV on GPU based systems for efficient performance is the principal interest in many researches. In this paper, we proposed a new method to optimize SpMV on GPUs via index compression. Our index compression method can reduce the index value of the access space. The memory space for recording each column index is significantly reduced from two bytes to one byte, which outperforms the previous work on access performance. The main contributions we make are as follows: (1) Only one byte for each column index is required, which can significantly reduce the working set of the column index and further improve the cache hit ration. (2) Our method can be applied to any kind of matrices, while the previous work can only apply to subset of the matrices. Computational experiments on problems according to the previous work reveal that the best performance improvement ration for ours is up to about 1.5.},
  issn      = {2381-0947},
  keywords  = {Indexes;Sparse matrices;Graphics processing units;Kernel;Bandwidth;Optimization;Memory management;Sparse Matrix-Vector Multiplication;GPU;CSR;CUDA},
}

@Online{Booth2018,
  author      = {Booth, Joshua Dennis and Bolet, Gregory},
  title       = {Javelin: A Scalable Implementation for Sparse Incomplete {LU} Factorization},
  year        = {2018},
  date        = {2018-12-13},
  month       = dec,
  abstract    = {In this work, we present a new scalable incomplete LU factorization framework called Javelin to be used as a preconditioner for solving sparse linear systems with iterative methods. Javelin allows for improved parallel factorization on shared-memory many-core systems, while packaging the coefficient matrix into a format that allows for high performance sparse matrix-vector multiplication and sparse triangular solves with minimal overheads. The framework achieves these goals by using a collection of traditional permutations, point-to-point thread synchronizations, tasking, and segmented prefix scans in a conventional compressed sparse row format. Using these changes, traditional fill-in and drop tolerance methods can be used, while still being able to have observed speedups of up to ~42$\times$ on 68 Intel Knights Landing cores and ~12$\times$ on 14 Intel Haswell cores.},
  eprint      = {1812.06160v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1812.06160v1:PDF},
}

@Article{Xu2019,
  author       = {Xu, Zhen and Chen, Xuhao and Shen, Jie and Zhang, Yang and Chen, Cheng and Yang, Canqun},
  title        = {{GARDENIA}: A Graph Processing Benchmark Suite for Next-Generation Accelerators},
  journaltitle = {ACM Journal on Emerging Technologies in Computing Systems},
  date         = {2019-01},
  volume       = {15},
  number       = {1},
  pages        = {9:1--9:13},
  issn         = {1550-4832},
  doi          = {10.1145/3283450},
  abstract     = {This article presents the Graph Algorithm Repository for Designing Next-generation Accelerators (GARDENIA), a benchmark suite for studying irregular graph algorithms on massively parallel accelerators. Applications with limited control and data irregularity are the main focus of existing generic benchmarks for accelerators, while available graph processing benchmarks do not apply state-of-the-art algorithms and/or optimization techniques. GARDENIA includes emerging graph processing workloads from graph analytics, sparse linear algebra, and machine-learning domains, which mimic massively multithreaded commercial programs running on modern large-scale datacenters. Our characterization shows that GARDENIA exhibits irregular microarchitectural behavior, which is quite different from structured workloads and straightforward-implemented graph benchmarks.},
  acmid        = {3283450},
  articleno    = {9},
  issue_date   = {January 2019},
  keywords     = {Benchmark suite, graph processing, irregular workloads, massive multithreading},
  location     = {New York, NY, USA},
  numpages     = {13},
  publisher    = {ACM},
}

@InProceedings{Loncaric2018,
  author    = {Loncaric, Calvin and Ernst, Michael D. and Torlak, Emina},
  title     = {Generalized Data Structure Synthesis},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  date      = {2018},
  series    = {ICSE'18},
  publisher = {ACM},
  location  = {Gothenburg, Sweden},
  isbn      = {978-1-4503-5638-1},
  pages     = {958--968},
  doi       = {10.1145/3180155.3180211},
  abstract  = {Data structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums.\\This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable.},
  acmid     = {3180211},
  address   = {New York, NY, USA},
  keywords  = {automatic programming, data structures, program synthesis},
  numpages  = {11},
}

@InProceedings{Idreos2019,
  author    = {Idreos, Stratos and Dayan, Niv and Qin, Wilson and Akmanalp, Mali and Hilgard, Sophie and Ross, Andrew and Lennon, James and Jain, Varun and Gupta, Harshita and Li, David and Zhu, Zichen},
  booktitle = {Biennial Conference on Innovative Data Systems Research},
  date      = {2019},
  title     = {Design Continuums and the Path Toward Self-Designing Key-Value Stores that Know and Learn},
  abstract  = {We introduce the concept of design continuums for the data layout of key-value stores. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1)~render what we consider up to now as fundamentally different data structures to be seen as views'' of the very same overall design space, and 2)~allow seeing'' new data structure designs with performance properties that are not feasible by existing designs. The core intuition behind the construction of design continuums is that all data structures arise from the very same set of fundamental design principles, i.e., a small set of data layout design concepts out of which we can synthesize any design that exists in the literature as well as new ones. We show how to construct, evaluate, and expand, design continuums and we also present the first continuum that unifies major data structure designs, i.e., B+Tree, BeTree, LSM-tree, and LSH-Table.\\The practical benefit of a design continuum is that it creates a fast inference engine for the design of data structures. For example, we can near instantly predict how a specific design change in the underlying storage of a data system would affect performance, or reversely what would be the optimal data structure (from a given set of designs) given workload characteristics and a memory budget. In turn, these properties allow us to envision a new class of self-designing key-value stores with a substantially improved ability to adapt to workload and hardware changes by transitioning between drastically different data structure designs to assume a diverse set of performance properties at will.},
}

@Report{Duff2019,
  author      = {Iain Duff, Jonathan Hogg and Lopez, Florent},
  title       = {A new sparse symmetric indefinite solver using A Posteriori Threshold Pivoting},
  type        = {Technical Report},
  institution = {Science \& Technology Facilities Council, UK},
  date        = {2019},
  number      = {RAL-TR-2018-008},
  abstract    = {The factorization of sparse symmetric indefinite systems is particularly challenging since pivoting is required to maintain stability of the factorization. Pivoting techniques generally offer limited parallelism and are associated with significant data movement hindering the scalability of these methods. Variants of the Threshold Partial Pivoting (TPP) algorithm for example have been often used because of its numerical robustness but standard implementations exhibit poor parallel performance. On the other hand, some methods trade stability for performance on parallel architectures such as the Supernode Bunch-Kaufman (SBK) used in the PARDISO solver. In this case, however, the factors obtained might not be used to accurately compute the solution of the system. For this reason we have designed a task-based $LDL^T$ factorization algorithm based on a new pivoting strategy called A Posteriori Threshold Pivoting (APTP) that is much more suitable for modern multicore architectures and has the same numerical robustness as the TPP strategy. We implemented our algorithm in a new version of the SPRAL Sparse Symmetric Indefinite Direct Solver (SSIDS) which initially supported GPU-only factorization. We have used OpenMP 4 task features to implement a multifrontal algorithm with dense factorizations using the novel APTP, and we show that it performs favourably compared to the state-of-the-art solvers HSL_MA86, HSL_MA97 and PARDISO both in terms of performance on a multicore machine and in terms of numerical robustness. Finally we show that this new solver is able to make use of GPU devices for accelerating the factorization on heterogeneous architectures.},
}

@Article{Rais2019,
  author       = {Rais, Helmi M. D. and Abed, Saad Adnan and Watada, Junzo},
  title        = {Computational Comparison of Major Proposed Methods for Graph Partitioning Problem},
  journaltitle = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
  date         = {2019},
  volume       = {23},
  number       = {1},
  pages        = {5--17},
  doi          = {10.20965/jaciii.2019.p0005},
  abstract     = {$k$-way graph partitioning is an NP-complete problem, which is applied to various tasks such as route planning, image segmentation, community detection, and high-performance computing. The approximate methods constitute a useful solution for these types of problems. Thus, many research studies have focused on developing meta-heuristic algorithms to tackle the graph partitioning problem. Local search is one of the earliest methods that has been applied efficiently to this type of problem. Recent studies have explored various types of local search methods and have improved them such that they can be used with the partitioning process. Moreover, local search methods are widely integrated with population-based approaches, to provide the best diversification and intensification for the problem space. This study emphasizes the local search approaches, as well as their combination with other graph partitioning approaches. At present, none of the surveys in the literature has focused on this class of state of the art approaches in much detail. In this study, the vital parts of these approaches including neighborhood structure, acceptance criterion, and the ways of combining them with other approaches, are highlighted. Additionally, we provide an experimental comparison that shows the variance in the performance of the reviewed methods. Hence, this study clarifies these methods to show their advantages and limitations for the targeted problem, and thus can aid in the direction of research flow towards the area of graph partitioning.},
}

@InProceedings{Muro2019,
  author    = {Muro, Ryo and Fujii, Akihiro and Tanaka, Teruo},
  title     = {Acceleration of Symmetric Sparse Matrix-Vector Product Using Improved Hierarchical Diagonal Blocking Format},
  booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
  date      = {2019},
  series    = {HPC Asia 2019},
  publisher = {ACM},
  location  = {Guangzhou, China},
  isbn      = {978-1-4503-6632-8},
  pages     = {63--70},
  doi       = {10.1145/3293320.3293332},
  abstract  = {In the previous study, Guy et al. proposed sparse matrix-vector product (SpMV) acceleration using the Hierarchical Diagonal Blocking (HDB) format that recursively repeated partitioning, reordering, and blocking on symmetric sparse matrix. The HDB format stores sparse matrix hierarchically using tree structure. Each node of tree structure of HDB format store small sparse matrices using CSR format.\\
In this present study, we examined two problems with the HDB format and provided a solution for each problem.\\
First, SpMV using the HDB format has a partial dependent relationship among hierarchies. The problem with the HDB format is that the parallelism of computation decreases as the hierarchy of nodes gets closer to the root. Thus, we propose cutting of dependency using work vectors to solve this problem.\\
Second, each node of the conventional HDB format is stored in Compressed Sparse Row (CSR) format. Block compressed Sparse Row (BSR) format often becomes faster than CSR format in SpMV performance. Thus, we evaluated the effectiveness of our proposed method with work vectors also for BSR-HDB format.\\
In addition, we compare the performance in the general format (CSR format, BSR format) using the Intel Math Kernel Library (MKL), the conventional HDB format, and the expanded HDB format by using 22 types of sparse matrix that from various field. The results showed that the SpMV performance was highest in the HDB format that we expanded in 19 types of sparse matrix, which was 1.99 times faster than the CSR format.},
  acmid     = {3293332},
  address   = {New York, NY, USA},
  keywords  = {Sparse Matrix Storage Format, Sparse Matrix-Vector Product, Task Parallelism},
  numpages  = {8},
}

@Article{Demirci2019,
  author       = {Demirci, Gunduz Vehbi and Aykanat, Cevdet},
  title        = {Scaling sparse matrix-matrix multiplication in the accumulo database},
  journaltitle = {Distributed and Parallel Databases},
  date         = {2019-01-28},
  issn         = {1573-7578},
  doi          = {10.1007/s10619-019-07257-y},
  abstract     = {We propose and implement a sparse matrix-matrix multiplication (SpGEMM) algorithm running on top of Accumulo's iterator framework which enables high performance distributed parallelism. The proposed algorithm provides write-locality while ingesting the output matrix back to database via utilizing row-by-row parallel SpGEMM. The proposed solution also alleviates scanning of input matrices multiple times by making use of Accumulo's batch scanning capability which is used for accessing multiple ranges of key-value pairs in parallel. Even though the use of batch-scanning introduces some latency overheads, these overheads are alleviated by the proposed solution and by using node-level parallelism structures. We also propose a matrix partitioning scheme which reduces the total communication volume and provides a balance of workload among servers. The results of extensive experiments performed on both real-world and synthetic sparse matrices show that the proposed algorithm scales significantly better than the outer-product parallel SpGEMM algorithm available in the Graphulo library. By applying the proposed matrix partitioning, the performance of the proposed algorithm is further improved considerably.},
  day          = {28},
}

@TechReport{Hoemmen2019,
  author      = {Hoemmen, Mark and Badwaik, Jayesh and Brucher, Matthieu and Iliopoulos, Athanasios (Nasos) and Michopoulos, John},
  title       = {Historical lessons for C++ linear algebra library standardization},
  institution = {ISO C++ standards meeting (Kona)},
  date        = {2019},
  type        = {techreport},
  number      = {P1417R0},
  url         = {http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2019/p1417r0.pdf},
  timestamp   = {2019.02.05},
}

@InProceedings{Hong2019,
  author    = {Hong, Changwan and Sukumaran-Rajam, Aravind and Nisa, Israt and Singh, Kunal and Sadayappan, P.},
  title     = {Adaptive Sparse Tiling for Sparse Matrix Multiplication},
  booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
  date      = {2019},
  series    = {PPoPP '19},
  publisher = {ACM},
  location  = {Washington, District of Columbia},
  isbn      = {978-1-4503-6225-2},
  pages     = {300--314},
  doi       = {10.1145/3293883.3295712},
  abstract  = {Tiling is a key technique for data locality optimization and is widely used in high-performance implementations of dense matrix-matrix multiplication for multicore/manycore CPUs and GPUs. However, the irregular and matrix-dependent data access pattern of sparse matrix multiplication makes it challenging to use tiling to enhance data reuse. In this paper, we devise an adaptive tiling strategy and apply it to enhance the performance of two primitives: SpMM (product of sparse matrix and dense matrix) and SDDMM (sampled dense-dense matrix multiplication). In contrast to studies that have resorted to non-standard sparse-matrix representations to enhance performance, we use the standard Compressed Sparse Row (CSR) representation, within which intra-row reordering is performed to enable adaptive tiling. Experimental evaluation using an extensive set of matrices from the Sparse Suite collection demonstrates significant performance improvement over currently available state-of-the-art alternatives.},
  acmid     = {3295712},
  address   = {New York, NY, USA},
  keywords  = {GPU, SDDMM, SpMM, multicore/manycore, sampled dense-dense matrix multiplication, sparse matrix-matrix multiplication, tiling},
  numpages  = {15},
}

@InProceedings{Winter2019,
  author    = {Winter, Martin and Mlakar, Daniel and Zayer, Rhaleb and Seidel, Hans-Peter and Steinberger, Markus},
  title     = {Adaptive Sparse Matrix-matrix Multiplication on the {GPU}},
  booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
  date      = {2019},
  series    = {PPoPP '19},
  publisher = {ACM},
  location  = {Washington, District of Columbia},
  isbn      = {978-1-4503-6225-2},
  pages     = {68--81},
  doi       = {10.1145/3293883.3295701},
  abstract  = {In the ongoing efforts targeting the vectorization of linear algebra primitives, sparse matrix-matrix multiplication (SpGEMM) has received considerably less attention than sparse Matrix-Vector multiplication (SpMV). While both are equally important, this disparity can be attributed mainly to the additional formidable challenges raised by SpGEMM.\\ In this paper, we present a dynamic approach for addressing SpGEMM on the GPU. Our approach works directly on the standard compressed sparse rows (CSR) data format. In comparison to previous SpGEMM implementations, our approach guarantees a homogeneous, load-balanced access pattern to the first input matrix and improves memory access to the second input matrix. It adaptively re-purposes GPU threads during execution and maximizes the time efficient on-chip scratchpad memory can be used. Adhering to a completely deterministic scheduling pattern guarantees bit-stable results during repetitive execution, a property missing from other approaches. Evaluation on an extensive sparse matrix benchmark suggests our approach being the fastest SpGEMM implementation for highly sparse matrices (80\% of the set). When bit-stable results are sought, our approach is the fastest across the entire test set.},
  acmid     = {3295701},
  address   = {New York, NY, USA},
  keywords  = {ESC, GPU, SpGEMM, adaptive, bit-stable, sparse matrix},
  numpages  = {14},
}

@InProceedings{Meng2019,
  author    = {Meng, Ke and Li, Jiajia and Tan, Guangming and Sun, Ninghui},
  title     = {A Pattern Based Algorithmic Autotuner for Graph Processing on {GPUs}},
  booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
  date      = {2019},
  series    = {PPoPP '19},
  publisher = {ACM},
  location  = {Washington, District of Columbia},
  isbn      = {978-1-4503-6225-2},
  pages     = {201--213},
  doi       = {10.1145/3293883.3295716},
  abstract  = {This paper proposes Gswitch, a pattern-based algorithmic auto-tuning system that dynamically switches between optimization variants with negligible overhead. Its novelty lies in a small set of algorithmic patterns that allow for the configurable assembly of variants of the algorithm. The fast transition of Gswitch is based on a machine learning model trained using 644 real graphs. Moreover, Gswitch provides a simple programming interface that conceals low-level tuning details from the user. We evaluate Gswitch on typical graph algorithms (BFS, CC, PR, SSSP, and BC) using Nvidia Kepler and Pascal GPUs. The results show that Gswitch runs up to 10$times$ faster than the best configuration of the state-of-the-art programmable GPU-based graph processing libraries on 10 representative graphs. Gswitch outperforms Gunrock on 92.4\% cases of 644 graphs which is the largest dataset evaluation reported to date.},
  acmid     = {3295716},
  address   = {New York, NY, USA},
  keywords  = {GPU, auto-tuning, graph processing},
  numpages  = {13},
}

@Article{Reguly2019,
  author       = {Reguly, I. Z. and Mudalige, G. R. and Giles, M. B. and Maheswaran, S.},
  title        = {Improving resilience of scientific software through a domain-specific approach},
  journaltitle = {Journal of Parallel and Distributed Computing},
  date         = {2019},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2019.01.015},
  url          = {http://www.sciencedirect.com/science/article/pii/S0743731519300917},
  abstract     = {In this paper we present research on improving the resilience of the execution of scientific software, an increasingly important concern in High Performance Computing (HPC). We build on an existing high-level abstraction framework, the Oxford Parallel library for Structured meshes (OPS), developed for the solution of multi-block structured mesh-based applications, and implement an algorithm in the library to carry out checkpointing automatically, without the intervention of the user. The target applications are a hydrodynamics benchmark application from the Mantevo Suite, CloverLeaf 3D, the sparse linear solver proxy application TeaLeaf, and the OpenSBLI compressible Navier--Stokes direct numerical simulation (DNS) solver. We present (1) the basic algorithm that OPS relies on to determine the optimal checkpoint in terms of size and location, (2) improvements that supply additional information to improve the decision, (3) techniques that reduce the cost of writing the checkpoints to non-volatile storage, (4) a performance analysis of the developed techniques on a single workstation and on several supercomputers, including ORNL’s Titan. Our results demonstrate the utility of the high-level abstractions approach in automating the checkpointing process and show that performance is comparable to, or better than the reference in all cases.},
  keywords     = {Domain specific language, High performance computing, Checkpointing, Resilience, Parallel I/O},
}

@Article{Zheng2019,
  author       = {Zheng, Hua and Vong, Seakweng and Liu, Ling},
  title        = {A direct preconditioned modulus-based iteration method for solving nonlinear complementarity problems of H-matrices},
  journaltitle = {Applied Mathematics and Computation},
  date         = {2019},
  volume       = {353},
  pages        = {396--405},
  issn         = {0096-3003},
  doi          = {10.1016/j.amc.2019.02.015},
  url          = {http://www.sciencedirect.com/science/article/pii/S0096300319301134},
  abstract     = {In this paper, we establish a direct preconditioned modulus-based iteration method for solving a class of nonlinear complementarity problems with the system matrix being an H-matrix. The convergence theorems of the proposed method are given, which generalize and improve the existing ones. Numerical examples show that the proposed method is efficient.},
  keywords     = {Nonlinear complementarity problem, Modulus-based matrix splitting iteration method, Precondition},
}

@Article{Aliaga2019,
  author       = {Aliaga, José I. and Dufrechou, Ernesto and Ezzatti, Pablo and Quintana-Ortí, Enrique S.},
  title        = {Accelerating the task/data-parallel version of {ILUPACK}’s {BiCG} in multi-{CPU}/{GPU} configurations},
  journaltitle = {Parallel Computing},
  date         = {2019},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2019.02.005},
  url          = {http://www.sciencedirect.com/science/article/pii/S0167819118301777},
  abstract     = {ILUPACK is a valuable tool for the solution of sparse linear systems via iterative Krylov subspace-based methods. Its relevance for the solution of real problems has motivated several efforts to enhance its performance on parallel machines. In this work we focus on exploiting the task-level parallelism derived from the structure of the BiCG method, in addition to the data-level parallelism of the internal matrix computations, with the goal of boosting the performance of a GPU (graphics processing unit) implementation of this solver. First, we revisit the use of dual-GPU systems to execute independent stages of the BiCG concurrently on both accelerators, while leveraging the extra memory space to improve the data access patterns. In addition, we extend our ideas to compute the BiCG method efficiently in multicore platforms with a single GPU. In this line, we study the possibilities offered by hybrid CPU-GPU computations, as well as a novel synchronization-free sparse triangular linear solver. The experimental results with the new solvers show important acceleration factors with respect to the previous data-parallel CPU and GPU versions.},
  keywords     = {Sparse linear systems, Iterative Krylov-subspace methods, Data parallelism, ILUPACK preconditioner, Graphics processing units (GPUs)},
}

@Article{Gould2019,
  author       = {Nicholas I. M. Gould, Tyrone Rees and Scott, Jennifer A.},
  title        = {Convergence and evaluation-complexity analysis of a regularized tensor-Newton method for solving nonlinear least-squares problems},
  journaltitle = {Computational Optimization and Applications},
  year         = {2019},
  doi          = {10.1007/s10589-019-00064-2},
  abstract     = {Given a twice-continuously differentiable vector-valued function $r(x)$, a local minimizer of $\Vert r(x) \Vert_2$ is sought. We propose and analyse tensor-Newton methods, in which $r(x)$ is replaced locally by its second-order Taylor approximation. Convergence is controlled by regularization of various orders. We establish global convergence to a first-order critical point of $\Vert r(x) \Vert_2$, and provide function evaluation bounds that agree with the best-known bounds for methods using second derivatives. Numerical experiments comparing tensor-Newton methods with regularized Gauss--Newton and Newton methods demonstrate the practical performance of the newly proposed method.},
}

@InProceedings{Flegar2017,
  author    = {Flegar, Goran and Anzt, Hartwig},
  title     = {Overcoming Load Imbalance for Irregular Sparse Matrices},
  booktitle = {Proceedings of the Seventh Workshop on Irregular Applications: Architectures and Algorithms},
  date      = {2017-11},
  series    = {IA3 2017},
  pages     = {1--8},
  doi       = {10.1145/3149704.3149767},
  abstract  = {In this paper we propose a load-balanced GPU kernel for computing the sparse matrix vector (SpMV) product. Making heavy use of the latest GPU programming features, we also enable satisfying performance for irregular and unbalanced matrices. In a performance comparison using 400 test matrices we reveal the new kernel being superior to the most popular SpMV implementations.},
  timestamp = {2019.03.07},
}

@Article{Lagraviere2019,
  author       = {Lagravière, Jèrèmie and Langguth, Johannes and Prugger, Martina and Einkemmer, Lukas and Ha, Phuong Hoai and Cai, Xing},
  title        = {Performance Optimization and Modeling of Fine-Grained Irregular Communication in {UPC}},
  journaltitle = {Scientific Programming},
  date         = {2019},
  volume       = {2019},
  doi          = {10.1155/2019/6825728},
  abstract     = {.e Unified Parallel C (UPC) programming language offers parallelism via logically partitioned shared memory, which typically spans physically disjoint memory subsystems. One convenient feature of UPC is its ability to automatically execute betweenthread data movement, such that the entire content of a shared data array appears to be freely accessible by all the threads. .e programmer friendliness, however, can come at the cost of substantial performance penalties. .is is especially true when indirectly indexing the elements of a shared array, for which the induced between-thread data communication can be irregular and have a fine-grained pattern. In this paper, we study performance enhancement strategies specifically targeting such finegrained irregular communication in UPC. Starting from explicit thread privatization, continuing with block-wise communication, and arriving at message condensing and consolidation, we obtained considerable performance improvement of UPC programs that originally require fine-grained irregular communication. Besides the performance enhancement strategies, the main contribution of the present paper is to propose performance models for the different scenarios, in the form of quantifiable formulas that hinge on the actual volumes of various data movements plus a small number of easily obtainable hardware characteristic parameters. .ese performance models help to verify the enhancements obtained, while also providing insightful predictions of similar parallel implementations, not limited to UPC, that also involve between-thread or between-process irregular communication. As a further validation, we also apply our performance modeling methodology and hardware characteristic parameters to an existing UPC code for solving a 2D heat equation on a uniform mesh.},
  timestamp    = {2019.03.10},
}

@Article{Demmel2014,
  author       = {Demmel, James and Diep Nguyen, Hong},
  title        = {Parallel Reproducible Summation},
  journaltitle = {IEEE Transactions on Computers},
  date         = {2014-01},
  volume       = {64},
  pages        = {1--1},
  doi          = {10.1109/TC.2014.2345391},
  abstract     = {Reproducibility, i.e. getting bitwise identical floating point results from multiple runs of the same program, is a property that many users depend on either for debugging or correctness checking in many codes [10]. However, the combination of dynamic scheduling of parallel computing resources, and floating point nonassociativity, makes attaining reproducibility a challenge even for simple reduction operations like computing the sum of a vector of numbers in parallel. We propose a technique for floating point summation that is reproducible independent of the order of summation. Our technique uses Rump’s algorithm for error-free vector transformation [7], and is much more efficient than using (possibly very) high precision arithmetic. Our algorithm reproducibly computes highly accurate results with an absolute error bound of $n cdot 2^{-28} \cdot macheps \cdot max _i |v_i|$ at a cost of $7n$ FLOPs and a small constant amount of extra memory usage. Higher accuracies are also possible by increasing the number of error-free transformations. As long as all operations are performed in to-nearest rounding mode, results computed by the proposed algorithms are reproducible for any run on any platform. In particular, our algorithm requires the minimum number of reductions, i.e. one reduction of an array of six double precision floating point numbers per sum, and hence is well suited for massively parallel environments.},
}

@Article{Villa2009,
  author       = {Villa, Oreste and Chavarría-Miranda, Daniel and Gurumoorthi, Vidhya and Marquez, Andres and Krishamoorthy, Sriram},
  title        = {Effects of Floating-Point non-Associativity on Numerical Computations on Massively Multithreaded Systems},
  journaltitle = {Cray User Group},
  date         = {2009-12},
  abstract     = {Floating-point operations, as defined in the IEEE-754 standard, are not associative. The ordering of large numbers of operations (such as summa-tions) that deal with operands of substantially dif-ferent magnitudes can significantly affect the final result. On massively multi-threaded systems, the non-deterministic nature of how machine floating-point operations are interleaved, combined with the fact that intermediate values have to be rounded or truncated to fit in the available precision leads to non-deterministic numerical error propagation. We have investigated on a Cray XMT system the effect of non-deterministic error propagation by observing the convergence rate of a conjugate gradient calcula-tion used as part of a Power State Estimation (PSE) application. As a possible mitigation strategy, we have explored quadruple precision accumulation, as well as a deterministic parallel tree scheme. The tree based approach has consistently outperformed the quadruple precision approach due to an improved convergence rate. As a consequence, we motivate the need for compile time mechanisms that enable enforcement of parallel deterministic operations on the Cray XMT.},
}

@TechReport{Harvey2019,
  author      = {Harvey, David and van der Hoeven, Joris},
  date        = {2019},
  institution = {HAL archives},
  title       = {Integer multiplication in time $O(n log n)$},
  eprint      = {02070778},
  eprinttype  = {HAL},
  url         = {https://hal.archives-ouvertes.fr/hal-02070778},
  abstract    = {We present an algorithm that computes the product of two $n$-bitintegers in $O(n log n)$ bit operations.},
}

@Article{Scott2019,
  author       = {Scott, J. and Tuma, M.},
  title        = {Sparse stretching for solving sparse-dense linear least-squares problems},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2019},
  issn         = {1095-7197},
  abstract     = {Large-scale linear least-squares problems arise in a wide range of practical applications. In some cases, the system matrix contains a small number of dense rows. These make the problem significantly harder to solve because their presence limits the direct applicability of sparse matrix techniques. In particular, the normal matrix is (close to) dense,
 so that forming it is impractical. One way to help overcome the dense row problem is to employ matrix stretching.
 Stretching is a sparse matrix technique that improves sparsity by making the least-squares problem larger.
 We show that standard stretching can still result in the normal matrix for the stretched problem having an unacceptably large amount of fill. This motivates us to propose a new sparse stretching strategy that performs the stretching so as to limit the fill in the normal matrix and its Cholesky factor. Numerical examples from real problems
 are used to illustrate the potential gains.},
  timestamp    = {2019.03.31},
}

@InProceedings{Blass2019,
  author    = {Blaß, Thorsten and Philippsen, Michael},
  title     = {Which Graph Representation to Select for Static Graph-Algorithms on a {CUDA}-capable {GPU}},
  booktitle = {Proceedings of the 12th Workshop on General Purpose Processing Using {GPUs}},
  date      = {2019},
  series    = {GPGPU '19},
  publisher = {ACM},
  location  = {Providence, RI, USA},
  isbn      = {978-1-4503-6255-9},
  pages     = {22--31},
  doi       = {10.1145/3300053.3319416},
  abstract  = {GPUs seem to be ideal for algorithms that work in parallel. A number of ways to represent graphs in GPU memory are known. But so far there are no guidelines to select the representation that is likely to result in the best performance.\\
This a comprehensive study investigates for CUDA-capable GPUs how different graph representations influence the performance of highly optimized graph processing algorithms that traverse the graphs without modifying them. We evaluate three different graph exchange formats and how efficiently they can be imported into eight graph data structures. We use ten state-of-the-art benchmarks that employ different traversals pattern. We evaluate them on 19 input graphs with different characteristics. The measurements show that there is not a single best data structure; the runtime performance can vary up to a factor of 2 between two representations.\\
The main contribution is a set of rules that helps in picking the best-performing graph representation for a given situation.},
  acmid     = {3319416},
  address   = {New York, NY, USA},
  keywords  = {CUDA, graph data structure, static graph algorithms},
  numpages  = {10},
}

@InProceedings{Nisa2019,
  author    = {Nisa, Israt and Li, Jiajia and Sukumaran-Rajam, Aravind and Vuduc, Richard and Sadayappan, P.},
  title     = {Load-Balanced Sparse {MTTKRP} on {GPUs}},
  booktitle = {Proceedings of the 2019 International Parallel and Distributed Processing Symposium},
  date      = {2019},
  series    = {IPDPS'19},
  abstract  = {Sparse matricized tensor times Khatri-Rao product (MTTKRP) is one of the most computationally expensive kernels in sparse tensor computations. This work focuses on optimizing the MTTKRP operation on GPUs, addressing both performance and storage requirements. We begin by identifying the performance bottlenecks in directly extending the state-ofthe-art CSF (compressed sparse fiber) format from CPUs to GPUs. A significant challenge with GPUs compared to multicore CPUs is that of utilizing the much greater degree of parallelism in a load-balanced fashion for irregular computations like sparse MTTKRP. To address this issue, we develop a new storage-efficient representation for tensors that enables highperformance, load-balanced execution of MTTKRP on GPUs. A GPU implementation of sparse MTTKRP using the new sparse tensor representation is shown to outperform all currently known parallel sparse CPU and GPU MTTKRP implementations.},
  timestamp = {2019.04.14},
}

@Online{Kawaguchi2019,
  author      = {Kawaguchi, Kenji and Pack Kaelbling, Leslie},
  title       = {Every Local Minimum is a Global Minimum of an Induced Model},
  date        = {2019-04},
  abstract    = {For non-convex optimization in machine learning, this paper proves that every local minimum achieves the global optimality of the perturbable gradient basis model at any differentiable point. As a result, non-convex machine learning is theoretically as supported as convex machine learning with a hand-crafted basis in terms of the loss at differentiable local minima, except in the case when a preference is given to the hand-crafted basis over the perturbable gradient basis. The proofs of these results are derived under mild assumptions. Accordingly, the proven results are directly applicable to many machine learning models, including practical deep neural networks, without any modification of practical methods. Furthermore, as special cases of our general results, this paper improves or complements several state-of-the-art theoretical results in the literature with a simple and unified proof technique.},
  eprint      = {1904.03673},
  eprinttype  = {arXiv},
  eprintclass = {stat.ML},
  keywords    = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@Online{Barratt2019,
  author     = {Barratt, Shane and Boyd, Stephen},
  title      = {Least Squares Auto-Tuning},
  date       = {2019},
  abstract   = {Least squares is by far the simplest and most commonly applied computational method in many fields. In almost all applications, the least squares objective is rarely the true objective. We account for this discrepancy by parametrizing the least squares problem and automatically adjusting these parameters using an optimization algorithm. We apply our method, which we call least squares auto-tuning, to data fitting.},
  eprint     = {1904.05460},
  eprinttype = {arXiv},
}

@Article{Li2019,
  author       = {Li, Yishui and Xie, Peizhen and Chen, Xinhai and Liu, Jie and Yang, Bo and Li, Shengguo and Gong, Chunye and Gan, Xinbiao and Xu, Han},
  title        = {VBSF: a new storage format for SIMD sparse matrix--vector multiplication on modern processors},
  journaltitle = {The Journal of Supercomputing},
  date         = {2019-04-10},
  issn         = {1573-0484},
  doi          = {10.1007/s11227-019-02835-4},
  abstract     = {Sparse matrix--vector multiplication (SpMV) is one of the most indispensable kernels of solving problems in numerous applications, but its performance of SpMV is limited by the need for frequent memory access. Modern processors exploit data-level parallelism to improve the performance using single-instruction multiple data (SIMD). In order to take full advantage of SIMD acceleration technology, a new storage format called Variable Blocked-$\sigma$-SIMD Format (VBSF) is proposed in this paper to change the irregular nature of traditional matrix storage formats. This format combines the adjacent nonzero elements into variable size blocks to ensure that SpMV can be computed with SIMD vector units. We compare the VBSF-based SpMV with traditional storage formats using 15 matrices as a benchmark suite on three computing platforms (FT2000, Intel Xeon E5 and Intel Silver) with different SIMD length. For the matrices in the benchmark suite, the VBSF obtains great performance improvement on three platforms, respectively, and it proves to have better storage efficiency compared with other storage formats.},
  day          = {10},
}

@Article{Solomonik2016,
  author       = {Solomonik, Edgar and Carson, Erin and Knight, Nicholas and Demmel, James},
  title        = {Trade-Offs Between Synchronization, Communication, and Computation in Parallel Linear Algebra Computations},
  journaltitle = {ACM Transaction on Parallel Computing},
  date         = {2016-01},
  volume       = {3},
  number       = {1},
  pages        = {3:1--3:47},
  issn         = {2329-4949},
  doi          = {10.1145/2897188},
  abstract     = {This article derives trade-offs between three basic costs of a parallel algorithm: synchronization, data movement, and computational cost. These trade-offs are lower bounds on the execution time of the algorithm that are independent of the number of processors but dependent on the problem size. Therefore, they provide lower bounds on the execution time of any parallel schedule of an algorithm computed by a system composed of any number of homogeneous processors, each with associated computational, communication, and synchronization costs. We employ a theoretical model that measures the amount of work and data movement as a maximum over that incurred along any execution path during the parallel computation. By considering this metric rather than the total communication volume over the whole machine, we obtain new insights into the characteristics of parallel schedules for algorithms with nontrivial dependency structures. We also present reductions from BSP and LogGP algorithms to our execution model, extending our lower bounds to these two models of parallel computation. We first develop our results for general dependency graphs and hypergraphs based on their expansion properties, and then we apply the theorem to a number of specific algorithms in numerical linear algebra, namely triangular substitution, Cholesky factorization, and stencil computations. We represent some of these algorithms as families of dependency graphs. We derive their communication lower bounds by studying the communication requirements of the hypergraph structures shared by these dependency graphs. In addition to these lower bounds, we introduce a new communication-efficient parallelization for stencil computation algorithms, which is motivated by results of our lower bound analysis and the properties of previously existing parallelizations of the algorithms.},
  acmid        = {2897188},
  articleno    = {3},
  issue_date   = {June 2016},
  keywords     = {Communication lower bounds, graph expansion, numerical linear algebra, stencil computations},
  location     = {New York, NY, USA},
  numpages     = {47},
  publisher    = {ACM},
}

@Online{Huckle2019,
  author    = {Huckle, Thomas K.},
  title     = {Accelerated Jacobi iterations for bidiagonal and sparse triangular matrices},
  date      = {2019},
  url       = {https://www5.in.tum.de/persons/huckle/it_triang.pdf},
  abstract  = {In many applications a sparse linear system of equations $Ax = b$ has to be solved. For applying iterative solvers like preconditioned conjugate gradient (pcg) or GMRES, effective preconditioners are necessary, e.g. Jacobi, Gauss-Seidel, or incomplete LU factorization (ILU). Often, effective preconditioners are given via sparse triangular matrices $L$, that have to be solved in every iteration step. Recent work by Edmond Chow introduced an easy to parallelize fixed-point iteration for computing approximations to (I)LU factorizations. Therefore, the aching handicap in parallel solution methods for sparse matrices is the solving of sparse triangular systems, e.g. bidiagonal matrices. In a parallel environment direct solvers can take only restricted advantage of parallelism. Therefore, in this paper we develop a fast iterative solution method for sparse triangular matrices. In contrast to direct solvers for triangular matrices $L$ like graph-based methods, sparse factorization methods, or Sherman-Morrison-Woodbury, here we want to consider stationary Jacobi iterations. In its original form the Jacobi iteration for ill-conditioned matrices can lead to very slow convergence. Therefore, we introduce different acceleration tools like preconditioning (block Jacobi and Incomplete Sparse Approximate Inverse ISAI), and a recursive acceleration of the Jacobi method. Here the Neumann series is replaced by the Euler expansion (see [4, 19, 8]). This is derived by a recursive computation of the Neumann series using powers of the initial Jacobi iteration matrix. The goal is to shift the major part of the operations from cheap but numerous iteration steps to better parallelizable cheap and sparse matrix-matrix products reducing the number of necessary iterations considerably, e.g. to less than $\log_2(n)$ for an $n \times n$ matrix.},
  timestamp = {2019.04.28},
}

@InProceedings{Nie2019,
  author    = {Nie, Q. and Malik, S.},
  title     = {{SpFlow}: Memory-Driven Data Flow Optimization for Sparse Matrix-Matrix Multiplication},
  booktitle = {Proceedings of the IEEE International Symposium on Circuits and Systems},
  date      = {2019-05},
  series    = {ISCAS'19},
  pages     = {1--5},
  doi       = {10.1109/ISCAS.2019.8702111},
  abstract  = {To improve the performance of sparse matrix-matrix multiplication (SpMM) running on a specialized architecture, orchestrating a data flow that maximizes data reuse in local memory is critical but challenging due to the irregular non-zero element locations and the wide range of sparsity. In this work, we proposed SpFlow, a memory-driven data flow optimization framework for SpMM. SpFlow can realize 54X fewer DRAM accesses and 97X fewer SRAM accesses on average than a GPU running the cuSPARSE kernel. And in comparison with a state-of-the-art accelerator, the performance can be improved by 3X, and SRAM accesses reduced by 5X on average.},
  issn      = {2158-1525},
  keywords  = {Random access memory;Sparse matrices;Graphics processing units;Indexes;Optimization;Memory management},
}

@Article{Lei2019,
  author       = {Lei, D. and Du, M. and Chen, H. and Li, Z. and Wu, Y.},
  title        = {Distributed Parallel Sparse Multinomial Logistic Regression},
  journaltitle = {IEEE Access},
  date         = {2019},
  volume       = {7},
  pages        = {55496--55508},
  issn         = {2169-3536},
  doi          = {10.1109/ACCESS.2019.2913280},
  abstract     = {Sparse Multinomial Logistic Regression (SMLR) is widely used in the field of image classification, multi-class object recognition, and so on, because it has the function of embedding feature selection during classification. However, it cannot meet the time and memory requirements for processing large-scale data. We have reinvestigated the classification accuracy and running efficiency of the algorithm for solving SMLR problems using the Alternating Direction Method of Multipliers (ADMM), which is called fast SMLR (FSMLR) algorithm in this paper. By reformulating the optimization problem of FSMLR, we transform the serial convex optimization problem to the distributed convex optimization problem, i.e., global consensus problem and sharing problem. Based on the distributed optimization problem, we propose two distribute parallel SMLR algorithms, sample partitioning-based distributed SMLR (SP-SMLR), and feature partitioning-based distributed SMLR (FP-SMLR), for a large-scale sample and large-scale feature datasets in big data scenario, respectively. The experimental results show that the FSMLR algorithm has higher accuracy than the original SMLR algorithm. The big data experiments show that our distributed parallel SMLR algorithms can scale for massive samples and large-scale features, with high precision. In a word, our proposed serial and distribute SMLR algorithms outperform the state-of-the-art algorithms.},
  keywords     = {Logistics;Convex functions;Optimization;Distributed databases;Partitioning algorithms;Machine learning algorithms;Task analysis;Alternating Direction Method of Multipliers;big data;distributed parallel;sparse multinomial logistic regression},
}

@InProceedings{Balaji2019,
  author    = {Balaji, Vignesh and Lucia, Brandon},
  title     = {Combining Data Duplication and Graph Reordering to Accelerate Parallel Graph Processing},
  booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
  date      = {2019-06},
  location  = {Phoenix, AX, USA},
  abstract  = {Performance of single-machine, shared memory graph processing is affected by expensive atomic updates and poor cache locality. Data duplication, a popular approach to eliminate atomic updates by creating thread-local copies of shared data, incurs extreme memory overheads due to the large sizes of typical input graphs. Even memory-efficient duplication strategies that exploit the power-law structure common to many graphs (by duplicating only the highly-connected "hub" vertices) suffer from overheads for having to dynamically identify the hub vertices. Degree Sorting, a popular graph reordering technique that re-assigns hub vertices consecutive IDs in a bid to improve spatial locality, is effective for single-threaded graph applications but suffers from increased false sharing in parallel executions. \\
The main insight of this work is that the combination of data duplication and Degree Sorting eliminates the overheads of each optimization. Degree Sorting improves the efficiency of data duplication by assigning hub vertices consecutive IDs which enables easy identification of the hub vertices. Additionally, duplicating the hub vertex data eliminates false sharing in Degree Sorting since each thread updates its local copy of the hub vertex data. We evaluate this mutually-enabling combination of power-law-specific data duplication and Degree Sorting in a system called RADAR. RADAR improves performance by eliminating atomic updates for hub vertices and improving the cache locality of graph applications, providing speedups of up to 166x (1.88x on average) across different graph applications and input graphs.},
  timestamp = {2019.05.09},
}

@Article{Anzt2019,
  author       = {Anzt, Hartwig and Flegar, Goran and Grützmacher, Thomas and Quintana-Ortí, Enrique S.},
  title        = {Toward a modular precision ecosystem for high-performance computing},
  journaltitle = {The International Journal of High Performance Computing Applications},
  date         = {2019},
  doi          = {10.1177/1094342019846547},
  abstract     = {With the memory bandwidth of current computer architectures being significantly slower than the (floating point) arithmetic performance, many scientific computations only leverage a fraction of the computational power in today’s high-performance architectures. At the same time, memory operations are the primary energy consumer of modern architectures, heavily impacting the resource cost of large-scale applications and the battery life of mobile devices. This article tackles this mismatch between floating point arithmetic throughput and memory bandwidth by advocating a disruptive paradigm change with respect to how data are stored and processed in scientific applications. Concretely, the goal is to radically decouple the data storage format from the processing format and, ultimately, design a “modular precision ecosystem” that allows for more flexibility in terms of customized data access. For memory-bounded scientific applications, dynamically adapting the memory precision to the numerical requirements allows for attractive resource savings. In this article, we demonstrate the potential of employing a modular precision ecosystem for the block-Jacobi preconditioner and the PageRank algorithm -- two applications that are popular in the communities and at the same characteristic representatives for the field of numerical linear algebra and data analytics, respectively.},
}

@Article{Bernaschi2019,
  author       = {Bernaschi, M. and Carrozzo, M. and Franceschini, A. and Janna, C.},
  title        = {A Dynamic Pattern Factored Sparse Approximate Inverse Preconditioner on Graphics Processing Units},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2019},
  volume       = {41},
  number       = {3},
  pages        = {C139--C160},
  doi          = {10.1137/18M1197461},
  abstract     = {One of the most time-consuming tasks in the procedures for the numerical study of PDEs is the solution to linear systems of equations. To that purpose, iterative solvers are viewed as a promising alternative to direct methods on high-performance computers since, in theory, they are almost perfectly parallelizable. Their main drawback is the need of finding a suitable preconditioner to accelerate convergence. The factorized sparse approximate inverse (FSAI), mainly in its adaptive form, has proven to be an effective parallel preconditioner for several problems. In the present work, we report about two novel ideas to dynamically compute, on graphics processing units (GPUs), the FSAI sparsity pattern, which is the main task in its setup. The first approach, borrowed from the CPU implementation, uses a global array as a nonzero indicator, whereas the second one relies on a merge-sort procedure of multiple arrays. We will show that the second approach requires significantly less memory and overcomes issues related to the limited global memory available on GPUs. Numerical tests prove that the GPU implementation of FSAI allows for an average speed-up of 7.5 over a parallel CPU implementation. Moreover, we will show that the preconditioner computation is still feasible using single precision arithmetic with a further 20\% reduction of the setup cost. Finally, the strong scalability of the overall approach in shown in a multi-GPU setting.},
}

@Article{Ernst2019,
  author     = {Dominik Ernst, Georg Hager, Jonas Thies and Wellein, Gerhard},
  title      = {Performance Engineering for a Tall \& Skinny Matrix Multiplication Kernel on {GPUs}},
  date       = {2019},
  eprint     = {1905.03136},
  eprinttype = {arXiv},
  abstract   = {General matrix-matrix multiplications (GEMM) in vendor-supplied BLAS libraries are best optimized for square matrices but often show bad performance for tall and skinny matrices, which are much taller than wide. Nvidia’s current CUBLAS implementation delivers only a fraction of the potential performance (as given by the roofline model) in this case. We describe the challenges and key properties of an implementation that can achieve perfect performance. We further evaluate different approaches of parallelization and thread distribution, and devise a flexible, configurable mapping scheme. A code generation approach enables a simultaneously flexible and specialized implementation with autotuning. This results in perfect performance for a large range of matrix sizes in the domain of interest, and at least 2/3 of maximum performance for the rest on an Nvidia Volta GPGPU.},
}

@Article{Hassan2019,
  author       = {Abdullahi Hassan, Ambra and Cardellini, Valeria and D’Ambra, Pasqua and di Serafino, Daniela and Filippone, Salvatore},
  title        = {Efficient Algebraic Multigrid Preconditioners on Clusters of {GPUs}},
  journaltitle = {Parallel Processing Letters},
  date         = {2019},
  volume       = {29},
  number       = {01},
  pages        = {1950001},
  doi          = {10.1142/S0129626419500014},
  abstract     = {Many scientific applications require the solution of large and sparse linear systems of equations using Krylov subspace methods; in this case, the choice of an effective preconditioner may be crucial for the convergence of the Krylov solver. Algebraic MultiGrid (AMG) methods are widely used as preconditioners, because of their optimal computational cost and their algorithmic scalability. The wide availability of GPUs, now found in many of the fastest supercomputers, poses the problem of implementing efficiently these methods on high-throughput processors. In this work we focus on the application phase of AMG preconditioners, and in particular on the choice and implementation of smoothers and coarsest-level solvers capable of exploiting the computational power of clusters of GPUs. We consider block-Jacobi smoothers using sparse approximate inverses in the solve phase associated with the local blocks. The choice of approximate inverses instead of sparse matrix factorizations is driven by the large amount of parallelism exposed by the matrix-vector product as compared to the solution of large triangular systems on GPUs. The selected smoothers and solvers are implemented within the AMG preconditioning framework provided by the MLD2P4 library, using suitable sparse matrix data structures from the PSBLAS library. Their behaviour is illustrated in terms of execution speed and scalability, on a test case concerning groundwater modelling, provided by the Jülich Supercomputing Center within the Horizon 2020 Project EoCoE.},
}

@InProceedings{Cools2019,
  author     = {Cools, Siegfried and Cornelis, Jeffrey and Ghysels, Pieter and Vanroose, Wim},
  title      = {Improving strong scaling of the Conjugate Gradient method for solving large linear systems using global reduction pipelining},
  booktitle  = {Proceedings of the 2019 EuroMPI conference},
  date       = {2019},
  series     = {EuroMPI'19},
  eprint     = {1905.06850},
  eprinttype = {arXiv},
  abstract   = {This paper presents performance results comparing MPI-based implementations of the popular Conjugate Gradient (CG) method and several of its communication hiding (or “pipelined”) variants. Pipelined CG methods are designed to efficiently solve SPD linear systems on massively parallel distributed memory hardware, and typically display significantly improved strong scaling compared to classic CG. This increase in parallel performance is achieved by overlapping the global reduction phase (MPI_Iallreduce) required to compute the inner products in each iteration by (chiefly local) computational work such as the matrix-vector product as well as other global communication. This work includes a brief introduction to the deep pipelined CG method for readers that may be unfamiliar with the specifics of the method. A brief overview of implementation details provides the practical tools required for implementation of the algorithm. Subsequently, easily reproducible strong scaling results on the US Department of Energy (DoE) NERSC machine “Cori” (Phase I -- Haswell nodes) on up to 1024 nodes with 16 MPI ranks per node are presented using an implementation of $p(l)$-CG that is available in the open source PETSc library. Observations on the staggering and overlap of the asynchronous, non-blocking global communication phases with communication and computational kernels are drawn from the experiments.},
}

@Article{Kong2019a,
  author       = {Kong, Fande},
  title        = {Parallel memory-efficient all-at-once algorithms for the sparse matrix triple products in multigrid methods},
  journaltitle = {The International Journal of High Performance Computing Applications},
  date         = {2019},
  eprint       = {1905.08423},
  eprinttype   = {arXiv},
  annotation   = {Multilevel/multigrid methods is one of the most popular approaches for solving a large sparse linear system of equations, typically, arising from the discretization of partial differential equations. One critical step in the multilevel/multigrid methods is to form coarse matrices through a sequence of sparse matrix triple products. A commonly used approach for the triple products explicitly involves two steps, and during each step a sparse matrix-matrix multiplication is employed. This approach works well for many applications with a good computational efficiency, but it has a high memory overhead since some auxiliary matrices need to be temporarily stored for accomplishing the calculations. In this work, we propose two new algorithms that construct a coarse matrix with taking one pass through the input matrices without involving any auxiliary matrices for saving memory. The new approaches are referred to as “all-atonce” and “merged all-at-once” (a modified version of “all-at-once”) since the new algorithms calculate the two sparse matrix-matrix multiplications simultaneously, and the traditional method is denoted as “two-step”. The all-at-once and the merged all-at-once algorithms are implemented based on hash tables in PETSc as part of this work with a careful consideration on the performance in terms of the compute time and the memory usage. In the new methods, the first sparse matrix-matrix multiplication is implemented using a row-wise algorithm, and the second one is based on an outer product. We numerically show that the proposed algorithms and their implementations are perfectly scalable in both the compute time and the memory usage with up to 32,768 processor cores for a model problem with 27 billions of unknowns. The scalability is also demonstrated for a realistic neutron transport problem with over 2 billion unknowns on a supercomputer with 10,000 processor cores. Compared with the traditional two-step method, the all-at-once and the merged all-at-once algorithms consume much less memory for both the model problem and the realistic neutron transport problem meanwhile they are able to maintain the computational efficiency.},
  timestamp    = {2019.05.26},
}

@InBook{Yamamoto2019,
  author    = {Yamamoto, Yusaku},
  title     = {High-Performance Algorithms for Numerical Linear Algebra},
  booktitle = {The Art of High Performance Computing for Computational Science, Vol. 1: Techniques of Speedup and Parallelization for General Purposes},
  date      = {2019},
  editor    = {Geshi, Masaaki},
  publisher = {Springer Singapore},
  location  = {Singapore},
  pages     = {113--136},
  doi       = {10.1007/978-981-13-6194-4_7},
  abstract  = {Matrix computations lie at the heart of many scientific computations. While sophisticated algorithms have been established for various numerical linear algebra problems such as the solution of linear simultaneous equationsLinear simultaneous equation and eigenvalue problemsEigenvalue problem, they require considerable modification with the advent of exaFLOPS- scale supercomputers, which are expected to have a huge number of computing cores, deep memory hierarchyMemory hierarchy, and increased probability of hardware errors. In this chapter, we discuss projected hardware characteristics of exaFLOPS machines and summarize the challenges to be faced by numerical linear algebra algorithms in the near future. Based on these preparations, we present a brief survey of recent research efforts in the field of numerical linear algebra targeted at meeting these challenges.},
}

@InBook{Mendoza2019,
  author    = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Urban, Matthias and Burkart, Michael and Dippel, Maximilian and Lindauer, Marius and Hutter, Frank},
  title     = {Towards Automatically-Tuned Deep Neural Networks},
  booktitle = {Automated Machine Learning: Methods, Systems, Challenges},
  date      = {2019},
  editor    = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-030-05318-5},
  pages     = {135--149},
  doi       = {10.1007/978-3-030-05318-5_7},
  abstract  = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. In this work, we present two versions of Auto-Net, which provide automatically-tuned deep neural networks without any human intervention. The first version, Auto-Net 1.0, builds upon ideas from the competition-winning system Auto-sklearn by using the Bayesian Optimization method SMAC and uses Lasagne as the underlying deep learning (DL) library. The more recent Auto-Net 2.0 builds upon a recent combination of Bayesian Optimization and HyperBand, called BOHB, and uses PyTorch as DL library. To the best of our knowledge, Auto-Net 1.0 was the first automatically-tuned neural network to win competition datasets against human experts (as part of the first AutoML challenge). Further empirical results show that ensembling Auto-Net 1.0 with Auto-sklearn can perform better than either approach alone, and that Auto-Net 2.0 can perform better yet.},
}

@Article{Jagode2019,
  author       = {Jagode, Heike and Danalis, Anthony and Anzt, Hartwig and Dongarra, Jack},
  title        = {PAPI software-defined events for in-depth performance analysis},
  journaltitle = {The International Journal of High Performance Computing Applications},
  date         = {2019},
  doi          = {10.1177/1094342019846287},
  abstract     = {The methodology and standardization layer provided by the Performance Application Programming Interface (PAPI) has played a vital role in application profiling for almost two decades. It has enabled sophisticated performance analysis tool designers and performance-conscious scientists to gain insights into their applications by simply instrumenting their code using a handful of PAPI functions that “just work” across different hardware components. In the past, PAPI development had focused primarily on hardware-specific performance metrics. However, the rapidly increasing complexity of software infrastructure poses new measurement and analysis challenges for the developers of large-scale applications. In particular, acquiring information regarding the behavior of libraries and runtimes—used by scientific applications—requires low-level binary instrumentation, or APIs specific to each library and runtime. No uniform API for monitoring events that originate from inside the software stack has emerged. In this article, we present our efforts to extend PAPI’s role so that it becomes the de facto standard for exposing performance-critical events, which we refer to as software-defined events (SDEs), from different software layers. Upgrading PAPI with SDEs enables monitoring of both types of performance events—hardware- and software-related events—in a uniform way, through the same consistent PAPI. The goal of this article is threefold. First, we motivate the need for SDEs and describe our design decisions regarding the functionality we offer through PAPI’s new SDE interface. Second, we illustrate how SDEs can be utilized by different software packages, specifically, by showcasing their use in the numerical linear algebra library MAGMA-Sparse, the tensor algebra library TAMM that is part of the NWChem suite, and the compiler-based performance analysis tool Byfl. Third, we provide a performance analysis of the overhead that results from monitoring SDEs and discuss the trade-offs between overhead and functionality.},
}

@InProceedings{Zhang2019,
  author    = {Zhang, Kai and Liu, Jun and Zhang, Jie and Wang, Jun},
  title     = {Greedy Orthogonal Pivoting Algorithm for Non-negative Matrix Factorization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  date      = {2019},
  series    = {PMLR'19},
  url       = {http://proceedings.mlr.press/v97/zhang19r/zhang19r.pdf},
  abstract  = {Non-negative matrix factorization is a powerful tool for learning useful representations in the data and has been widely applied in many problems such as data mining and signal processing. Orthogonal NMF, which can further improve the locality of decomposition, has drawn considerable interest in clustering problems. However, imposing simultaneous non-negative and orthogonal structure can be difficult, and so existing algorithms can only solve it approximately. To address this challenge, we propose an innovative procedure called Greedy Orthogonal Pivoting Algorithm (GOPA). The GOPA method fully exploits the sparsity of non-negative orthogonal solutions to break the global problem into a series of local optimizations, in which an adaptive subset of coordinates are updated in a greedy, closed-form manner. The biggest advantage of GOPA is that it promotes exact orthogonality and provides solid empirical evidence that stronger orthogonality does contribute favorably to better clustering performance. On the other hand, we have designed randomized and batch-mode version of GOPA, which can further reduce the computational cost and improve accuracy, making it suitable for large data.},
}

@Article{Yuan2019,
  author     = {Yuan, Ganzhao and Shen, Li and Zheng, Wei-Shi},
  title      = {A Block Decomposition Algorithm for Sparse Optimization},
  date       = {2019},
  eprint     = {1905.11031},
  eprinttype = {arXiv},
  abstract   = {Sparse optimization is a central problem in machine learning and computer vision. However, this problem is inherently NP-hard and thus difficult to solve in general. Combinatorial search methods find the global optimal solution but are confined to small-sized problems, while coordinate descent methods are efficient but often suffer from poor local minima. This paper considers a new block decomposition algorithm that combines the effectiveness of combinatorial search methods and the efficiency of coordinate descent methods. Specifically, we consider a random strategy or/and a greedy strategy to select a subset of coordinates as the working set, and then perform a global combinatorial search over the working set based on the original objective function. We show that our method finds stronger stationary points than Amir Beck et al.’s coordinate-wise optimization method. In addition, we establish the global convergence and convergence rate of our block decomposition algorithm. Our experiments on solving sparse regularized and sparsity constrained least squares optimization problems demonstrate that our method achieves state-ofthe-art performance in terms of accuracy.},
  timestamp  = {2019.06.02},
}

@Article{Yu2019,
  author       = {Yu, Ting and Liu, Mengchi},
  title        = {A Memory Efficient Clique Enumeration Method for Sparse Graphs with a Parallel Implementation},
  journaltitle = {Parallel Computing},
  date         = {2019},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2019.05.005},
  url          = {http://www.sciencedirect.com/science/article/pii/S0167819118301297},
  abstract     = {Maximal clique enumeration (MCE) is a widely studied problem that plays a crucial role in structure mining of undirected graphs. The increasing scale of real-world graphs has brought the challenges of high memory cost and high CPU workload to the problem. In this paper, we propose a memory efficient method named CMC-bit for MCE on sparse graphs. It reduces the memory cost via minimizing the candidate cliques and representing them by the data structure bitset. It generates an appropriate order for the vertex set according to two optimized principles to reduce the CPU cost. We further design a partition-based CMC-bit algorithm with a one-side extending strategy to solve the memory-limited problem. We parallelize the CMC-bit algorithm based on MapReduce with a range-based partition strategy to make an optimal trade-off between the shuffling workload of graph decomposition and load balance in the Reduce phase. We conduct extensive experiments on 30 real-world datasets. The results demonstrate that both the CMC-bit algorithm and its parallel implementation significantly outperform the respective state-of-the-art algorithms in speed. We also show that the parallel CMC-bit algorithm achieves good performance on the scalability with respect to both the reducer number and the CPU number.},
  keywords     = {Maximal clique enumeration, Graph algorithms, MapReduce, Parallel graph algorithms},
}

@Article{Das2019,
  author     = {Das, Swapnil and Demmel, James and Fountoulakis, Kimon and Grigori, Laura and Mahoney, Michael. W.},
  title      = {Parallel and Communication Avoiding Least Angle Regression},
  date       = {2019},
  eprint     = {1905.11340},
  eprinttype = {arXiv},
  abstract   = {We are interested in parallelizing the Least Angle Regression (LARS) algorithm for fitting linear regression models to high-dimensional data. We consider two parallel and communication avoiding versions of the basic LARS algorithm. The two algorithms apply to data that have different layout patterns (one is appropriate for row-partitioned data, and the other is appropriate for column-partitioned data), and they have different asymptotic costs and practical performance. The first is bLARS, a block version of LARS algorithm, where we update b columns at each iteration. Assuming that the data are row-partitioned, bLARS reduces the number of arithmetic operations, latency, and bandwidth by a factor of b. The second is Tournament-bLARS (T-bLARS), a tournament version of LARS, in which case processors compete, by running several LARS computations in parallel, to choose b new columns to be added into the solution. Assuming that the data are column-partitioned, T-bLARS reduces latency by a factor of b. Similarly to LARS, our proposed methods generate a sequence of linear models. We present extensive numerical experiments that illustrate speed-ups up to 25$\times$ compared to LARS.},
  timestamp  = {2019.06.02},
}

@InProceedings{Fuchs2019,
  author    = {Fuchs, Adi and Wentzlaff, David},
  title     = {The Accelerator Wall: Limits of Chip Specialization},
  booktitle = {Proceedings of the 25th IEEE International Symposium on High-Performance Computer Architecture},
  date      = {2019},
  series    = {HPCA'18},
  abstract  = {Specializing chips using hardware accelerators has become the prime means to alleviate the gap between the growing computational demands and the stagnating transistor budgets caused by the slowdown of CMOS scaling. Much of the benefits of chip specialization stems from optimizing a computational problem within a given chip’s transistor budget. Unfortunately, the stagnation of the number of transistors available on a chip will limit the accelerator design optimization space, leading to diminishing specialization returns, ultimately hitting an accelerator wall.\\ In this work, we tackle the question of what are the limits of future accelerators and chip specialization? We do this by characterizing how current accelerators depend on CMOS scaling, based on a physical modeling tool that we constructed using datasheets of thousands of chips. We identify key concepts used in chip specialization, and explore case studies to understand how specialization has progressed over time in different applications and chip platforms (e.g., GPUs, FPGAs, ASICs). Utilizing these insights, we build a model which projects forward to see what future gains can and cannot be enabled from chip specialization. A quantitative analysis of specialization returns and technological boundaries is critical to help researchers understand the limits of accelerators and develop methods to surmount them.},
  timestamp = {2019.06.02},
}

@Article{Liu2019,
  author       = {Liu, Hui and Tian, Ye and Zong, Hongming and Ma, Qingping and Wang, Michael Yu and Zhang, Liang},
  title        = {Fully parallel level set method for large-scale structural topology optimization},
  journaltitle = {Computers \& Structures},
  date         = {2019},
  volume       = {221},
  pages        = {13--27},
  issn         = {0045-7949},
  doi          = {10.1016/j.compstruc.2019.05.010},
  url          = {http://www.sciencedirect.com/science/article/pii/S0045794918316511},
  abstract     = {To realize large-scale or high-resolution structural topology optimization design, a fully parallel parameterized level set method with compactly supported radial basis functions (CSRBFs) is developed based on both the uniform and non-uniform structured meshes. In this work, the whole computation process is parallelized, including mesh generation, sensitivity analysis, calculation and assembly of the element stiffness matrices, solving of the structural state equation, parameterization and updating of the level set function, and output of the computational results during the optimization iterations. In addition, some typical numerical examples, in which the calculation scale is up to 7 million 8-node hexahedral elements, are carried out for verifying the effectiveness of the proposed method. Finally, the computing time is also analyzed in detail. It is found that: (1) In the optimized structures, the thin sheet-like components gradually replace the truss-like ones when refining the mesh, (2) the parameterization process of the level set function will become fast as long as the non-uniformity of mesh is not very high and the supported radius of CSRBF is small enough, and (3) more than 80\% of the total computing time is always consumed for solving the structural state equation during the finite element analysis (FEA).},
  keywords     = {Parallel computing, Level set method, Large-scale structural topology optimization, Uniform and non-uniform structured meshes, Compactly supported radial basis function},
}

@Article{Wu2019,
  author       = {Wu, Rongteng},
  title        = {Dynamic Scheduling Strategy for Block Parallel Cholesky Factorization Based on Activity on Edge Network},
  journaltitle = {{IEEE} Access},
  date         = {2019},
  volume       = {7},
  pages        = {66317--66324},
  issn         = {2169-3536},
  doi          = {10.1109/ACCESS.2019.2917714},
  abstract     = {The efficient development of system software and design applications in parallel architecture is a notable challenge considering various aspects, such as load balancing, memory spaces, communication, and synchronization. This paper presents a block parallel Cholesky factorization algorithm for a multicore system, which is developed based on activity on edge network. First, the basic block computing tasks and their dependencies are taken as vertices and edges, respectively, and a directed acyclic graph corresponding to the specific block parallel Cholesky factorization is generated. Next, each edge of the directed acyclic graph is assigned to a weight equal to the processing time of the initial vertex of the edge, and the directed acyclic graph becomes an activity on edge network with only one starting and one ending vertex. Finally, a queuing algorithm is designed for the basic block computing tasks according to the edge activity on edge network, and a dynamic scheduling strategy is developed for block parallel Cholesky factorization. The results of the experiments concerning the parallel execution time of the algorithm in multicore systems with different configurations demonstrate that the proposed algorithm has notable advantages compared with the traditional static scheduling algorithm, and it exhibits satisfactory load balancing, parallelism, and scalability capacities.},
  keywords     = {Task analysis;Load management;Multicore processing;Program processors;Heuristic algorithms;Linear algebra;Dynamic scheduling;Cholesky factorization;dense linear algebra;dynamic schedule strategy;load balancing;multicore computing},
}

@InProceedings{Jun2018,
  author    = {Jun, S. and Wright, A. and Zhang, S. and Xu, S. and Arvind},
  title     = {GraFBoost: Using Accelerated Flash Storage for External Graph Analytics},
  booktitle = {ACM/IEEE 45th Annual International Symposium on Computer Architecture},
  date      = {2018-06},
  pages     = {411--424},
  doi       = {10.1109/ISCA.2018.00042},
  abstract  = {We describe GraFBoost, a flash-based architecture with hardware acceleration for external analytics of multi-terabyte graphs. We compare the performance of GraFBoost with 1 GB of DRAM against various state-of-the-art graph analytics software including FlashGraph, running on a 32-thread Xeon server with 128 GB of DRAM. We demonstrate that despite the relatively small amount of DRAM, GraFBoost achieves high performance with very large graphs no other system can handle, and rivals the performance of the fastest software platforms on sizes of graphs that existing platforms can handle. Unlike in-memory and semi-external systems, GraFBoost uses a constant amount of memory for all problems, and its performance decreases very slowly as graph sizes increase, allowing GraFBoost to scale to much larger problems than possible with existing systems while using much less resources on a single-node system. The key component of GraFBoost is the sort-reduce accelerator, which implements a novel method to sequentialize fine-grained random accesses to flash storage. The sort-reduce accelerator logs random update requests and then uses hardware-accelerated external sorting with interleaved reduction functions. GraFBoost also stores newly updated vertex values generated in each superstep of the algorithm lazily with the old vertex values to further reduce I/O traffic. We evaluate the performance of GraFBoost for PageRank, breadth-first search and betweenness centrality on our FPGA-based prototype (Xilinx VC707 with 1 GB DRAM and 1 TB flash) and compare it to other graph processing systems including a pure software implementation of GrapFBoost.},
  issn      = {2575-713X},
  keywords  = {DRAM chips;field programmable gate arrays;graph theory;sorting;DRAM;sort-reduce accelerator;graph analytics software;FlashGraph;I-O traffic;PageRank;breadth-first search;betweenness centrality;FPGA;Xilinx VC707;Xeon server;semiexternal systems;multiterabyte graphs;external graph analytics;accelerated flash storage;graph processing systems;GraFBoost;hardware-accelerated external sorting;Random access memory;Hardware;Software;Benchmark testing;Software algorithms;Field programmable gate arrays;Programming;graph analytics;flash storage;FPGA;sort-reduce;hardware acceleration},
}

@TechReport{Muhammad2019,
  author      = {Osama, Muhammad and Truong, Minh and Yang, Carl and Buluç, Aydın and Owens, John D.},
  title       = {Graph Coloring on the {GPU}},
  institution = {UC Davis: College of Engineering},
  date        = {2019},
  url         = {https://escholarship.org/uc/item/6kp4p18t},
  abstract    = {We design and implement parallel graph coloring algorithms on the GPU using two different abstractions—one datacentric (Gunrock), the other linear-algebra-based (GraphBLAS). We analyze the impact of variations of a baseline independent-set algorithm on quality and runtime. We study how optimizations such as hashing, avoiding atomics, and a max-min heuristic affect performance. Our Gunrock graph coloring implementation has a peak 2$\times$ speed-up, a geomean speed-up of 1.3$\times$ and produces 1.6$\times$ more colors over previous hardwired state-of-theart implementations on real-world datasets. Our GraphBLAS implementation of Luby’s algorithm produces 1.9$\times$ fewer colors than the previous state-of-the-art parallel implementation at the cost of 3$\times$ extra runtime, and 1.014$\times$ fewer colors than a greedy, sequential algorithm with a geomean speed-up of 2.6$\times$.},
  timestamp   = {2019.06.09},
}

@InProceedings{Mohammadi2019,
  author    = {Mohammadi, Mahdi Soltan and Yuki, Tomofumi and Cheshmi, Kazem and Davis, Eddie C. and Hall, Mary and Dehnavi, Maryam Mehri and Nandy, Payal and Olschanowsky, Catherine and Venkat, Anand and Strout, Michelle Mills},
  title     = {Sparse Computation Data Dependence Simplification for Efficient Compiler-generated Inspectors},
  booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  date      = {2019},
  series    = {PLDI 2019},
  publisher = {ACM},
  location  = {Phoenix, AZ, USA},
  isbn      = {978-1-4503-6712-7},
  pages     = {594--609},
  doi       = {10.1145/3314221.3314646},
  abstract  = {This paper presents a combined compile-time and runtime loop-carried dependence analysis of sparse matrix codes and evaluates its performance in the context of wavefront parallellism. Sparse computations incorporate indirect memory accesses such as x[col[j]] whose memory locations cannot be determined until runtime. The key contributions of this paper are two compile-time techniques for significantly reducing the overhead of runtime dependence testing: (1) identifying new equality constraints that result in more efficient runtime inspectors, and (2) identifying subset relations between dependence constraints such that one dependence test subsumes another one that is therefore eliminated. New equality constraints discovery is enabled by taking advantage of domain-specific knowledge about index arrays, such as col[j]. These simplifications lead to automatically-generated inspectors that make it practical to parallelize such computations. We analyze our simplification methods for a collection of seven sparse computations. The evaluation shows our methods reduce the complexity of the runtime inspectors significantly. Experimental results for a collection of five large matrices show parallel speedups ranging from 2x to more than 8x running on a 8-core CPU.},
  acmid     = {3314646},
  address   = {New York, NY, USA},
  keywords  = {Presburger arithmetic with uninterpreted functions, SMT solvers, data dependence simplification, dependence analysis, inspector-executor strategies, sparse matrices},
  numpages  = {16},
}

@InProceedings{Augustine2019,
  author    = {Augustine, Travis and Sarma, Janarthanan and Pouchet, Louis-Noël and Rodríguez, Gabriel},
  title     = {Generating Piecewise-regular Code from Irregular Structures},
  booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  date      = {2019},
  series    = {PLDI 2019},
  publisher = {ACM},
  location  = {Phoenix, AZ, USA},
  isbn      = {978-1-4503-6712-7},
  pages     = {625--639},
  doi       = {10.1145/3314221.3314615},
  abstract  = {Irregular data structures, as exemplified with sparse matrices, have proved to be essential in modern computing. Numerous sparse formats have been investigated to improve the overall performance of Sparse Matrix-Vector multiply (SpMV). But in this work we propose instead to take a fundamentally different approach: to automatically build sets of regular sub-computations by mining for regular sub-regions in the irregular data structure. Our approach leads to code that is specialized to the sparsity structure of the input matrix, but which does not need anymore any indirection array, thereby improving SIMD vectorizability. We particularly focus on small sparse structures (below 10M nonzeros), and demonstrate substantial performance improvements and compaction capabilities compared to a classical CSR implementation and Intel MKL IE's SpMV implementation, evaluating on 200+ different matrices from the SuiteSparse repository.},
  acmid     = {3314615},
  address   = {New York, NY, USA},
  keywords  = {Polyhedral compilation, SpMV, sparse data structure, trace compression},
  numpages  = {15},
}

@InProceedings{Anzt2019a,
  author    = {Anzt, Hartwig and Chen, Yen-Chen and Cojean, Terry and Dongarra, Jack and Flegar, Goran and Nayak, Pratik and Quintana-Ortí, Enrique S. and Tsai, Yuhsiang M. and Wang, Weichung},
  title     = {Towards Continuous Benchmarking: An Automated Performance Evaluation Framework for High Performance Software},
  booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
  date      = {2019},
  series    = {PASC '19},
  publisher = {ACM},
  location  = {Zurich, Switzerland},
  isbn      = {978-1-4503-6770-7},
  pages     = {1--11},
  doi       = {10.1145/3324989.3325719},
  abstract  = {We present an automated performance evaluation framework that enables an automated workflow for testing and performance evaluation of software libraries. Integrating this component into an ecosystem enables sustainable software development, as a community effort, via a web application for interactively evaluating the performance of individual software components. The performance evaluation tool is based exclusively on web technologies, which removes the burden of downloading performance data or installing additional software. We employ this framework for the Ginkgo software ecosystem, but the framework can be used with essentially any software project, including the comparison between different software libraries. The Continuous Integration (CI) framework of Ginkgo is also extended to automatically run a benchmark suite on predetermined HPC systems, store the state of the machine and the environment along with the compiled binaries, and collect results in a publicly accessible performance data repository based on Git. The Ginkgo performance explorer (GPE) can be used to retrieve the performance data from the repository, and visualizes it in a web browser. GPE also implements an interface that allows users to write scripts, archived in a Git repository, to extract particular data, compute particular metrics, and visualize them in many different formats (as specified by the script). The combination of these approaches creates a workflow which enables performance reproducibility and software sustainability of scientific software. In this paper, we present example scripts that extract and visualize performance data for Ginkgo's SpMV kernels that allow users to identify the optimal kernel for specific problem characteristics.},
  acmid     = {3325719},
  address   = {New York, NY, USA},
  articleno = {9},
  keywords  = {automated performance benchmarking, continuous integration, healthy software lifecycle, interactive performance visualization},
  numpages  = {11},
}

@InProceedings{Xie2019,
  author    = {Xie, Zhen and Tan, Guangmin and Liu, Weifeing and Sun, Ninghui},
  title     = {{IA-SpGEMM}: An Input-aware Auto-tuning Framework for Parallel Sparse Matrix-Matrix Multiplication},
  booktitle = {Proceedings of the 33rd ACM Conference on Supercomputing},
  date      = {2019},
  series    = {ICS '19},
  location  = {Phoenix, AZ, USA},
  url       = {https://folk.idi.ntnu.no/weifengl/papers/spgemm_xie_ics19.pdf},
  abstract  = {Sparse matrix-matrix multiplication (SpGEMM) is a sparse kernel that is used in a number of scientific applications. Although several SpGEMM algorithms have been proposed, almost all of them are restricted to the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. The particular format and algorithm that yield the best performance for SpGEMM also remain undetermined.\\ In this work, we conduct a prospective study on format-specific parallel SpGEMM algorithms, and analyze their pros and cons. We then propose IA-SpGEMM, an input-aware auto-tuning Framework for SpGEMM, that provides a unified programming interface in the CSR format and automatically determines the best format and algorithm for arbitrary sparse matrices. For this purpose, we set-up an algorithm set and design a deep learning model called MatNet that is trained by over 2,700 matrices from the SuiteSparse Matrix Collection to quickly and accurately predict the best solution by using sparse features and density representations. We evaluate our framework on CPUs and a GPU, and the results show that IA-SpGEMM is on average 3.27$\times$ and 13.17$\times$ faster than MKL on an Intel and an AMD platform, respectively, and is 2.23$\times$ faster than cuSPARSE on an NVIDIA GPU.},
  timestamp = {2019.06.16},
}

@InProceedings{Montagne2019,
  author    = {Montagne, E. and Surós, R.},
  title     = {Systolic Sparse Matrix Vector Multiply in the Age of TPUs and Accelerators},
  booktitle = {2019 Spring Simulation Conference (SpringSim)},
  date      = {2019-04},
  pages     = {1--10},
  doi       = {10.23919/SpringSim.2019.8732860},
  abstract  = {Tensor Processing Units has brought back systolic arrays as a computational alternative to high performance computing. Recently Google presented a Tensor Processing Unit for handling matrix multiplication using systolic arrays. This unit is designed for dense matrices only. As they stated, sparse architectural support was omitted momentarily but they will focus on sparsity in future designs. We propose a systolic array to compute the Sparse Matrix Vector product in $T2(n)\approx \displaystyle \lceil\frac{nnz}{2}\rceil+2n+2$ using 2 n+2 processing elements. The systolic array we propose also use accumulators to collect the partial results of the resulting vector and supports adapting tiling.},
  keywords  = {Sparse matrices;Arrays;Adders;Artificial neural networks;Indexes;systolic arrays;sparse matrix;spmv product;tensor processing unit(tpu)},
}

@Article{Burkhardt2019,
  author     = {Burkhardt, Paul},
  title      = {Optimal algebraic Breadth-First Search for sparse graphs},
  year       = {2019},
  eprint     = {arXiv:1906.03113v1},
  eprinttype = {arXiv},
  abstract   = {There has been a rise in the popularity of algebraic methods for graph algorithms given the development of the GraphBLAS library and other sparse matrix methods. These are useful in practice because many graph algorithms are amenable to sparse matrix multiplication. An exemplar for these approaches is Breadth-First Search (BFS). The algebraic BFS algorithm is simply a recursion of matrix-vector multiplications with the $n \times n$ adjacency matrix. Despite many redundant operations over nonzeros that ultimately lead to suboptimal performance, the algebraic BFS is appealing for practical implementations because it is simple and embarrassingly parallel. By using highly tuned matrix libraries it can be faster in practice than the theoretically optimal combinatorial algorithm. Therefore an optimal algebraic BFS should be of keen interest especially if it is easily integrated with existing matrix methods.\\ Current methods, notably in the GraphBLAS, use a Sparse Matrix Sparse Vector (SpMSpV) multiplication in which the input vector is kept in a sparse representation in each step of the BFS. But simply applying SpMSpV in BFS does not lead to optimal runtime. Each nonzero in the vector must be masked in subsequent steps. This has been an area of recent recent in GraphBLAS and other libraries. While in theory these masking methods are asymptotically optimal on sparse graphs, many add work that leads to suboptimal runtime. We give a new optimal, algebraic BFS for sparse graphs that is also a constant factor faster than theoretically optimal SpMSpV methods. We show how to eliminate redundant operations so an element in the adjacency matrix is operated upon no more than once, thus taking $O(m)$ operations for a graph with $O(m)$ edges.\\ Our method multiplies progressively smaller submatrices of the adjacency matrix at each step. The matrix remains unchanged, rather we are masking the rows and columns in the matrix that corresponds to previously visited vertices. The input vector in each step is also effectively masked so it is a sparse vector. Thus our method multiplies a sparse submatrix by a sparse vector in decreasing size each step. Our sequential algebraic BFS algorithm takes $O(m)$ algebraic operations on a sparse graph as opposed to $O(mn)$ operations of other sparse matrix approaches. Our analysis closes a gap in the literature.},
  timestamp  = {2019.06.16},
}

@Article{Ioannidis2019,
  author     = {Ioannidis, E. I. and Cheimarios, N. and Spyropoulos, A. N. and Boudouvis, A. G.},
  title      = {On the performance of various parallel {GMRES} implementations on {CPU} and {GPU} clusters},
  date       = {2019},
  eprint     = {arXiv:1906.04051v1},
  eprinttype = {arXiv},
  annotation = {As the need for computational power and efficiency rises, parallel systems become increasingly popular among various scientific fields. While multiple core-based architectures have been the center of attention for many years, the rapid development of general purposes GPU-based architectures takes high performance computing to the next level. In this work, different implementations of a parallel version of the preconditioned GMRES - an established iterative solver for large and sparse linear equation sets - are presented, each of them on different computing architectures: From distributed and shared memory core-based to GPU-based architectures. The computational experiments emanate from the dicretization of a benchmark boundary value problem with the finite element method. Major advantages and drawbacks of the various implementations are addressed in terms of parallel speedup, execution time and memory issues. Among others, comparison of the results in the different architectures, show the high potentials of GPU-based architectures.},
  timestamp  = {2019.06.16},
}

@Article{Sao2019,
  author       = {Sao, Piyush and Li, Xiaoye S. and Vuduc, Richard},
  title        = {A communication-avoiding 3D algorithm for sparse LU factorization on heterogeneous systems},
  journaltitle = {Journal of Parallel and Distributed Computing},
  date         = {2019},
  issn         = {0743-7315},
  doi          = {10.1016/j.jpdc.2019.03.004},
  url          = {http://www.sciencedirect.com/science/article/pii/S0743731518305197},
  abstract     = {We propose a new algorithm to improve the strong scalability of right-looking sparse LU factorization on distributed memory systems. Our 3D algorithm for sparse LU uses a three-dimensional MPI process grid, exploits elimination tree parallelism, and trades off increased memory for reduced per-process communication. We also analyze the asymptotic improvements for planar graphs (e.g., those arising from 2D grid or mesh discretizations) and certain non-planar graphs (specifically for 3D grids and meshes). For a planar graph with n vertices, our algorithm reduces communication volume asymptotically in n by a factor of Ologn and latency by a factor of Ologn. For non-planar cases, our algorithm can reduce the per-process communication volume by 3$\times$ and latency by On13 times. In all cases, the memory needed to achieve these gains is a constant factor. We implemented our algorithm by extending the 2D data structure used in SuperLU_DIST. Our new 3D code achieves empirical speedups up to 27$\times$ for planar graphs and up to 3.3$\times$ for non-planar graphs over the baseline 2D SuperLU_DIST when run on 24,000 cores of a Cray XC30. We extend the 3D algorithm for heterogeneous architectures by adding the Highly Asynchronous Lazy Offload (Halo) algorithm for co-processor offload [44]. On 4096 nodes of a Cray XK7 with 32,768 CPU cores and 4096 Nvidia K20x GPUs, the 3D algorithm achieves empirical speedups up to 24$\times$ for planar graphs and 3.5$\times$ for non-planar graphs over the baseline 2D SuperLU_DIST with co-processor acceleration.},
  timestamp    = {2019.06.16},
}

@Article{Elgohary2019,
  author       = {Elgohary, Ahmed and Boehm, Matthias and Haas, Peter J. and Reiss, Frederick R. and Reinwald, Berthold},
  title        = {Compressed Linear Algebra for Declarative Large-scale Machine Learning},
  journaltitle = {Communications of the ACM},
  date         = {2019-04},
  volume       = {62},
  number       = {5},
  pages        = {83--91},
  issn         = {0001-0782},
  doi          = {10.1145/3318221},
  abstract     = {Large-scale Machine Learning (ML) algorithms are often iterative, using repeated read-only data access and I/O-bound matrix-vector multiplications. Hence, it is crucial for performance to fit the data into single-node or distributed main memory to enable fast matrix-vector operations. General-purpose compression struggles to achieve both good compression ratios and fast decompression for block-wise uncompressed operations. Therefore, we introduce Compressed Linear Algebra (CLA) for lossless matrix compression. CLA encodes matrices with lightweight, value-based compression techniques and executes linear algebra operations directly on the compressed representations. We contribute effective column compression schemes, cache-conscious operations, and an efficient sampling-based compression algorithm. Our experiments show good compression ratios and operations performance close to the uncompressed case, which enables fitting larger datasets into available memory. We thereby obtain significant end-to-end performance improvements.},
  acmid        = {3318221},
  issue_date   = {May 2019},
  location     = {New York, NY, USA},
  numpages     = {9},
  publisher    = {ACM},
  timestamp    = {2019.06.16},
}

@TechReport{Chen2016a,
  author      = {Chen, Zhangxin and Liu, Hui and Yang, Bo},
  date        = {2016},
  institution = {The Computing Research Repository},
  title       = {Parallel Triangular Solvers on {GPU}},
  url         = {http://arxiv.org/abs/1606.00541},
  abstract    = {In this paper, we investigate GPU based parallel triangular solvers systematically. The parallel triangular solvers are fundamental to incomplete LU factorization family preconditioners and algebraic multigrid solvers. We develop a new matrix format suitable for GPU devices. Parallel lower triangular solvers and upper triangular solvers are developed for this new data structure. With these solvers, ILU preconditioners and domain decomposition preconditioners are developed. Numerical results show that we can speed triangular solvers around seven times faster.},
  bibsource   = {dblp computer science bibliography, http://dblp.org},
  biburl      = {http://dblp2.uni-trier.de/rec/bib/journals/corr/ChenLY16},
  timestamp   = {Fri, 01 Jul 2016 17:39:49 +0200},
  volume      = {abs/1606.00541},
}

@TechReport{Sutton2016a,
  author      = {Sutton, Michael and Ben{-}Nun, Tal and Barak, Amnon and Pai, Sreepathi and Pingali, Keshav},
  date        = {2016},
  institution = {The Computing Research Repository},
  title       = {Adaptive Work-Efficient Connected Components on the {GPU}},
  url         = {http://arxiv.org/abs/1612.01178},
  abstract    = {This report presents an adaptive work-efficient approach for implementing the Connected Components algorithm on GPUs. The results show a considerable increase in performance (up to 6.8$\times$) over current state-of-the-art solutions.},
  bibsource   = {dblp computer science bibliography, http://dblp.org},
  biburl      = {http://dblp.uni-trier.de/rec/bib/journals/corr/SuttonBBPP16},
  timestamp   = {Mon, 02 Jan 2017 11:09:15 +0100},
  volume      = {abs/1612.01178},
}

@Article{Yang2018d,
  author     = {Yang, Carl and Buluç, Aydın and Owens, John D.},
  date       = {2018},
  title      = {Implementing Push-Pull Efficiently in GraphBLAS},
  eprint     = {1804.03327},
  eprinttype = {arXiv},
  volume     = {abs/1804.03327},
  abstract   = {We factor Beamer's push-pull, also known as direction-optimized breadth-first-search (DOBFS) into 3 separable optimizations, and analyze them for generalizability, asymptotic speedup, and contribution to overall speedup. We demonstrate that masking is critical for high performance and can be generalized to all graph algorithms where the sparsity pattern of the output is known a priori. We show that these graph algorithm optimizations, which together constitute DOBFS, can be neatly and separably described using linear algebra and can be expressed in the GraphBLAS linear-algebra-based framework. We provide experimental evidence that with these optimizations, a DOBFS expressed in a linear-algebra-based graph framework attains competitive performance with state-of-the-art graph frameworks on the GPU and on a multi-threaded CPU, achieving 101 GTEPS on a Scale 22 RMAT graph.},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/bib/journals/corr/abs-1804-03327},
  timestamp  = {Mon, 13 Aug 2018 16:48:40 +0200},
}

@Article{Agullo2019,
  author       = {Agullo, Emmanuel and Giraud, Luc and Poirel, Louis},
  date         = {2019},
  journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
  title        = {Robust preconditioners via generalized eigenproblems for hybrid sparse linear solvers},
  url          = {https://hal.inria.fr/hal-02074474/document},
  abstract     = {The solution of large sparse linear systems is one of the most time consuming kernels in many numerical simulations. The domain decomposition community has developed many efficient and robust methods in the last decades. While many of these solvers fall into the abstract Schwarz (aS) framework, their robustness has originally been demonstrated on a case-by-case basis. In this paper, we propose a bound for the condition number of all deflated aS methods provided that the coarse grid consists of the assembly of local components that contain the kernel of some local operators. We show that classical results from the literature on particular instances of aS methods can be retrieved from this bound. We then show that such a coarse grid correction can be explicitly obtained algebraically via generalized eigenproblems, leading to a condition number independent of the number of domains. This result can be readily applied to retrieve or improve the bounds previously obtained via generalized eigenproblems in the particular cases of Neumann-Neumann (NN), Additive Schwarz (AS) and optimized Robin but also generalizes them when applied with approximate local solvers. Interestingly, the proposed methodology turns out to be a comparison of the considered particular aS method with generalized versions of both NN and AS for tackling the lower and upper part of the spectrum, respectively. We furthermore show that the application of the considered grid corrections in an additive fashion is robust in the AS case although it is not robust for aS methods in general. In particular, the proposed framework allows for ensuring the robustness of the AS method applied on the Schur complement (AS/S), either with deflation or additively, and with the freedom of relying on an approximate local Schur complement. Numerical experiments illustrate these statements.},
  timestamp    = {2019.06.16},
}

@TechReport{Georgieva2019,
  author      = {Georgieva, Irina and Harizanov, Stanislav and Hofreither, Clemens},
  date        = {2019},
  institution = {Johann Radon Institute for Computational and Applied Mathematics (RICAM)},
  title       = {Iterative Low-rank Approximation Solvers for the Extension Method for Fractional Diffusion},
  number      = {RICAM-Report 2019-14},
  abstract    = {We consider the numerical method for fractional diffusion problems which is based on an extension to a mixed boundary value problem for a local operator in a higher dimensional space. We observe that, when this problem is discretized using tensor product spaces as is commonly done, the solution can be very well approximated by low-rank tensors. This motivates us to apply iterative low-rank approximation algorithms in order to efficiently solve this extended problem. In particular, we employ a recently proposed greedy Tucker approximation method as well as a more classical greedy rank one update method. Throughout, all objects of interest are kept in suitable low-rank approximations, which dramatically reduces the required amount of memory compared to the full formulation of the extended problem.\\
Our approach can be used for general, non-structured space discretizations. If the space discretization itself has tensor product structure, we can further decompose the problem in order to deal with even lower dimensional objects. We also note that the approach can be directly applied to higher-order discretizations both in space and the extended variable.\\
In several numerical examples, we demonstrate the convergence behaviour of the proposed methods. In particular, the Tucker approximation approach requires only a few iterations in order to reach the discretization error in all tested settings.},
  timestamp   = {2019.06.16},
}

@Article{Zhou2019,
  author       = {Zhou, Gan and Feng, Yanjun and Bo, Rui and Zhang, Tao},
  date         = {2019},
  journaltitle = {International Journal of Electrical Power \& Energy Systems},
  title        = {{GPU}-accelerated sparse matrices parallel inversion algorithm for large-scale power systems},
  doi          = {10.1016/j.ijepes.2019.03.074},
  issn         = {0142-0615},
  pages        = {34--43},
  url          = {http://www.sciencedirect.com/science/article/pii/S0142061518325109},
  volume       = {111},
  abstract     = {State-of-the-art Graphics Processing Unit (GPU) has superior performances on float-pointing calculation and memory bandwidth, and therefore has great potential in many computationally intensive power system applications, one of which is the inversion of large-scale sparse matrix. It is a fundamental component for many power system analyses which requires to solve massive number of forward and backward substitution (F\&B) subtasks and seems to be a good GPU-accelerated candidate application. By means of solving multiple F\&B subtasks concurrently and a serial of performance tunings in compliance with GPU’s architectures, we successfully develop a batch F\&B algorithm on GPUs, which not only extracts the intra-level and intra-level parallelisms inside single F\&B subtask but also explores a more regular parallelism among massive F\&B subtasks, called inter-task parallelism. Case study on a 9241-dimension case shows that the proposed batch F\&B solver consumes 2.92 $\mu$s per forward substitution (FS) subtask when the batch size is equal to 3072, achieving 65 times speedup relative to KLU library. And on the basis the complete design process of GPU-based inversion algorithm is proposed. By offloading the tremendous computational burden to GPU, the inversion of 9241-dimension case consumes only 97 ms, which can achieve 8.1 times speedup relative to the 12-core CPU inversion solver based on KLU library. The proposed batch F\&B solver is practically very promising in many other power system applications requiring solving massive F\&B subtasks, such as probabilistic power flow analysis.},
  keywords     = {Inversion, Forward substitution, Backward substitution, Spares matrix, GPU, Accelerated, Parallelism, Power flow},
  timestamp    = {2019.06.16},
}

@Article{Xiao2019,
  author       = {Xiao, G. and Li, K. and Chen, Y. and He, W. and Zomaya, A. and Li, T.},
  date         = {2019},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  title        = {{CASpMV}: A Customized and Accelerative SpMV Framework for the Sunway TaihuLight},
  doi          = {10.1109/TPDS.2019.2907537},
  issn         = {1045-9219},
  pages        = {1--1},
  abstract     = {The Sunway TaihuLight, equipped with 10 million cores, is currently the world's third fastest supercomputer. SpMV is one of core algorithms in many high-performance computing applications. This paper implements a fine-grained design for generic parallel SpMV based on the special Sunway architecture and finds three main performance limitations, i.e., storage limitation, load imbalance, and huge overhead of irregular memory accesses. To address these problems, this paper introduces a customized and accelerative framework for SpMV (CASpMV) on the Sunway. The CASpMV customizes an auto-tuning four-way partition scheme for SpMV based on the proposed statistical model, which describes the sparse matrix structure characteristics, to make it better fit in with the computing architecture and memory hierarchy of the Sunway. Moreover, the CASpMV provides an accelerative method and customized optimizations to avoid irregular memory accesses and further improve its performance on the Sunway. Our CASpMV achieves a performance improvement that ranges from 588.05\% to 2118.62\% over the generic parallel SpMV on a CG (which corresponds to an MPI process) of the Sunway on average and has good scalability on multiple CGs. The performance comparisons of the CASpMV with state-of-the-art methods on the Sunway indicate that the sparsity and irregularity of data structures have less impact on CASpMV.},
  keywords     = {Sparse matrices;Computer architecture;Parallel processing;Acceleration;Supercomputers;Kernel;Graphics processing units;Heterogeneous many-core processor;matrix partition;optimization;parallelism;SpMV;Sunway TaihuLight supercomputer},
  timestamp    = {2019.06.16},
}

@Article{Ma2019,
  author       = {Ma, S. and Liu, Z. and Chen, S. and Huang, L. and Guo, Y. and Wang, Z. and Zhang, M.},
  date         = {2019},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  title        = {Coordinated DMA: Improving the DRAM Access Efficiency for Matrix Multiplication},
  doi          = {10.1109/TPDS.2019.2906891},
  issn         = {1045-9219},
  pages        = {1--1},
  abstract     = {High performance implementation of matrix multiplication is essential for scientific computing. The memory access procedure is quite possible to be the bottleneck of matrix multiplication. The widely used GotoBLAS GEMM implementation divides the integral matrix into several partitions to be assigned to different cores for parallelization. Traditionally, each core deploys a DMA transfer to access its own partition in the DRAM memory. However, deploying an independent DMA transfer for each core cannot efficiently exploit the inter-core locality. Also, multiple concurrent DMA transfers interfere with each other, further reducing the DRAM access efficiency. We observe that the same row of neighboring partitions is in the same DRAM page, which means that there is significant locality inherent in the address layout. We propose the coordinated DMA to efficiently exploit the locality. It invokes one transfer to serve all cores and moves data in a row-major manner to improve the DRAM access efficiency. Compared with a baseline design, the coordinated DMA improves the bandwidth by 84.8\% and reduces DRAM energy consumption by 43.1\% for micro-benchmarks. It achieves higher performance for the GEMM and Linpack benchmark. With much less hardware costs, the coordinated DMA significantly outperforms an out-of-order memory controller.},
  keywords     = {Random access memory;Bandwidth;Out of order;Computer architecture;Layout;Graphics processing units;Hardware;Coordinated DMA;DRAM Access Efficiency;Matrix Multiplication},
  timestamp    = {2019.06.16},
}

@InProceedings{Li2019a,
  author    = {Li, Jiajia and Uçar, Bora and Çatalyürek, {\"U}mit V. and Sun, Jimeng and Barker, Kevin and Vuduc, Richard},
  title     = {Efficient and Effective Sparse Tensor Reordering},
  booktitle = {Proceedings of the ACM International Conference on Supercomputing},
  date      = {2019},
  series    = {ICS '19},
  publisher = {ACM},
  location  = {Phoenix, Arizona},
  isbn      = {978-1-4503-6079-1},
  pages     = {227--237},
  doi       = {10.1145/3330345.3330366},
  abstract  = {This paper formalizes the problem of reordering a sparse tensor to improve the spatial and temporal locality of operations with it, and proposes two reordering algorithms for this problem, which we call BFS-MCS and Lexi-Order. The BFS-MCS method is a Breadth First Search (BFS)-like heuristic approach based on the maximum cardinality search family; Lexi-Order is an extension of doubly lexical ordering of matrices to tensors. We show the effects of these schemes within the context of a widely used tensor computation, the CANDECOMP/PARAFAC decomposition (CPD), when storing the tensor in three previously proposed sparse tensor formats: coordinate (COO), compressed sparse fiber (CSF), and hierarchical coordinate (HiCOO). A new partition-based superblock scheduling is also proposed for HiCOO format to improve load balance. On modern multicore CPUs, we show Lexi-Order obtains up to 4.14$\times$ speedup on sequential HiCOO-Mttkrp and 11.88$\times$ speedup on its parallel counterpart. The performance of COO- and CSF-based Mttkrps also improves. Our two reordering methods are more effective than state-of-the-art approaches. The code is released as part of Parallel Tensor Infrastructure (ParTI!): https://github.com/hpcgarage/ParTI.},
  acmid     = {3330366},
  address   = {New York, NY, USA},
  keywords  = {HiCOO, hierarchical coordinate, reordering, sparse tensor, tensor decomposition},
  numpages  = {11},
}

@InProceedings{Sao2019a,
  author    = {Sao, Piyush and Kannan, Ramakrishnan and Li, Xiaoye Sherry and Vuduc, Richard},
  title     = {A Communication-avoiding 3D Sparse Triangular Solver},
  booktitle = {Proceedings of the ACM International Conference on Supercomputing},
  date      = {2019},
  series    = {ICS '19},
  publisher = {ACM},
  location  = {Phoenix, Arizona},
  isbn      = {978-1-4503-6079-1},
  pages     = {127--137},
  doi       = {10.1145/3330345.3330357},
  abstract  = {We present a novel distributed memory algorithm to improve the strong scalability of the solution of a sparse triangular system. This operation appears in the solve phase of direct methods for solving general sparse linear systems, $Ax = b$. Our 3D sparse triangular solver employs several techniques, including a 3D MPI process grid, elimination tree parallelism, and data replication, all of which reduce the per-process communication when combined. We present analytical models to understand the communication cost of our algorithm and show that our 3D sparse triangular solver can reduce the per-process communication volume asymptotically by a factor of $O(n^{1/4})$ and $O(n^{1/6})$ for problems arising from the finite element discretizations of 2D "planar" and 3D "non-planar" PDEs, respectively. We implement our algorithm for use in SuperLU_DIST3D, using a hybrid MPI+OpenMP programming model. Our 3D triangular solve algorithm, when run on 12k cores of Cray XC30, outperforms the current state-of-the-art 2D algorithm by 7.2$\times$ for planar and 2.7$\times$ for the non-planar sparse matrices, respectively.},
  acmid     = {3330357},
  address   = {New York, NY, USA},
  keywords  = {communication-avoiding algorithms, distributed-memory parallelism, sparse matrix computations},
  numpages  = {11},
}

@InBook{Alyahya2020,
  author    = {Alyahya, Hana and Mehmood, Rashid and Katib, Iyad},
  title     = {Parallel Iterative Solution of Large Sparse Linear Equation Systems on the Intel {MIC} Architecture},
  booktitle = {Smart Infrastructure and Applications: Foundations for Smarter Cities and Societies},
  date      = {2020},
  editor    = {Mehmood, Rashid and See, Simon and Katib, Iyad and Chlamtac, Imrich},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-030-13705-2},
  pages     = {377--407},
  doi       = {10.1007/978-3-030-13705-2_16},
  abstract  = {Many important scientific, engineering, and smart city applications require solving large sparse linear equation systems. The numerical methods for solving linear equations can be categorised into direct methods and iterative methods. Jacobi method is one of the iterative solvers that has been widely used due to its simplicity and efficiency. Its performance is affected by factors including the storage format, the specific computational algorithm, and its implementation. While the performance of Jacobi has been studied extensively on conventional CPU architectures, research on its performance on emerging architectures, such as the Intel Many Integrated Core (MIC) architecture, is still in its infancy. In this chapter, we investigate the performance of parallel implementations of the Jacobi method on Knights Corner (KNC), the first generation of the Intel MIC architectures. We implement Jacobi with two storage formats, Compressed Sparse Row (CSR) and Modified Sparse Row (MSR), and measure their performance in terms of execution time, offloading time, and speedup. We report results of sparse matrices with over 28 million rows and 640 million non-zero elements acquired from 13 diverse application domains. The experimental results show that our Jacobi parallel implementation on MIC achieves speedups of up to 27.75$\times$ compared to the sequential implementation. It also delivers a speedup of up to 3.81$\times$ compared to a powerful node comprising 24 cores in two Intel Xeon E5-2695v2 processors.},
}

@InBook{AlAhmadi2020,
  author    = {AlAhmadi, Sarah and Muhammed, Thaha and Mehmood, Rashid and Albeshri, Aiiad},
  title     = {Performance Characteristics for Sparse Matrix-Vector Multiplication on {GPUs}},
  booktitle = {Smart Infrastructure and Applications: Foundations for Smarter Cities and Societies},
  date      = {2020},
  editor    = {Mehmood, Rashid and See, Simon and Katib, Iyad and Chlamtac, Imrich},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-030-13705-2},
  pages     = {409--426},
  doi       = {10.1007/978-3-030-13705-2_17},
  abstract  = {The massive parallelism provided by the graphics processing units (GPUs) offers tremendous performance in many high-performance computing applications. One such application is Sparse Matrix-Vector (SpMV) multiplication, which is an essential building block for numerous scientific and engineering applications. Researchers who propose new storage techniques for sparse matrix-vector multiplication focus mainly on a single evaluation metrics or performance characteristics which is usually the throughput performance of sparse matrix-vector multiplication in FLOPS. However, such an evaluation does not provide a deeper insight nor allow to compare new SpMV techniques with their competitors directly. In this chapter, we explain the notable performance characteristics of the GPU architectures and SpMV computations. We discuss various strategies to improve the performance of SpMV on GPUs. We also discuss a few performance criteria that are usually overlooked by the researchers during the evaluation process. We also analyze various well-known schemes such as COO, CSR, ELL, DIA, HYB, and CSR5 using the discussed performance characteristics.},
}

@Online{Artemov2019,
  author     = {Artemov, Anton G.},
  title      = {Sparse approximate matrix multiplication in a fully recursive distributed task-parallel framework},
  date       = {2019},
  abstract   = {In this paper we consider parallel implementations of approximate multiplication of large matrices with exponential decay of elements. Such matrices arise in computations related to electronic structure calculations and some other fields of science. Commonly, sparsity is introduced by truncation of input matrices. In turn, the sparse approximate multiplication algorithm [M. Challacombe and N. Bock, arXiv preprint 1011.3534, 2010] performs truncation of sub-matrix products. We consider these two methods and their combination, i.e. truncation of both input matrices and sub-matrix products. Implementations done using the Chunks and Tasks programming model and library [E. H. Rubensson and E. Rudberg, Parallel Comput., 40:328343, 2014] are presented and discussed. The absolute error asymptotic behavior is derived. A comparison between the three methods in terms of performance is done on a model problem. The algorithms are also applied to matrices coming from large chemical systems with $\approx$106 atoms.},
  eprint     = {1906.0814},
  eprinttype = {arXiv},
}

@InProceedings{Anzalone2019,
  author   = {Anzalone, E. and Capra, M. and Peloso, R. and Martina, M. and Masera, G.},
  title    = {Low-power Hardware Accelerator for Sparse Matrix Convolution in Deep Neural Network"},
  date     = {2019},
  abstract = {Deep Neural Networks (DNN) have reached an outstanding accuracy in the past years, often going beyond human abilities. Nowadays, DNNs are widely used in many Artificial Intelligence (AI) applications such as computer vision, natural language processing and autonomous driving. However, these incredible performance come at a high computational cost, requiring complex hardware platforms. Therefore, the need for dedicated hardware accelerators able to drastically speed up the execution by preserving a low-power attitude arise. This paper presents innovative techniques able to tackle matrix sparsity in convolutional DNNs due to non-linear activation functions. Developed architectures allow to skip unnecessary operations, like zero multiplications, without sacrificing accuracy or throughput and improving the energy efficiency. Such improvement could enhance the performance of embedded limited-budget battery applications, where cost-effective hardware, accuracy and duration are critical to expanding the deployment of AI.},
}

@Article{Davydov2019,
  author        = {Davydov, Denis and Kronbichler, Martin},
  title         = {Algorithms and data structures for matrix-free finite element operators with MPI-parallel sparse multi-vectors},
  date          = {2019-07},
  eprint        = {1907.01005},
  eprinttype    = {arXiv},
  eprintclass   = {cs.MS},
  abstract      = {Traditional solution approaches for problems in quantum mechanics scale as $O(M3)$, where $M$ is the number of electrons. Various methods have been proposed to address this issue and obtain linear scaling $O(M)$. One promising formulation is the direct minimization of energy. Such methods take advantage of physical localization of the solution, namely that the solution can be sought in terms of non-orthogonal orbitals with local support. In this work a numerically efficient implementation of sparse parallel vectors within the open-source finite element library deal.II is proposed. The main algorithmic ingredient is the matrix-free evaluation of the Hamiltonian operator by cell-wise quadrature. Based on an a-priori chosen support for each vector we develop algorithms and data structures to perform (i) matrix-free sparse matrix multivector products (SpMM), (ii) the projection of an operator onto a sparse sub-space (inner products), and (iii) post-multiplication of a sparse multivector with a square matrix. The node-level performance is analyzed using a roofline model. Our matrix-free implementation of finite element operators with sparse multivectors achieves the performance of 157 GFlop/s on Intel Cascade Lake architecture. Strong and weak scaling results are reported for a typical benchmark problem using quadratic and quartic finite element bases.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190701005D},
  keywords      = {Computer Science - Mathematical Software, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis, Physics - Computational Physics},
}

@Article{Choi2019a,
  author       = {Choi, Dongjin and Jang, Jun-Gi and Kang, U.},
  title        = {S3CMTF: Fast, accurate, and scalable method for incomplete coupled matrix-tensor factorization},
  journaltitle = {{PloS} one},
  date         = {2019},
  volume       = {14},
  number       = {6},
  issn         = {1932-6203},
  doi          = {10.1371/journal.pone.0217316},
  abstract     = {How can we extract hidden relations from a tensor and a matrix data simultaneously in a fast, accurate, and scalable way? Coupled matrix-tensor factorization (CMTF) is an important tool for this purpose. Designing an accurate and efficient CMTF method has become more crucial as the size and dimension of real-world data are growing explosively. However, existing methods for CMTF suffer from lack of accuracy, slow running time, and limited scalability. In this paper, we propose S3CMTF, a fast, accurate, and scalable CMTF method. In contrast to previous methods which do not handle large sparse tensors and are not parallelizable, S3CMTF provides parallel sparse CMTF by carefully deriving gradient update rules. S3CMTF asynchronously updates partial gradients without expensive locking. We show that our method is guaranteed to converge to a quality solution theoretically and empirically. S3CMTF further boosts the performance by carefully storing intermediate computation and reusing them. We theoretically and empirically show that S3CMTF is the fastest, outperforming existing methods. Experimental results show that S3CMTF is up to 930$\times$ faster than existing methods while providing the best accuracy. S3CMTF shows linear scalability on the number of data entries and the number of cores. In addition, we apply S3CMTF to Yelp rating tensor data coupled with 3 additional matrices to discover interesting patterns.},
}

@InProceedings{Lee2019,
  author    = {Lee, Chao-Lin and Chao, Chen-Ting and Lee, Jenq-Kuen and Huang, Chung-Wen and Hung, Ming-Yu},
  title     = {Sparse-Matrix Compression Primitives with OpenCL Framework to Support Halide},
  booktitle = {Proceedings of the International Workshop on OpenCL},
  date      = {2019},
  series    = {IWOCL'19},
  publisher = {ACM},
  location  = {Boston, MA, USA},
  isbn      = {978-1-4503-6230-6},
  pages     = {1--2},
  doi       = {10.1145/3318170.3318179},
  abstract  = {Halide and OpenCL now play important roles for heterogeneous multi-core computing. OpenCL provides vendor-level support and Halide provides domain-specific support such as vision processing and AI model (TVM Halide IR). Halide also provides flexible scheduling for applications on target machines. OpenCL plays a supporting role for Halide environments. In this work, we investigate the research issues in supporting sparse computation with Halide and their corresponding OpenCL support. We present sparse matrix compression primitives on Halide for sparse matrix matrix (SpMM) multiplication with OpenCL framework. Halide is a programming language designed to process image and array from numerous algorithms and scheduling primitives to achieve state-of-art performance including SIMD and heterogeneous computation. This paper proposed the implementation of sparse matrix compression for Halide scheduling primitives including COO, CSR, and hybrid CSR. The design of experiments includes Halide primitives for sparse matrix compression and matrix computations. The experimental result of computation with compressing matrix shows the performance are improved by up to 85\% compared to the baseline without compression.},
  acmid     = {3318179},
  address   = {New York, NY, USA},
  articleno = {24},
  keywords  = {Halide, OpenCL, Sparse Matrix},
  numpages  = {2},
}

@Article{Grutzmacher2019,
  author       = {Grützmacher, Thomas and Cojean, Terry and Flegar, Goran and Göbel, Fritz and Anzt, Hartwig},
  title        = {A customized precision format based on mantissa segmentation for accelerating sparse linear algebra},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2019},
  doi          = {10.1002/cpe.5418},
  eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5418},
  abstract     = {In this work, we pursue the idea of radically decoupling the floating point format used for arithmetic operations from the format used to store the data in memory. We complement this idea with a customized precision memory format derived by splitting the mantissa (significand) of standard IEEE formats into segments, such that values can be accessed faster if lower accuracy is acceptable. Combined with precision-aware algorithms that dynamically adapt the data access accuracy to the numerical requirements, the customized precision memory format can render attractive runtime savings without impacting the memory footprint of the data or the accuracy of the final result. In an experimental analysis using the adaptive precision Jacobi method on diagonalizable test problems, we assess the benefits of the mantissa-segmenting customized precision format on recent multi- and manycore architectures.},
  keywords     = {adaptive precision Jacobi, GPUs, mantissa segmentation, modular precision ecosystem, multiprecision algorithm, multicore},
}

@Article{Goldenberg2019,
  author       = {Goldenberg, S. and Stathopoulos, A. and Romero, E.},
  title        = {A Golub--Kahan Davidson Method for Accurately Computing a Few Singular Triplets of Large Sparse Matrices},
  journaltitle = {{SIAM} Journal on Scientific Computing},
  date         = {2019},
  volume       = {41},
  number       = {4},
  pages        = {A2172-A2192},
  doi          = {10.1137/18M1222004},
  abstract     = {Obtaining high accuracy singular triplets for large sparse matrices is a significant challenge, especially when searching for the smallest triplets. Due to the difficulty and size of these problems, efficient methods must function iteratively, with preconditioners, and under strict memory constraints. In this research, we present a Golub--Kahan Davidson method (GKD), which satisfies these requirements and includes features such as soft-locking with orthogonality guarantees, an inner correction equation similar to Jacobi--Davidson, locally optimal +k restarting, and the ability to find real zero singular values in both square and rectangular matrices. Additionally, our method achieves full accuracy while avoiding the augmented matrix, which often converges slowly for the smallest triplets due to the difficulty of interior eigenvalue problems. We describe our method in detail, including implementation issues that arise. Our experimental results confirm the efficiency and stability of our method over the current implementation of PHSVDS in the PRIMME software package.},
}

@Article{Choi2019,
  author     = {Choi, Bosu and Christlieb, Andrew and Wang, Yang},
  title      = {Multiscale High-Dimensional Sparse Fourier Algorithms for Noisy Data},
  year       = {2019},
  eprint     = {1907.03692},
  eprinttype = {arXiv},
  abstract   = {We develop an efficient and robust high-dimensional sparse Fourier algorithm for noisy samples. Earlier in the paper Multi-dimensional sublinear sparse Fourier algorithm (2016) [3], an efficient sparse Fourier algorithm with $O(ds log s)$ average-case runtime and $O(ds)$ sampling complexity under certain assumptions was developed for signals that are s-sparse and bandlimited in the d-dimensional Fourier domain, i.e. there are at most s energetic frequencies and they are in $[-N/2, N/2)^d \cap \mathbb{Z}^d$. However, in practice the measurements of signals often contain noise, and in some cases may only be nearly sparse in the sense that they are well approximated by the best s Fourier modes. In this paper, we propose a multiscale sparse Fourier algorithm for noisy samples that proves to be both robust against noise and efficient.},
}

@Article{Klowckiewicz2019,
  author     = {Klowckiewicz, Bazyl and Darve, Eric},
  title      = {Sparse hierarchical preconditioners using piecewise smooth approximations of eigenvectors},
  date       = {2019},
  eprint     = {1907.0},
  eprinttype = {arXiv},
  abstract   = {When solving linear systems arising from PDE discretizations, iterative methods (such as Conjugate Gradient, GMRES, or MINRES) are often the only practical choice. To converge in a small number of iterations, however, they have to be coupled with an efficient preconditioner. The efficiency of the preconditioner depends largely on its accuracy on the eigenvectors corresponding to small eigenvalues, and unfortunately, black-box methods typically cannot guarantee sufficient accuracy on these eigenvectors. Thus, constructing the preconditioner becomes a very problemdependent task. We describe a hierarchical approximate factorization approach which addresses this issue by focusing on improving the accuracy on smooth eigenvectors (such eigenvectors typically correspond to the small eigenvalues). The improved accuracy is achieved by preserving the action of the factorized matrix on piecewise polynomial functions of the PDE domain. Based on the factorization, we propose a family of sparse preconditioners with $O (n)$ or $O (n log n)$ construction complexities. Our methods exhibit the optimal $O (n)$ solution times in benchmarks run on large elliptic problems of different types, arising for example in flow or mechanical simulations. In the case of the linear elasticity equation the preconditioners are exact on the near-kernel rigid body modes.},
}

@PhdThesis{Yang2019,
  author      = {Yang, Carl Y.},
  title       = {High-Performance Linear Algebra-based Graph Framework on the {GPU}},
  institution = {University of California, Davis},
  date        = {2019},
  url         = {https://escholarship.org/uc/item/37j8j27d},
  abstract    = {High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs, because of three challenges: (1) difficulty of coming up with graph building blocks, (2) load imbalance on parallel hardware, and (3) graph problems having low arithmetic ratio. To address these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based in sparse linear algebra, which will allow graph algorithms to be expressed in a performant, succinct, composable and portable manner. Initial research efforts in implementing GraphBLAS on GPUs has been promising, but performance still trails by an order of magnitude compared to state-of-the-art graph frameworks using the traditional graph-centric approach of describing operations on vertices or edges.\\ This dissertation examines the performance challenges of a linear algebra-based approach to building graph frameworks and describes new design principles for overcoming these bottlenecks. Among the new design principles is making exploiting input sparsity a first-class citizen in the framework. This is an especially important optimization, because it allows users to write graph algorithms without specifying certain implementation details thus permitting the software backend to choose the optimal implementation based on the input sparsity. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. We examine when it is profitable to exploit this output sparsity to reduce computational complexity. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics.\\ The design principles described in the thesis have been implemented in GraphBLAST, an open-source high-performance graph framework on GPU developed as part of this dissertation. It is notable for being the first graph framework based in linear algebra to get comparable or faster performance compared to the traditional, vertex-centric backends. The benefits of design principles described in this thesis have been shown to be important for single GPU, and it will grow in importance when it serves as a building block for distributed implementation in the future and as a single GPU backend for higher-level languages such as Python. A graph framework based in linear algebra not only improves performance of existing graph algorithms, but in quickly prototyping new algorithms as well.},
}

@Article{Camacho2019,
  author     = {Camacho, J. and Smilde, A. K. and Saccenti, E. and Westerhuis, J. A.},
  title      = {All Sparse PCA Models Are Wrong, But Some Are Useful. Part I: Computation of Scores, Residuals and Explained Variance},
  date       = {2019},
  eprint     = {1907.03989},
  eprinttype = {arXiv},
  abstract   = {Sparse Principal Component Analysis (sPCA) is a popular matrix factorization approach based on Principal Component Analysis (PCA) that combines variance maximization and sparsity with the ultimate goal of improving data interpretation. When moving from PCA to sPCA, there are a number of implications that the practitioner needs to be aware of. A relevant one is that scores and loadings in sPCA may not be orthogonal. For this reason, the traditional way of computing scores, residuals and variance explained that is used in the classical PCA cannot directly be applied to sPCA models. This also affects how sPCA components should be visualized. In this paper we illustrate this problem both theoretically and numerically using simulations for several state-ofthe-art sPCA algorithms, and provide proper computation of the different elements mentioned. We show that sPCA approaches present disparate and limited performance when modeling noisefree, sparse data. In a follow-up paper, we discuss the theoretical properties that lead to this problem.},
}

@Article{Bertsimas2019,
  author     = {Bertsimas, Dimitris and Stellato, Bartolomeo},
  title      = {Online Mixed-Integer Optimization in Milliseconds},
  year       = {2019},
  eprint     = {1907.02206},
  eprinttype = {arXiv},
  abstract   = {We propose a method to solve online mixed-integer optimization (MIO) problems at very high speed using machine learning. By exploiting the repetitive nature of online optimization, we are able to greatly speedup the solution time. Our approach encodes the optimal solution into a small amount of information denoted as strategy using the Voice of Optimization framework proposed in [BS18]. In this way the core part of the optimization algorithm becomes a multiclass classification problem which can be solved very quickly. In this work we extend that framework to real-time and high-speed applications focusing on parametric mixed-integer quadratic optimization (MIQO). We propose an extremely fast online optimization algorithm consisting of a feedforward neural network (NN) evaluation and a linear system solution where the matrix has already been factorized. Therefore, this online approach does not require any solver nor iterative algorithm. We show the speed of the proposed method both in terms of total computations required and measured execution time. We estimate the number of floating point operations (flops) required to completely recover the optimal solution as a function of the problem dimensions. Compared to state-of-the-art MIO routines, the online running time of our method is very predictable and can be lower than a single matrix factorization time. We benchmark our method against the state-ofthe-art solver Gurobi obtaining from two to three orders of magnitude speedups on benchmarks with real-world data.},
  timestamp  = {2019.07.09},
}

@InBook{Bollhofer2019,
  author     = {Bollhöfer, Matthias and Schenk, Olaf and Janalík, Radim and Hamm, Steve and Gullapalli, Kiran},
  title      = {State-of-The-Art Sparse Direct Solvers},
  date       = {2019},
  eprint     = {1907.05309},
  eprinttype = {arXiv},
  abstract   = {In this chapter we will give an insight into modern sparse elimination methods. These are driven by a preprocessing phase based on combinatorial algorithms which improve diagonal dominance, reduce fill-in and improve concurrency to allow for parallel treatment. Moreover, these methods detect dense submatrices which can be handled by dense matrix kernels based on multi-threaded level-3 BLAS. We will demonstrate for problems arising from circuit simulation how the improvement in recent years have advanced direct solution methods significantly},
}

@Article{Alappat2019,
  author     = {Alappat, Christie and Hager, Georg and Schenk, Olaf and Thies, Jonas and Basermann, Achim and Bishop, Alan R. and Fehske, Holger and Wellein, Gerhard},
  title      = {A Recursive Algebraic Coloring Technique for Hardware-Efficient Symmetric Sparse Matrix-Vector Multiplication},
  date       = {2019},
  eprint     = {1907.06487},
  eprinttype = {arXiv},
  abstract   = {The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important building block for many numerical linear algebra kernel operations or graph traversal applications. Parallelizing SymmSpMV on today's multicore platforms with up to 100 cores is difficult due to the need to manage conflicting updates on the result vector. Coloring approaches can be used to solve this problem without data duplication, but existing coloring algorithms do not take load balancing and deep memory hierarchies into account, hampering scalability and full-chip performance. In this work, we propose the recursive algebraic coloring engine (RACE), a novel coloring algorithm and open-source library implementation, which eliminates the shortcomings of previous coloring methods in terms of hardware efficiency and parallelization overhead. We describe the level construction, distance-k coloring, and load balancing steps in RACE, use it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices with other state-of-the-art coloring techniques and Intel MKL on two modern multicore processors. RACE outperforms all other approaches substantially and behaves in accordance with the roofline model. Outliers are discussed and analyzed in detail. While we focus on SymmSpMV in this paper, our algorithm and software is applicable to any sparse matrix operation with data dependencies that can be resolved by distance-k coloring.},
}

@PhdThesis{Shi2019,
  author      = {Shi, Yang},
  title       = {Efficient Tensor Operations via Compression and Parallel Computation},
  institution = {University of California, Irvine},
  date        = {2019},
  url         = {https://escholarship.org/uc/item/2wm4k3sn},
  abstract    = {Linear algebra is the foundation of machine learning, especially for handling big data. We want to extract useful information that can represent the behavior of the data. For data with underlying known structures, it is straightforward to apply algorithms that maintain that structure. For instance, singular value decomposition (SVD) is one way to approximate lowrank matrices. The generalized SVD, tensor decomposition, is the crux of model estimation for tensors. However, not all data has a trivial structure. Multi-modality data that contains information from different sources can be complex and hard to extract the structure. A data-independent randomized algorithm, such as sketching, is the solution for this case. Under both scenarios, the information extraction process may be statistically challenging as the problems are non-convex optimization problems. More importantly, the large size and the high-dimensionality of the data have been significant obstacles in discovering hidden variables and summarizing them. Thus, how to improve high-dimensional data computation efficiency is vitally important.\\ This thesis contains the theoretical analysis for learning the underlying information from high-dimensional structured or non-structured data via tensor operations such as tensor decomposition and tensor sketching. It is easy to consider tensors as multi-dimensional vectors or matrices and apply vector/matrix-based algorithms to find the solution. However, these methods omit multi-dimensionality of the data and can be computational inefficient than considering the tensor as a whole. We show the superiority of our approximation algorithms over these methods from computation and memory efficiency point of views.\\ This thesis also discusses optimizing tensor operation computations from the high-performance computing aspect. Conventional methods treat tensors as flattened matrices or vectors. Operations between tensors may require lots of permutations and reshapes. We propose new tensor algebra computation routines that avoid the prepossessing as much as possible. The value of this approach and its applications are recognized by NVIDIA. The proposed interface exists in the CUBLAS 8.0.},
}

@PhdThesis{Falco2019,
  author      = {Falco, Aurélien},
  date        = {2019},
  institution = {Université de Bordeaux},
  title       = {Bridging the Gap Between H-Matrices and Sparse Direct Methods for the Solution of Large Linear Systems},
  abstract    = {Many physical phenomena may be studied through modeling and numerical simulations, commonplace in scientific applications. To be tractable on a computer, appropriated discretization techniques must be considered, which often lead to a set of linear equations whose features depend on the discretization techniques. Among them, the Finite Element Method usually leads to sparse linear systems whereas the Boundary Element Method leads to dense linear systems. The size of the resulting linear systems depends on the domain where the studied physical phenomenon develops and tends to become larger and larger as the performance of the computer facilities increases. For the sake of numerical robustness, the solution techniques based on the factorization of the matrix associated with the linear system are the methods of choice when affordable. In that respect, hierarchical methods based on low-rank compression have allowed a drastic reduction of the computational requirements for the solution of dense linear systems over the last two decades. For sparse linear systems, their application remains a challenge which has been studied by both the community of hierarchical matrices and the community of sparse matrices. On the one hand, the first step taken by the community of hierarchical matrices most often takes advantage of the sparsity of the problem through the use of nested dissection. While this approach benefits from the hierarchical structure, it is not, however, as efficient as sparse solvers regarding the exploitation of zeros and the structural separation of zeros from non-zeros. On the other hand, sparse factorization is organized so as to lead to a sequence of smaller dense operations, enticing sparse solvers to use this property and exploit compression techniques from hierarchical methods in order to reduce the computational cost of these elementary operations. Nonetheless, the globally hierarchical structure may be lost if the compression of hierarchical methods is used only locally on dense submatrices. We here review the main techniques that have been employed by both those communities, trying to highlight their common properties and their respective limits with a special emphasis on studies that have aimed to bridge the gap between them. With these observations in mind, we propose a class of hierarchical algorithms based on the symbolic analysis of the structure of the factors of a sparse matrix. These algorithms rely on a symbolic information to cluster and construct a hierarchical structure coherent with the non-zero pattern of the matrix. Moreover, the resulting hierarchical matrix relies on low-rank compression for the reduction of the memory consumption of large submatrices as well as the time to solution of the solver. We also compare multiple ordering techniques based on geometrical or topological properties. Finally, we open the discussion to a coupling between the Finite Element Method and the Boundary Element Method in a unified computational framework.},
}

@Article{Massias2019,
  author     = {Massias, Mathurin and Vaiter, Samuel and Gramfort, Alexandre and Salmon, Joseph},
  title      = {Dual Extrapolation for Sparse Generalized Linear Models},
  date       = {2019},
  eprint     = {1907.05830},
  eprinttype = {arXiv},
  abstract   = {Generalized Linear Models (GLM) form a wide class of regression and classification models, where prediction is a function of a linear combination of the input variables. For statistical inference in high dimension, sparsity inducing regularizations have proven to be useful while offering statistical guarantees. However, solving the resulting optimization problems can be challenging: even for popular iterative algorithms such as coordinate descent, one needs to loop over a large number of variables. To mitigate this, techniques known as screening rules and working sets diminish the size of the optimization problem at hand, either by progressively removing variables, or by solving a growing sequence of smaller problems. For both techniques, significant variables are identified thanks to convex duality arguments. In this paper, we show that the dual iterates of a GLM exhibit a Vector AutoRegressive (VAR) behavior after sign identification, when the primal problem is solved with proximal gradient descent or cyclic coordinate descent. Exploiting this regularity, one can construct dual points that offer tighter certificates of optimality, enhancing the performance of screening rules and helping to design competitive working set algorithms.},
}

@InProceedings{Behnezhad2019,
  author    = {Behnezhad, Soheil and Brandt, Sebastian and Derakhshan, Mahsa and Fischer, Manuela and Hajiaghayi, MohammadTaghi and Karp, Richard M. and Uitto, Jara},
  title     = {Massively Parallel Computation of Matching and MIS in Sparse Graphs},
  booktitle = {Proceedings of the 2019 {ACM} Symposium on Principles of Distributed Computing},
  date      = {2019},
  series    = {PODC '19},
  publisher = {ACM},
  location  = {Toronto ON, Canada},
  isbn      = {978-1-4503-6217-7},
  pages     = {481--490},
  doi       = {10.1145/3293611.3331609},
  abstract  = {The Massively Parallel Computation (MPC) model serves as a common abstraction of many modern large-scale parallel computation frameworks and has recently gained a lot of importance, especially in the context of classic graph problems. In this work, we mainly consider maximal matching and maximal independent set problems in the MPC model.\\
  These problems are known to admit efficient MPC algorithms if the space available per machine is near-linear in the number n of nodes. This is not only often significantly more than what we can afford, but also allows for easy if not trivial solutions for sparse graphs -- which are common in real-world large-scale graphs. We are, therefore, interested in the low-memory MPC model, where the space per machine is restricted to be strongly sublinear, that is, $n^\delta$ for any constant $0 < \delta < 1$. \\
  We parametrize our algorithms by the arboricity $\lambda$ of the input graph. Our key ingredient is a degree reduction technique that reduces these problems in graphs with arboricity $\lambda$ to the corresponding problems in graphs with maximum degree $poly(\lambda, log n)$ in $O(log^2 log n)$ rounds, giving rise to $O(\sqrt{log \lambda} \cdot log log \lambda + log 2 log n)$-round algorithms.\\
  Our result is particularly interesting for graphs with $poly log n$ arboricity as for such graphs, we get $O(log^2 log n)$-round algorithms. This covers most natural families of sparse graphs and almost exponentially improves over previous algorithms that all required log $O(1)$ n rounds in this regime of MPC.\\
  Finally, our maximal matching algorithm can be employed to obtain a $(1+\epsilon)$-approximate maximum cardinality matching, a $(2+\epsilon)$-approximate maximum weighted matching, as well as a 2-approximate minimum vertex cover in essentially the same number of rounds.},
  acmid     = {3331609},
  address   = {New York, NY, USA},
  keywords  = {approximation algorithms, arboricity, massively parallel computation, matching, maximal independent set, sparse graphs, sublinear memory},
  numpages  = {10},
}

@InProceedings{Chang2019,
  author    = {Chang, Yi-Jun and Fischer, Manuela and Ghaffari, Mohsen and Uitto, Jara and Zheng, Yufan},
  title     = {The Complexity of {$\Delta +1$} Coloring in Congested Clique, Massively Parallel Computation, and Centralized Local Computation},
  booktitle = {Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing},
  date      = {2019},
  series    = {PODC '19},
  publisher = {ACM},
  location  = {Toronto ON, Canada},
  isbn      = {978-1-4503-6217-7},
  pages     = {471--480},
  doi       = {10.1145/3293611.3331607},
  abstract  = {In this paper, we present new randomized algorithms that improve the complexity of the classic $(\Delta+1)$-coloring problem, and its generalization $(\Delta+1)$-list-coloring, in three well-studied models of distributed, parallel, and centralized computation: Distributed Congested Clique: We present an $O(1)$-round randomized algorithm for $(\Delta + 1)$-list-coloring in the congested clique model of distributed computing. This settles the asymptotic complexity of this problem. It moreover improves upon the $O(log^\star \Delta)$-round randomized algorithms of Parter and Su [DISC'18] and $O((log log \Delta) \cdot log^\star \Delta)$-round randomized algorithm of Parter [ICALP'18].\\ Massively Parallel Computation: We present a randomized $(\Delta + 1)$-list-coloring algorithm with round complexity $O(\sqrt{log log n})$ in the Massively Parallel Computation (MPC) model with strongly sublinear memory per machine. This algorithm uses a memory of $O(n\alpha)$ per machine, for any desirable constant $\alpha > 0$, and a total memory of $\tilde{O} (m)$, where m is the number of edges in the graph. Notably, this is the first coloring algorithm with sublogarithmic round complexity, in the sublinear memory regime of MPC. For the quasilinear memory regime of MPC, an $O(1)$-round algorithm was given very recently by Assadi et al. [SODA'19].\\ Centralized Local Computation: We show that $(\Delta + 1)$-list-coloring can be solved by a randomized algorithm with query complexity $\Delta O(1) \dots O(log n)$, in the centralized local computation model. The previous state of the art for $(\Delta+1)$-list-coloring in the centralized local computation model are based on simulation of known LOCAL algorithms. The deterministic $O(\sqrt{\Delta poly log \Delta} + log^\star n)$-round LOCAL algorithm of Fraigniaud et al. [FOCS'16] can be implemented in the centralized local computation model with query complexity $\Delta^O(\sqrt{ \Delta poly log \Delta}) \dots O(log^\star n)$; the randomized $O(log^\star \Delta) + 2^O(\sqrt{log log n})$-round LOCAL algorithm of Chang et al. [STOC'18] can be implemented in the centralized local computation model with query complexity $\Delta^O(log\star \Delta) \dots O(log n)$.},
  acmid     = {3331607},
  address   = {New York, NY, USA},
  keywords  = {centralized local computation, coloring, congested clique, massively parallel computation},
  numpages  = {10},
}

@InProceedings{Kurzak2019,
  author    = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
  title     = {Massively Parallel Automated Software Tuning},
  booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
  date      = {2019},
  series    = {ICPP 2019},
  publisher = {ACM},
  location  = {Kyoto, Japan},
  isbn      = {978-1-4503-6295-5},
  pages     = {92:1--92:10},
  doi       = {10.1145/3337821.3337908},
  abstract  = {This article presents an implementation of a distributed autotuning engine developed as part of the Bench-testing OpenN Software Autotuning Infrastructure project. The system is geared towards performance optimization of computational kernels for graphics processing units, and allows for the deployment of vast autotuning sweeps to massively parallel machines. The software implements dynamic work scheduling to distributed-memory resources and takes advantage of multithreading for parallel compilation and dispatches kernel launches to multiple accelerators. This paper lays out the main design principles of the system and discusses the basic mechanics of the initial implementation. Preliminary performance results are presented, encountered challenges are discussed, and the future directions are outlined.},
  acmid     = {3337908},
  address   = {New York, NY, USA},
  articleno = {92},
  keywords  = {automated software tuning, graphics processing unit},
  numpages  = {10},
}

@InProceedings{Argueta2019,
  author    = {Argueta, Arturo and Chiang, David},
  title     = {Accelerating Sparse Matrix Operations in Neural Networks on Graphics Processing Units},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  date      = {2019},
  location  = {Florence, IT},
  pages     = {6215--6224},
  abstract  = {Graphics Processing Units (GPUs) are commonly used to train and evaluate neural networks efficiently. While previous work in deep learning has focused on accelerating operations on dense matrices/tensors on GPUs, efforts have concentrated on operations involving sparse data structures. Operations using sparse structures are common in natural language models at the input and output layers, because these models operate on sequences over discrete alphabets. We present two new GPU algorithms: one at the input layer, for multiplying a matrix by a few-hot vector (generalizing the more common operation of multiplication by a one-hot vector) and one at the output layer, for a fused softmax and top-N selection (commonly used in beam search). Our methods achieve speedups over state-of-theart parallel GPU baselines of up to 7$\times$ and 50$\times$, respectively. We also illustrate how our methods scale on different GPU architectures.},
  timestamp = {2019.08.02},
}

@InProceedings{Li2019b,
  author    = {Li, Ming and Hawrylak, Peter and Hale, John},
  title     = {Combining OpenCL and MPI to Support Heterogeneous Computing on a Cluster},
  booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
  date      = {2019},
  series    = {PEARC '19},
  publisher = {ACM},
  location  = {Chicago, IL, USA},
  isbn      = {978-1-4503-7227-5},
  pages     = {5:1--5:6},
  doi       = {10.1145/3332186.3333059},
  abstract  = {This paper presents an implementation of a heterogeneous programming model which combines Open Computing Language (OpenCL) and Message Passing Interface (MPI). The model is applied to solving a Markov decision process (MDP) with value iteration method. The performance test is conducted on a high performance computing cluster. At peak performance, the model is able to achieve a 57$\times$ speedup over a serial implementation. For an extremely large input MDP, which has 1,000,000 states, the obtained speedup is still over 12$\times$, showing that this heterogeneous programming model can solve MDPs more efficiently than the serial solver does.},
  acmid     = {3333059},
  address   = {New York, NY, USA},
  articleno = {5},
  keywords  = {HPC, MDP, MPI, OpenCL, heterogeneous computing, parallelism},
  numpages  = {6},
}

@PhdThesis{Mamooler2019,
  author      = {Mamooler, Parisa},
  title       = {The domain decomposition method of Bank and Jimack as an optimized Schwarz method},
  institution = {University of Geneva},
  date        = {2019},
  abstract    = {The aim of this thesis is to introduce the Bank-Jimack domain decomposition method and study its convergence behavior. We are interested in understanding what the precise contribution of the outer coarse mesh is to the convergence behavior of the domain decomposition method proposed by Bank and Jimack. We show for a two subdomain decomposition that the outer coarse mesh can be interpreted as computing an approximation to the optimal transmission condition represented by the Dirichlet to Neumann map, and thus the method of Bank and Jimack can be viewed as an optimized Schwarz method, i.e. a Schwarz method that uses Robin or higher order transmission conditions instead of theclassical Dirichlet ones.},
  timestamp   = {2019.08.17},
}

@Article{Kong2019,
  author      = {Kong, Fande},
  title       = {A parallel monolithic multilevel Schwarz preconditioner for the neutron transport criticality calculations with a nonlinear diffusion acceleration method},
  date        = {2019},
  eprint      = {1907.12590v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {The multigroup neutron transport criticality calculations using modern supercomputers have been widely employed in a nuclear reactor analysis for studying whether or not a system is self-sustaining. However, the design and development of an efficient parallel algorithm for the transport criticality calculations is a challenging task especially when the number of processor cores is large and the unstructured mesh is adopted since both the compute time and the memory usage need to be taken into consideration. In this paper, we study a monolithic multilevel Schwarz preconditioner for the transport criticality calculations using the nonlinear diffusion acceleration (NDA). In NDA, the linear systems of equations arising from the discretizations of the nonlinear diffusion equations and the transport equations need to be efficiently solved. To achieve this goal, we propose a monolithically coupled approach equipped with several important ingredients; e.g., subspace-based coarsening, aggressive coarsening and strength matrix thresholding. The proposed monolithic multilevel method is capable of efficiently handling the linear systems of equations for both the transport system and the diffusion system. In the multilevel method, the construction of coarse spaces is nontrivial and expensive. We propose a subspace-based coarsening algorithm to resolve this issue by exploring the matrix structures of the transport equations and the nonlinear diffusion equations. We numerically demonstrate that the monolithic multilevel preconditioner with the subspace-based coarsening algorithm is twice as fast as that equipped with a full space based coarsening approach on thousands of processor cores for an unstructured mesh neutron transport problem with billions of unknowns.},
  file        = {online:http\://arxiv.org/pdf/1907.12590v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@InProceedings{Regunta2019,
  author    = {Regunta, S. C. and Tondomker, S. H. and Kothapalli, K.},
  title     = {BRICS -- Efficient Techniques for Estimating the Farness-Centrality in Parallel},
  booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium Workshops},
  date      = {2019-05},
  series    = {IPDPSW'19},
  pages     = {645--654},
  doi       = {10.1109/IPDPSW.2019.00110},
  abstract  = {In this paper, we study scalable parallel algorithms for estimating the farness-centrality value of the nodes in a given undirected and connected graph. Our algorithms consider approaches that are more suitable for sparse graphs. To this end, we propose four optimization techniques based on removing redundant nodes, removing identical nodes, removing chain nodes, and making use of decomposition based on the biconnected components of the input graph. We test our techniques on a collection of real-world graphs for the time taken and the average error percentage. We further analyze the applicability of our techniques on various classes of real-world graphs. We suggest why certain techniques work better on certain classes of graphs.},
  keywords  = {Optimization;Measurement;Parallel algorithms;Scalability;Silicon;Social networking (online);Estimation;closeness centrality;parallel;real world graphs;estimation},
}

@Article{Carson2019,
  author      = {Carson, Erin C.},
  title       = {An Adaptive $s$-step Conjugate Gradient Algorithm with Dynamic Basis Updating},
  year        = {2019},
  date        = {2019-08-12},
  eprint      = {1908.04081v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {The adaptive $s$-step CG algorithm is a solver for sparse, symmetric positive definite linear systems designed to reduce the synchronization cost per iteration while still achieving a user-specified accuracy requirement. In this work, we improve the adaptive $s$-step conjugate gradient algorithm by use of iteratively updated estimates of the largest and smallest Ritz values, which give approximations of the largest and smallest eigenvalues of $A$, using a technique due to Meurant and Tichý [G. Meurant and P. Tichý, Numer. Algs. (2018), pp.~1--32]. The Ritz value estimates are used to dynamically update parameters for constructing Newton or Chebyshev polynomials so that the conditioning of the $s$-step bases can be continuously improved throughout the iterations. These estimates are also used to automatically set a variable related to the ratio of the sizes of the error and residual, which was previously treated as an input parameter. We show through numerical experiments that in many cases the new algorithm improves upon the previous adaptive $s$-step approach both in terms of numerical behavior and reduction in number of synchronizations.},
  file        = {online:http\://arxiv.org/pdf/1908.04081v1:PDF},
  keywords    = {math.NA, cs.DC, cs.NA},
}

@Article{Yang2019a,
  author      = {Yang, Carl and Buluç, Aydın and Owens, John D.},
  title       = {{GraphBLAST}: A High-Performance Linear Algebra-based Graph Framework on the {GPU}},
  date        = {2019},
  eprint      = {1908.01407v2},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs, because of three challenges: (1) difficulty of coming up with graph building blocks, (2) load imbalance on parallel hardware, and (3) graph problems having low arithmetic intensity. To address these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based in sparse linear algebra, which will allow graph algorithms to be expressed in a performant, succinct, composable and portable manner. In this paper, we examine the performance challenges of a linear algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in "GraphBLAST", the first open-source linear algebra-based graph framework on GPU targeting high-performance computing. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model.},
  file        = {online:http\://arxiv.org/pdf/1908.01407v2:PDF},
  keywords    = {cs.DC},
}

@Article{Iwashita2019,
  author      = {Iwashita, Takeshi and Li, Senxi and Fukaya, Takeshi},
  title       = {Hierarchical Block Multi-Color Ordering: A New Parallel Ordering Method for Vectorization and Parallelization of the Sparse Triangular Solver in the ICCG Method},
  date        = {2019},
  eprint      = {1908.00741v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a new parallel ordering method to vectorize and parallelize the sparse triangular solver, which is called hierarchical block multi-color ordering. In this method, the parallel forward and backward substitutions can be vectorized while preserving the advantages of block multi-color ordering, that is, fast convergence and fewer thread synchronizations. To evaluate the proposed method in a parallel ICCG (Incomplete Cholesky Conjugate Gradient) solver, numerical tests were conducted using five test matrices on three types of computational nodes. The numerical results indicate that the proposed method outperforms the conventional block and nodal multi-color ordering methods in 13 out of 15 test cases, which confirms the effectiveness of the method.},
  file        = {online:http\://arxiv.org/pdf/1908.00741v1:PDF},
  keywords    = {cs.DC},
}

@Article{Kong2019b,
  author       = {Kong, Qian and Jing, Yan-Fei and Huang, Ting-Zhu and An, Heng-Bin},
  title        = {Acceleration of the Scheduled Relaxation Jacobi method: promising strategies for solving large, sparse linear systems},
  journaltitle = {Journal of Computational Physics},
  date         = {2019},
  pages        = {108862},
  issn         = {0021-9991},
  doi          = {10.1016/j.jcp.2019.108862},
  url          = {http://www.sciencedirect.com/science/article/pii/S0021999119305467},
  abstract     = {The main aim of this paper is to develop two algorithms based on the Scheduled Relaxation Jacobi (SRJ) method [J. Comput. Phys., 274 (2014), pp. 695-708] for solving problems arising from the finite-difference discretization of elliptic partial differential equations on large grids. These two algorithms are the Alternating Anderson-Scheduled Relaxation Jacobi (AASRJ) method by utilizing Anderson mixing after each SRJ iteration cycle and the Minimal Residual Scheduled Relaxation Jacobi (MRSRJ) method by minimizing residual after each SRJ iteration cycle, respectively. Through numerical experiments, we show that AASRJ is competitive with the optimal version of the SRJ method [J. Comput. Phys., 332 (2017), pp. 446-460] in most problems we considered here, and MRSRJ outperforms SRJ in all cases. The properties of AASRJ and MRSRJ are demonstrated. Both of them are promising strategies for solving large, sparse linear systems while maintaining the simplicity of the Jacobi method.},
  keywords     = {Acceleration, The Scheduled Relaxation Jacobi method, Anderson mixing},
}

@Article{Usman2019,
  author       = {Usman, Sardar and Mehmood, Rashid and Katib, Iyad and Albeshri, Aiiad and Altowaijri, Saleh M.},
  title        = {ZAKI: A Smart Method and Tool for Automatic Performance Optimization of Parallel SpMV Computations on Distributed Memory Machines},
  journaltitle = {Mobile Networks and Applications},
  date         = {2019},
  doi          = {10.1007/s11036-019-01318-3},
  abstract     = {SpMV is a vital computing operation of many scientific, engineering, economic and social applications, increasingly being used to develop timely intelligence for the design and management of smart societies. Several factors affect the performance of SpMV computations, such as matrix characteristics, storage formats, software and hardware platforms. The complexity of the computer systems is on the rise with the increasing number of cores per processor, different levels of caches, processors per node and high speed interconnect. There is an ever-growing need for new optimization techniques and efficient ways of exploiting parallelism. In this paper, we propose ZAKI, a data-driven, machine-learning approach and tool, to predict the optimal number of processes for SpMV computations of an arbitrary sparse matrix on a distributed memory machine. The aim herein is to allow application scientists to automatically obtain the best configuration, and hence the best performance, for the execution of SpMV computations. We train and test the tool using nearly 2000 real world matrices obtained from 45 application domains including computational fluid dynamics (CFD), computer vision, and robotics. The tool uses three machine learning methods, decision trees, random forest, gradient boosting, and is evaluated in depth. A discussion on the applicability of our proposed tool to energy efficiency optimization of SpMV computations is given. This is the first work where the sparsity structure of matrices have been exploited to predict the optimal number of processes for a given matrix in distributed memory environments by using different base and ensemble machine learning methods.},
  timestamp    = {2019.08.17},
}

@Article{Peng2019,
  author      = {Peng, Shaoyi and Tan, Sheldon X. D.},
  title       = {{GLU3.0}: Fast {GPU}-based Parallel Sparse {LU} Factorization for Circuit Simulation},
  date        = {2019},
  eprint      = {1908.00204v2},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {In this article, we propose a new GPU-based sparse LU factorization method, called GLU3.0, solves the aforementioned problems. First, it introduces a much more efficient double-U dependency detection algorithm to make the detection much simpler. Second, we observe that the potential parallelism is different as the matrix factorization goes on. We then develop three different modes of GPU kernel to adapt to different stages to accommodate the computing task changes in the factorization. As a result, the new GLU can dynamically allocate GPU blocks and wraps based on the number of columns in a level to better balance the computing demands and resources during the LU factorization process. Experimental results on circuit matrices from University of Florida Sparse Matrix Collection (UFL) show that the GLU3.0 can deliver 2-3 orders of magnitude speedup over GLU2.0 for the data dependency detection. Furthermore, GLU3.0 achieve 13.0$\times$ (arithmetic mean) and 6.7$\times$ (geometric mean) speedup over GLU2.0 and 7.1$\times$ (arithmetic mean) and 4.8$\times$ (geometric mean) over the recently proposed enhanced GLU2.0 sparse LU solver on the same set of circuit matrices.},
  file        = {online:http\://arxiv.org/pdf/1908.00204v2:PDF},
  keywords    = {cs.DC, cs.DS},
}

@InProceedings{Ribizel2019,
  author    = {Ribizel, T. and Anzt, H.},
  title     = {Approximate and Exact Selection on {GPUs}},
  booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium Workshops},
  date      = {2019-05},
  series    = {IPDPSW'19},
  pages     = {471--478},
  doi       = {10.1109/IPDPSW.2019.00088},
  abstract  = {We present a novel algorithm for parallel selection on GPUs. The algorithm requires no assumptions on the input data distribution, and has a much lower recursion depth compared to many state-of-the-art algorithms. We implement the algorithm for different GPU generations, always using the respectively-available low-level communication features, and assess the performance on server-line hardware. The computational complexity of our SampleSelect algorithm is comparable to specialized algorithms designed for - and exploiting the characteristics of - "pleasant" data distributions. At the same time, as the SampleSelect does not work on the actual values but the ranks of the elements only, it is robust to the input data and can complete significantly faster for adversarial data distributions. Additionally to the exact SampleSelect, we address the use case of approximate selection by designing a variant that radically reduces the computational cost while preserving high approximation accuracy.},
  keywords  = {Approximation algorithms;Sorting;Indexes;Kernel;Graphics processing units;Partitioning algorithms;Hardware;parallel selection algorithm;GPU;kth order statistics;approximate threshold selection},
}

@InProceedings{Xie2019a,
  author    = {Xie, Jiaming and Liang, Yun},
  title     = {SPART: Optimizing CNNs by Utilizing Both Sparsity of Weights and Feature Maps},
  booktitle = {Advanced Parallel Processing Technologies},
  date      = {2019},
  editor    = {Yew, Pen-Chung and Stenström, Per and Wu, Junjie and Gong, Xiaoli and Li, Tao},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-030-29611-7},
  pages     = {71--85},
  abstract  = {Intense convolution computation and great memory requirement in CNNs constraint their wider deployments and applications. Although both the weights and feature maps in CNNs can be sparse, directly mapping sparse convolution to spGEMM in HPC domain fails to improve the actual performance. Besides, existing sparse formats like CSR are not suitable for encoding the sparse feature maps because convolution operates across rows.},
}

@InProceedings{Anzt2019b,
  author    = {Anzt, Hartwig and Flegar, Goran},
  title     = {Are we Doing the Right Thing? A Critical Analysis of the Academic {HPC} Community},
  booktitle = {2019 {IEEE} International Parallel and Distributed Processing Symposium Workshops},
  date      = {2019-05},
  series    = {IPDPSW'19},
  publisher = {{IEEE}},
  doi       = {10.1109/ipdpsw.2019.00122},
  abstract  = {Like in any other research field, academically surviving in the High Performance Computing (HPC) community generally requires to publish papers, in the bast case many of them and in high-ranked journals or at top-tier conferences. As a result, the number of scientific papers published each year in this relatively small community easily outnumbers what a single researcher can read. At the same time, many of the proposed and analyzed strategies, algorithms, and hardware-optimized implementations never make it beyond the prototype stage, as they are abandoned once they served the single purpose of yielding (another) publication. In a time and field where high-quality manpower is a scarce resource, this is extremely inefficient. In this position paper we promote a radical paradigm shift towards accepting high-quality software patches to community software packages as legitimate conference contributions. In consequence, the reputation and appointability of researchers is no longer based on the classical scientific metrics, but on the quality and documentation of open source software contributions -- effectively improving and accelerating the collaborative development of community software.},
}

@InProceedings{Garstka2019,
  author    = {Garstka, Michael and Cannon, Mark and Goulart, Paul},
  title     = {{COSMO}: A conic operator splitting method for large convex problems},
  booktitle = {2019 18th European Control Conference ({ECC})},
  date      = {2019-06},
  publisher = {{IEEE}},
  doi       = {10.23919/ecc.2019.8796161},
  abstract  = {This paper describes the Conic Operator Splitting Method (COSMO), an operator splitting algorithm for convex optimisation problems with quadratic objective function and conic constraints. At each step the algorithm alternates between solving a quasi-definite linear system with a constant coefficient matrix and a projection onto convex sets. The solver is able to exploit chordal sparsity in the problem data and to detect infeasible problems. The low per-iteration computational cost makes the method particularly efficient for large problems, e.g. semidefinite programs in portfolio optimisation, graph theory, and robust control. Our Julia implementation is open-source, extensible, integrated into the Julia optimisation ecosystem and performs well on a variety of large convex problem classes.},
}

@Article{Stramondo2019,
  author       = {Stramondo, Giulio and Ciobanu, Cătălin Bogdan and Laat, Cees and Varbanescu, Ana Lucia},
  title        = {Designing and building application-centric parallel memories},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2019-08},
  doi          = {10.1002/cpe.5485},
  abstract     = {Memory bandwidth is a critical performance factor for many applications and architectures. Intuitively, a parallel memory could be a good solution for any bandwidth-limited application, yet building application-centric custom parallel memories remains a challenge. In this work, we present a comprehensive approach to tackle this challenge and demonstrate how to systematically design and implement application-centric parallel memories. Specifically, our approach (1) analyzes the application memory access traces to extract parallel accesses, (2) configures our parallel memory for maximum performance, and (3) builds the actual application-centric memory system. We further provide a simple performance prediction model for the constructed memory system. We evaluate our approach with two sets of experiments. First, we demonstrate how our parallel memories provide performance benefits for a broad range of memory access patterns. Second, we prove the feasibility of our approach and validate our performance model by implementing and benchmarking the designed parallel memories using FPGA hardware and a sparse version of the STREAM benchmark.},
  publisher    = {Wiley},
}

@InProceedings{Nie2019a,
  author    = {Nie, Jing and Zhang, Chunlei and Zou, Dan and Xia, Fei and Lu, Lina and Wang, Xiang and Zhao, Fei},
  title     = {Adaptive Sparse Matrix-Vector Multiplication on {CPU}-{GPU} Heterogeneous Architecture},
  booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
  date      = {2019},
  series    = {HPCCT'19},
  publisher = {{ACM} Press},
  doi       = {10.1145/3341069.3341072},
  abstract  = {SpMV is the core algorithm in solving the sparse linear equations, which is widely used in many research and engineering application field. GPU is the most common coprocessor in high-performance computing domain, and has already been proven to researchers the practical value in accelerating various algorithms. A lot of reletead work has been carried out to optimize parallel SpMV on CPU-GPU platforms, which mainly focuses on reducing the computing overhead on the GPU, including branch divergence and cache missing, and little attention was paid to the overall efficiency of the heterogeneous platform. In this paper, we describe the design and implementation of an adaptive sparse matrix-vector multiplication (SpMV) on CPU-GPU heterogeneous architecture. We propose a dynamic task scheduling framework for CPU-GPU platform to improve the utilization of both CPU and GPU. A double buffering scheme is also presented to hide the data transfer overhead between CPU and GPU. Two deeply optimized SpMV kernels are deployed for CPU and GPU respectively. The evaluation on typical sparse matrices indicates that the proposed algorithm obtains both significant performance increase and adaptability to different types of sparse matrices.},
}

@Article{Silvestri2019,
  author      = {Silvestri, Francesco and Vella, Flavio},
  title       = {A Computational Model for Tensor Core Units},
  date        = {2019-08-19},
  eprint      = {arXiv:1908.06649v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {To respond to the need of efficient training and inference of deep neural networks, a pletora of domain-specific hardware architectures have been introduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A common feature of these architectures is a hardware circuit for efficiently computing a dense matrix multiplication of a given small size. In order to broad the class of algorithms that exploit these systems, we propose a computational model, named TCU model, that captures the ability to natively multiply small matrices. We then use the TCU model for designing fast algorithms for linear algebra problems, including dense and sparse matrix multiplication, FFT, integer multiplication, and polynomial evaluation. We finally highlight a relation between the TCU model and the external memory model.},
  file        = {:http\://arxiv.org/pdf/1908.06649v1:PDF},
  keywords    = {cs.DS, cs.AR, cs.DC, cs.LG},
}

@InProceedings{Besta2017,
  author    = {Besta, Maciej and Podstawski, Michał and Groner, Linus and Solomonik, Edgar and Hoefler, Torsten},
  title     = {To Push or To Pull},
  booktitle = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
  date      = {2017},
  series    = {HPDC'17},
  publisher = {{ACM} Press},
  doi       = {10.1145/3078597.3078616},
  abstract  = {We reduce the cost of communication and synchronization in graph processing by analyzing the fastest way to process graphs: pushing the updates to a shared state or pulling the updates to a private state. We investigate the applicability of this push-pull dichotomy to various algorithms and its impact on complexity, performance, and the amount of used locks, atomics, and reads/writes. We consider 11 graph algorithms, 3 programming models, 2 graph abstractions, and various families of graphs. The conducted analysis illustrates surprising differences between push and pull variants of different algorithms in performance, speed of convergence, and code complexity; the insights are backed up by performance data from hardware counters. We use these findings to illustrate which variant is faster for each algorithm and to develop generic strategies that enable even higher speedups. Our insights can be used to accelerate graph processing engines or libraries on both massively-parallel shared-memory machines as well as distributed-memory systems.},
}

@InProceedings{Zheng2019a,
  author    = {Zheng, Tianqi and Zhang, Zhibin and Cheng, Xueqi},
  title     = {SilverChunk: An Efficient In-Memory Parallel Graph Processing System},
  booktitle = {Database and Expert Systems Applications},
  date      = {2019},
  editor    = {Hartmann, Sven and Küng, Josef and Chakravarthy, Sharma and Anderst-Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
  publisher = {Springer International Publishing},
  isbn      = {978-3-030-27618-8},
  pages     = {222--236},
  abstract  = {One of the main constructs of graph processing is the two-level nested loop structure. Parallelizing nested loops is notoriously unfriendly to both CPU and memory access when dealing with real graph data due to its skewed distribution. To address this problem, we present SilverChunk, a high performance graph processing system. SilverChunk builds edge chunks of equal size from original graphs and unfolds nested loops statically in pull-based executions (VR-Chunk) and dynamically in push-based executions (D-Chunk). VR-Chunk slices the entire graph into several chunks. A virtual vertex is generated pointing to the first half of each sliced edge list so that no edge list lives in more than one chunk. D-Chunk builds its chunk list via binary searching over the prefix degree sum array of the active vertices. Each chunk has a local buffer for conflict-free maintenance of the next frontier. By changing the units of scheduling from edges to chunks, SilverChunk achieves better CPU and memory utilization. SilverChunk provides a high level programming interface combined with multiple optimization techniques to help developing efficient graph processing applications. Our evaluation results reveal that SilverChunk outperforms state-of-the-art shared-memory graph processing systems by up to $4\times4$, including Gemini, Grazelle, etc. Moreover, it has lower memory overheads and nearly zero pre-processing time.},
}

@Article{Williams-Young2019,
  author      = {Williams-Young, David B. and Beckman, Paul G. and Yang, Chao},
  title       = {A Shift Selection Strategy for Parallel Shift-Invert Spectrum Slicing in Symmetric Self-Consistent Eigenvalue Computation},
  year        = {2019},
  date        = {2019-08-16},
  eprint      = {1908.06043},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {The central importance of large scale eigenvalue problems in scientific computation necessitates the development massively parallel algorithms for their solution. Recent advances in dense numerical linear algebra have enabled the routine treatment of eigenvalue problems with dimensions on the order of hundreds of thousands on the world's largest supercomputers. In cases where dense treatments are not feasible, Krylov subspace methods offer an attractive alternative due to the fact that they do not require storage of the problem matrices. However, demonstration of scalability of either of these classes of eigenvalue algorithms on computing architectures capable of expressing excessive parallelism is non-trivial due to communication requirements and serial bottlenecks, respectively. In this work, we introduce the SISLICE method: a parallel shift-invert algorithm for the solution of the symmetric self-consistent field (SCF) eigenvalue problem. The SISLICE method drastically reduces the communication requirement of current parallel shift-invert eigenvalue algorithms through various shift selection and migration techniques based on density of states estimation and k-means clustering, respectively. This work demonstrates the robustness and parallel performance of the SISLICE method on a representative set of SCF eigenvalue problems and outlines research directions which will be explored in future work.},
  keywords    = {math.NA, cs.DC, cs.NA, physics.comp-ph},
}

@Article{Sid-Lakhdar2019,
  author      = {Sid-Lakhdar, Wissam M. and Aznaveh, Mohsen Mahmoudi and Li, Xiaoye S. and Demmel, James W.},
  title       = {Multitask and Transfer Learning for Autotuning Exascale Applications},
  date        = {2019-08-15},
  eprint      = {1908.05792},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {Multitask learning and transfer learning have proven to be useful in the field of machine learning when additional knowledge is available to help a prediction task. We aim at deriving methods following these paradigms for use in autotuning, where the goal is to find the optimal performance parameters of an application treated as a black-box function. We show comparative results with state-of-the-art autotuning techniques. For instance, we observe an average $1.5\times$ improvement of the application runtime compared to the OpenTuner and HpBandSter autotuners. We explain how our approaches can be more suitable than some state-of-the-art autotuners for the tuning of any application in general and of expensive exascale applications in particular.},
  keywords    = {cs.LG, cs.DC, stat.ML},
}

@Article{Karypis1998,
  author       = {Karypis, George and Kumar, Vipin},
  title        = {Multilevelk-way Partitioning Scheme for Irregular Graphs},
  journaltitle = {Journal of Parallel and Distributed Computing},
  date         = {1998-01},
  volume       = {48},
  number       = {1},
  pages        = {96--129},
  doi          = {10.1006/jpdc.1997.1404},
  abstract     = {In this paper, we present and study a class of graph partitioning algorithms that reduces the size of the graph by collapsing vertices and edges, we find a $k$-way partitioning of the smaller graph, and then we uncoarsen and refine it to construct a $k$-way partitioning for the original graph. These algorithms compute a $k$-way partitioning of a graph $G= (V,E)$ in $O(|E|)$ time, which is faster by a factor of $O(log k)$ than previously proposed multilevel recursive bisection algorithms. A key contribution of our work is in finding a high-quality and computationally inexpensive refinement algorithm that can improve upon an initialk-way partitioning. We also study the effectiveness of the overall scheme for a variety of coarsening schemes. We present experimental results on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. Our experiments show that this new scheme produces partitions that are of comparable or better quality than those produced by the multilevel bisection algorithm and requires substantially smaller time. Graphs containing up to 450,000 vertices and 3,300,000 edges can be partitioned in 256 domains in less than 40 s on a workstation such as SGI's Challenge. Compared with the widely used multilevel spectral bisection algorithm, our new algorithm is usually two orders of magnitude faster and produces partitions with substantially smaller edge-cut.},
  publisher    = {Elsevier {BV}},
}

@Article{Buluc2013,
  author      = {Buluç, Aydın and Meyerhenke, Henning and Safro, Ilya and Sanders, Peter and Schulz, Christian},
  title       = {Recent Advances in Graph Partitioning},
  date        = {2013-11-13},
  eprint      = {1311.3144},
  eprinttype  = {arXiv},
  eprintclass = {cs.DS},
  abstract    = {We survey recent trends in practical algorithms for balanced graph partitioning together with applications and future research directions.},
  file        = {:http\://arxiv.org/pdf/1311.3144v3:PDF},
  keywords    = {cs.DS, cs.DC, math.CO},
}

@InProceedings{Herrmann2017,
  author    = {Herrmann, J. and Kho, J. and Uçar, B. and Kaya, K. and Çatalyürek, {\"U}. V.},
  title     = {Acyclic Partitioning of Large Directed Acyclic Graphs},
  booktitle = {2017 IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
  date      = {2017-05},
  series    = {CCGRID'17},
  pages     = {371--380},
  doi       = {10.1109/CCGRID.2017.101},
  abstract  = {Finding a good partition of a computational directed acyclic graph associated with an algorithm can help find an execution pattern improving data locality, conduct an analysis of data movement, and expose parallel steps. The partition is required to be acyclic, i.e., the inter-part edges between the vertices from different parts should preserve an acyclic dependency structure among the parts. In this work, we adopt the multilevel approach with coarsening, initial partitioning, and refinement phases for acyclic partitioning of directed acyclic graphs and develop a direct k-way partitioning scheme. To the best of our knowledge, no such scheme exists in the literature. To ensure the acyclicity of the partition at all times, we propose novel and efficient coarsening and refinement heuristics. The quality of the computed acyclic partitions is assessed by computing the edge cut, the total volume of communication between the parts, and the critical path latencies. We use the solution returned by well-known undirected graph partitioners as a baseline to evaluate our acyclic partitioner, knowing that the space of solution is more restricted in our problem. The experiments are run on large graphs arising from linear algebra applications.},
}

@Article{Herrmann2019,
  author       = {Herrmann, J. and Özkaya, M. and Uçar, B. and Kaya, K. and Çatalyürek, {\"U}.},
  title        = {Multilevel Algorithms for Acyclic Partitioning of Directed Acyclic Graphs},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2019},
  volume       = {41},
  number       = {4},
  pages        = {A2117--A2145},
  doi          = {10.1137/18M1176865},
  abstract     = {We investigate the problem of partitioning the vertices of a directed acyclic graph into a given number of parts. The objective function is to minimize the number or the total weight of the edges having end points in different parts, which is also known as the edge cut. The standard load balancing constraint of having an equitable partition of the vertices among the parts should be met. Furthermore, the partition is required to be acyclic; i.e., the interpart edges between the vertices from different parts should preserve an acyclic dependency structure among the parts. In this work, we adopt the multilevel approach with coarsening, initial partitioning, and refinement phases for acyclic partitioning of directed acyclic graphs. We focus on two-way partitioning (sometimes called bisection), as this scheme can be used in a recursive way for multiway partitioning. To ensure the acyclicity of the partition at all times, we propose novel and efficient coarsening and refinement heuristics. The quality of the computed acyclic partitions is assessed by computing the edge cut. We also propose effective ways to use the standard undirected graph partitioning methods in our multilevel scheme. We perform a large set of experiments on a dataset consisting of (i) graphs coming from an application and (ii) some others corresponding to matrices from a public collection. We report significant improvements compared to the current state of the art.},
}

@Article{Nagasaka2019,
  author       = {Nagasaka, Yusuke and Matsuoka, Satoshi and Azad, Ariful and Bulu{\c{c}}, Ayd{\i}n},
  title        = {Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors},
  journaltitle = {Parallel Computing},
  date         = {2019-08},
  pages        = {102545},
  doi          = {10.1016/j.parco.2019.102545},
  abstract     = {Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional numerical applications to recent big data analysis and machine learning. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi- and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with memory management and thread scheduling on Intel Xeon Phi (Knights Landing or KNL). Specifically targeting many-core processors, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real graph algorithms, such as multi-source breadth-first search or triangle counting. Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases while different algorithms dominate the other scenarios with different matrix size, sparsity, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. We build the performance model for hash-table and heap-based algorithms, which supports the recipe. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix. Finally, we integrate our implementations into a large-scale protein clustering code named HipMCL, accelerating its SpGEMM kernel by up to 10X and achieving an overall performance boost for the whole HipMCL application by 2.6$\times$.},
  publisher    = {Elsevier {BV}},
}

@Article{Jia2018,
  author      = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
  title       = {Dissecting the {NVIDIA} {Volta} {GPU} Architecture via Microbenchmarking},
  date        = {2018-04-18},
  eprint      = {arXiv:1804.06826v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
  file        = {:http\://arxiv.org/pdf/1804.06826v1:PDF},
  keywords    = {cs.DC, cs.PF},
}

@Article{Jia2019,
  author      = {Jia, Zhe and Maggioni, Marco and Smith, Jeffrey and Scarpazza, Daniele Paolo},
  title       = {Dissecting the {NVidia} {Turing} {T4} {GPU} via Microbenchmarking},
  date        = {2019-03-18},
  eprint      = {arXiv:1903.07486v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {In 2019, the rapid rate at which GPU manufacturers refresh their designs, coupled with their reluctance to disclose microarchitectural details, is still a hurdle for those software designers who want to extract the highest possible performance. Last year, these very reasons motivated us to dissect the Volta GPU architecture using microbenchmarks. The introduction in August 2018 of Turing, NVidia's latest architecture, pressed us to update our study. In this report, we examine Turing and compare it quantitatively against previous NVidia GPU generations. Specifically, we study the T4 GPU: a low-power board aiming at inference applications. We describe its improvements against its inference-oriented predecessor: the P4 GPU based on the Pascal architecture. Both T4 and P4 GPUs achieve significantly higher frequency-per-Watt figures than their full-size counterparts. We study the performance of the T4's TensorCores, finding a much higher throughput on low-precision operands than on the P4 GPU. We reveal that Turing introduces new instructions that express matrix math more succinctly. We map Turing's instruction space, finding the same encoding as Volta, and additional instructions. We reveal that the Turing TU104 chip has the same memory hierarchy depth as the Volta GV100; cache levels sizes on the TU104 are frequently twice as large as those found on the Pascal GP104. We benchmark each constituent of the T4 memory hierarchy and find substantial overall performance improvements over its P4 predecessor. We studied how clock throttling affects compute-intensive workloads that hit power or thermal limits. Many of our findings are novel, published here for the first time. All of them can guide high-performance software developers get closer to the GPU's peak performance.},
  file        = {:http\://arxiv.org/pdf/1903.07486v1:PDF},
  keywords    = {cs.DC},
}

@Article{Charara2019,
  author       = {Charara, Ali and Keyes, David and Ltaief, Hatem},
  title        = {Batched Triangular Dense Linear Algebra Kernels for Very Small Matrix Sizes on {GPUs}},
  journaltitle = {ACM Transactions on Mathematcal Software},
  date         = {2019-05},
  volume       = {45},
  number       = {2},
  pages        = {15:1--15:28},
  issn         = {0098-3500},
  doi          = {10.1145/3267101},
  abstract     = {Batched dense linear algebra kernels are becoming ubiquitous in scientific applications, ranging from tensor contractions in deep learning to data compression in hierarchical low-rank matrix approximation. Within a single API call, these kernels are capable of simultaneously launching up to thousands of similar matrix computations, removing the expensive overhead of multiple API calls while increasing the occupancy of the underlying hardware. A challenge is that for the existing hardware landscape (x86, GPUs, etc.), only a subset of the required batched operations is implemented by the vendors, with limited support for very small problem sizes. We describe the design and performance of a new class of batched triangular dense linear algebra kernels on very small data sizes (up to 256) using single and multiple GPUs. By deploying recursive formulations, stressing the register usage, maintaining data locality, reducing threads synchronization, and fusing successive kernel calls, the new batched kernels outperform existing state-of-the-art implementations.},
  acmid        = {3267101},
  articleno    = {15},
  issue_date   = {June 2019},
  keywords     = {KBLAS, batched BLAS kernels, dense linear algebra, hardware accelerators, recursive formulation},
  location     = {New York, NY, USA},
  numpages     = {28},
  publisher    = {ACM},
}

@Article{Blanchard2019a,
  author     = {Blanchard, Pierre and Higham, Nicholas J. and Lopez, Florent and Mary, Theo and Pranesh, Srikara},
  title      = {Mixed Precision Block Fused Multiply-Add: Error Analysis and Application to {GPU} Tensor Cores},
  date       = {2019},
  eprint     = {MIMS EPrint:2019.18},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2727/1/paper.pdf},
  abstract   = {Block low-rank (BLR) matrices possess a blockwise low-rank property that can be exploited to reduce the complexity of numerical linear algebra algorithms. The impact of these lowrank approximations on the numerical stability of the algorithms in floating-point arithmetic has not previously been analyzed. We present rounding error analysis for the solution of a linear system by LU factorization of BLR matrices. Assuming that a stable pivoting scheme is used, we prove backward stability: the relative backward error is bounded by a modest constant times $\epsilon$, where the low-rank threshold $\epsilon$ is the parameter controlling the accuracy of the blockwise low-rank approximations. In addition to this key result, our analysis offers three new insights into the numerical behavior of BLR algorithms. First, we compare the use of a global or local low-rank threshold and find that a global one should be preferred. Second, we show that performing intermediate recompressions during the factorization can significantly reduce its cost without compromising numerical stability. Third, we consider different BLR factorization variants and determine the compress--factor--update (CFU) variant to be the best. Tests on a wide range of matrices from various real-life applications show that the predictions from the analysis are realized in practice.},
}

@Article{Blanchard2019,
  author      = {Blanchard, Pierre and Higham, Desmond J. and Higham, Nicholas J.},
  title       = {Accurate Computation of the Log-Sum-Exp and Softmax Functions},
  date        = {2019-09-08},
  eprint      = {1909.03469},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones and that the shifted softmax formula is typically more accurate than a division-free variant.},
  file        = {:http\://arxiv.org/pdf/1909.03469v1:PDF},
  keywords    = {math.NA, cs.NA, 97N20, G.1.3; I.2.8; G.3; G.4},
}

@InBook{Nowak2019,
  author    = {Nowak, Ivo and Muts, Pavlo and Hendrix, Eligius M. T.},
  title     = {Multi-Tree Decomposition Methods for Large-Scale Mixed Integer Nonlinear Optimization},
  booktitle = {Large Scale Optimization in Supply Chains and Smart Manufacturing: Theory and Applications},
  date      = {2019},
  editor    = {Velásquez-Bermúdez, J. and Khakifirooz, M. and Fathi, M.},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-030-22788-3},
  pages     = {27--58},
  doi       = {10.1007/978-3-030-22788-3_2},
  abstract  = {Most industrial optimization problems are sparse and can be formulated as block-separable mixed-integer nonlinear programmingMixed integer nonlinear programming(MINLP) problems, defined by linking low-dimensional sub-problems by (linear) coupling constraints. Decomposition methods solve a block-separable MINLP by alternately solving master problems and sub-problems. In practice, decomposition methods are sometimes the only possibility to compute high-quality solutions of large-scale optimization problems. However, efficient implementations may require expert knowledge and problem-specific features. Recently, there is renewed interest in making these methods accessible to general users by developing generic decomposition frameworks and modelling support. The focus of this chapter is on so-called multi-tree decomposition methods, which iteratively approximate the feasible area without using a single (global) branch-and-bound tree, i.e. branch-and-bound is only used for solving sub-problems. After an introduction, we describe first outer approximation (OA) decomposition methods, Outer approximationincluding the adaptive, multivariate partitioning (AMP)Adaptive, Multivariate Partitioning (AMP) algorithmand the novel decomposition-based outer approximation (DECOA) algorithmDecomposition-based outer approximation (DECOA). This is followed by a description of multi-tree methods using a reduced master problem for solving large-scale industrial optimization problems. The first method to be described applies parallel column generationColumn generation(CG) and iterative fixing for solving nonconvex transport optimization problems with several hundred millions of variables and constraints. The second method is based on a novel approach combining CG and compact outer approximation. The last methodology to be discussed is the general Benders decomposition methodBenders decomposition methodfor globally solving large nonconvex stochastic programs using a reduced mixed-integer programming (MIP) master problem.},
}

@TechReport{Shaiek2019,
  author       = {Shaiek, Hejer and Tomov, Stanimire and Ayala, Alan and Haidar, Azzam and Dongarra, Jack},
  title        = {{GPUDirect} {MPI} Communications and Optimizations to Accelerate {FFTs} on Exascale Systems},
  institution  = {ICL},
  date         = {2019-09},
  type         = {Extended Abstract},
  number       = {icl-ut-19-06},
  abstract     = {Fast Fourier transforms (FFTs) are used in applications ranging from molecular dynamics and spectrum estimation to machine learn- ing, fast convolution and correlation, signal modulation, wireless multimedia applications, and others. However, FFTs are memory bound, and therefore, to accelerate them, it is crucial to avoid and optimize the FFTs communications. To this end, we present a 3-D FFT design for distributed graphics processing unit (GPU) systems that: (1) efficiently uses GPUs high bandwidth, (2) reduces global communications algorithmically, when possible, and (3) employs GPUDirect technologies as well as MPI optimizations in the development of high-performance FFTs for large-scale GPU-accelerated systems. We show that these developments and optimizations lead to very good strong scalability and a performance that is close to 90\% of the theoretical peak.},
  journaltitle = {EuroMPI'19 Posters, Zurich, Switzerland},
  keywords     = {CUDA-Aware MPI, ECP, FFT, FFT-ECP, gpu, GPUDirect},
}

@Article{Cheng2019,
  author       = {Cheng, Wanyou and Dai, Yu-Hong},
  title        = {An active set Newton-CG method for {$\ell_1$} optimization},
  journaltitle = {Applied and Computational Harmonic Analysis},
  date         = {2019},
  issn         = {1063-5203},
  doi          = {10.1016/j.acha.2019.08.005},
  url          = {http://www.sciencedirect.com/science/article/pii/S1063520318300459},
  abstract     = {In this paper, we investigate the active set identification technique of ISTA and provide some good properties. An active set Newton-CG method is then proposed for $\ell_1$ optimization. Under appropriate conditions, we show that the proposed method is globally convergent with some nonmonotone line search. The numerical comparisons with several state-of-art methods demonstrate the efficiency of the proposed method.},
  keywords     = {optimization, Shrinkage operator, Barzilai-Borwein method, Newton-CG method},
}

@PhdThesis{Ramesh2019,
  author      = {Ramesh, Chinthala},
  title       = {Hardware-Software Co-Design Accelerators for Sparse BLAS},
  institution = {Centre for Nano Science and Engineering (CeNSE), Indian Institute of Science, Bangalore},
  date        = {2019},
  eprint      = {http://etd.iisc.ac.in/handle/2005/4276},
  abstract    = {Sparse Basic Linear Algebra Subroutines (Sparse BLAS) is an important library. Sparse BLAS includes three levels of subroutines. Level 1, Level2 and Level 3 Sparse BLAS routines. Level 1 Sparse BLAS routines do computations over sparse vector and spare/dense vector. Level 2 deals with sparse matrix and vector operations. Level 3 deals with sparse matrix and dense matrix operations. The computations of these Sparse BLAS routines on General Purpose Processors (GPPs) not only suffer from less utilization of hardware resources but also takes more compute time than the workload due to poor data locality of sparse vector/matrix storage formats. In the literature, tremendous efforts have been put into software to improve these Sparse BLAS routines performance on GPPs. GPPs best suit for applications with high data locality, whereas Sparse BLAS routines operate on applications with less data locality hence, GPPs performance is poor. Various Custom Function Units (Hardware Accelerators) are proposed in the literature and are proved to be efficient than soft wares which tried to accelerate Sparse BLAS subroutines. Though existing hardware accelerators improved the Sparse BLAS performance compared to software Sparse BLAS routines, there is still lot of scope to improve these accelerators. This thesis describes both the existing software and hardware software co-designs (HW/SW co-design) and identifies the limitations of these existing solutions. We propose a new sparse data representation called Sawtooth Compressed Row Storage (SCRS) and corresponding SpMV and SpMM algorithms. SCRS based SpMV and SpMM are performing better than existing software solutions. Even though SCRS based SpMV and SpMM algorithms perform better than existing solutions, they still could not reach theoretical peak performance. The knowledge gained from the study of limitations of these existing solutions including the proposed SCRS based SpMV and SpMM is used to propose new HW/SW co-designs. Software accelerators are limited by the hardware properties of GPPs, and GPUs itself, hence, we propose HW/SW co-designs to accelerate few basic Sparse BLAS operations (SpVV and SpMV). Our proposed Parallel Sparse BLAS HW/SW co-design achieves near theoretical peak performance with reasonable hardware resources.},
}

@InProceedings{Mniszewski2019,
  author    = {Mniszewski, Susan M.},
  title     = {Graph Partitioning As Quadratic Unconstrained Binary Optimization {(QUBO)} on Spiking Neuromorphic Hardware},
  booktitle = {Proceedings of the International Conference on Neuromorphic Systems},
  date      = {2019},
  series    = {ICONS'19},
  publisher = {ACM},
  location  = {Knoxville, TN, USA},
  isbn      = {978-1-4503-7680-8},
  pages     = {4:1--4:5},
  doi       = {10.1145/3354265.3354269},
  abstract  = {In this work, graph partitioning (GP) is explored using quadratic unconstrained binary optimization (QUBO) on the IBM TrueNorth spiking neuromorphic architecture. GP splits a graph into similar-sized parts while minimizing the number of cut edges between parts. Classical approaches to GP rely on heuristics and approximation algorithms. The GP QUBO formulation was inspired by previous work using the D-Wave quantum annealer. This approach is not limited to graph algorithms, but is applicable to solving a spectrum of NP-hard optimization problems. A classical pseudo simulated annealing metaheuristic is used to solve the QUBO. Implementation on the IBM TrueNorth using a spiking framework is described. Results as converged high-energy solutions are shown to be "good enough" or optimal for partitioning a graph into 2 parts.},
  acmid     = {3354269},
  address   = {New York, NY, USA},
  articleno = {4},
  keywords  = {IBM TrueNorth, QUBO, graph partitioning, quadratic unconstrained binary optimization, spiking neuromorphic hardware},
  numpages  = {5},
}

@Article{Ballard2019,
  author      = {Ballard, Grey and Demmel, James and Dumitriu, Ioana and Rusciano, Alexander},
  title       = {A Generalized Randomized Rank-Revealing Factorization},
  date        = {2019},
  eprint      = {1909.06524},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We introduce a Generalized Randomized QR-decomposition that may be applied to arbitrary products of matrices and their inverses, without needing to explicitly compute the products or inverses. This factorization is a critical part of a communication-optimal spectral divide-and-conquer algorithm for the nonsymmetric eigenvalue problem. In this paper, we establish that this randomized QR-factorization satisfies the strong rank-revealing properties. We also formally prove its stability, making it suitable in applications. Finally, we present numerical experiments which demonstrate that our theoretical bounds capture the empirical behavior of the factorization.},
  file        = {:http\://arxiv.org/pdf/1909.06524v1:PDF},
  keywords    = {math.NA, cs.DS, cs.NA, 15-04},
}

@Article{Boulmier2019,
  author      = {Boulmier, Anthony and Raynaud, Franck and Abdennadher, Nabil and Chopard, Bastien},
  title       = {On the Benefits of Anticipating Load Imbalance for Performance Optimization of Parallel Applications},
  year        = {2019},
  date        = {2019-09-16},
  eprint      = {1909.07168},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {In parallel iterative applications, computational efficiency is essential for addressing large problems. Load imbalance is one of the major performance degradation factors of parallel applications. Therefore, distributing, cleverly, and as evenly as possible, the workload among processing elements (PE) maximizes application performance. So far, the standard load balancing method consists in distributing the workload evenly between PEs and, when load imbalance appears, redistributing the extra load from overloaded PEs to underloaded PEs. However, this does not anticipate the load imbalance growth that may continue during the next iterations. In this paper, we present a first step toward a novel philosophy of load balancing that unloads the PEs that will be overloaded in the near future to let the application rebalance itself via its own dynamics. Herein, we present a formal definition of our new approach using a simple mathematical model and discuss its advantages compared to the standard load balancing method. In addition to the theoretical study, we apply our method to an application that reproduces the computation of a fluid model with non-uniform erosion. The performance validates the benefit of anticipating load imbalance. We observed up to 16\% performance improvement compared to the standard load balancing method.},
  file        = {:http\://arxiv.org/pdf/1909.07168v1:PDF},
  keywords    = {cs.DC},
}

@PhdThesis{Hannah2019,
  author      = {Hannah, Robert Rafaeil},
  date        = {2019},
  institution = {{UCLA}},
  title       = {Fundamental Results on Asynchronous Parallel Optimization Algorithms},
  abstract    = {In this thesis, we present a body of work on the performance and convergence properties of asynchronous-parallel algorithms completed over the course of my doctorate degree (Hannah, Feng, and Wotao Yin 2018; Hannah and Wotao Yin 2017b; T. Sun, Hannah, and Wotao Yin 2017; Hannah and Wotao Yin 2017a). Asynchronous algorithms eliminate the costly synchronization penalty of traditional synchronous-parallel algorithms. They do this by having computing nodes utilize the most recently available information to compute updates. However, it's not immediately clear whether the trade-off of eliminating synchronization penalty at the cost of using outdated information is favorable.\\ We first give a comprehensive theoretical justification of the performance advantages of asynchronous algorithms, which we summarize as "Faster Iterations, Same Quality" (Hannah and Wotao Yin 2017a). Under a well-justified model, we show that asynchronous algorithms complete "Faster Iterations". Using renewal theory, we demonstrate how network delays, heterogeneous sub-problem difficulty and computing power greatly hinder synchronous algorithms, but have no impact on their asynchronous counterparts. We next prove the first exact convergence rate results for a variety of synchronous algorithms including synchronous ARock and synchronous randomized block coordinate descent (sync-RBCD). This allows us to make a fair comparison between these algorithms and their asynchronous counterparts.\\ Finally we show that a variety of asynchronous algorithms have a convergence rate that essentially matches the previously derived exact rates for synchronous counterparts so long as the delays are not too large. Hence asynchronous algorithms complete faster iteration that are of the "Same Quality" as synchronous algorithms. Therefore we conclude that a wide variety of asynchronous algorithms will always outcompete their synchronous counterparts if the delays are not too large, and especially at scale. \\ Next we present the first asynchonous Nesterov-accelerated algorithm that attains a speedup: A2BCD (Hannah, Feng, and Wotao Yin 2018). We first prove that A2BCD attains NU_ACDM’s complexity to highest order. NU_ACDM is a state-of-the-art accelerated coordinate descent algorithm (Allen-Zhu, Qu, et al. 2016). Then we show that both A2BCD and NU_ACDM both have optimal complexity. Hence because A2BCD has faster iterations, and optimal complexity, it should be the fastest coordinate descent algorithm. We verify this with numerical experiments comparing A2BCD with NU_ACDM. We find that A2BCD is up to 4-5$\times$ faster than NU_ACDM, and hence conclude that our algorithm is the current fastest coordinate descent algorithm that exists. Finally we derive a second-order ODE, which is the continuoustime limit of A2BCD. The ODE analysis motivates and clarifies our proof strategy. \\ Lastly, we present earlier foundational work that comprises the basis of the technical innovations that made the previous results possible (Hannah and Wotao Yin 2017b). We show that ARock and its many special cases may converge even under unbounded delays (both stochastic and deterministic). These results sidestep longstanding impossibility results derived in the 1980s by making slightly stronger assumptions. They were also an early demonstration of the power of meticulous Lyapunov-function construction techniques pioneered in this body of work.},
}

@Article{Higham2019,
  author     = {Higham, Nicholas and Mary, Theo},
  title      = {Solving Block Low-Rank Linear Systems by {LU} Factorization is Numerically Stable},
  date       = {2019},
  eprint     = {MIMS EPrint:2019.15},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2733/1/paper.pdf},
  abstract   = {Computing units that carry out a fused multiply-add (FMA) operation with matrix arguments, referred to as tensor units by some vendors, have great potential for use in scientific computing. However, these units are inherently mixed precision and existing rounding error analyses do not support them. We consider a mixed precision block FMA that generalizes both the usual scalar FMA and existing tensor units. We describe how to exploit such a block FMA in the numerical linear algebra kernels of matrix multiplication and LU factorization and give rounding error analyses of both kernels. An important application is to GMRES-based iterative refinement with block FMAs, for which our analysis provides new insight. Our framework is applicable to the tensor core units in the NVIDIA Volta and Turing GPUs. For these we compare matrix multiplication and LU factorization with TC16 and TC32 forms of FMA, which differ in the precision used for the output of the tensor cores. Our experiments on an NVDIA V100 GPU confirm the predictions of the analysis that the TC32 variant is much more accurate than the TC16 one, while achieving almost the same performance.},
}

@Article{Cugu2019,
  author       = {Çuğu, İlke and Manguoğlu, Murat},
  title        = {A parallel multithreaded sparse triangular linear system solver},
  journaltitle = {Computers \& Mathematics with Applications},
  date         = {2019},
  issn         = {0898-1221},
  doi          = {10.1016/j.camwa.2019.09.012},
  url          = {http://www.sciencedirect.com/science/article/pii/S0898122119304602},
  abstract     = {We propose a parallel sparse triangular linear system solver based on the Spike algorithm. Sparse triangular systems are required to be solved in many applications. Often, they are a bottleneck due to their inherently sequential nature. Furthermore, typically many successive systems with the same coefficient matrix and with different right hand side vectors are required to be solved. The proposed solver decouples the problem at the cost of extra arithmetic operations as in the banded case. Compared to the banded case, there are extra savings due to the sparsity of the triangular coefficient matrix. We show the parallel performance of the proposed solver against the state-of-the-art parallel sparse triangular solver in Intel’s Math Kernel Library (MKL) on a multicore architecture. We also show the effect of various sparse matrix reordering schemes. Numerical results show that the proposed solver outperforms MKL’s solver in $\sim$80\% of cases by a factor of 2.47, on average.},
  keywords     = {Sparse triangular linear systems, Direct methods, Parallel computing},
}

@Article{Tasseff2019,
  author      = {Tasseff, Byron and Coffrin, Carleton and Wächter, Andreas and Laird, Carl},
  title       = {Exploring Benefits of Linear Solver Parallelism on Modern Nonlinear Optimization Applications},
  date        = {2019-09-17},
  eprint      = {1909.08104},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {The advent of efficient interior point optimization methods has enabled the tractable solution of large-scale linear and nonlinear programming (NLP) problems. A prominent example of such a method is seen in Ipopt, a widely-used, open-source nonlinear optimization solver. Algorithmically, Ipopt depends on the use of a sparse symmetric indefinite linear system solver, which is heavily employed within the optimization of barrier subproblems. As such, the performance and reliability of Ipopt is dependent on the properties of the selected linear solver. Inspired by a trend in mathematical programming toward solving larger and more challenging NLPs, this work explores two core questions: first, how does the scalability of available linear solvers, many of which exhibit shared-memory parallelism, impact Ipopt performance; and second, does the best linear solver vary across NLP problem classes, including nonlinear network problems and problems constrained by partial differential equations? To better understand these properties, this paper first describes available open- and closed-source, serial and parallel linear solvers and the fundamental differences among them. Second, it introduces the coupling of a new open-source linear solver capable of heterogeneous parallelism over multi-core central processing units and graphics processing units. Third, it compares linear solvers using a variety of mathematical programming problems, including standard test problems for linear and nonlinear optimization, optimal power flow benchmarks, and scalable two- and three-dimensional partial differential equation and optimal control problems. Finally, linear solver recommendations are provided to maximize Ipopt performance across different application domains.},
  file        = {:http\://arxiv.org/pdf/1909.08104v1:PDF},
  keywords    = {math.OC},
}

@InProceedings{LeGorrec2019,
  author    = {le Gorrec, Luce and Mouysset, Sandrine and Duff, Iain S. and Knight, Philip A. and Ruiz, Daniel},
  title     = {Uncovering Hidden Block Structure for Clustering},
  booktitle = {Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery},
  date      = {2019},
  series    = {ECMLPKDD'19},
  abstract  = {We present a multistage procedure to cluster directed and undirected weighted graphs by finding the block structure of their adjacency matrices. A central part of the process is to scale the adjacency matrix into a doubly-stochastic form, which permits detection of the whole matrix block structure with minimal spectral information (theoretically a single pair of singular vectors suffices).We present the different stages of our method, namely the impact of the doubly-stochastic scaling on singular vectors, detection of the block structure by means of these vectors, and details such as cluster refinement and a stopping criterion. Then we test thealgorithm's effectiveness by using it on two unsupervised classification tasks: community detection in networks and shape detection in cloudsof points in two dimensions. By comparing results of our approach with thoseof widely used algorithms designed for specific purposes, we observe that our method is competitive (for community detection) if not superior (for shape detection) in comparison with existing methods.},
}

@Article{Gataric2017,
  author      = {Gataric, Milana and Wang, Tengyao and Samworth, Richard J.},
  title       = {Sparse principal component analysis via axis-aligned random projections},
  date        = {2017-12-15},
  eprint      = {1712.05630},
  eprinttype  = {arXiv},
  eprintclass = {stat.ME},
  abstract    = {We introduce a new method for sparse principal component analysis, based on the aggregation of eigenvector information from carefully-selected axis-aligned random projections of the sample covariance matrix. Unlike most alternative approaches, our algorithm is non-iterative, so is not vulnerable to a bad choice of initialisation. We provide theoretical guarantees under which our principal subspace estimator can attain the minimax optimal rate of convergence in polynomial time. In addition, our theory provides a more refined understanding of the statistical and computational trade-off in the problem of sparse principal component estimation, revealing a subtle interplay between the effective sample size and the number of random projections that are required to achieve the minimax optimal rate. Numerical studies provide further insight into the procedure and confirm its highly competitive finite-sample performance.},
  file        = {:http\://arxiv.org/pdf/1712.05630v4:PDF},
  keywords    = {stat.ME, math.ST, stat.ML, stat.TH, 62H25},
}

@Article{Xu2019a,
  author      = {Xu, Weizhi and Fan, Shengyu and Wang, Tiantian and Zhou, Yufeng},
  title       = {Blocking and sparsity for optimization of convolution calculation algorithm on GPUs},
  date        = {2019-09-22},
  eprint      = {1909.09927},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Convolution neural network (CNN) plays a paramount role in machine learning, which has made significant contributions, such as medical image classification, natural language processing, and recommender system. The success convolution neural network achieved excellent performance with fast execution time. Due to the convolution operation dominate the total operation time of Convolution neural network. In this paper, we propose a novel convolution method of Graphic Processing Units (GPUs), which reduce the convolution operation time and improve the execution speed approximately 2$\times$ than the state of the art convolution algorithm. Our work based on the observation is that the sparsity of the input feature map of convolution operation is relatively large, and the zero value of the feature map is redundancy for convolution result. Therefore, we skip the zero value calculation and improve the speed by compressing the feature map. Besides, the shape of the feature map for the deep network is small, and the number of threads is limited. Therefore, for a limited number of threads, it is necessary to reduce the amount of calculation to increase the calculation speed. Our algorithm has a good effect on the convolution operation of the feature map of the deep network with large sparsity and small size. In this work, our contributions can be summarized as follows: 1) A novel store format for hight-sparsity feature map. 2) A novel convolution algorithm based on block compression and Shared memory is proposed. 3) A feature map data-set for convolution algorithm optimization. 4) We performed a single-layer convolution comparison experiment with CuDNN for different models, and it is best to achieve 3.5$\times$ speedup. We also implemented the algorithm on the VGG-19 model, which can achieve 1.3$\times$$\sim$2.9$\times$ speedup in deep convolution operation, and the entire network can achieve 2.3$\times$ speedup.},
  file        = {:http\://arxiv.org/pdf/1909.09927v1:PDF},
  keywords    = {cs.DC},
}

@Article{Hu2019,
  author       = {Hu, Yuangming and Li, Tzu-Mao and Anderson, Luke and Ragan-Kelley, Jonathan and Durand, Frédo},
  title        = {Taichi: A Language for High-Performance Computation on Spatially Sparse Data Structures},
  journaltitle = {ACM Transactions on Graphics},
  date         = {2019},
  volume       = {38},
  number       = {6},
  abstract     = {3D visual computing data are often spatially sparse. To exploit such sparsity, people have developed hierarchical sparse data structures, such as multilevel sparse voxel grids, particles, and 3D hash tables. However, developing and using these high-performance sparse data structures is challenging, due to their intrinsic complexity and overhead. We propose Taichi, a new data-oriented programming language for efficiently authoring, accessing, and maintaining such data structures. The language offers a high-level, data structure-agnostic interface for writing computation code. The user independently specifies the data structure. We provide several elementary components with different sparsity properties that can be arbitrarily composed to create a wide range of multi-level sparse data structures. This decoupling of data structures from computation makes it easy to experiment with different data structures without changing computation code, and allows users to write computation as if they are working with a dense array. Our compiler then uses the semantics of the data structure and index analysis to automatically optimize for locality, remove redundant operations for coherent accesses, maintain sparsity and memory allocations, and generate efficient parallel and vectorized instructions for CPUs and GPUs. \\ Our approach yields competitive performance on common computational kernels such as stencil applications, neighbor lookups, and particle scattering. We demonstrate our language by implementing simulation, rendering, and vision tasks including a material point method simulation, finite element analysis, a multigrid Poisson solver for pressure projection, volumetric path tracing, and 3D convolution on sparse grids. Our computation-data structure decoupling allows us to quickly experiment with different data arrangements, and to develop high-performance data structures tailored for specific computational tasks. With $\frac{1}{10}$th as many lines of code, we achieve 4.55$\times$ higher performance on average, compared to hand-optimized reference implementations.},
}

@Article{Yasar2019,
  author      = {Yaşar, Abdurrahman and Çatalyürek, {\"U}mit V.},
  title       = {Heuristics for Symmetric Rectilinear Matrix Partitioning},
  date        = {2019-09-26},
  eprint      = {1909.12209},
  eprinttype  = {arXiv},
  eprintclass = {cs.DS},
  abstract    = {Partitioning sparse matrices and graphs is a common and important problem in many scientific and graph analytics applications. In this work, we are concerned with a spatial partitioning called rectilinear partitioning (also known as generalized block distribution) of sparse matrices, which is needed for tiled (or {\em blocked}) execution of sparse matrix and graph analytics kernels. More specifically, in this work, we address the problem of symmetric rectilinear partitioning of square matrices. By symmetric, we mean having the same partition on rows and columns of the matrix, yielding a special tiling where the diagonal tiles (blocks) will be squares. We propose five heuristics to solve two different variants of this problem, and present a thorough experimental evaluation showing the effectiveness of the proposed algorithms.},
  file        = {:http\://arxiv.org/pdf/1909.12209v1:PDF},
  keywords    = {cs.DS},
}

@InProceedings{Mukkara2019,
  author       = {Mukkara, Anurag and Beckmann, Nathan and Sanchez, Daniel},
  title        = {{PHI}: Architectural Support for Synchronization- and Bandwidth-Efficient Commutative Scatter Updates},
  booktitle    = {Proceedings of the 52nd Annual {IEEE}/{ACM} International Symposium on MIcroarchitecture},
  date         = {2019},
  series       = {MICRO-52},
  location     = {Columbus, OH, USA},
  month        = oct,
  pages        = {14},
  abstract     = {Many applications perform frequent scatter update operations to large data structures. For example, in push-style graph algorithms, processing each vertex requires updating the data of all its neighbors. Neighbors are often scattered over the whole graph, so these scatter updates have poor spatial and temporal locality. In current systems, scatter updates suffer high synchronization costs and high memory traffic. These drawbacks make push-style execution unattractive, and, when algorithms allow it, programmers gravitate towards pull-style implementations based on gather reads instead. \\ We present PHI, a push cache hierarchy that makes scatter updates synchronization- and bandwidth-efficient. PHI adds support for pushing sparse, commutative updates from cores towards main memory. PHI adds simple compute logic at each cache level to buffer and coalesce these commutative updates throughout the hierarchy. This avoids synchronization, exploits temporal locality, and produces a load balanced execution. Moreover, PHI exploits spatial locality by selectively deferring updates with poor spatial locality, batching them to achieve sequential main memory transfers. \\ PHI is the first system to leverage both the temporal and spatial locality benefits of commutative scatter updates, some of which do not apply to gather reads. As a result, PHI not only makes push algorithms efficient, but makes them consistently faster than pull ones. We evaluate PHI on graph algorithms and other sparse applications processing large inputs. PHI improves performance by 4.7$\times$ on average (and by up to 11$\times$), and reduces memory traffic by 2$\times$ (and by up to 5$\times$).},
  journaltitle = {The 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
}

@PhdThesis{Nisa2019a,
  author      = {Nisa, Israt J.},
  title       = {Architecture-aware Algorithm Design of Sparse Tensor/Matrix Primitives for {GPU}s},
  institution = {Ohio State University},
  date        = {2019},
  abstract    = {Sparse matrix/tensor operations have been a common computational motif in a wide spectrum of domains - numerical linear algebra, graph analytics, machine learning, health-care, etc. Sparse kernels play a key role in numerous machine learning algorithms and the rising popularity of this domain increases the significance of the primitives like SpMV (Sparse Matrix-Vector Multiplication), SDDMM (Sampled Dense-Dense Matrix Multiplication), MF/TF(Sparse Matrix/Tensor Factorization), etc. These primitives are data-parallel and highly suitable for GPU-like architectures that provide massive parallelism. Real-world matrices and tensors are large-scale and have millions of data points, which is sufficient to utilize all the cores of a GPU. Yet, a data parallel algorithm can become the bottleneck of an application and perform way below than the upper bound of the roofline model. Some common reasons are frequent irregular global memory access, low data reuse, and imbalanced work distribution. However, efficient utilization of GPU memory hierarchy, reduced thread communication, increased data locality , and an even workload distribution can provide ample opportunities for significant performance improvement. The challenge lies in utilizing the techniques across applications and achieve an even performance in spite of the irregularity of the input matrices or tensors. In this work, we systematically identify the performance bottlenecks of the important sparse algorithms and provide optimized and high performing solutions.\\At the beginning of this dissertation, we explore the application of cost-effective ML techniques in solving the format selection and performance modeling problem in the SpMV domain. By identifying a small set of sparse matrix features to use in training the ML models, we are able to select the best storage format, and predict the execution time of an SpMV kernel as well. Next, we optimize the SDDMM kernel, which is a key bottleneck in factor analysis and topic modeling algorithms like ALS, LDA, GaP, ALS, etc. The performance constraints are addressed by exploiting data reuse and increasing parallelism using virtual warping, multi-level tiling and effective use of on-chip memory (shared memory), etc. Rest of the following works are on the optimization of factorization techniques of sparse matrix and tensors on GPUs. For matrix factorization, we optimize the cyclic coordinate descent (CCD++), which is the state-of-the-art factorization method. An efficient GPU implementation is devised by using kernel fusion, tiling and binning. Next, we extend the optimization of the factorization problem to higher order data - tensor. MTTKRP (Matricized Tensor Times Khatri-Rao Products) is a key bottleneck of one of the most common tensor factorization techniques - CPD (CANDECOMP/PARAFAC decomposition). We develop new storage-efficient representations, B-CSF and HB-CSF, for tensors that enables high-performance and load-balanced execution of MTTKRP on GPUs. However, for a tensor with d modes, CPD requires a sequence of d tensor computations. To guarantee efficient memory access with respect to different modes, many storage formats store d distinct representations despite d-fold space overhead. Hence, we devise MM-CSF, a compact mixed-mode representation where better performance is achieved compared to existing solutions while utilizing a small fraction of the space.},
}

@Article{Demmel2019,
  author      = {Demmel, James and Grigori, Laura and Rusciano, Alexander},
  title       = {An improved analysis and unified perspective on deterministic and randomized low rank matrix approximations},
  date        = {2019-10-01},
  eprint      = {1910.00223},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We introduce a Generalized LU-Factorization (GLU) for low-rank matrix approximation. We relate this to past approaches and extensively analyze its approximation properties. The established deterministic guarantees are combined with sketching ensembles satisfying Johnson-Lindenstrauss properties to present complete bounds. Particularly good performance is shown for the sub-sampled randomized Hadamard transform (SRHT) ensemble. Moreover, the factorization is shown to unify and generalize many past algorithms. It also helps to explain the effect of sketching on the growth factor during Gaussian Elimination.},
  file        = {:http\://arxiv.org/pdf/1910.00223v1:PDF},
  keywords    = {math.NA, cs.DS, cs.NA, 15-04},
}

@InProceedings{Davis2019,
  author    = {Davis, Timothy A. and Aznaveh, Mohsen and Kolodziej, Scott},
  title     = {Write Quick, Run Fast: Sparse Deep Neural Network in 20 Minutes of Development Time via SuiteSparse:GraphBLAS},
  booktitle = {Proceedings of the 23rd IEEE Conference on High Performance Extreme Computing},
  date      = {2019},
  series    = {HPEC'19},
  location  = {Waltham, MA, USA},
  abstract  = {SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which provides a powerful and expressive framework for creating graph algorithms based on the elegant mathematics of sparse matrix operations on a semiring. Algorithms written in GraphBLAS achieve high performance with minimal development time. Using GraphBLAS, it took a mere 20 minutes to write a first-cut computational kernel that solves the Sparse Deep Neural Network Graph Challenge. Understanding the problem description and file format, writing code to read in the files that define the problem, and comparing our results with the reference solution took a full day. The kernel consists of a single for-loop around 4 lines of code, all of which are calls to GraphBLAS, and it worked perfectly the first time it was compiled. The sequential performance of the GraphBLAS solution is 3$\times$ to 5$\times$ faster than the MATLAB reference implementation. OpenMP parallelism gives an additional 10$\times$ to 15$\times$ speedup on a 20-core Intel processor, 17$\times$ on an IBM Power8 system, and 20$\times$ on a Power9 system, for the largest problems. Since SuiteSparse:GraphBLAS does not yet employ MPI, this was added at the application level, a development effort that took one week, primarily because of difficulties in resolving a load-balancing issue in the MPI-based parallel algorithm.},
}

@Article{Brock2019,
  author      = {Brock, Benjamin and Chen, Yuxin and Yan, Jiakun and Owens, John and Buluç, Aydın and Yelick, Katherine},
  title       = {{RDMA} vs. {RPC} for Implementing Distributed Data Structures},
  date        = {2019-10-04},
  eprint      = {1910.02158},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Distributed data structures are key to implementing scalable applications for scientific simulations and data analysis. In this paper we look at two implementation styles for distributed data structures: remote direct memory access (RDMA) and remote procedure call (RPC). We focus on operations that require individual accesses to remote portions of a distributed data structure, e.g., accessing a hash table bucket or distributed queue, rather than global operations in which all processors collectively exchange information. We look at the trade-offs between the two styles through microbenchmarks and a performance model that approximates the cost of each. The RDMA operations have direct hardware support in the network and therefore lower latency and overhead, while the RPC operations are more expressive but higher cost and can suffer from lack of attentiveness from the remote side. We also run experiments to compare the real-world performance of RDMA- and RPC-based data structure operations with the predicted performance to evaluate the accuracy of our model, and show that while the model does not always precisely predict running time, it allows us to choose the best implementation in the examples shown. We believe this analysis will assist developers in designing data structures that will perform well on current network architectures, as well as network architects in providing better support for this class of distributed data structures.},
  file        = {:http\://arxiv.org/pdf/1910.02158v1:PDF},
  keywords    = {cs.DC},
}

@TechReport{Tomov2019,
  author      = {Tomov, Stanimire and Haidar, Azzam and Ayala, Alan and Shaiek, Hejer and Dongarra, Jack},
  title       = {{FFT}-{ECP} Implementation Optimizations and Features Phase},
  institution = {Innovative Computing Laboratory, University of Tennessee},
  date        = {2019-09},
  type        = {ECP WBS 2.3.3.09 Milestone Report},
  number      = {FFT-ECP ST-MS-10-1440},
  note        = {revision 09-2019},
  url         = {https://www.icl.utk.edu/files/publications/2019/icl-utk-1263-2019.pdf},
  abstract    = {The goal of this milestone was the imlementation optimizations and features phase of 3-D FFTs in the FFT-ECP project. The target architectures are large-scale distributed GPU-accelerated platforms. \\ In this milestone we describe the implmentation optimizations, features, and performance of the 3-D FFTs that we developed for heterogeneous systems with GPUs. Specifically, this milestone delivered on the following sub-tasks: \begin{itemize} \item Extend FFT-ECP to support various precisions, including real, and investigate the feasibility of mixed precision FFT solvers; \item Develop support for flexible data layouts and enable the new library to handle data conversion/communication on the backend in an optimized and dynamic adaptive fashion based on the communication cost model analyzed in the previous milestone; \item Optimize the distributed 3-D FFT-ECP solver to enable multiple FFTs per MPI process (with accelerators) and multiple GPUs per node. \end{itemize} A main part of this milestone were the performance optimizations and the additions of features that targeted ECP applications need. \\ The artifacts delivered include the performance optimizations and features added to the solvers, and a tuned FFT-ECP software, freely available on the FFT-ECP’s Git repository hosted on Bitbucket, \url{https://bitbucket.org/icl/heffte/}. This is the first software release under the FFT-ECP project. Released is a new FFT library, called heFFTe version 0.1 (Highly Efficient FFTs for Exascale).\\ See also the FFT-ECP website, \url{http://icl.utk.edu/fft/} for more details on the FFT-ECP project.},
}

@Article{Zhang2019a,
  author      = {Zhang, Zecheng and Wu, Xiaoxiao and Zhang, Naijing and Zhang, Siyuan and Solomonik, Edgar},
  title       = {Enabling Distributed-Memory Tensor Completion in Python using New Sparse Tensor Kernels},
  date        = {2019-10-06},
  eprint      = {1910.02371},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Tensor computations are increasingly prevalent numerical techniques in data science.However, innovation and deployment of methods on large sparse tensor datasets are made challenging by the difficulty of efficient implementation thereof.We provide a Python extension to the Cyclops tensor algebra library, which fully automates the management of distributed-memory parallelism and sparsity for NumPy-style operations on multidimensional arrays.We showcase this functionality with novel high-level implementations of three algorithms for the tensor completion problem: alternating least squares (ALS) with an implicit conjugate gradient method, stochastic gradient descent (SGD), and coordinate descent (CCD++).To make possible tensor completion for very sparse tensors, we introduce a new multi-tensor routine that is asymptotically more efficient than pairwise tensor contraction for key components of the tensor completion methods.Further, we add support for hypersparse matrix representations to Cyclops.We provide microbenchmarking results on the Stampede2 supercomputer to demonstrate the efficiency of this functionality.Finally, we study the accuracy and performance of the tensor completion methods for a synthetic tensor with 10 billion nonzeros and the Netflix dataset.},
  file        = {:http\://arxiv.org/pdf/1910.02371v1:PDF},
  keywords    = {cs.DC, cs.MS, cs.NA, math.NA},
}

@Article{Chehade2019,
  author      = {Chehade, Abdallah and Shi, Zunya},
  title       = {The Sparse Reverse of Principal Component Analysis for Fast Low-Rank Matrix Completion},
  date        = {2019-10-04},
  eprint      = {1910.02155},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {Matrix completion constantly receives tremendous attention from many research fields. It is commonly applied for recommender systems such as movie ratings, computer vision such as image reconstruction or completion, multi-task learning such as collaboratively modeling time-series trends of multiple sensors, and many other applications. Matrix completion techniques are usually computationally exhaustive and/or fail to capture the heterogeneity in the data. For example, images usually contain a heterogeneous set of objects, and thus it is a challenging task to reconstruct images with high levels of missing data. In this paper, we propose the sparse reverse of principal component analysis for matrix completion. The proposed approach maintains smoothness across the matrix, produces accurate estimates of the missing data, converges iteratively, and it is computationally tractable with a controllable upper bound on the number of iterations until convergence. The accuracy of the proposed technique is validated on natural images, movie ratings, and multisensor data. It is also compared with common benchmark methods used for matrix completion.},
  file        = {:http\://arxiv.org/pdf/1910.02155v1:PDF},
  keywords    = {cs.LG, eess.SP, stat.ML},
}

@Article{Montoison2019,
  author      = {Montoison, Alexis and Orban, Dominique},
  title       = {{BiLQ}: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property},
  date        = {2019-10-07},
  doi         = {10.13140/RG.2.2.18287.59042},
  eprint      = {1910.02598},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We introduce an iterative method named BiLQ for solving general square linear systems Ax = b based on the Lanczos biorthogonalization process defined by least-norm subproblems, and that is a natural companion to BiCG and QMR. Whereas the BiCG (Fletcher, 1976), CGS (Sonneveld, 1989) and BiCGSTAB (van der Vorst, 1992) iterates may not exist when the tridiagonal projection of A is singular, BiLQ is reliable on compatible systems even if A is ill-conditioned or rank deficient. As in the symmetric case, the BiCG residual is often smaller than the BiLQ residual and, when the BiCG iterate exists, an inexpensive transfer from the BiLQ iterate is possible. Although the Euclidean norm of the BiLQ error is usually not monotonic, it is monotonic in a different norm that depends on the Lanczos vectors. We establish a similar property for the QMR (Freund and Nachtigal, 1991) residual. BiLQ combines with QMR to take advantage of two initial vectors and solve a system and an adjoint system simultaneously at a cost similar to that of applying either method. We derive an analogous combination of USYMLQ and USYMQR based on the orthogonal tridiagonalization process (Saunders, Simon, and Yip, 1988). The resulting combinations, named BiLQR and TriLQR, may be used to estimate integral functionals involving the solution of a primal and an adjoint system. We compare BiLQR and TriLQR with Minres-qlp on a related augmented system, which performs a comparable amount of work and requires comparable storage. In our experiments, BiLQR terminates earlier than TriLQR and MINRES-QLP in terms of residual and error of the primal and adjoint systems.},
  file        = {:http\://arxiv.org/pdf/1910.02598v1:PDF},
  keywords    = {math.NA, cs.DS, cs.NA, 15A06, 65F10, 65F25, 65F50, 93E24 90C06},
}

@Article{Serafino2019,
  author      = {di Serafino, Daniela and Orban, Dominique},
  title       = {Constraint-Preconditioned Krylov Solvers for Regularized Saddle-Point Systems},
  date        = {2019-10-07},
  doi         = {10.5281/zenodo.3473542},
  eprint      = {1910.02552},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We consider the iterative solution of regularized saddle-point systems. When the leading block is symmetric and positive semi-definite on an appropriate subspace, Dollar, Gould, Schilders, and Wathen (2006) describe how to apply the conjugate gradient (CG) method coupled with a constraint preconditioner, a choice that has proved to be effective in optimization applications. We investigate the design of constraint-preconditioned variants of other Krylov methods for regularized systems by focusing on the underlying basis-generation process. We build upon principles laid out by Gould, Orban, and Rees (2014) to provide general guidelines that allow us to specialize any Krylov method to regularized saddle-point systems. In particular, we obtain constraint-preconditioned variants of Lanczos and Arnoldi-based methods, including the Lanczos version of CG, MINRES, SYMMLQ, GMRES(m) and DQGMRES. We also provide MATLAB implementations in hopes that they are useful as a basis for the development of more sophisticated software. Finally, we illustrate the numerical behavior of constraint-preconditioned Krylov solvers using symmetric and nonsymmetric systems arising from constrained optimization.},
  file        = {:http\://arxiv.org/pdf/1910.02552v1:PDF},
  keywords    = {math.NA, cs.NA, 65F08, 65F10, 65F50, 90C20},
}

@InProceedings{Kanellopoulos2019,
  author    = {Kanellopoulos, Konstantinos and Vijaykumar, Nandita and Giannoula, Christina and Azizi, Roknoddin and Koppula, Skanda and Ghiasi, Nika Mansouri and Shahroodi, Taha and Luna, Juan Gomez and Mutlu, Onur},
  title     = {{SMASH}: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  date      = {2019},
  series    = {MICRO '52},
  publisher = {ACM},
  location  = {Columbus, OH, USA},
  isbn      = {978-1-4503-6938-1},
  pages     = {600--614},
  doi       = {10.1145/3352460.3358286},
  abstract  = {Important workloads, such as machine learning and graph analytics applications, heavily involve sparse linear algebra operations. These operations use sparse matrix compression as an effective means to avoid storing zeros and performing unnecessary computation on zero elements. However, compression techniques like Compressed Sparse Row (CSR) that are widely used today introduce significant instruction overhead and expensive pointer-chasing operations to discover the positions of the non-zero elements. In this paper, we identify the discovery of the positions (i.e., indexing) of non-zero elements as a key bottleneck in sparse matrix-based workloads, which greatly reduces the benefits of compression.\\ We propose SMASH, a hardware-software cooperative mechanism that enables highly-efficient indexing and storage of sparse matrices. The key idea of SMASH is to explicitly enable the hardware to recognize and exploit sparsity in data. To this end, we devise a novel software encoding based on a hierarchy of bitmaps. This encoding can be used to efficiently compress any sparse matrix, regardless of the extent and structure of sparsity. At the same time, the bitmap encoding can be directly interpreted by the hardware. We design a lightweight hardware unit, the Bitmap Management Unit (BMU), that buffers and scans the bitmap hierarchy to perform highly-efficient indexing of sparse matrices. SMASH exposes an expressive and rich ISA to communicate with the BMU, which enables its use in accelerating any sparse matrix computation.\\We demonstrate the benefits of SMASH on four use cases that include sparse matrix kernels and graph analytics applications. Our evaluations show that SMASH provides average performance improvements of 38\% for Sparse Matrix Vector Multiplication and 44\% for Sparse Matrix Matrix Multiplication, over a state-of-the-art CSR implementation, on a wide variety of matrices with different characteristics. SMASH incurs a very modest hardware area overhead of up to 0.076\% of an out-of-order CPU core.},
  acmid     = {3358286},
  address   = {New York, NY, USA},
  keywords  = {accelerators, compression, efficiency, graph processing, hardware-software cooperation, linear algebra, memory, sparse matrices, specialized architectures},
  numpages  = {15},
}

@InProceedings{Hegde2019,
  author     = {Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W.},
  title      = {{ExTensor}: An Accelerator for Sparse Tensor Algebra},
  booktitle  = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  date       = {2019},
  series     = {MICRO '52},
  publisher  = {ACM},
  location   = {Columbus, OH, USA},
  isbn       = {978-1-4503-6938-1},
  pages      = {319--333},
  doi        = {10.1145/3352460.3358275},
  acmid      = {3358275},
  address    = {New York, NY, USA},
  annotation = {Generalized tensor algebra is a prime candidate for acceleration via customized ASICs. Modern tensors feature a wide range of data sparsity, with the density of non-zero elements ranging from 10-6\% to 50\%. This paper proposes a novel approach to accelerate tensor kernels based on the principle of hierarchical elimination of computation in the presence of sparsity. This approach relies on rapidly finding intersections---situations where both operands of a multiplication are non-zero---enabling new data fetching mechanisms and avoiding memory latency overheads associated with sparse kernels implemented in software.\\ We propose the ExTensor accelerator, which builds these novel ideas on handling sparsity into hardware to enable better bandwidth utilization and compute throughput. We evaluate ExTensor on several kernels relative to industry libraries (Intel MKL) and state-of-the-art tensor algebra compilers (TACO). When bandwidth normalized, we demonstrate an average speedup of 3.4$\times$, 1.3$\times$, 2.8$\times$, 24.9$\times$, and 2.7$\times$ on SpMSpM, SpMM, TTV, TTM, and SDDMM kernels respectively over a server class CPU.},
  keywords   = {Hardware Acceleration, Sparse Computation, Tensor Algebra},
  numpages   = {15},
}

@InProceedings{Zhu2019,
  author    = {Zhu, Maohua and Zhang, Tao and Gu, Zhenyu and Xie, Yuan},
  title     = {Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  date      = {2019},
  series    = {MICRO '52},
  publisher = {ACM},
  location  = {Columbus, OH, USA},
  isbn      = {978-1-4503-6938-1},
  pages     = {359--371},
  doi       = {10.1145/3352460.3358269},
  abstract  = {Deep neural networks have become the compelling solution for the applications such as image classification, object detection, speech recognition, and machine translation. However, the great success comes at the cost of excessive computation due to the over-provisioned parameter space. To improve the computation efficiency of neural networks, many pruning techniques have been proposed to reduce the amount of multiply-accumulate (MAC) operations, which results in high sparsity in the networks.\\ Unfortunately, the sparse neural networks often run slower than their dense counterparts on modern GPUs due to their poor device utilization rate. In particular, as the sophisticated hardware primitives (e.g., Tensor Core) have been deployed to boost the performance of dense matrix multiplication by an order of magnitude, the performance of sparse neural networks lags behind significantly.\\ In this work, we propose an algorithm and hardware co-design methodology to accelerate the sparse neural networks. A novel pruning algorithm is devised to improve the workload balance and reduce the decoding overhead of the sparse neural networks. Meanwhile, new instructions and micro-architecture optimization are proposed in Tensor Core to adapt to the structurally sparse neural networks. Our experimental results show that the pruning algorithm can achieve 63\% performance gain with model accuracy sustained. Furthermore, the hardware optimization gives an additional 58\% performance gain with negligible area overhead.},
  acmid     = {3358269},
  address   = {New York, NY, USA},
  keywords  = {graphics processing units, neural networks, pruning},
  numpages  = {13},
}

@InProceedings{Gondimalla2019,
  author    = {Gondimalla, Ashish and Chesnut, Noah and Thottethodi, Mithuna and Vijaykumar, T. N.},
  title     = {{SparTen}: A Sparse Tensor Accelerator for Convolutional Neural Networks},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  date      = {2019},
  series    = {MICRO '52},
  publisher = {ACM},
  location  = {Columbus, OH, USA},
  isbn      = {978-1-4503-6938-1},
  pages     = {151--165},
  doi       = {10.1145/3352460.3358291},
  abstract  = {Convolutional neural networks (CNNs) are emerging as powerful tools for image processing. Recent machine learning work has reduced CNNs' compute and data volumes by exploiting the naturally-occurring and actively-transformed zeros in the feature maps and filters. While previous semi-sparse architectures exploit one-sided sparsity either in the feature maps or the filters, but not both, a recent fully-sparse architecture, called Sparse CNN (SCNN), exploits two-sided sparsity to improve performance and energy over dense architectures. However, sparse vector-vector dot product, a key primitive in sparse CNNs, would be inefficient using the representation adopted by SCNN. The dot product requires finding and accessing non-zero elements in matching positions in the two sparse vectors -- an inner join using the position as the key with a single value field. SCNN avoids the inner join by performing a Cartesian product capturing the relevant multiplications. However, SCNN's approach incurs several considerable overheads and is not applicable to non-unit-stride convolutions. Further, exploiting reuse in sparse CNNs fundamentally causes systematic load imbalance not addressed by SCNN. We propose SparTen which achieves efficient inner join by providing support for native two-sided sparse execution and memory storage. To tackle load imbalance, SparTen employs a software scheme, called greedy balancing, which groups filters by density via two variants, a software-only one which uses whole-filter density and a software-hardware hybrid which uses finer-grain density. Our simulations show that, on average, SparTen performs 4.7$\times$, 1.8$\times$, and 3$\times$ better than a dense architecture, one-sided sparse architecture, and SCNN, respectively. An FPGA implementation shows that SparTen performs 4.3$\times$ and 1.9$\times$ better than a dense architecture and a one-sided sparse architecture, respectively.},
  acmid     = {3358291},
  address   = {New York, NY, USA},
  keywords  = {Accelerators, Convolutional neural networks, Sparse tensors},
  numpages  = {15},
}

@Article{Tang2019,
  author      = {Tang, Yu-Hang and Selvitopi, Oguz and Popovici, Doru and Buluç, Aydın},
  title       = {A High-Throughput Solver for Marginalized Graph Kernels on {GPU}},
  date        = {2019-10-14},
  eprint      = {1910.06310},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {We present the design and optimization of a solver for efficient and high-throughput computation of the marginalized graph kernel on General Purpose GPUs. The graph kernel is computed using the conjugate gradient method to solve a generalized Laplacian of the tensor product between a pair of graphs. To cope with the large gap between the instruction throughput and the memory bandwidth of the GPUs, our solver forms the graph tensor product on-the-fly without storing it in memory. This is achieved by using threads in a warp cooperatively to stream the adjacency and edge label matrices of individual graphs by small square matrix blocks called tiles, which are then staged in registers and the shared memory for later reuse. Warps across a thread block can further share tiles via the shared memory to increase data reuse. We exploit the sparsity of the graphs hierarchically by storing only non-empty tiles using a coordinate format and nonzero elements within each tile using bitmaps. We propose a new partition-based reordering algorithm for aggregating nonzero elements of the graphs into fewer but denser tiles to further exploit sparsity. We carry out extensive theoretical analyses on the graph tensor product primitives for tiles of various density and evaluate their performance on synthetic and real-world datasets. Our solver delivers three to four orders of magnitude speedup over existing CPU-based solvers such as GraKeL and GraphKernels. The capability of the solver enables kernel-based learning tasks at unprecedented scales.},
  file        = {:http\://arxiv.org/pdf/1910.06310v2:PDF},
  keywords    = {cs.DC, cs.LG, cs.PF},
}

@Article{Sivkov2019,
  author      = {Sivkov, Ilia and Lazzaro, Alfio and Hutter, Juerg},
  title       = {{DBCSR}: A Library for Dense Matrix Multiplications on Distributed GPU-Accelerated Systems},
  date        = {2019-10-10},
  eprint      = {1910.04796},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Most, if not all the modern scientific simulation packages utilize matrix algebra operations. Among the operation of the linear algebra, one of the most important kernels is the multiplication of matrices, dense and sparse. Examples of application of such a kernel are in electronic structure calculations, machine learning, data mining, graph processing, and digital signal processing. Several optimized libraries exist that can achieve high-performance on distributed systems. Only a few of them target distributed GPU-accelerated systems. In most of the cases, these libraries are provided and optimized by system vendors for their specific computer systems. In this paper, we present the DBCSR library (Distributed Block Compressed Sparse Row) for the distributed dense matrix-matrix multiplications. Although the library is specifically designed for block-sparse matrix-matrix multiplications, we optimized it for the dense case on GPU-accelerated systems. We show that the DBCSR outperforms the multiplication of matrices of different sizes and shapes provided by a vendor optimized GPU version of the ScaLAPACK library up to 2.5$\times$ (1.4$\times$ on average).},
  file        = {:http\://arxiv.org/pdf/1910.04796v1:PDF},
  keywords    = {cs.DC, cs.MS},
}

@Article{Zhang2019b,
  author      = {Zhang, Yongzhe and Azad, Ariful and Hu, Zhenjiang},
  title       = {{FastSV}: A Distributed-Memory Connected Component Algorithm with Fast Convergence},
  date        = {2019-10-14},
  eprint      = {1910.05971},
  eprinttype  = {arXiv},
  eprintclass = {cs.DS},
  abstract    = {This paper presents a new distributed-memory algorithm called FastSV for finding connected components in an undirected graph. Our algorithm simplifies the classic Shiloach-Vishkin algorithm and employs several novel and efficient hooking strategies for faster convergence. We map different steps of FastSV to linear algebraic operations and implement them with the help of scalable graph libraries. FastSV uses sparse operations to avoid redundant work and optimized MPI communication to avoid bottlenecks. The resultant algorithm shows high-performance and scalability as it can find the connected components of a hyperlink graph with over 134B edges in 30 seconds using 262K cores on a Cray XC40 supercomputer. FastSV outperforms the state-of-the-art algorithm by an average speedup of 2.21$\times$ (max 4.27$\times$) on a variety of real-world graphs.},
  file        = {:http\://arxiv.org/pdf/1910.05971v1:PDF},
  keywords    = {cs.DS, cs.DC},
}

@InProceedings{Ayala19,
  author    = {Ayala, Alan and Tomov, Stanimire and Luo, Xi and Shaiek, Hejer and Haidar, Azzam and Bosilca, George and Dongarra, Jack},
  title     = {Impacts of Multi-{GPU} {MPI} Collective Communications on Large {FFT} Computation},
  booktitle = {Proceedings of the Workshop on Exascale MPI (ExaMPI)},
  date      = {2019},
  series    = {SC'19},
  location  = {Denver, CO},
  abstract  = {Most applications targeting exascale, such as those part of the Exascale Computing Project (ECP), are designed for heterogeneous architectures and rely on the Message Passing Interface (MPI) as their underlying parallel programming model. In this paper we analyze the limitations of collective MPI communication for the computation of fast Fourier transforms (FFTs), which are relied on heavily for large-scale particle simulations. We present experiments made at one of the largest heterogeneous platforms, the Summit supercomputer at ORNL. We discuss communication models from state-of-the-art FFT libraries, and propose a new FFT library, named HEFFTE (Highly Efficient FFTs for Exascale), which supports heterogeneous architectures and yields considerable speedups compared with CPU libraries, while maintaining good weak as well as strong scalability.},
  keywords  = {Collective MPI, Exascale applications, FFT, Heterogeneous systems, scalable},
}

@Article{Anzt2019c,
  author       = {Anzt, Hartwig and Dongarra, Jack and Flegar, Goran and Higham, Nicholas J. and Quintana-Orti, Enrique S.},
  title        = {Adaptive precision in block-Jacobi preconditioning for iterative sparse linear system solvers},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2019},
  volume       = {31},
  number       = {6},
  pages        = {e4460},
  doi          = {10.1002/cpe.4460},
  abstract     = {We propose an adaptive scheme to reduce communication overhead caused by data movement by selectively storing the diagonal blocks of a block-Jacobi preconditioner in different precision formats (half, single, or double). This specialized preconditioner can then be combined with any Krylov subspace method for the solution of sparse linear systems to perform all arithmetic in double precision. We assess the effects of the adaptive precision preconditioner on the iteration count and data transfer cost of a preconditioned conjugate gradient solver. A preconditioned conjugate gradient method is, in general, a memory bandwidth-bound algorithm, and therefore its execution time and energy consumption are largely dominated by the costs of accessing the problem{\textquoteright}s data in memory. Given this observation, we propose a model that quantifies the time and energy savings of our approach based on the assumption that these two costs depend linearly on the bit length of a floating point number. Furthermore, we use a number of test problems from the SuiteSparse matrix collection to estimate the potential benefits of the adaptive block-Jacobi preconditioning scheme.},
  keywords     = {adaptive precision, block-Jacobi preconditioning, communication reduction, energy efficiency, Krylov subspace methods, sparse linear systems},
}

@InProceedings{Kiran2019,
  author    = {Kiran, Utpal and Sanfui, Subhajit and Ratnakar, Shashi Kant and Gautam, Sachin Singh and Sharma, Deepak},
  title     = {Comparative Analysis of {GPU}-Based Solver Libraries for a Sparse Linear System of Equations},
  booktitle = {Advances in Computational Methods in Manufacturing},
  date      = {2019},
  editor    = {Narayanan, R. Ganesh and Joshi, Shrikrishna N. and Dixit, Uday Shanker},
  publisher = {Springer Singapore},
  location  = {Singapore},
  isbn      = {978-981-32-9072-3},
  pages     = {889--897},
  abstract  = {In this paper, a comparison of GPU-based linear solverLinear solver libraries for the solution of sparse positive-definite matrices is presented. These large sparse matrices arise in a number of computational disciplines seeking a solution for partial differential equations. The solution of these matrices is often a time-consuming process that can be reduced by parallel computingParallel computing. Since the development of GPU for general-purpose computing, a number of numerical solver libraries have evolved that can accelerate the solution procedure. The performance of three solver libraries has been evaluated in this paper for five different test matrices. These test matrices have been taken from different application domains with different sparsity patterns. Results demonstrate a higher speedup from the iterative solver over the direct solver on GPU and also over a multithreaded CPU implementation.},
}

@InProceedings{Sadi2019,
  author    = {Sadi, Fazle and Sweeney, Joe and Low, Tze Meng and Hoe, James C. and Pileggi, Larry and Franchetti, Franz},
  title     = {Efficient {SpMV} Operation for Large and Highly Sparse Matrices Using Scalable Multi-way Merge Parallelization},
  booktitle = {Proceedings of the 52nd Annual {IEEE/ACM} International Symposium on Microarchitecture},
  date      = {2019},
  series    = {MICRO '52},
  publisher = {ACM},
  location  = {Columbus, OH, USA},
  isbn      = {978-1-4503-6938-1},
  pages     = {347--358},
  doi       = {10.1145/3352460.3358330},
  abstract  = {The importance of Sparse Matrix dense Vector multiplication (SpMV) operation in graph analytics and numerous scientific applications has led to development of custom accelerators that are intended to over-come the difficulties of sparse data operations on general purpose architectures. However, efficient SpMV operation on large problem (i.e. working set exceeds on-chip storage) is severely constrained due to strong dependence on limited amount of fast random access memory to scale. Additionally, unstructured matrix with high sparsity pose difficulties as most solutions rely on exploitation of data locality. This work presents an algorithm co-optimized scalable hardware architecture that can efficiently operate on very large (~billion nodes) and/or highly sparse (avg. degree <10) graphs with significantly less on-chip fast memory than existing solutions. A novel parallelization methodology for implementing large and high throughput multi-way merge network is the key enabler of this high performance SpMV accelerator. Additionally, a data compression scheme to reduce off-chip traffic and special computation for nodes with exceptionally large number of edges, commonly found in power-law graphs, are presented. This accelerator is demonstrated with 16-nm fabricated ASIC and Stratix 10 FPGA platforms. Experimental results show more than an order of magnitude improvement over current custom hardware solutions and more than two orders of magnitude improvement over commercial off-the-shelf (COTS) architectures for both performance and energy efficiency.},
  acmid     = {3358330},
  address   = {New York, NY, USA},
  keywords  = {SpMV, custom hardware, merge parallelization, sparse matrices},
  numpages  = {12},
}

@Article{Jiang2019,
  author      = {Jiang, Yuning and Kouzoupis, Dimitris and Yin, Haoyu and Diehl, Moritz and Houska, Boris},
  title       = {Decentralized Optimization over Tree Graphs},
  date        = {2019-10-21},
  eprint      = {1910.09206},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This paper presents a decentralized algorithm for non-convex optimization over tree-structured networks. We assume that each node of this network can solve small-scale optimization problems and communicate approximate value functions with its neighbors based on a novel multi-sweep communication protocol. In contrast to existing parallelizable optimization algorithms for non-convex optimization the nodes of the network are neither synchronized nor assign any central entity. None of the nodes needs to know the whole topology of the network, but all nodes know that the network is tree-structured. We discuss conditions under which locally quadratic convergence rates can be achieved. The method is illustrated by running the decentralized asynchronous multi-sweep protocol on a radial AC power network case study.},
  file        = {:http\://arxiv.org/pdf/1910.09206v1:PDF},
  keywords    = {math.OC},
}

@InProceedings{Kepner2016,
  author    = {Kepner, J. and Aaltonen, P. and Bader, D. and Buluç, A. and Franchetti, F. and Gilbert, J. and Hutchison, D. and Kumar, M. and Lumsdaine, A. and Meyerhenke, H. and McMillan, S. and Yang, C. and Owens, J. D. and Zalewski, M. and Mattson, T. and Moreira, J.},
  title     = {Mathematical foundations of the {GraphBLAS}},
  booktitle = {IEEE High Performance Extreme Computing Conference},
  date      = {2016-09},
  series    = {HPEC '16},
  pages     = {1--9},
  doi       = {10.1109/HPEC.2016.7761646},
  abstract  = {The GraphBLAS standard (GraphBlas.org) is being developed to bring the potential of matrix-based graph algorithms to the broadest possible audience. Mathematically, the GraphBLAS defines a core set of matrix-based graph operations that can be used to implement a wide class of graph algorithms in a wide range of programming environments. This paper provides an introduction to the mathematics of the GraphBLAS. Graphs represent connections between vertices with edges. Matrices can represent a wide range of graphs using adjacency matrices or incidence matrices. Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data. Fortunately, the two are easily connected by matrix multiplication. A key feature of matrix mathematics is that a very small number of matrix operations can be used to manipulate a very wide range of graphs. This composability of a small number of operations is the foundation of the GraphBLAS. A standard such as the GraphBLAS can only be effective if it has low performance overhead. Performance measurements of prototype GraphBLAS implementations indicate that the overhead is low.},
  keywords  = {graph theory;mathematics computing;matrix algebra;programming environments;mathematical foundations;GraphBLAS standard;GraphBlas.org;matrix-based graph algorithms;matrix-based graph operations;programming environments;adjacency matrices;incidence matrices;matrix multiplication;matrix mathematics;Matrices;Sparse matrices;Finite element analysis;Standards;Additives},
}

@Article{Macintosh2019,
  author       = {Macintosh, Hamish J. and Banks, Jasmine E. and Kelson, Neil A.},
  title        = {Implementing and Evaluating an Heterogeneous, Scalable, Tridiagonal Linear System Solver with {OpenCL} to Target {FPGAs}, {GPUs}, and {CPUs}},
  journaltitle = {International Journal of Reconfigurable Computing},
  date         = {2019-10},
  volume       = {2019},
  pages        = {1--13},
  doi          = {10.1155/2019/3679839},
  abstract     = {Solving diagonally dominant tridiagonal linear systems is a common problem in scientific high-performance computing (HPC). Furthermore, it is becoming more commonplace for HPC platforms to utilise a heterogeneous combination of computing devices. Whilst it is desirable to design faster implementations of parallel linear system solvers, power consumption concerns are increasing in priority. This work presents the oclspkt routine. The oclspkt routine is a heterogeneous OpenCL implementation of the truncated SPIKE algorithm that can use FPGAs, GPUs, and CPUs to concurrently accelerate the solving of diagonally dominant tridiagonal linear systems. The routine is designed to solve tridiagonal systems of any size and can dynamically allocate optimised workloads to each accelerator in a heterogeneous environment depending on the accelerator’s compute performance. The truncated SPIKE FPGA solver is developed first for optimising OpenCL device kernel performance, global memory bandwidth, and interleaved host to device memory transactions. The FPGA OpenCL kernel code is then refactored and optimised to best exploit the underlying architecture of the CPU and GPU. An optimised TDMA OpenCL kernel is also developed to act as a serial baseline performance comparison for the parallel truncated SPIKE kernel since no FPGA tridiagonal solver capable of solving large tridiagonal systems was available at the time of development. The individual GPU, CPU, and FPGA solvers of the oclspkt routine are 110\%, 150\%, and 170\% faster, respectively, than comparable device-optimised third-party solvers and applicable baselines. Assessing heterogeneous combinations of compute devices, the GPU + FPGA combination is found to have the best compute performance and the FPGA-only configuration is found to have the best overall estimated energy efficiency.},
  publisher    = {Hindawi Limited},
}

@Article{Nataf2019,
  author      = {Nataf, F.},
  title       = {Adaptive Domain Decomposition method for Saddle Point problem in Matrix Form},
  year        = {2019},
  eprint      = {1911.01858},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {We introduce an adaptive domain decomposition (DD) method for solving saddle point problems defined as a block two by two matrix. The algorithm does not require any knowledge of the constrained space. We assume that all sub matrices are sparse and that the diagonal blocks are the sum of positive semi definite matrices. The latter assumption enables the design of adaptive coarse space for DD methods.},
  file        = {:http\://arxiv.org/pdf/1911.01858v1:PDF},
  keywords    = {cs.DC, cs.NA, math.NA, math.OC},
}

@Article{Anastos2019,
  author      = {Anastos, Michael and Lamaison, Ander and Steiner, Raphael and Szabó, Tibor},
  title       = {Majority Colorings of Sparse Digraphs},
  date        = {2019-11-05},
  eprint      = {1911.01954},
  eprinttype  = {arXiv},
  eprintclass = {math.CO},
  abstract    = {A majority coloring of a directed graph is a vertex-coloring in which every vertex has the same color as at most half of its out-neighbors. Kreutzer, Oum, Seymour, van der Zypen and Wood proved that every digraph has a majority 4-coloring and conjectured that every digraph admits a majority 3-coloring. We verify this conjecture for digraphs with chromatic number at most 6 or dichromatic number at most 3. We obtain analogous results for list coloring: We show that every digraph with list chromatic number at most 6 or list dichromatic number at most 3 is majority 3-choosable. We deduce that digraphs with maximum out-degree at most 4 or maximum degree at most 7 are majority 3-choosable. On the way to these results we investigate digraphs admitting a majority 2-coloring. We show that every digraph without odd directed cycles is majority 2-choosable. We answer an open question posed by Kreutzer et al. negatively, by showing that deciding whether a given digraph is majority 2-colorable is NP-complete. Finally we deal with a fractional relaxation of majority coloring proposed by Kreutzer et al. and show that every digraph has a fractional majority 3.9602-coloring. We show that every digraph with minimum out-degree $\Omega\left((1/\varepsilon)^2\ln(1/\varepsilon)\right)$ has a fractional majority $(2+\varepsilon)$-coloring.},
  file        = {:http\://arxiv.org/pdf/1911.01954v1:PDF},
  keywords    = {math.CO, 05C15, 05C20},
}

@Article{Sivkov2019a,
  author      = {Sivkov, Ilia and Seewald, Patrick and Lazzaro, Alfio and Hutter, Juerg},
  title       = {{DBCSR}: A Blocked Sparse Tensor Algebra Library},
  date        = {2019-10-29},
  eprint      = {1910.13555},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Advanced algorithms for large-scale electronic structure calculations are mostly based on processing multi-dimensional sparse data. Examples are sparse matrix-matrix multiplications in linear-scaling Kohn-Sham calculations or the efficient determination of the exact exchange energy. When going beyond mean field approaches, e.g. for Moller-Plesset perturbation theory, RPA and Coupled-Cluster methods, or the GW methods, it becomes necessary to manipulate higher-order sparse tensors. Very similar problems are also encountered in other domains, like signal processing, data mining, computer vision, and machine learning. With the idea that the most of the tensor operations can be mapped to matrices, we have implemented sparse tensor algebra functionalities in the frames of the sparse matrix linear algebra library DBCSR (Distributed Block Compressed Sparse Row). DBCSR has been specifically designed to efficiently perform blocked-sparse matrix operations, so it becomes natural to extend its functionality to include tensor operations. We describe the newly developed tensor interface and algorithms. In particular, we introduce the tensor contraction based on a fast rectangular sparse matrix multiplication algorithm.},
  file        = {:http\://arxiv.org/pdf/1910.13555v1:PDF},
  keywords    = {cs.DC},
}

@Article{Liu2019a,
  author      = {Liu, Changxi and Yang, Hailong and Liu, Xu and Luan, Zhongzhi and Qian, Depei},
  title       = {Intelligent-Unrolling: Exploiting Regular Patterns in Irregular Applications},
  date        = {2019-10-24},
  eprint      = {1910.13346},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Modern optimizing compilers are able to exploit memory access or computation patterns to generate vectorization codes. However, such patterns in irregular applications are unknown until runtime due to the input dependence. Thus, either compiler's static optimization or profile-guided optimization based on specific inputs cannot predict the patterns for any common input, which leads to suboptimal code generation. To address this challenge, we develop Intelligent-Unroll, a framework to automatically optimize irregular applications with vectorization. Intelligent-Unroll allows the users to depict the computation task using \textit{code seed} with the memory access and computation patterns represented in \textit{feature table} and \textit{information-code tree}, and generates highly efficient codes. Furthermore, Intelligent-Unroll employs several novel optimization techniques to optimize reduction operations and gather/scatter instructions. We evaluate Intelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph applications. Experimental results show that Intelligent-Unroll is able to generate more efficient vectorization codes compared to the state-of-the-art implementations.},
  file        = {:http\://arxiv.org/pdf/1910.13346v1:PDF},
  keywords    = {cs.DC, cs.PF, cs.PL},
}

@Article{Dhulipala2019,
  author      = {Dhulipala, Laxman and McGuffey, Charlie and Kang, Hongbo and Gu, Yan and Blelloch, Guy E. and Gibbons, Phillip B. and Shun, Julian},
  title       = {Semi-Asymmetric Parallel Graph Algorithms for {NVRAMs}},
  date        = {2019-10-27},
  eprint      = {1910.12310},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Emerging non-volatile main memory (NVRAM) technologies provide novel features for large-scale graph analytics, combining byte-addressability, low idle power, and improved memory-density. Systems are likely to have an order of magnitude more NVRAM than traditional memory (DRAM), allowing large graph problems to be solved efficiently at a modest cost on a single machine. However, a significant challenge in achieving high performance is in accounting for the fact that NVRAM writes can be significantly more expensive than NVRAM reads. In this paper, we propose an approach to parallel graph analytics in which the graph is stored as a read-only data structure (in NVRAM), and the amount of mutable memory is kept proportional to the number of vertices. Similar to the popular semi-external and semi-streaming models for graph analytics, the approach assumes that the vertices of the graph fit in a fast read-write memory (DRAM), but the edges do not. In NVRAM systems, our approach eliminates writes to the NVRAM, among other benefits. We present a model, the Parallel Semi-Asymmetric Model (PSAM), to analyze algorithms in the setting, and run experiments on a 48-core NVRAM system to validate the effectiveness of these algorithms. To this end, we study over a dozen graph problems. We develop parallel algorithms for each that are efficient, often work-optimal, in the model. Experimentally, we run all of the algorithms on the largest publicly-available graph and show that our PSAM algorithms outperform the fastest prior algorithms designed for DRAM or NVRAM. We also show that our algorithms running on NVRAM nearly match the fastest prior algorithms running solely in DRAM, by effectively hiding the costs of repeatedly accessing NVRAM versus DRAM.},
  file        = {:http\://arxiv.org/pdf/1910.12310v1:PDF},
  keywords    = {cs.DC, cs.DS},
}

@InProceedings{Mattson2019,
  author    = {Mattson, T. and Davis, T. A. and Kumar, M. and Buluc, A. and McMillan, S. and Moreira, J. and Yang, C.},
  title     = {{LAGraph}: A Community Effort to Collect Graph Algorithms Built on Top of the GraphBLAS},
  booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium Workshops},
  date      = {2019-05},
  series    = {IPDPSW'19},
  pages     = {276--284},
  doi       = {10.1109/IPDPSW.2019.00053},
  abstract  = {In 2013, we released a position paper to launch a community effort to define a common set of building blocks for constructing graph algorithms in the language of linear algebra. This led to the GraphBLAS. We released a specification for the C programming language binding to the GraphBLAS in 2017. Since that release, multiple libraries that conform to the GraphBLAS C specification have been produced. In this position paper, we launch the next phase of this ongoing community effort: a project to assemble a set of high level graph algorithms built on top of the GraphBLAS. While many of these algorithms are well-known with high quality implementations available, they have not been assembled in one place and integrated with the GraphBLAS. We call this project the LAGraph graph algorithms project and with this position paper, we put out a call for collaborators to join us. While the initial goal is to just assemble these algorithms into a single framework, the long term goal is a library of production-worthy code, with the LAGraph library serving as an open source repository of verified graph algorithms that use the GraphBLAS.},
  keywords  = {C language;formal specification;graph theory;mathematics computing;C programming language specification;open source repository;linear algebra;LAGraph graph algorithms project;ongoing community effort;GraphBLAS;Libraries;Sparse matrices;Linear algebra;Matlab;Software algorithms;Hardware;Data structures;GraphBLAS;Graph Algorithms},
}

@InBook{Cook2019,
  author    = {Cook, William},
  title     = {Computing in Combinatorial Optimization},
  booktitle = {Lecture Notes in Computer Science},
  date      = {2019},
  editor    = {Steffen B., Woeginger G.},
  volume    = {10000},
  publisher = {Springer, Cham.},
  isbn      = {978-3-319-91908-9},
  doi       = {10.1007/978-3-319-91908-9_3},
  abstract  = {Research in combinatorial optimization successfully combines diverse ideas drawn from computer science, mathematics, and operations research. We give a tour of this work, focusing on the early development of the subject and the central role played by linear programming. The paper concludes with a short wish list of future research directions.},
}

@Article{Higham2019b,
  author     = {Higham, Nicholas and Pranesh, Srikara},
  title      = {Exploiting Lower Precision Arithmetic in Solving Symmetric Positive Definite Linear Systems and Least Squares Problems},
  date       = {2019},
  eprint     = {MIMS EPrint:2019.20},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2736/1/paper.pdf},
  abstract   = {What is the fastest way to solve a linear system $Ax= b$ in arithmetic of a given precision when $A$ is symmetric positive definite and otherwise unstructured? The usual answer is by Cholesky factorization, assuming that $A$ can be factorized. We develop an algorithm that can be faster, given an arithmetic of precision lower than the working precision as well as (optionally) one of higher precision. The arithmetics might, for example, be of precisions half, single, and double; half and double, possibly with quadruple; or single and double, possibly with quadruple. We compute a Cholesky factorization at the lower precision and use the factors as preconditioners in GMRES-based iterative refinement. To avoid breakdown of the factorization we shift the matrix by a small multiple of its diagonal. We explain why this is preferable to the common approach of shifting by a multiple of the identity matrix, We also incorporate scaling in order to avoid overflow and reduce the chance of underflow when working in IEEE half precision arithmetic. We extend the algorithm to solve a linear least squares problem with a well conditioned coefficient matrix by forming and solving the normal equations. In both algorithms most of the work is done at low precision provided that iterative refinement and the inner iterative solver converge quickly. We explain why replacing GMRES by the conjugate gradient method causes convergence guarantees to be lost, but show that this change has little effect on convergence in practice. Our numerical experiments confirm the potential of the new algorithms to provide faster solutions in environments that support multiple precisions of arithmetic.},
}

@Article{Zhao2019,
  author       = {Zhao, K. and Su, J. and Yu, J. X. and Zhang, H.},
  title        = {{SQL-G}: Efficient Graph Analytics by {SQL}},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date         = {2019},
  doi          = {10.1109/TKDE.2019.2950620},
  abstract     = {Querying graphs and conducting graph analytics become important in data processing since there are many real applications dealing with massive graphs, such as online social networks, Semantic Web, knowledge graphs, etc. Over the years, many distributed graph processing systems have been developed to support graph analytics using various programming models, and many graph querying languages have been proposed. A natural question that arises is how to integrate graph data and traditional non-graph data in a distributed system for users to conduct analytics. There are two issues. One issue is related to expressiveness on how to specify graph analytics as well as data analytics by a querying language. The other issue is related to efficiency on how to process analytics in a distributed system. For the first issue, SQL is a best candidate, since SQL is a well-accepted language for data processing. We concentrate on SQL for graph analytics. Our early work shows that graph analytics can be supported by SQL in a way from "semiring + while" to "relational algebra + while" via the enhanced recursive SQL queries. In this paper, we focus on the second issue on how to process such enhanced recursive SQL queries based on the GAS (Gather-Apply-Scatter) model under which efficient graph processing systems can be developed. To demonstrate the efficiency, we implemented a system by tightly coupling Spark SQL and GraphX on Spark which is one of the most popular in-memory data-flow processing platforms. First, we enhance Spark SQL by adding the capability of supporting the enhanced recursive SQL queries for graph analytics. In this regard, graph analytics can be processed using a distributed SQL engine alone. Second, we further propose new transformation rules to optimize/translate the operations for recursive SQL queries to the operations by GraphX. In this regard, graph analytics by SQL can be processed in a similar way as done by a distributed graph processing system using the APIs provided by the system. We conduct extensive performance studies to test graph analytics using large real graphs. We show that our approach can achieve similar or even higher efficiency, in comparison to the built-in graph algorithms in the existing graph processing systems.},
  keywords     = {Structured Query Language;Sparks;Algebra;Engines;Programming;Distributed databases;Data processing},
}

@InProceedings{Helal2019,
  author    = {Helal, Ahmed E. and Aji, Ashwin M. and Chu, Michael L. and Beckmann, Bradford M. and Feng, Wu-chun},
  title     = {Adaptive Task Aggregation for High-Performance Sparse Solvers on {GPUs}},
  booktitle = {The 28th International Conference on Parallel Architectures and Compilation},
  date      = {2019},
  series    = {PACT'19},
  abstract  = {Sparse solvers are heavily used in computational fluid dynamics (CFD), computer-aided design (CAD), and other important application domains. These solvers remain challenging to execute on massively parallel architectures, due to the sequential dependencies between the fine-grained application tasks. In particular, parallel sparse solvers typically suffer from substantial scheduling and dependency-management overheads relative to the compute operations. We propose adaptive task aggregation (ATA) to efficiently execute such irregular computations on GPU architectures via hierarchical dependency management and lowlatency task scheduling. On a gamut of  representative problems with different data-dependency structures, ATA significantly outperforms existing GPU task-execution approaches, achieving a geometric mean speedup of 2.2$\times$ to 3.7$\times$ across different sparse kernels (with speedups of up to two orders of magnitude).},
}

@Article{Kim2019,
  author       = {Kim, Hyunjun and Hong, Sungin and Park, Jeonghwan and Han, Hwansoo},
  title        = {Static code transformations for thread-dense memory accesses in {GPU} computing},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  date         = {2019-10},
  doi          = {10.1002/cpe.5512},
  abstract     = {Due to the GPU's complex memory system and massive thread-level parallelism, application programmers often have difficulty optimizing GPU programs. An essential approach to memory optimization is to utilize low-latency on-chip memory to avoid high latency of off-chip memory accesses. Shared memory is an on-chip memory, which is explicitly managed by programmers. Shared memory has a read/write latency similar to that of the L1 cache, but poor data management can degrade performance. In this paper, we present a static code transformation that preloads dataset in GPU's shared memory. Our static analysis primarily targets global memory requests with high thread-density for preloading in shared memory. The thread-dense memory access pattern is a pattern in which many threads efficiently manage the address space of shared memory, as well as reuse the same data in a thread block. We limit the usage of shared memory so that thread-level parallelism remains at the same level when selecting datasets for preloading. Finally, our source-to-source compiler allows to preload selected datasets in shared memory by transforming non-optimized GPU kernel code. Our methods achieve 1.26$\times$ and 1.62$\times$ speedups on average (geometric mean), respectively with GTX980 and P100 GPUs.},
  publisher    = {Wiley},
}

@InProceedings{Elafrou2019,
  author    = {Elafrou, Athena and Goumas, Georgios and Koziris, Nectarios},
  title     = {Conflict-free Symmetric Sparse Matrix-vector Multiplication on Multicore Architectures},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  date      = {2019},
  series    = {SC '19},
  publisher = {ACM},
  location  = {Denver, Colorado},
  isbn      = {978-1-4503-6229-0},
  pages     = {48:1--48:15},
  doi       = {10.1145/3295500.3356148},
  abstract  = {Exploiting the numeric symmetry in sparse matrices to reduce their memory footprint is very tempting for optimizing the memory-bound Sparse Matrix-Vector Multiplication (SpMV) kernel. Despite being very beneficial for serial computation, storing the upper or lower triangular part of the matrix introduces race conditions in the updates to the output vector in a parallel execution. Previous work has suggested using local, per-thread vectors to circumvent this problem, introducing a work-inefficient reduction step that limits the scalability of SpMV. In this paper, we address this issue with Conflict-Free Symmetric (CFS) SpMV, an optimization strategy that organizes the parallel computation into phases of conflict-free execution. We identify such phases through graph coloring and propose heuristics to improve the coloring quality for SpMV in terms of load balancing and locality to the input and output vectors. We evaluate our approach on two multicore shared-memory systems and demonstrate improved performance over the state-of-the-art.},
  acmid     = {3356148},
  address   = {New York, NY, USA},
  articleno = {48},
  keywords  = {conjugate gradient, sparse linear algebra, sparse matrix vector multiplication, symmetric sparse matrices},
  numpages  = {15},
}

@InProceedings{Fujiki2019,
  author    = {Fujiki, Daichi and Chatterjee, Niladrish and Lee, Donghyuk and O'Connor, Mike},
  title     = {Near-memory Data Transformation for Efficient Sparse Matrix Multi-vector Multiplication},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  date      = {2019},
  series    = {SC '19},
  publisher = {ACM},
  location  = {Denver, Colorado},
  isbn      = {978-1-4503-6229-0},
  pages     = {55:1--55:17},
  doi       = {10.1145/3295500.3356154},
  abstract  = {Efficient manipulation of sparse matrices is critical to a wide range of HPC applications. Increasingly, GPUs are used to accelerate these sparse matrix operations. We study one common operation, Sparse Matrix Multi-Vector Multiplication (SpMM), and evaluate the impact of the sparsity, distribution of non-zero elements, and tile-traversal strategies on GPU implementations. Using these insights, we determine that operating on these sparse matrices in a Densified Compressed Sparse Row (DCSR) is well-suited to the parallel warp-synchronous execution model of the GPU processing elements.\\ Preprocessing or storing the sparse matrix in the DCSR format, however, often requires significantly more memory storage than conventional Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) formats. Given that SpMM kernels are often bottlenecked on DRAM bandwidth, the increase in DRAM traffic to access the larger DCSR formatted data structure can result in a slowdown for many matrices.\\ We propose a near-memory transform engine to dynamically create DCSR formatted tiles for the GPU processing elements from the CSC formatted matrix in memory. This work enhances a GPU's last-level cache/memory controller unit to act as an efficient translator between the compute-optimized representation of data and its corresponding storage/bandwidth-optimized format to accelerate sparse workloads. Our approach achieves 2.26$\times$ better performance on average compared to the vendor supplied optimized library for sparse matrix operations, cuSPARSE.},
  acmid     = {3356154},
  address   = {New York, NY, USA},
  articleno = {55},
  numpages  = {17},
}

@InProceedings{Zhu2019a,
  author    = {Zhu, G. and Jiang, P. and Agrawal, G.},
  title     = {A Methodology for Characterizing Sparse Datasets and Its Application to SIMD Performance Prediction},
  booktitle = {Proceedings of the 28th International Conference on Parallel Architectures and Compilation Techniques},
  date      = {2019-09},
  series    = {PACT '19},
  pages     = {445--456},
  doi       = {10.1109/PACT.2019.00042},
  abstract  = {Irregular computations are commonly seen in many scientific and engineering domains that use unstructured meshes or sparse matrices. The performance of an irregular application is very dependent upon the dataset. This paper poses the following question: "given an unstructured mesh or a graph, what method(s) can be used to sample it, such that the execution on the resulting sampled dataset can accurately reflect performance characteristics on the full dataset". Our first insight is that developing a universal sampling approach for all sparse matrices is unpractical. According to the non-zero distribution of the sparse matrix, we propose two novel sampling strategies: Stride Average sampling and Random Tile sampling, which are suitable for uniform and skewed sparse matrices respectively. To help categorize a sparse matrix as uniform or skewed, we introduce clustering coefficient as an important feature which can be propagated into the decision tree model. We also adapt Random Node Neighbor sampling approach for efficient estimation of clustering coefficient. We apply our unstructured dataset characterization approach to modeling the performance for SIMD irregular applications, where the sampled dataset obtained is used to predict cache miss rate and SIMD utilization ratio. We also build analytical models to estimate overheads incurred by load imbalance among threads. With knowledge of these factors, we adapt a code skeleton framework SKOPE to capture the workload behaviors and aggregate performance statistics for execution time prediction.},
  keywords  = {Sparse matrices;Parallel processing;Adaptation models;Instruction sets;Load modeling;Predictive models;Analytical models;irregular application;performance prediction;SIMD;sampling},
}

@InProceedings{Nisa2019b,
  author    = {Nisa, Israt and Li, Jiajia and Sukumaran-Rajam, Aravind and Rawat, Prasant Singh and Krishnamoorthy, Sriram and Sadayappan, P.},
  title     = {An Efficient Mixed-mode Representation of Sparse Tensors},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  date      = {2019},
  series    = {SC '19},
  publisher = {ACM},
  location  = {Denver, Colorado},
  isbn      = {978-1-4503-6229-0},
  pages     = {49:1--49:25},
  doi       = {10.1145/3295500.3356216},
  abstract  = {The Compressed Sparse Fiber (CSF) representation for sparse tensors is a generalization of the Compressed Sparse Row (CSR) format for sparse matrices. For a tensor with d modes, typical tensor methods such as CANDECOMP/PARAFAC decomposition (CPD) require a sequence of d tensor computations, where efficient memory access with respect to different modes is required for each of them. The straightforward solution is to use d distinct representations of the tensor, with each one being efficient for one of the d computations. However, a d-fold space overhead is often unacceptable in practice, especially with memory-constrained GPUs. In this paper, we present a mixed-mode tensor representation that partitions the tensor's nonzero elements into disjoint sections, each of which is compressed to create fibers along a different mode. Experimental results demonstrate that better performance can be achieved while utilizing only a small fraction of the space required to keep d distinct CSF representations.},
  acmid     = {3356216},
  address   = {New York, NY, USA},
  articleno = {49},
  keywords  = {CANDECOMP/PARAFAC decomposition, GPU, MTTKRP, sparse tensors},
  numpages  = {25},
}

@InProceedings{Besta2019,
  author    = {Besta, Maciej and Weber, Simon and Gianinazzi, Lukas and Gerstenberger, Robert and Ivanov, Andrey and Oltchik, Yishai and Hoefler, Torsten},
  title     = {Slim Graph: Practical Lossy Graph Compression for Approximate Graph Processing, Storage, and Analytics},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  date      = {2019},
  series    = {SC '19},
  publisher = {ACM},
  location  = {Denver, Colorado},
  isbn      = {978-1-4503-6229-0},
  pages     = {35:1--35:25},
  doi       = {10.1145/3295500.3356182},
  abstract  = {We propose Slim Graph: the first programming model and framework for practical lossy graph compression that facilitates high-performance approximate graph processing, storage, and analytics. Slim Graph enables the developer to express numerous compression schemes using small and programmable compression kernels that can access and modify local parts of input graphs. Such kernels are executed in parallel by the underlying engine, isolating developers from complexities of parallel programming. Our kernels implement novel graph compression schemes that preserve numerous graph properties, for example connected components, minimum spanning trees, or graph spectra. Finally, Slim Graph uses statistical divergences and other metrics to analyze the accuracy of lossy graph compression. We illustrate both theoretically and empirically that Slim Graph accelerates numerous graph algorithms, reduces storage used by graph datasets, and ensures high accuracy of results. Slim Graph may become the common ground for developing, executing, and analyzing emerging lossy graph compression schemes.},
  acmid     = {3356182},
  address   = {New York, NY, USA},
  articleno = {35},
  numpages  = {25},
}

@Article{Han2019,
  author       = {Han, J. and Xiong, K. and Nie, F. and Li, X.},
  title        = {Structured Graph Reconstruction for Scalable Clustering},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date         = {2019},
  doi          = {10.1109/TKDE.2019.2948850},
  abstract     = {Spectral clustering is a quite simple but effective method for solving graph clustering problem. It first embeds the original data into a lower dimensional space with spectral analysis, and then relys on an algorithm to obtain the final cluster labels. Since it involves eigen-decomposition of the graph Laplacian matrix for spectral embedding, spectral clustering suffers from high computational cost as data grow in scale. It is also limited by the performance of post-processing algorithm such as kmeans. To address these two issues, in this paper, we propose a novel approach denoted by Orthogonal and Nonnegative Graph Reconstruction (ONGR) for large scale clustering. The two constraints are served as the structure constraint under which the graph reconstructed by the indicator matrix is structured. The proposed method mainly needs to perform economical singular value decomposition for small size matrix thus it scales linearly with the data size. Moreover, the interpretability of the indicator matrix is offered due to the nonnegative constraint. Therefore, the final cluster labels can be directly obtained without post-processing. Extensive experiments show the effectiveness of the proposed method.},
  keywords     = {Laplace equations;Matrix decomposition;Scalability;Clustering algorithms;Sparse matrices;Linear programming;Clustering methods;scalable clustering;interpretability;structure constraints;graph reconstruction},
}

@Article{Lee2019a,
  author       = {Lee, Dongha and Oh, Jinoh and Yu, Hwanjo},
  title        = {{OCam}: Out-of-core coordinate descent algorithm for matrix completion},
  journaltitle = {Information Sciences},
  date         = {2019},
  issn         = {0020-0255},
  doi          = {10.1016/j.ins.2019.09.077},
  url          = {http://www.sciencedirect.com/science/article/pii/S0020025519309284},
  abstract     = {Recently, there are increasing reports that most datasets can be actually stored in disks of a single off-the-shelf workstation, and utilizing out-of-core methods is much cheaper and even faster than using a distributed system. For these reasons, out-of-core methods have been actively developed for machine learning and graph processing. The goal of this paper is to develop an efficient out-of-core matrix completion method based on coordinate descent approach. Coordinate descent-based matrix completion (CD-MC) has two strong benefits over other approaches: 1) it does not involve heavy computation such as matrix inversion and 2) it does not have step-size hyper-parameters, which reduces the effort for hyper-parameter tuning. Existing solutions for CD-MC have been developed and analyzed for in-memory setting and they do not take disk-I/O into account. Thus, we propose OCam, a novel out-of-core coordinate descent algorithm for matrix completion. Our evaluation results and cost analyses provide sound evidences supporting the following benefits of OCam: (1) Scalability -- OCam is a truly scalable out-of-core method and thus decomposes a matrix larger than the size of memory, (2) Efficiency -- OCam is super fast. OCam is up to 10$\times$ faster than the state-of-the-art out-of-core method, and up to 4.1$\times$ faster than a competing distributed method when using eight machines. The source code of OCam will be available for reproducibility.},
  keywords     = {Matrix completion, Out-of-core method, Coordinate descent},
}

@InProceedings{Dong2019,
  author    = {Dong, X. and Liu, L. and Zhao, P. and Li, G. and Li, J. and Wang, X. and Feng, X.},
  title     = {Acorns: A Framework for Accelerating Deep Neural Networks with Input Sparsity},
  booktitle = {Proceedings of the 28th International Conference on Parallel Architectures and Compilation Techniques},
  date      = {2019-09},
  series    = {PACT '19},
  pages     = {178--191},
  doi       = {10.1109/PACT.2019.00022},
  abstract  = {Deep neural networks have been employed in a broad range of applications, including face detection, natural language processing, and autonomous driving. Yet, the neural networks with the capability to tackle real-world problems are intrinsically expensive in computation, hindering the usage of these models. Sparsity in the input data of neural networks provides an optimizing opportunity. However, harnessing the potential performance improvement on modern CPU faces challenges raised by sparse computations of the neural network, such as cache-unfriendly memory accesses and efficient sparse kernel implementation. In this paper, we propose Acorns, a framework to accelerate deep neural networks with input sparsity. In Acorns, sparse input data is organized into our designed sparse data layout, which allows memory-friendly access for kernels in neural networks and opens the door for many performance-critical optimizations. Upon that, Acorns generates efficient sparse kernels for operators in neural networks from kernel templates, which combine directions that express specific optimizing transformations to be performed, and straightforward code that describes the computation. Comprehensive evaluations demonstrate Acorns can outperform state-of-the-art baselines by significant speedups. On the real-world detection task in autonomous driving, Acorns demonstrates 1.8-22.6$\times$ performance improvement over baselines. Specifically, the generated programs achieve 1.8-2.4$\times$ speedups over Intel MKL-DNN, 3.0-8.8$\times$ speedups over TensorFlow, and 11.1-13.2$\times$ speedups over Intel MKL-Sparse.},
  keywords  = {Neural networks;Kernel;Layout;Convolution;Autonomous vehicles;Optimization;Deep Learning;Sparse;Optimization;Compiler},
}

@InProceedings{Gates2019,
  author    = {Gates, Mark and Kurzak, Jakub and Charara, Ali and YarKhan, Asim and Dongarra, Jack},
  title     = {{SLATE}: Design of a Modern Distributed and Accelerated Linear Algebra Library},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  date      = {2019},
  series    = {SC '19},
  publisher = {ACM},
  location  = {Denver, Colorado},
  isbn      = {978-1-4503-6229-0},
  pages     = {26:1--26:18},
  doi       = {10.1145/3295500.3356223},
  abstract  = {The SLATE (Software for Linear Algebra Targeting Exascale) library is being developed to provide fundamental dense linear algebra capabilities for current and upcoming distributed high-performance systems, both accelerated CPU-GPU based and CPU based. SLATE will provide coverage of existing ScaLAPACK functionality, including the parallel BLAS; linear systems using LU and Cholesky; least squares problems using QR; and eigenvalue and singular value problems. In this respect, it will serve as a replacement for ScaLAPACK, which after two decades of operation, cannot adequately be retrofitted for modern accelerated architectures. SLATE uses modern techniques such as communication-avoiding algorithms, lookahead panels to overlap communication and computation, and task-based scheduling, along with a modern C++ framework. Here we present the design of SLATE and initial reports of several of its components.},
  acmid     = {3356223},
  address   = {New York, NY, USA},
  articleno = {26},
  keywords  = {GPU computing, dense linear algebra, distributed computing},
  numpages  = {18},
}

@Article{Higham2019a,
  author       = {Higham, Catherine F. and Higham, Desmond J.},
  title        = {Deep Learning: An Introduction for Applied Mathematicians},
  journaltitle = {{SIAM} Review},
  date         = {2019-01},
  volume       = {61},
  number       = {3},
  pages        = {860--891},
  doi          = {10.1137/18m1165748},
  abstract     = {Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics, notably from calculus, approximation theory, optimization, and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: What is a deep neural network? How is a network trained? What is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also demonstrate the use of state-of-the-art software on a large scale image classification problem. We finish with references to the current literature.},
  publisher    = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Kouyialis2019,
  author       = {Kouyialis, Georgia and Wang, Xiaoyu and Misener, Ruth},
  title        = {Symmetry Detection for Quadratic Optimization Using Binary Layered Graphs},
  journaltitle = {Processes},
  date         = {2019-11},
  volume       = {7},
  number       = {11},
  pages        = {838},
  doi          = {10.3390/pr7110838},
  abstract     = {Symmetry in mathematical optimization may create multiple, equivalent solutions. In nonconvex optimization, symmetry can negatively affect algorithm performance, e.g., of branch-and-bound when symmetry induces many equivalent branches. This paper develops detection methods for symmetry groups in quadratically-constrained quadratic optimization problems. Representing the optimization problem with adjacency matrices, we use graph theory to transform the adjacency matrices into binary layered graphs. We enter the binary layered graphs into the software package nauty that generates important symmetric properties of the original problem. Symmetry pattern knowledge motivates a discretization pattern that we use to reduce computation time for an approximation of the point packing problem. This paper highlights the importance of detecting and classifying symmetry and shows that knowledge of this symmetry enables quick approximation of a highly symmetric optimization problem},
  publisher    = {{MDPI} {AG}},
}

@Article{Yang2019b,
  author       = {Yang, Tianbao},
  title        = {Advancing Non-Convex and Constrained Learning: Challenges and Opportunities},
  journaltitle = {{AI} Matters},
  date         = {2019},
  volume       = {5},
  issue        = {3},
  pages        = {29--39},
  doi          = {10.1145/3362077.3362085},
}

@Article{Adoni2019,
  author       = {Adoni, Hamilton Wilfried Yves and Nahhal, Tarik and Krichen, Moez and Aghezzaf, Brahim and Elbyed, Abdeltif},
  title        = {A survey of current challenges in partitioning and processing of graph-structured data in parallel and distributed systems},
  journaltitle = {Distributed and Parallel Databases},
  date         = {2019-11},
  doi          = {10.1007/s10619-019-07276-9},
  abstract     = {One of the concepts that attracts attention since entering of big data era is the graph-structured data. Suitable frameworks to handle such data would face several constraints, especially scalability, partitioning challenges, processing complexity and hardware configurations. Unfortunately, although several works deal with big data issues, there is a lack of literature review concerning the challenges related to query answering on large-scale graph data. In this survey paper, we review current problems related to the partitioning and processing of graph-structured data. We discuss existing graph processing systems and provide some insights to know how to choose the right system for parallel and distributed processing of large-scale graph data. Finally, we survey current open challenges in this field.},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Article{Donnat2019,
  author      = {Donnat, Claire and Holmes, Susan},
  title       = {Convex Hierarchical Clustering for Graph-Structured Data},
  date        = {2019-11-08},
  eprint      = {1911.03417},
  eprinttype  = {arXiv},
  eprintclass = {stat.AP},
  abstract    = {Convex clustering is a recent stable alternative to hierarchical clustering. It formulates the recovery of progressively coalescing clusters as a regularized convex problem. While convex clustering was originally designed for handling Euclidean distances between data points, in a growing number of applications, the data is directly characterized by a similarity matrix or weighted graph. In this paper, we extend the robust hierarchical clustering approach to these broader classes of similarities. Having defined an appropriate convex objective, the crux of this adaptation lies in our ability to provide: (a) an efficient recovery of the regularization path and (b) an empirical demonstration of the use of our method. We address the first challenge through a proximal dual algorithm, for which we characterize both the theoretical efficiency as well as the empirical performance on a set of experiments. Finally, we highlight the potential of our method by showing its application to several real-life datasets, thus providing a natural extension to the current scope of applications of convex clustering.},
  file        = {:http\://arxiv.org/pdf/1911.03417v1:PDF},
  keywords    = {stat.AP, cs.LG, stat.ML},
}

@Article{Giles2019,
  author      = {Giles, Michael B. and Jentzen, Arnulf and Welti, Timo},
  title       = {Generalised multilevel Picard approximations},
  date        = {2019-11-08},
  eprint      = {http://arxiv.org/abs/1911.03188v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations (PDEs). In particular, most of the numerical approximation schemes studied in the scientific literature suffer under the curse of dimensionality in the sense that the number of computational operations needed to compute an approximation with an error of size at most $ \varepsilon > 0 $ grows at least exponentially in the PDE dimension $ d \in \mathbb{N} $ or in the reciprocal of $ \varepsilon $. Recently, so-called full-history recursive multilevel Picard (MLP) approximation methods have been introduced to tackle the problem of approximately solving high-dimensional PDEs. MLP approximation methods currently are, to the best of our knowledge, the only methods for parabolic semi-linear PDEs with general time horizons and general initial conditions for which there is a rigorous proof that they are indeed able to beat the curse of dimensionality. The main purpose of this work is to investigate MLP approximation methods in more depth, to reveal more clearly how these methods can overcome the curse of dimensionality, and to propose a generalised class of MLP approximation schemes, which covers previously analysed MLP approximation schemes as special cases. In particular, we develop an abstract framework in which this class of generalised MLP approximations can be formulated and analysed and, thereafter, apply this abstract framework to derive a computational complexity result for suitable MLP approximations for semi-linear heat equations. These resulting MLP approximations for semi-linear heat equations essentially are generalisations of previously introduced MLP approximations for semi-linear heat equations.},
  file        = {:http\://arxiv.org/pdf/1911.03188v1:PDF},
  keywords    = {math.NA, cs.NA, math.AP, math.PR, 65M75, 65C05},
}

@InProceedings{Shi2019a,
  author    = {Shi, Zai and Eryilmaz, Atilla},
  title     = {Cubic Regularized {ADMM} with Convergence to a Local Minimum in Non-convex Optimization},
  booktitle = {Proceedings of the 57th Annual Allerton Conference on Communication, Contrl, and Computing},
  date      = {2019},
  series    = {Allerton},
  abstract  = {How to escape saddle points is a critical issue in nonconvex optimization. Previous methods on this issue mainly assume that the objective function is Hessian-Lipschitz, which leave a gap for applications using non-Hessian-Lipschitz functions. In this paper, we propose Cubic Regularized Alternating Direction Method of Multipliers (CR-ADMM) to escape saddle points of separable non-convex functions containing a non-HessianLipschitz component. By carefully choosing a parameter, we prove that CR-ADMM converges to a local minimum of the original function with a rate of $O(\frac{1}{T} \frac{1}{3})$ in time horizon $T$, which is faster than gradient-based methods. We also show that when one or more steps of CR-ADMM are not solved exactly, CRADMM can converge to a neighborhood of the local minimum. Through the experiments of matrix factorization problems, CRADMM is shown to have a faster rate and a lower optimality gap compared with other gradient-based methods. Our approach can also find applications in other scenarios where regularized non-convex cost minimization is performed, such as parameter optimization of deep neural networks.},
}

@Article{Steck2019,
  author      = {Steck, Daniel and Kanzow, Christian},
  title       = {Regularization of Limited Memory Quasi-Newton Methods for Large-Scale Nonconvex Minimization},
  date        = {2019-11-11},
  eprint      = {1911.04584},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This paper deals with the unconstrained optimization of smooth objective functions. It presents a class of regularized quasi-Newton methods whose globalization turns out to be more efficient than standard line search or trust-region strategies. The focus is therefore on the solution of large-scale problems using limited memory quasi-Newton techniques. Global convergence of the regularization methods is shown under mild assumptions. The details of the regularized limited memory quasi-Newton updates are discussed including their compact representations. Numerical results using all large-scale test problems from the CUTEst collection indicate that the regularization method outperforms the standard line search limited memory BFGS method.},
  file        = {:http\://arxiv.org/pdf/1911.04584v1:PDF},
  keywords    = {math.OC, cs.NA, math.NA},
}

@InProceedings{Agarwal2019,
  author    = {Agarwal, Abhishek and Peng, Jianhao and Milenkovic, Olgica},
  title     = {Online Convex Matrix Factorization with Representative Regions},
  booktitle = {Proceddings of the 33rd Conference on Neural Information Processing Systems},
  date      = {2019},
  series    = {NeurIPS' 19},
  location  = {Vancouver, CA},
  abstract  = {Matrix factorization (MF) is a versatile learning method that has found wide applications in various data-driven disciplines. Still, many MF algorithms do not adequately scale with the size of available datasets and/or lack interpretability. To improve the computational efficiency of the method, an online (streaming) MF algorithm was proposed in [1]. To enable data interpretability, a constrained version of MF, termed convex MF, was introduced in [2]. In the latter work, the basis vectors are required to lie in the convex hull of the data samples, thereby ensuring that every basis can be interpreted as a weighted combination of data samples. No current algorithmic solutions for online convex MF are known as it is challenging to find adequate convex bases without having access to the complete dataset. We address both problems by proposing the first online convex MF algorithm that maintains a collection of constant-size sets of representative data samples needed for interpreting each of the basis [2] and has the same almost sure convergence guarantees as the online learning algorithm of [1]. Our proof techniques combine random coordinate descent algorithms with specialized quasi-martingale convergence analysis. Experiments on synthetic and real world datasets show significant computational savings of the proposed online convex MF method compared to classical convex MF. Since the proposed method maintains small representative sets of data samples needed for convex interpretations, it is related to a body of work in theoretical computer science, pertaining to generating point sets [3], and in computer vision, pertaining to archetypal analysis [4]. Nevertheless, it differs from these lines of work both in terms of the objective and algorithmic implementations.},
}

@Article{Bento2019,
  author       = {Bento, José and Ioannidis, Stratis},
  title        = {A family of tractable graph metrics},
  journaltitle = {Applied Network Science},
  date         = {2019-11-15},
  volume       = {4},
  number       = {1},
  pages        = {107},
  issn         = {2364-8228},
  doi          = {10.1007/s41109-019-0219-z},
  abstract     = {Important data mining problems such as nearest-neighbor search and clustering admit theoretical guarantees when restricted to objects embedded in a metric space. Graphs are ubiquitous, and clustering and classification over graphs arise in diverse areas, including, e.g., image processing and social networks. Unfortunately, popular distance scores used in these applications, that scale over large graphs, are not metrics and thus come with no guarantees. Classic graph distances such as, e.g., the chemical distance and the Chartrand-Kubiki-Shultz distance are arguably natural and intuitive, and are indeed also metrics, but they are intractable: as such, their computation does not scale to large graphs. We define a broad family of graph distances, that includes both the chemical and the Chartrand-Kubiki-Shultz distances, and prove that these are all metrics. Crucially, we show that our family includes metrics that are tractable. Moreover, we extend these distances by incorporating auxiliary node attributes, which is important in practice, while maintaining both the metric property and tractability.},
  day          = {15},
}

@InProceedings{Koutis2019,
  author    = {Koutis, Ioannis and Le, Huong},
  title     = {Spectral Modification of Graphs for Improved Spectral Clustering},
  booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems},
  date      = {2019},
  series    = {NeurIPS' 19},
  location  = {Vancouver, CA},
  abstract  = {Spectral clustering algorithms provide approximate solutions to hard optimization problems that formulate graph partitioning in terms of the graph conductance. It is well understood that the quality of these approximate solutions is negatively affected by a possibly significant gap between the conductance and the second eigenvalue of the graph. In this paper we show that for any graph $G$, there exists a ‘spectral maximizer’ graph $H$ which is cut-similar to $G$, but has eigenvalues that are near the theoretical limit implied by the cut structure of $G$. Applying then spectral clustering on $H$ has the potential to produce improved cuts that also exist in $G$ due to the cut similarity. This leads to the second contribution of this work: we describe a practical spectral modification algorithm that raises the eigenvalues of the input graph, while preserving its cuts. Combined with spectral clustering on the modified graph, this yields demonstrably improved cuts.},
}

@InProceedings{Dumitrasc2019,
  author       = {Dumitrasc, Andrei and Leleux, Philippe and Rüde, Ulrich},
  title        = {Block partitioning of sparse rectangular matrices},
  date         = {2019},
  volume       = {19},
  series       = {PAMM '19},
  number       = {1},
  doi          = {10.1002/pamm.201900287},
  abstract     = {Abstract We present a means of reordering large, sparse rectangular matrices such that their nonzeros are closer to the diagonal. This enables a block-partitioning which is useful in parallel contexts. We use the Reverse Cuthill-McKee (RCM) algorithm on the adjacency matrix of the associated bipartite graph. The resulting, reordered matrix has a block bidiagonal structure.},
  journaltitle = {Proceedings of Applied Mathematics and Mechanics},
}

@InProceedings{Piccinotti2019,
  author    = {Piccinotti, D. and Ramalli, E. and Parravicini, A. and Brondolin, R. and Santambrogio, M.},
  title     = {Solving write conflicts in {GPU}-accelerated graph computation: A {PageRank} case-study},
  booktitle = {Proceedings of the IEEE 5th International forum on Research and Technology for Society and Industry},
  date      = {2019-09},
  series    = {RTSI '19},
  pages     = {144--148},
  doi       = {10.1109/RTSI.2019.8895572},
  abstract  = {Graph ranking algorithms, such as PageRank, are widely used in a number of real-world applications like web search. As the size of the graphs on which these algorithms are applied gets bigger and bigger, it becomes necessary to devise powerful and flexible techniques to accelerate and parallelize the computation, both at software and hardware level. Leveraging GPUs is a promising direction due to their highly parallel computing capabilities, but execution time is often hampered by write conflicts. In this paper, we present a solution to handle write conflicts in GPU computations exploiting high level of parallelism, and show how this technique can effectively be used to accelerate the computation of PageRank by a factor of 5$\times$, with respect to a baseline in which conflicts are not handled. Our solution is implemented at software level, and doesn't require specific hardware resources.},
  issn      = {2687-6809},
  keywords  = {Graph Algorithms;GPU;Atomic Addition;PageRank},
}

@Article{Ribizel2019a,
  author       = {Ribizel, Tobias and Anzt, Hartwig},
  title        = {Parallel Selection on {GPUs}},
  journaltitle = {Parallel Computing},
  date         = {2019},
  pages        = {102588},
  issn         = {0167-8191},
  doi          = {10.1016/j.parco.2019.102588},
  url          = {http://www.sciencedirect.com/science/article/pii/S0167819119301796},
  abstract     = {We present a novel parallel selection algorithm for GPUs capable of handling single rank selection (single selection) and multiple rank selection (multiselection). The algorithm requires no assumptions on the input data distribution, and has a much lower recursion depth compared to many state-of-the-art algorithms. We implement the algorithm for different GPU generations, always leveraging the respectively-available low-level communication features, and assess the performance on server-line hardware. The computational complexity of our SampleSelect algorithm is comparable to specialized algorithms designed for – and exploiting the characteristics of – “pleasant” data distributions. At the same time, as the proposed SampleSelect algorithm does not work on the actual element values but on the element ranks of the elements only, it is robust to the input data and can complete significantly faster for adversarial data distributions. We also address the use case of approximate selection by designing a variant that radically reduces the computational cost while preserving high approximation accuracy.},
  keywords     = {Parallel selection algorithm, GPU, Multiselection, th order statistics, Approximate selection},
}

@InProceedings{Anzt2019d,
  author       = {Anzt, Hartwig and Cojean, Terry and Kühn, Eileen},
  title        = {Towards a New Peer Review Concept for Scientific Computing ensuring Technical Quality, Software Sustainability, and Result Reproducibility},
  date         = {2019},
  volume       = {19},
  series       = {PAMM '19},
  number       = {1},
  doi          = {10.1002/pamm.201900490},
  eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.201900490},
  abstract     = {In this position paper we argue for implementing an alternative peer review process for scientific computing contributions that promotes high quality scientific software developments as fully-recognized conference submission. The idea is based on leveraging the code reviewers' feedback on scientific software contributions to community software developments as a third-party review involvement. Providing open access to this technical review would complement the scientific review of the contribution, efficiently reduce the workload of the undisclosed reviewers, improve the algorithm implementation quality and software sustainability, and ensure full reproducibility of the reported results. Using this process creates incentives to publish scientific algorithms in open source software – instead of designing prototype algorithms with the unique purpose of publishing a paper. In addition, the comments and suggestions of the community being archived in the versioning control systems ensure that also community reviewers are receiving credit for the review contributions – unlike reviewers in the traditional peer review process. Finally, it reflects the particularity of the scientific computing community using conferences rather than journals as the main publication venue.},
  journaltitle = {Proceedings of Applied Mathematics and Mechanics},
}

@InProceedings{Sahasrabudhe2019,
  author    = {Sahasrabudhe, Damodar and Phipps, Eric T. and Rajamanickam, Sivasankaran and Berzins, Martin},
  title     = {A Portable {SIMD} Primitive Using Kokkos for Heterogeneous Architectures},
  booktitle = {Proceedings of the 6th Workshop on Accelerator Programming Using Directives},
  date      = {2019},
  publisher = {ACM},
  location  = {Denver, Colorado},
  pages     = {1--23},
  abstract  = {As computer architectures are rapidly evolving (e.g. those designed for exascale), multiple portability frameworks have been developed to avoid new architecture-specific development and tuning. However, portability frameworks depend on compilers for auto-vectorization and may lack support for explicit vectorization on heterogeneous platforms. Alternatively, programmers can use intrinsics-based primitives to achieve more efficient vectorization, but the lack of a gpu back-end for these primitives makes such code non-portable. A unified, portable, Single Instruction Multiple Data (SIMD) primitive proposed in this work, allows intrinsics-based vectorization on cpus and many-core architectures such as Intel Knights Landing (KNL), and also facilitates Single Instruction Multiple Threads (SIMT) based execution on gpus. This unified primitive, coupled with the Kokkos portability ecosystem, makes it possible to develop explicitly vectorized code, which is portable across heterogeneous platforms. The new SIMD primitive is used on different architectures to test the performance boost against hard-to-auto-vectorize baseline, to measure the overhead against efficiently vectroized baseline, and to evaluate the new feature called the “logical vector length” (LVL). The SIMD primitive provides portability across cpus and gpus without any performance degradation being observed experimentally.},
}

@Article{Han2019a,
  author      = {Han, Ruobing and You, Yang and Demmel, James},
  title       = {Auto-Precision Scaling for Distributed Deep Learning},
  date        = {2019-11-20},
  eprint      = {1911.08907},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {In recent years, large-batch optimization is becoming the key of distributed deep learning. However, large-batch optimization is hard. Straightforwardly porting the code often leads to a significant loss in testing accuracy. As some researchers suggested that large batch optimization leads to a low generalization performance, and they further conjectured that large-batch training needs a higher floating-point precision to achieve a higher generalization performance. To solve this problem, we conduct an open study in this paper. Our target is to find the number of bits that large-batch training needs. To do so, we need a system for customized precision study. However, state-of-the-art systems have some limitations that lower the efficiency of developers and researchers. To solve this problem, we design and implement our own system CPD: A High Performance System for Customized-Precision Distributed DL. In our experiments, our application often loses accuracy if we use a very-low precision (e.g. 8 bits or 4 bits). To solve this problem, we proposed the APS (Auto-Precision-Scaling) algorithm, which is a layer-wise adaptive scheme for gradients shifting. With APS, we are able to make the large-batch training converge with only 4 bits.},
  file        = {:http\://arxiv.org/pdf/1911.08907v1:PDF},
  keywords    = {cs.DC, cs.LG},
}

@Article{Ayall2019,
  author       = {Ayall, T. and Duan, H. and Liu, C.},
  title        = {Edge Property Based Stream Order Reduce the Performance of Stream Edge Graph Partition},
  journaltitle = {Journal of Physics: Conference Series},
  date         = {2019-11},
  volume       = {1395},
  pages        = {1--8},
  doi          = {10.1088/1742-6596/1395/1/012010},
  abstract     = {The graph data exist everywhere in various disciplines such as in social network, biological network, chemical compound, and computer vision, etc. Currently, the size of a graph data have dramatically increased; all disciplines have extracted knowledge from a graph by partitioning and distributing a graph into different clusters node using the distributed graph processing system or graph database, however the graph partition has reduced the performance of those system. Even if the stream edge graph partition has shown better partition quality than vertex graph partition for skew degree distribution of a graph and support big graph partition, the stream order has affected the quality of stream edge graph partition. In this study,we propose two edge properties based stream order models such as TFB (Tree edges First then Backward edges follow), BFT (Backward edges First then Tree edges follow) and study the effect of stream order on stream edge graph partition algorithms. The result shows that TFB and BFT models significantly affect the quality of stream edge partition except hashing and all algorithms show best quality of partition on Random order than other order such as TFB, BFT, BFS (Breadth First Search), and DFS (Depth First Search)},
  publisher    = {{IOP} Publishing},
}

@Article{Zhang2019c,
  author      = {Zhang, Yunming and Brahmakshatriya, Ajay and Chen, Xinyi and Dhulipala, Laxman and Kamil, Shoaib and Amarasinghe, Saman and Shun, Julian},
  title       = {{PriorityGraph}: A Unified Programming Model for Optimizing Ordered Graph Algorithms},
  date        = {2019-11-17},
  eprint      = {1911.07260},
  eprinttype  = {arXiv},
  eprintclass = {cs.PL},
  abstract    = {Many graph problems can be solved using ordered parallel graph algorithms that achieve significant speedup over their unordered counterparts by reducing redundant work. This paper introduces PriorityGraph, a new programming framework that simplifies writing high-performance parallel ordered graph algorithms. The key to PriorityGraph is a novel priority-based abstraction that enables vertices to be processed in a dynamic order while hiding low-level implementation details from the user. PriorityGraph is implemented as a language and compiler extension to GraphIt. The PriorityGraph compiler extension leverages new program analyses, transformations, and code generation to produce fast implementations of ordered parallel graph algorithms. We also introduce bucket fusion, a new performance optimization that fuses together different rounds of ordered algorithms to reduce synchronization overhead, resulting in 1.2$\times$ to 3$\times$ speedup over the fastest existing ordered algorithm implementations on road networks with large diameters. PriorityGraph achieves up to 3$\times$ speedup on six ordered graph algorithms over state-of-the-art frameworks and hand-optimized implementations (Julienne, Galois, and GAPBS) that support ordered algorithms.},
  file        = {:http\://arxiv.org/pdf/1911.07260v1:PDF},
  keywords    = {cs.PL, cs.DC},
}

@Article{Apers2019,
  author      = {Apers, Simon and de Wolf, Ronald},
  title       = {Quantum Speedup for Graph Sparsification, Cut Approximation and Laplacian Solving},
  date        = {2019-11-17},
  eprint      = {1911.07306},
  eprinttype  = {arXiv},
  eprintclass = {quant-ph},
  abstract    = {Graph sparsification underlies a large number of algorithms, ranging from approximation algorithms for cut problems to solvers for linear systems in the graph Laplacian. In its strongest form, "spectral sparsification" reduces the number of edges to near-linear in the number of nodes, while approximately preserving the cut and spectral structure of the graph. The breakthrough work by Bencz\'ur and Karger (STOC'96) and Spielman and Teng (STOC'04) showed that sparsification can be done optimally in time near-linear in the number of edges of the original graph. \\ In this work we show that quantum algorithms allow to speed up spectral sparsification, and thereby many of the derived algorithms. Given adjacency-list access to a weighted graph with $n$ nodes and $m$ edges, our algorithm outputs an $\epsilon$-spectral sparsifier in time $\widetilde{O}(\sqrt{mn}/\epsilon)$. We prove that this is tight up to polylog-factors. The algorithm builds on a string of existing results, most notably sparsification algorithms by Spielman and Srivastava (STOC'08) and Koutis and Xu (TOPC'16), a spanner construction by Thorup and Zwick (STOC'01), a single-source shortest-paths quantum algorithm by D\"urr et al. (ICALP'04) and an efficient $k$-wise independent hash construction by Christiani, Pagh and Thorup (STOC'15). Combining our sparsification algorithm with existing classical algorithms yields the first quantum speedup, roughly from $\widetilde{O}(m)$ to $\widetilde{O}(\sqrt{mn})$, for approximating the max cut, min cut, min $st$-cut, sparsest cut and balanced separator of a graph. Combining our algorithm with a classical Laplacian solver, we demonstrate a similar speedup for Laplacian solving, for approximating effective resistances, cover times and eigenvalues of the Laplacian, and for spectral clustering.},
  file        = {:http\://arxiv.org/pdf/1911.07306v1:PDF},
  keywords    = {quant-ph, cs.CC, cs.DS},
}

@TechReport{Regev2019,
  author      = {Regev, Shaked and Saunders, Michael A.},
  title       = {{SSAI}: A Symmetric Sparse Approximate Inverse Preconditioner for the Conjugate Gradient Method},
  institution = {Stanford University},
  date        = {2019},
  abstract    = {We propose a method for solving a Hermitian positive definite linear system $Ax = b$, where $A$ is an explicit sparse matrix (real or complex). A sparse approximate right inverse $M$ is computed and replaced by $\tilde{M} = (M + M^H)/2$, which is used as a left-right preconditioner in a modified version of the preconditioned conjugate gradient (PCG) method. M is formed column by column and can therefore be computed in parallel. PCG requires only matrix-vector multiplications with $A$ and $\tilde{M}$ (not solving a linear system with the preconditioner), and so too can be carried out in parallel. We compare it with incomplete Cholesky factorization (the gold standard for PCG) and with MATLAB’s backslash operator (sparse Cholesky) on matrices from various applications.},
}

@Article{Enayati2019,
  author       = {Enayati, Shakiba and Özaltın, Osman Y.},
  title        = {Optimal Influenza Vaccine Distribution with Equity},
  journaltitle = {European Journal of Operational Research},
  date         = {2019-11},
  doi          = {10.1016/j.ejor.2019.11.025},
  abstract     = {This paper is concerned with the optimal influenza vaccine distribution in a heterogeneous population consisting of multiple subgroups. We employ a compartmental model for influenza transmission and formulate a mathematical program to minimize the number of vaccine doses distributed to effectively extinguish an emerging outbreak in its early stages. We propose an equity constraint to help public health authorities consider fairness when making vaccine distribution decisions. We develop an exact solution approach that generates a vaccine distribution policy with a solution quality guarantee. We perform sensitivity analyses on key epidemic parameters in order to illustrate the application of the proposed model. We then analyze the scalability of the solution approach for a population consisting of subgroups based on geographic location and age. We finally demonstrate the proposed model’s ability to consider vaccine coverage inequity and discuss a derivative-free optimization approach, as an alternative solution method which can consider various different objective functions and constraints. Our results indicate that consideration of group-specific transmission dynamics is paramount to the optimal distribution of influenza vaccines.},
  publisher    = {Elsevier {BV}},
}

@Article{Elsen2019,
  author      = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  title       = {Fast Sparse ConvNets},
  date        = {2019-11-21},
  eprint      = {1911.09723},
  eprinttype  = {arXiv},
  eprintclass = {cs.CV},
  abstract    = {Historically, the pursuit of efficient inference has been one of the driving forces behind research into new deep learning architectures and building blocks. Some recent examples include: the squeeze-and-excitation module, depthwise separable convolutions in Xception, and the inverted bottleneck in MobileNet v2. Notably, in all of these cases, the resulting building blocks enabled not only higher efficiency, but also higher accuracy, and found wide adoption in the field. In this work, we further expand the arsenal of efficient building blocks for neural network architectures; but instead of combining standard primitives (such as convolution), we advocate for the replacement of these dense primitives with their sparse counterparts. While the idea of using sparsity to decrease the parameter count is not new, the conventional wisdom is that this reduction in theoretical FLOPs does not translate into real-world efficiency gains. We aim to correct this misconception by introducing a family of efficient sparse kernels for ARM and WebAssembly, which we open-source for the benefit of the community as part of the XNNPACK library. Equipped with our efficient implementation of sparse primitives, we show that sparse versions of MobileNet v1, MobileNet v2 and EfficientNet architectures substantially outperform strong dense baselines on the efficiency-accuracy curve. On Snapdragon 835 our sparse networks outperform their dense equivalents by $1.3-2.4\times$ -- equivalent to approximately one entire generation of MobileNet-family improvement. We hope that our findings will facilitate wider adoption of sparsity as a tool for creating efficient and accurate deep learning architectures.},
  file        = {:http\://arxiv.org/pdf/1911.09723v1:PDF},
  keywords    = {cs.CV},
}

@Article{Hernandez2019,
  author      = {Hernandez, Taylor M. and Beeumen, Roel Van and Caprio, Mark A. and Yang, Chao},
  title       = {A greedy algorithm for computing eigenvalues of a symmetric matrix},
  date        = {2019-11-20},
  eprint      = {1911.10041},
  eprinttype  = {arXiv},
  eprintclass = {physics.comp-ph},
  abstract    = {We present a greedy algorithm for computing selected eigenpairs of a large sparse matrix $H$ that can exploit localization features of the eigenvector. When the eigenvector to be computed is localized, meaning only a small number of its components have large magnitudes, the proposed algorithm identifies the location of these components in a greedy manner, and obtains approximations to the desired eigenpairs of $H$ by computing eigenpairs of a submatrix extracted from the corresponding rows and columns of $H$. Even when the eigenvector is not completely localized, the approximate eigenvectors obtained by the greedy algorithm can be used as good starting guesses to accelerate the convergence of an iterative eigensolver applied to $H$. We discuss a few possibilities for selecting important rows and columns of $H$ and techniques for constructing good initial guesses for an iterative eigensolver using the approximate eigenvectors returned from the greedy algorithm. We demonstrate the effectiveness of this approach with examples from nuclear quantum many-body calculations and many-body localization studies of quantum spin chains.},
  file        = {:http\://arxiv.org/pdf/1911.10041v1:PDF},
  keywords    = {physics.comp-ph},
}

@Article{Gong2019,
  author      = {Gong, Zhangxiaowen and Ji, Houxiang and Fletcher, Christopher and Hughes, Christopher and Torrellas, Josep},
  title       = {SparseTrain:Leveraging Dynamic Sparsity in Training DNNs on General-Purpose SIMD Processors},
  date        = {2019-11-22},
  eprint      = {1911.10175},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {Our community has greatly improved the efficiency of deep learning applications, including by exploiting sparsity in inputs. Most of that work, though, is for inference, where weight sparsity is known statically, and/or for specialized hardware. We propose a scheme to leverage dynamic sparsity during training. In particular, we exploit zeros introduced by the ReLU activation function to both feature maps and their gradients. This is challenging because the sparsity degree is moderate and the locations of zeros change over time. We also rely purely on software. We identify zeros in a dense data representation without transforming the data and performs conventional vectorized computation. Variations of the scheme are applicable to all major components of training: forward propagation, backward propagation by inputs, and backward propagation by weights. Our method significantly outperforms a highly-optimized dense direct convolution on several popular deep neural networks. At realistic sparsity, we speed up the training of the non-initial convolutional layers in VGG16, ResNet-34, ResNet-50, and Fixup ResNet-50 by 2.19$\times$, 1.37$\times$, 1.31$\times$, and 1.51$\times$ respectively on an Intel Skylake-X CPU.},
  file        = {:http\://arxiv.org/pdf/1911.10175v1:PDF},
  keywords    = {cs.LG, cs.DC, stat.ML},
}

@TechReport{Buttari2019a,
  author      = {Buttari, Alfredo and Hauberg, Søren and Kodsi, Costy},
  title       = {Parallel {QR} factorization of block-tridiagonal matrices},
  institution = {Institut de recherche en informatique de Toulouse (IRIT)},
  date        = {2019},
  abstract    = {In this work, we deal with the QR factorization of block-tridiagonal matrices, where the blocks are dense and rectangular. This work is motivated by a novel method for computing geodesics over Riemannian manifolds. If blocks are reduced sequentially along the diagonal, only limited parallelism is available. We propose a matrix permutation approach based on the Nested Dissection method which improves parallelism at the cost of additional computations and storage. We provide a detailed analysis of the approach showing that this extra cost is bounded. Finally, we present an implementation for shared memory systems relying on task parallelism and the use of a runtime system. Experimental results support the conclusions of our analysis and show that the proposed approach leads to good performance and scalability.},
}

@Article{Buttari2019,
  author       = {Buttari, Alfredo. and Orban, Dominique. and Ruiz, Daniel. and Titley-Peloquin, David.},
  title        = {A Tridiagonalization Method for Symmetric Saddle-Point Systems},
  journaltitle = {SIAM Journal on Scientific Computing},
  date         = {2019},
  volume       = {41},
  number       = {5},
  pages        = {S409--S432},
  doi          = {10.1137/18M1194900},
  abstract     = {We propose an iterative method for the solution of symmetric saddle-point systems that exploits the orthogonal tridiagonalization method of Saunders, Simon, and Yip (1988). By contrast with methods based on the Golub and Kahan (1965) bidiagonalization process, our method takes advantage of two initial vectors and splits the system into the sum of a least-squares and a least-norm problem. Our method typically requires fewer operator-vector products than MINRES, yet performs a comparable amount of work per iteration and has comparable storage requirements.},
}

@InProceedings{Wang2019,
  author    = {Wang, J. and Huang, Z. and Kong, L. and Xiao, J. and Wang, P. and Zhang, L. and Li, C.},
  title     = {Performance of Training Sparse Deep Neural Networks on {GPUs}},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--5},
  doi       = {10.1109/HPEC.2019.8916506},
  abstract  = {Deep neural networks have revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. The sizes of deep neural networks (DNNs) are rapidly outgrowing the capacity of hardware to fast store and train them. Over the past few decades, researches have explored the prospect of sparse DNNs before, during, and after training by pruning edges from the underlying topology. After the above operation, the generated neural network is known as a sparse neural network. More recent works have demonstrated the remarkable results that certain sparse DNNs can train to the same precision as dense DNNs at lower runtime and storage cost. Although existing methods ease the situation that high demand for computation resources severely hinders the deployment of large-scale DNNs in resource-constrained devices, DNNs can be trained at a faster speed and lower cost. In this work, we propose a Fine-tune Structured Sparsity Learning (FSSL) method to regularize the structures of DNNs and accelerate the training of DNNs. FSSL can: (1) learn a compact structure from large sparse DNN to reduce computation cost; (2) obtain a hardware-friendly to accelerate the DNNs evaluation efficiently. Experimental results of the training time and the compression rate show that superior performance and efficiency than the Matlab example code. These speedups are about twice speedups of non-structured sparsity.},
  issn      = {2377-6943},
  keywords  = {sparse neural networks;sparse matrices;graph analysis;GPU Computing},
}

@TechReport{Ucar2019,
  author      = {Uçar, Bora},
  title       = {Partitioning, matching, and ordering: Combinatorial scientific computing with matrices and tensors},
  institution = {École Normale Supérieure de Lyon},
  date        = {2019},
  abstract    = {This document investigates three classes of problems at the interplay of discrete algorithms, combinatorial optimization, and numerical methods. The general research area is called combinatorial scientific computing (CSC). In CSC, the contributions have practical and theoretical flavor. For all problems discussed in this document, we have the design, analysis, and implementation of algorithms along with many experiments. The theoretical results are included in this document, some with proofs; the reader is invited to the original papers for the omitted proofs. A similar approach is taken for presenting the experiments. While most results for observing theoretical findings in practice are included, the reader is referred to the original papers for some other results (e.g., run time analysis).},
}

@InProceedings{Pandey2019,
  author    = {Pandey, S. and Li, X. S. and Buluç, A. and Xu, J. and Liu, H.},
  title     = {{H-INDEX}: Hash-Indexing for Parallel Triangle Counting on GPUs},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--7},
  doi       = {10.1109/HPEC.2019.8916492},
  abstract  = {Triangle counting is a graph algorithm that calculates the number of triangles involving each vertex in a graph. Briefly, a triangle encompasses three vertices from a graph, where every vertex possesses at least one incidental edge to the other two vertices from the triangle. Consequently, list intersection, which identifies the incidental edges, becomes the core algorithm for triangle counting. At the meantime, attracted by the enormous parallel computing potential of Graphics Processing Units (GPUs), numerous efforts have been devoted to deploy triangle counting algorithms on GPUs.While state-of-the-art intersection algorithms, such as merge-path and binary-search, perform well on traditional multi-core CPU systems, deploying them on massively parallel GPUs turns out to be challenging. In particular, merge-path based approach experiences the hardship of evenly distributing the workload across vast GPU threads and irregular memory accesses. Binary-search based approach often suffers from the potential problem of high time complexity. Furthermore, both approaches require sorted neighbor lists from the input graphs, which involves nontrivial preprocessing overhead. To this end, we introduce H-INDEX, a hash-indexing assisted triangle counting algorithm that overcomes all the aforementioned shortcomings. Notably, HINDEX achieves 141.399 billion TEPS computing rate on a Protein K-mer V2a graph with 64 GPUs. To the best of our knowledge, this is the first work that advances triangle counting beyond the 100 billion TEPS rate.},
  issn      = {2377-6943},
}

@InProceedings{Milroy2019,
  author    = {Milroy, D. J. and Baker, A. H. and Dennis, J. M. and Gettelman, A. and Hammerling, D. M.},
  title     = {Investigating the Impact of Mixed Precision on Correctness for a Large Climate Code},
  booktitle = {Proceedings of the Third International Workshop on Software Correctness for HPC Applications},
  date      = {2019},
  abstract  = {Earth system models (ESMs) are computationally expensive and represent many complex processes on a wide range of scales from molecular to global. Certain ESM computations require high precision while others, such as atmospheric microphysics (e.g., precipitation) which are approximated by bulk properties, should not. As such, atmospheric microphysics models are prime candidates for conversion to single precision, which afford distinct computational and memory advantages over typical double-precision numbers. However, care must be taken as indiscriminate type casting to single precision can result in numerical instability and divergent output when applied naively. In this work we relate our experiences attempting to improve the performance of the Morrison-Gettelman microphysics package (MG2) in a popular ESM by modifying it to compute in single precision without sacrificing correctness. We find that modification of the entire MG2 package to compute with singleprecision floats achieves a respectable performance increase but does not appear to be correct in terms of maintaining consistency with double-precision MG2. On the other hand, narrowing the scope of our conversion to a couple expensive subprograms yields more satisfying results in terms of correctness but with negligible overall performance improvement. We evaluate correctness with both an objective statistical tool and traditional approaches more familiar to climate scientists. While we are still working toward our ultimate goal of improving the performance of MG2 without negatively affecting model output, we believe that our experiences may be helpful to other groups pursuing similar goals.},
}

@InProceedings{Kuppannagari2019,
  author    = {Kuppannagari, S. R. and Rajat, R. and Kannan, R. and Dasu, A. and Prasanna, V. K.},
  title     = {IP Cores for Graph Kernels on FPGAs},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--7},
  doi       = {10.1109/HPEC.2019.8916363},
  abstract  = {Graphs are a powerful abstraction for representing networked data in many real-world applications. The need for performing large scale graph analytics has led to widespread adoption of dedicated hardware accelerators such as FPGA for this purpose. In this work, we develop IP cores for several key graph kernels. Our IP cores use graph processing over partitions (GPOP) programming paradigm to perform computations over graph partitions. Partitioning the input graph into nonoverlapping partitions improves on-chip data reuse. Additional optimizations to exploit intra and interpartition parallelism and to reduce external memory accesses are also discussed. We generate FPGA designs for general graph algorithms with various vertex attributes and update propagation functions, such as Sparse Matrix Vector Multiplication (SpMV), PageRank (PR), Single Source Shortest Path (SSSP), and Weakly Connected Component (WCC). We target a platform consisting of large external DDR4 memory to store the graph data and Intel Stratix FPGA to accelerate the processing. Experimental results show that our accelerators sustain a high throughput of up to 2250, 2300, 3378, and 2178 Million Traversed Edges Per Second (MTEPS) for SpMV, PR, SSSP and WCC, respectively. Compared with several highly-optimized multi-core designs, our FPGA framework achieves up to 20.5$\times$ speedup for SpMV, 16.4$\times$ speedup for PR, 3.5$\times$ speedup for SSSP, and 35.1$\times$ speedup for WCC, and compared with two state-of-the-art FPGA frameworks, our designs demonstrate up to 5.3$\times$ speedup for SpMV, 1.64$\times$ speedup for PR, and 1.8$\times$ speedup for WCC, respectively. We develop a performance model for our GPOP paradigm. We then perform performance predictions of our designs assuming the graph is stored in HBM2 instead of DRAM. We further discuss extensions to our optimizations to improve the throughput.},
  issn      = {2377-6943},
}

@InProceedings{Hossain2019,
  author    = {Hossain, S. and Mahmud, M. S.},
  title     = {On Computing with Diagonally Structured Matrices},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--6},
  doi       = {10.1109/HPEC.2019.8916325},
  abstract  = {We present a storage scheme for storing matrices by diagonals and algorithms for performing matrix-matrix and matrix-vector multiplication by diagonals. Matrix elements are accessed with stride-1 and involve no indirect referencing. Access to the transposed matrix requires no additional effort. The proposed storage scheme handles dense matrices and matrices with special structure e.g., banded, triangular, symmetric in a uniform manner. Test results from preliminary numerical experiments with an OpenMP implementation of our method are encouraging.},
  issn      = {2377-6943},
  keywords  = {matrix-matrix multiplication;matrix-vector multiplication;diagonal storage;task parallelism;banded matrices},
}

@Article{Croci2019,
  author      = {Croci, M. and Giles, M. B. and Farrell, P. E.},
  title       = {Multilevel quasi Monte Carlo methods for elliptic {PDEs} with random field coefficients via fast white noise sampling},
  date        = {2019},
  eprint      = {1911.12099},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {When solving partial differential equations with random fields as coefficients the efficient sampling of random field realisations can be challenging. In this paper we focus on the fast sampling of Gaussian fields using quasi-random points in a finite element and multilevel quasi Monte Carlo (MLQMC) setting. Our method uses the SPDE approach combined with a new fast (ML)QMC algorithm for white noise sampling. We express white noise as a wavelet series expansion that we divide in two parts. The first part is sampled using quasi-random points and contains a finite number of terms in order of decaying importance to ensure good QMC convergence. The second part is a correction term which is sampled using standard pseudo-random numbers. We show how the sampling of both terms can be performed in linear time and memory complexity in the number of mesh cells via a supermesh construction, yielding an overall linear cost. Furthermore, our technique can be used to enforce the MLQMC coupling even in the case of non-nested mesh hierarchies. We demonstrate the efficacy of our method with numerical experiments.},
  file        = {:http\://arxiv.org/pdf/1911.12099v1:PDF},
  keywords    = {math.NA, cs.NA, 65C05, 60G60, 65N30, 60H35, 35R60},
}

@TechReport{Abdelfattah2019,
  author      = {Abdelfattah, Ahmad and Tomov, Stanimire and Dongarra, Jack},
  title       = {Towards Half-Precision Computation for Complex Matrices: A Case Study for Mixed-Precision Solvers on {GPUs}},
  institution = {Innovative Computing Laboratory, University of Tennessee},
  date        = {2019},
  abstract    = {The use of low-precision computations is popular in accelerating machine learning and artificial intelligence (AI) applications. Hardware architectures, such as high-end graphics processing units (GPUs), now support native 16-bit floating-point arithmetic (i.e., half-precision). While half precision provides a natural 2$\times$/4$\times$ speedup against the performance of single/double precisions, respectively, modern GPUs are equipped with hardware accelerators that further boost the FP16 performance. These accelerators, known as tensor cores (TCs), have a theoretical peak performance that is 8$\times$/16$\times$ faster than FP32/FP64 performance, respectively. Such a high level of performance has encouraged researchers to harness the compute power of TCs outside AI applications. \\ This paper presents a mixed-precision dense linear solver ($Ax = b$) for complex matrices using the GPU’s TC units. Unlike similar efforts that have discussed accelerating $Ax = b$ in real FP16 arithmetic, this paper focuses on complex FP16 precisions. The developed solution uses a “half-complex” precision to accelerate the solution of Ax = b while maintaining complex FP32 precision accuracy. The proposed solver requires the development of a high-performance mixed-precision matrix multiplication (CGEMM-FP16) that accepts half-complex inputs, and uses the TCs’ full-precision products and FP32 accumulations for the computation. We discuss two designs and their performance. Similar to the way fast GEMMs power the performance of LAPACK, the mixed-precision CGEMMFP16 can enable the development of mixed-precision LAPACK algorithms. We illustrate this by integrating both CGEMM-FP16s into the development of mixed-precision LU factorizations of complex matrices. Finally, an iterative refinement solver is used to deliver complex FP32 accuracy using a preconditioned GMRES solver. Our experiments, conducted on V100 GPUs, show that the mixed-precision solver can be up to 2.5$\times$ faster than a full single-complex precision solver.},
}

@InProceedings{Acer2019,
  author    = {Acer, S. and Yaşar, A. and Rajamanickam, S. and Wolf, M. and Çatalyürek, \"U. V.},
  title     = {Scalable Triangle Counting on Distributed-Memory Systems},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--5},
  doi       = {10.1109/HPEC.2019.8916302},
  abstract  = {Triangle counting is a foundational graph-analysis kernel in network science. It has also been one of the challenge problems for the “Static Graph Challenge”. In this work, we propose a novel, hybrid, parallel triangle counting algorithm based on its linear algebra formulation. Our framework uses MPI and Cilk to exploit the benefits of distributed-memory and shared-memory parallelism, respectively. The problem is partitioned among MPI processes using a two-dimensional (2D) Cartesian block partitioning. One-dimensional (1D) rowwise partitioning is used within the Cartesian blocks for shared-memory parallelism using the Cilk programming model. Besides exhibiting very good strong scaling behavior in almost all tested graphs, our algorithm achieves the fastest time on the 1.4B edge real-world twitter graph, which is 3.217 seconds, on 1,092 cores. In comparison to past distributed-memory parallel winners of the graph challenge, we demonstrate a speed up of 2.7$\times$ on this twitter graph. This is also the fastest time reported for parallel triangle counting on the twitter graph when the graph is not replicated.},
  issn      = {2377-6943},
  keywords  = {triangle counting;distributed-memory systems;scale-free graphs;two-dimensional partitioning},
}

@InProceedings{Deakin2019,
  author    = {Deakin, Tom J. and McIntosh-Smith, Simon N. and Price, James and Poenaru, Andrei and Atkinson, Patrick R. and Popa, Codrin and Salmon, Justin},
  title     = {Performance Portability across Diverse Computer Architectures},
  booktitle = {2019 IEEE International Workshop on Performance, Portability and Productivity in HPC},
  date      = {2019-09-25},
  language  = {English},
  series    = {P3HPC '19},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  location  = {United States},
  abstract  = {Previous studies into performance portability have typically analysed a single application (and its various implementations) in isolation. In this study we explore the wider landscape of performance portability by considering a number of applications from across the space of dwarfs, written in multiple parallel programming models, and across a diverse set of architectures. We apply rigorous performance portability metrics, as defined by Pennycook et al [1]. We believe this is the broadest and most rigorous performance portability study to date, representing a far reaching exploration of the state of performance portability that is achievable today. We will present a summary of the performance portability of each application and programming model across our diverge range of twelve computer architectures, including six different server CPUs from five different vendors, five different GPUs from two different vendors, and one vector architecture. We will conclude with an analysis of the performance portability of key programming models in general, across different application spaces as well across differing architectures, allowing us to comment on more general performance portability principles.},
  day       = {25},
  keywords  = {performance portability, productivity, mini-app, programming models},
}

@InProceedings{Yasar2019a,
  author    = {Yaşar, Abdurrahman and Rajamanickam, Sivasankaran and Berry, Jonathan W. and Wolf, Michael M. and Young, Jeffrey and Çatalyürek, {\"U}mit V.},
  title     = {Linear Algebra-Based Triangle Counting via Fine-Grained Tasking on Heterogeneous Environments},
  booktitle = {Proceedings of the IEEE High Performance Extreme Computing Conference},
  date      = {2019-09},
  series    = {HPEC '19},
  pages     = {1--4},
  doi       = {10.1109/HPEC.2019.8916492},
  abstract  = {Triangle counting is a representative graph problem that shows the challenges of improving graph algorithm performance using algorithmic techniques and adopting graph algorithms to new architectures. In this paper, we describe an update to the linear-algebraic formulation of the triangle counting problem. Our new approach relies on fine-grained tasking based on a tile layout. We adopt this task based algorithm to heterogeneous architectures (CPUs and GPUs) for up to 10.8$\times$ speed up over past year's graph challenge submission. This implementation also results in the fastest kernel time known at time of publication for real-world graphs like twitter (3.7 second) and friendster (1.8 seconds) on GPU accelerators when the graph is GPU resident. This is a 1.7 and 1.2 time improvement over previous state-of-the-art triangle counting on GPUs. We also improved end-to-end execution time by overlapping computation and communication of the graph to the GPUs. In terms of end-toend execution time, our implementation also achieves the fastest end-to-end times due to very low overhead costs.},
  issn      = {2377-6943},
}

@Article{Li2019c,
  author       = {Li, M. and Liu, Y. and Yang, H. and Luan, Z. and Gan, L. and Yang, G. and Qian, D.},
  title        = {Accelerating Sparse Cholesky Factorization on Sunway Manycore Architecture},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  date         = {2019},
  issn         = {2161-9883},
  doi          = {10.1109/TPDS.2019.2953852},
  abstract     = {To improve the performance of Sparse Cholesky factorization, existing research divides the adjacent columns of the sparse matrix with the same nonzero patterns into supernodes for parallelization. However, due to the various structures of sparse matrices, the computation of the generated supernodes varies significantly, and thus hard to optimize when computed by dense matrix kernels. Therefore, how to efficiently map sparse Choleksy factorization to the emerging architectures, such as Sunway many-core processor, remains an active research direction. In this paper, we propose swCholesky, which is a highly optimized implementation of sparse Cholesky factorization on Sunway processor. Specifically, we design three kernel task queues and a dense matrix library to dynamically adapt to the kernel characteristics and architecture features. In addition, we propose an auto-tuning mechanism to search for the optimal settings of the important parameters in swCholesky. Our experiments show that swCholesky achieves better performance than state-of-the-art implementations.},
  keywords     = {Sunway architecture;Sparse Cholesky factorization;Performance Optimization},
}

@Article{Arjevani2019,
  author      = {Arjevani, Yossi and Carmon, Yair and Duchi, John C. and Foster, Dylan J. and Srebro, Nathan and Woodworth, Blake},
  title       = {Lower Bounds for Non-Convex Stochastic Optimization},
  date        = {2019-12-05},
  eprint      = {1912.02365},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We lower bound the complexity of finding $\epsilon$-stationary points (with gradient norm at most $\epsilon$) using stochastic first-order methods. In a well-studied model where algorithms access smooth, potentially non-convex functions through queries to an unbiased stochastic gradient oracle with bounded variance, we prove that (in the worst case) any algorithm requires at least $\epsilon^{-4}$ queries to find an $\epsilon$ stationary point. The lower bound is tight, and establishes that stochastic gradient descent is minimax optimal in this model. In a more restrictive model where the noisy gradient estimates satisfy a mean-squared smoothness property, we prove a lower bound of $\epsilon^{-3}$ queries, establishing the optimality of recently proposed variance reduction techniques.},
  file        = {:http\://arxiv.org/pdf/1912.02365v1:PDF},
  keywords    = {math.OC, cs.IT, cs.LG, math.IT, stat.ML},
}

@Article{Cifuentes2019,
  author      = {Cifuentes, Diego and Moitra, Ankur},
  title       = {Polynomial time guarantees for the {Burer-Monteiro} method},
  date        = {2019-12-03},
  eprint      = {1912.01745},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {The Burer-Monteiro method is one of the most widely used techniques for solving large-scale semidefinite programs (SDP). The basic idea is to solve a nonconvex program in $Y$, where $Y$ is an $n \times p$ matrix such that $X = Y Y^T$. In this paper, we show that this method can solve SDPs in polynomial time in an smoothed analysis setting. More precisely, we consider an SDP whose domain satisfies some compactness and smoothness assumptions, and slightly perturb the cost matrix and the constraints. We show that if $p \gtrsim \sqrt{2(1+\eta)m}$, where $m$ is the number of constraints and $\eta>0$ is any fixed constant, then the Burer-Monteiro method can solve SDPs to any desired accuracy in polynomial time, in the setting of smooth analysis. Our bound on $p$ approaches the celebrated Barvinok-Pataki bound in the limit as $\eta$ goes to zero, beneath which it is known that the nonconvex program can be suboptimal. Previous analyses were unable to give polynomial time guarantees for the Burer-Monteiro method, since they either assumed that the criticality conditions are satisfied exactly, or ignored the nontrivial problem of computing an approximately feasible solution. We address the first problem through a novel connection with tubular neighborhoods of algebraic varieties. For the feasibility problem we consider a least squares formulation, and provide the first guarantees that do not rely on the restricted isometry property.},
  file        = {:http\://arxiv.org/pdf/1912.01745v1:PDF},
  keywords    = {math.OC, stat.ML, 90C22 (Primary) 90C26 (Secondary)},
}

@Article{Quirynen2019,
  author      = {Quirynen, Rien and Cairano, Stefano Di},
  title       = {{PRESAS}: Block-Structured Preconditioning of Iterative Solvers within a Primal Active-Set Method for fast {MPC}},
  date        = {2019-12-04},
  eprint      = {1912.02122},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Model predictive control (MPC) for linear dynamical systems requires solving an optimal control structured quadratic program (QP) at each sampling instant. This paper proposes a primal active-set strategy (PRESAS) for the efficient solution of such block-sparse QPs, based on a preconditioned iterative solver to compute the search direction in each iteration. Rank-one factorization updates of the preconditioner result in a per-iteration computational complexity of $\mathcal{O}(N m^2)$, where $m$ denotes the number of state and control variables and $N$ the number of control intervals. Three different block-structured preconditioning techniques are presented and their numerical properties are studied further. In addition, an augmented Lagrangian based implementation is proposed to avoid a costly initialization procedure to find a primal feasible starting point. Based on a standalone C code implementation, we illustrate the computational performance of PRESAS against current state of the art QP solvers for multiple linear and nonlinear MPC case studies. We also show that the solver is real-time feasible on a dSPACE MicroAutoBox-II rapid prototyping unit for vehicle control applications, and numerical reliability is illustrated based on experimental results from a testbench of small-scale autonomous vehicles.},
  file        = {:http\://arxiv.org/pdf/1912.02122v1:PDF},
  keywords    = {math.OC, cs.SY, eess.SY},
}

@InProceedings{Dufrechou2019,
  author    = {Dufrechou, E. and Ezzatti, P. and Quintana-Orti, E. S.},
  title     = {Automatic Selection of Sparse Triangular Linear System Solvers on {GPUs} through Machine Learning Techniques},
  booktitle = {Proceeding of the 31st International Symposium on Computer Architecture and High Performance Computing},
  date      = {2019-10},
  series    = {SBAC-PAD '19},
  pages     = {41--47},
  doi       = {10.1109/SBAC-PAD.2019.00020},
  abstract  = {The solution of sparse triangular linear systems is often the most time-consuming stage of preconditioned iterative methods to solve general sparse linear systems, where it has to be applied several times for the same sparse matrix. For this reason, its computational performance has a strong impact on a wide range of scientific and engineering applications, which has motivated the study of its efficient execution on massively parallel platforms. In this sense, several methods have been proposed to tackle this operation on graphics processing units (GPUs), which can be classified under either the level-set or the self-scheduling paradigms. The results obtained from the experimental evaluation of the different methods suggest that both paradigms perform well for certain problems but poorly for others. Additionally, the relation between the properties of the linear systems and the performance of the different solvers is not evident a-priori. In this context, techniques that allow to predict inexpensively which is be the best solver for a particular linear system can lead to important runtime reductions. Our approach leverages machine learning techniques to select the best sparse triangular solver for a given linear system, with focus on the case where a small number of triangular systems has to be solved for the same matrix. We study the performance of several methods using different features derived from the sparse matrices, obtaining models with more than 80\% of accuracy and acceptable prediction speed. These results are an important advance towards the automatic selection of the best GPU solver for a given sparse triangular linear system, and the characterization of the performance of these kernels.},
  issn      = {1550-6533},
  keywords  = {Linear systems;Sparse matrices;Machine learning;Kernel;Graphics processing units;Instruction sets;Indexes;graphics processors;sparse triangular linear systems;high performance;machine learning},
}

@InProceedings{Marques2019,
  author    = {Marques, S. M. V. N. and Medeiros, T. S. and Rossi, F. D. and Luizelli, M. C. and Girardi, A. G. and Beck, A. C. S. and Lorenzon, A. F.},
  title     = {The Impact of Turbo Frequency on the Energy, Performance, and Aging of Parallel Applications},
  booktitle = {Proceedings of the IFIP/IEEE 27th International Conference on Very Large Scale Integration},
  date      = {2019-10},
  series    = {VLSI-SoC '19},
  pages     = {149--154},
  doi       = {10.1109/VLSI-SoC.2019.8920389},
  abstract  = {Technologies that improve the performance of parallel applications by increasing the nominal operating frequency of processors respecting a given TDP (Thermal Design Power) have been widely used. However, they may impact on other non-functional requirements in different ways (e.g. increasing energy consumption or aging). Therefore, considering the huge number of configurations available, represented by the range of all possible combinations among different parallel applications, amount of threads, dynamic voltage and frequency scaling (DVFS) governors, boosting technologies and simultaneous multithreading (SMT), selecting the one that offers the best tradeoff for a non-functional requirement is extremely challenging for software designers. Given that, in this work we assess the impact of changing these configurations on the energy consumption, performance, and aging of parallel applications on a turbo-compliant processor. Results show that there is no single configuration that would provide the best solution for all nonfunctional requirements at once. For instance, we demonstrate that the configuration that offers the best performance is the same one that has the worst impact on aging, accelerating it by up to 1.75 times. With our experiments, we provide guidelines for the developer when it comes to tuning performance using turbo boosting to save as much energy as possible and increase the lifespan of the hardware components1.1This study was financed in part by the Coordenao de Aperfeioamento de Pessoal de Nvel Superior - Brasil (CAPES) - Finance Code 001},
  issn      = {2324-8432},
  keywords  = {Program processors;Boosting;Energy consumption;Aging;Games;Benchmark testing;Hardware;Turbo Frequency;TLP Exploitation;Aging;Energy},
}

@Article{Curtis2019,
  author      = {Curtis, Frank E. and Robinson, Daniel P. and Royer, Clément and Wright, Stephen J.},
  title       = {Trust-Region {Newton-CG} with Strong Second-Order Complexity Guarantees for Nonconvex Optimization},
  date        = {2019-12-09},
  eprint      = {1912.04365},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Worst-case complexity guarantees for nonconvex optimization algorithms have been a topic of growing interest. Multiple frameworks that achieve the best known complexity bounds among a broad class of first- and second-order strategies have been proposed. These methods have often been designed primarily with complexity guarantees in mind and, as a result, represent a departure from the algorithms that have proved to be the most effective in practice. In this paper, we consider trust-region Newton methods, one of the most popular classes of algorithms for solving nonconvex optimization problems. By introducing slight modifications to the original scheme, we obtain two methods---one based on exact subproblem solves and one exploiting inexact subproblem solves as in the popular "trust-region Newton-Conjugate-Gradient" (Newton-CG) method---with iteration and operation complexity bounds that match the best known bounds for the aforementioned class of first- and second-order methods. The resulting Newton-CG method also retains the attractive practical behavior of classical trust-region Newton-CG, which we demonstrate with numerical comparisons on a standard benchmark test set.},
  file        = {:http\://arxiv.org/pdf/1912.04365v1:PDF},
  keywords    = {math.OC},
}

@Article{Dvurechensky2019,
  author      = {Dvurechensky, Pavel and Gasnikov, Alexander and Ostroukhov, Petr and Uribe, César A. and Ivanova, Anastasiya},
  title       = {Near-optimal tensor methods for minimizing the gradient norm of convex function},
  date        = {2019-12-06},
  eprint      = {arXiv: 1912.03381},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Motivated by convex problems with linear constraints and, in particular, by entropy-regularized optimal transport, we consider the problem of finding $\varepsilon$-approximate stationary points, i.e. points with the norm of the objective gradient less than $\varepsilon$, of convex functions with Lipschitz $p$-th order derivatives. Lower complexity bounds for this problem were recently proposed in [Grapiglia and Nesterov, arXiv:1907.07053]. However, the methods presented in the same paper do not have optimal complexity bounds. We propose two optimal up to logarithmic factors methods with complexity bounds $\tilde{O}(\varepsilon^{-2(p+1)/(3p+1)})$ and $\tilde{O}(\varepsilon^{-2/(3p+1)})$ with respect to the initial objective residual and the distance between the starting point and solution respectively.},
  file        = {:http\://arxiv.org/pdf/1912.03381v1:PDF},
  keywords    = {math.OC},
}

@Article{Crowley2019,
  author      = {Crowley, Colin and Rodriguez, Jose Israel and Weiker, Jacob and Zoromski, Jacob},
  title       = {Regeneration graphs for polynomial system solving},
  date        = {2019-12-09},
  eprint      = {1912.04394},
  eprinttype  = {arXiv},
  eprintclass = {math.AG},
  abstract    = {Regeneration is a popular method for describing the solution set of a system of polynomial equations. In this paper we introduce regeneration graphs to solve polynomial systems. This translates the problem of solving a polynomial system to that of traversing a directed acyclic graph. Previous regeneration algorithms can be viewed in our context as breadth first traversal, and we formulate a depth first alternative which is useful in many applications because it quickly produces a subset of the solutions and is not ``all or nothing.''},
  file        = {:http\://arxiv.org/pdf/1912.04394v1:PDF},
  keywords    = {math.AG},
}

@Article{Ozaslan2019,
  author      = {Ozaslan, Ibrahim Kurban and Pilanci, Mert and Arikan, Orhan},
  title       = {Regularized Momentum Iterative Hessian Sketch for Large Scale Linear System of Equations},
  date        = {2019-12-07},
  eprint      = {1912.03514},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this article, Momentum Iterative Hessian Sketch (M-IHS) techniques, a group of solvers for large scale linear Least Squares (LS) problems, are proposed and analyzed in detail. The proposed techniques are obtained by incorporating the Heavy Ball Acceleration into the Iterative Hessian Sketch algorithm and they provide significant improvements over the randomized preconditioning techniques. Through the error analyses of the M-IHS variants, lower bounds on the sketch size for various randomized distributions to converge at a pre-determined rate with a constant probability are established. The bounds present the best results in the current literature for obtaining a solution approximation and they suggest that the sketch size can be chosen proportional to the statistical dimension of the regularized problem regardless of the size of the coefficient matrix. The statistical dimension is always smaller than the rank and it gets smaller as the regularization parameter increases. By using approximate solvers along with the iterations, the M-IHS variants are capable of avoiding all matrix decompositions and inversions, which is one of the main advantages over the alternative solvers such as the Blendenpik and the LSRN. Similar to the Chebyshev Semi-iterations, the M-IHS variants do not use any inner products and eliminate the corresponding synchronizations steps in hierarchical or distributed memory systems, yet the M-IHS converges faster than the Chebyshev Semi-iteration based solvers.},
  file        = {:http\://arxiv.org/pdf/1912.03514v1:PDF},
  keywords    = {math.OC, cs.CC, 15B52, 65F08, 65F10, 65F22, 65F50, 68W20, 90C06},
}

@InProceedings{Barkalov,
  author    = {Barkalov, Konstantin and Lebedev, Ilya},
  title     = {Parallel Global Optimization for Non-convex Mixed-Integer Problems},
  booktitle = {Proceedings of the 5th Russian Supercomputing Days Conference},
  year      = {2019},
  editor    = {Voevodin, Vladimir and Sobolev, Sergey},
  series    = {RuSCDays'19},
  publisher = {Springer International Publishing},
  isbn      = {978-3-030-36592-9},
  pages     = {98--109},
  abstract  = {The paper considers the mixed-integer global optimization problems. A novel parallel algorithm for solving the problems of this class based on the index algorithm for solving the continuous global optimization problems has been proposed. The comparison of this algorithm with known analogs demonstrates the efficiency of the developed approach. The proposed algorithm allows an efficient parallelization including the employment of the graphics accelerators. The results of performed numerical experiments (solving a series of 100 multiextremal mixed-integer problems) confirm a good speedup of the algorithm with the use of GPU.},
}

@Article{Sun2019,
  author      = {Ruoyu Sun},
  title       = {Optimization for deep learning: theory and algorithms},
  date        = {2019-12-19},
  eprint      = {1912.08957},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.},
  file        = {:http\://arxiv.org/pdf/1912.08957v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Article{Jakovetic2019,
  author      = {Dusan Jakovetic and Dragana Bajovic and Joao Xavier and Jose M. F. Moura},
  title       = {Primal-dual optimization methods for large-scale and distributed data analytics},
  date        = {2019-12-18},
  eprint      = {1912.08546},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {The augmented Lagrangian method (ALM) is a classical optimization tool that solves a given "difficult" (constrained) problem via finding solutions of a sequence of "easier"(often unconstrained) sub-problems with respect to the original (primal) variable, wherein constraints satisfaction is controlled via the so-called dual variables. ALM is highly flexible with respect to how primal sub-problems can be solved, giving rise to a plethora of different primal-dual methods. The powerful ALM mechanism has recently proved to be very successful in various large scale and distributed applications. In addition, several significant advances have appeared, primarily on precise complexity results with respect to computational and communication costs in the presence of inexact updates and design and analysis of novel optimal methods for distributed consensus optimization. We provide a tutorial-style introduction to ALM and its analysis via control-theoretic tools, survey recent results, and provide novel insights in the context of two emerging applications: federated learning and distributed energy trading.},
  file        = {:http\://arxiv.org/pdf/1912.08546v1:PDF},
  keywords    = {math.OC, cs.IT, math.IT},
}

@InProceedings{Nguyen2020,
  author    = {Nguyen, Duc Manh},
  title     = {A Combination of {CMAES-APOP} Algorithm and {Quasi-Newton} Method},
  booktitle = {Advanced Computational Methods for Knowledge Engineering},
  year      = {2020},
  editor    = {Le Thi, Hoai An and Le, Hoai Minh and Pham Dinh, Tao and Nguyen, Ngoc Thanh},
  publisher = {Springer International Publishing},
  isbn      = {978-3-030-38364-0},
  pages     = {64--74},
  abstract  = {In this paper, we present an approach for combining the CMAES-APOP with a local search in order to make a hybrid evolutionary algorithm. This combination is based on the information of population size in the evolution process of the CMAES-APOP algorithm while the local search is quasi-Newton line search algorithm. We will give some conditions to efficiently active the local search inside CMAES-APOP. Some numerical experiments on multi-modal optimization problems will show the efficiency of proposed approach.},
  address   = {Cham},
}

@Article{Ren2019,
  author      = {Yuxiang Ren and Lin Meng and Jiawei Zhang},
  title       = {Scalable Heterogeneous Social Network Alignment through Synergistic Graph Partition},
  date        = {2019-12-18},
  eprint      = {1912.08372},
  eprinttype  = {arXiv},
  eprintclass = {cs.SI},
  abstract    = {Social network alignment has been an important research problem for social network analysis in recent years. With the identified shared users across networks, it will provide researchers with the opportunity to achieve a more comprehensive understanding of users' social activities both within and across networks. Social network alignment is a very difficult problem. Besides the challenges introduced by the network heterogeneity, the network alignment problem can be reduced to a combinatorial optimization problem with an extremely large search space. The learning effectiveness and efficiency of existing alignment models will be degraded significantly as the network size increases. In this paper, we will focus on studying the scalable heterogeneous social network alignment problem, and propose to address it with a novel two-stage network alignment model, namely \textbf{S}calable \textbf{H}eterogeneous \textbf{N}etwork \textbf{A}lignment (SHNA). Based on a group of intra- and inter-network meta diagrams, SHNA first partitions the social networks into a group of sub-networks synergistically. Via the partially known anchor links, SHNA will extract the partitioned sub-network correspondence relationships. Instead of aligning the complete input network, SHNA proposes to identify the anchor links between the matched sub-network pairs, while those between the unmatched sub-networks will be pruned to effectively shrink the search space. Extensive experiments have been done to compare SHNA with the state-of-the-art baseline methods on a real-world aligned social networks dataset. The experimental results have demonstrated both the effectiveness and efficiency of the model in addressing the problem.},
  file        = {:http\://arxiv.org/pdf/1912.08372v1:PDF},
  keywords    = {cs.SI, physics.soc-ph},
}

@TechReport{Grapiglia2019,
  author      = {Geovani Nunes Grapiglia and Yurii Nesterov},
  title       = {Tensor methods for minimizing convex functions with {Hölder} continuous higher-order derivatives},
  institution = {Center for Operations Research and Economics, University of Lauvain},
  year        = {2019},
  date        = {2019},
  abstract    = {In this paper we study $p$-order methods for unconstrained minimization of convex functions that are $p$-times differentiable ($p \ge 2$) with $\nu$-Hölder continuous $p$th derivatives. We propose tensor schemes with and without acceleration. For the schemes without acceleration, we establish iteration complexity bounds of $\mathcal{O}(\epsilon^{-1/(p+\nu-1)}$ for reducing the functional residual below a given $\epsilon \in (0, 1)$. Assuming that $\nu$ is known, we obtain an improved complexity bound of $\mathcal{O}(\epsilon^{-1/(p+\nu)})$ for the corresponding accelerated scheme. For the case in which $\nu$ is unknown, we present a universal accelerated tensor scheme with iteration complexity of $\mathcal{O}(\epsilon^{-p/(p+1)(p+\nu-1)})$. A lower complexity bound of $\mathcal{O}(\epsilon^{-2/[3(p+\nu)-2]})$ is also obtained for this problem class.},
}

@Article{Wang2019a,
  author      = {Jie Wang and Victor Magron and Jean-Bernard Lasserre},
  title       = {{TSSOS}: A Moment-{SOS} hierarchy that exploits term sparsity},
  date        = {2019-12-18},
  eprint      = {1912.08899},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This paper is concerned with polynomial optimization problems. We show how to exploit term (or monomial) sparsity of the input polynomials to obtain a new converging hierarchy of semidefinite programming relaxations. The novelty (and distinguishing feature) of such relaxations is to involve block-diagonal matrices obtained in an iterative procedure performing completion of the connected components of certain adjacency graphs. The graphs are related to the terms arising in the original data and not to the links between variables. Our theoretical framework is then applied to compute lower bounds for polynomial optimization problems either randomly generated or coming from the networked systems literature.},
  file        = {:http\://arxiv.org/pdf/1912.08899v1:PDF},
  keywords    = {math.OC},
}

@Article{Ahmad2019,
  author     = {Ahmad, Khalid and Sundar, Hari and Hall, Mary},
  title      = {Data-driven Mixed Precision Sparse Matrix Vector Multiplication for GPUs},
  journal    = {ACM Transactions on Architecture and Code Optimization},
  year       = {2019},
  volume     = {16},
  number     = {4},
  month      = dec,
  pages      = {51:1--51:24},
  issn       = {1544-3566},
  doi        = {10.1145/3371275},
  url        = {http://doi.acm.org/10.1145/3371275},
  abstract   = {We optimize Sparse Matrix Vector multiplication (SpMV) using a mixed precision strategy (MpSpMV) for Nvidia V100 GPUs. The approach has three benefits: (1) It reduces computation time, (2) it reduces the size of the input matrix and therefore reduces data movement, and (3) it provides an opportunity for increased parallelism. MpSpMV’s decision to lower to single precision is data driven, based on individual nonzero values of the sparse matrix. On all real-valued matrices from the Sparse Matrix Collection, we obtain a maximum speedup of 2.61$\times$ and average speedup of 1.06$\times$ over double precision, while maintaining higher accuracy compared to single precision.},
  acmid      = {3371275},
  address    = {New York, NY, USA},
  articleno  = {51},
  issue_date = {December 2019},
  keywords   = {Mixed precision, SpMV, correctness, sparse matrices},
  numpages   = {24},
  publisher  = {ACM},
}

@InProceedings{Vatai2020,
  author    = {Vatai, Emil and Singhal, Utsav and Suda, Reiji},
  title     = {Diamond Matrix Powers Kernels},
  booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
  year      = {2020},
  series    = {HPCAsia2020},
  publisher = {ACM},
  location  = {Fukuoka, Japan},
  isbn      = {978-1-4503-7236-7},
  pages     = {102--113},
  doi       = {10.1145/3368474.3368494},
  url       = {http://doi.acm.org/10.1145/3368474.3368494},
  abstract  = {Matrix powers kernel calculates the vectors Akv, for k = 1, 2,..., m and they are the heart of various scientific computations, including communication avoiding iterative solvers. In this paper we propose diamond matrix powers kernel - DMPK, which has the purpose to apply the "diamond tiling" stencil algorithm to general matrices. It can also be considered as an extension of the PA1 and PA2 algorithms, introduced by Demmel et al. Our approach enables us to control the balance between the amount of communication avoidance and redundant computation inherently present in communication avoiding algorithms. We present a proof of concept implementation of the algorithm using MPI routines. The experiments we performed show that the control of the amount of computation and communication is achievable, and with more thorough optimisations, DMPK is a promising alternative to existing MPK approaches.},
  acmid     = {3368494},
  address   = {New York, NY, USA},
  numpages  = {12},
}

@Article{Stoll2019,
  author      = {Martin Stoll},
  title       = {A literature survey of matrix methods for data science},
  date        = {2019-12-17},
  eprint      = {1912.07896},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {Efficient numerical linear algebra is a core ingredient in many applications across almost all scientific and industrial disciplines. With this survey we want to illustrate that numerical linear algebra has played and is playing a crucial role in enabling and improving data science computations with many new developments being fueled by the availability of data and computing resources.},
  file        = {:http\://arxiv.org/pdf/1912.07896v1:PDF},
  keywords    = {math.NA, cs.LG, cs.NA},
}

@Article{Zhang2019d,
  author      = {Hong Zhang and Emil M. Constantinescu and Barry F. Smith},
  title       = {{PETSc} {TSAdjoint}: a discrete adjoint {ODE} solver for first-order and second-order sensitivity analysis},
  date        = {2019-12-16},
  eprint      = {arXiv: 1912.07696},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {We present a new software system PETSc TSAdjoint for first-order and second-order adjoint sensitivity analysis of time-dependent nonlinear differential equations. The derivative calculation in PETSc TSAdjoint is essentially a high-level algorithmic differentiation process. The adjoint models are derived by differentiating the timestepping algorithms and implemented based on the parallel infrastructure in PETSc. Full differentiation of the library code including MPI routines thus is avoided, and users do not need to derive their own adjoint models for their specific applications. PETSc TSAdjoint can compute the first-order derivative, that is, the gradient of a scalar functional, and the Hessian-vector product that carries second-order derivative information, while requiring minimal input (a few callbacks) from the users. Optimal checkpointing schemes are employed by the adjoint model in a manner that is transparent to users. Usability, efficiency, and scalability are demonstrated through examples from a variety of applications.},
  file        = {:http\://arxiv.org/pdf/1912.07696v1:PDF},
  keywords    = {cs.MS},
}

@Article{Chung2019,
  author      = {Julianne Chung and Matthias Chung and J. Tanner Slagel and Luis Tenorio},
  title       = {Sampled Limited Memory Methods for Massive Linear Inverse Problems},
  date        = {2019-12-17},
  eprint      = {1912.07962},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {In many modern imaging applications the desire to reconstruct high resolution images, coupled with the abundance of data from acquisition using ultra-fast detectors, have led to new challenges in image reconstruction. A main challenge is that the resulting linear inverse problems are massive. The size of the forward model matrix exceeds the storage capabilities of computer memory, or the observational dataset is enormous and not available all at once. Row-action methods that iterate over samples of rows can be used to approximate the solution while avoiding memory and data availability constraints. However, their overall convergence can be slow. In this paper, we introduce a sampled limited memory row-action method for linear least squares problems, where an approximation of the global curvature of the underlying least squares problem is used to speed up the initial convergence and to improve the accuracy of iterates. We show that this limited memory method is a generalization of the damped block Kaczmarz method, and we prove linear convergence of the expectation of the iterates and of the error norm up to a convergence horizon. Numerical experiments demonstrate the benefits of these sampled limited memory row-action methods for massive 2D and 3D inverse problems in tomography applications.},
  file        = {:http\://arxiv.org/pdf/1912.07962v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Lin2019,
  author      = {Tianyi Lin and Michael. I. Jordan},
  title       = {A Control-Theoretic Perspective on Optimal High-Order Optimization},
  date        = {2019-12-16},
  eprint      = {1912.07168},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we provide a control-theoretic perspective on optimal tensor optimization algorithms for minimizing a convex function in a finite-dimensional Euclidean space. Given a function $\Phi: \mathbb{R}^d \rightarrow \mathbb{R}$ that is convex and twice-continuously differentiable, we study an ordinary differential equation (ODE) that is governed by the gradient operator $\nabla \Phi$ and a positive control parameter $\lambda(t)$ that tends to infinity as $t \rightarrow +\infty$. The tuning of $\lambda(\cdot)$ is achieved via a closed-loop control law based on the \textit{algebraic equation} $[\lambda(t)]^p\|\nabla\Phi(x(t))\|^{p-1} = \theta$ for a given $\theta > 0$. We prove the existence and uniqueness of a local solution to this closed-loop ODE by the Banach fixed-point theorem. We then present a Lyapunov function that allows us to establish the existence and uniqueness of a global solution and analyze the convergence properties of trajectories. The rate of convergence is $\mathcal{O}(t^{-(3p+1)/2})$ in terms of objective gap and $\mathcal{O}(t^{-3p})$ in terms of squared gradient norm. We present two frameworks for implicit time discretization of the ODE, one of which generalizes the \textsf{large-step A-HPE} framework of~[Monteiro2013], and the other of which leads to a new $p$-th order tensor algorithm. A highlight of our analysis is that we show that all of the $p$-th order optimal tensor algorithms in this paper minimize the squared gradient norm at a rate of $\mathcal{O}(k^{-3p})$.},
  file        = {:http\://arxiv.org/pdf/1912.07168v1:PDF},
  keywords    = {math.OC, cs.CC, cs.DS},
}

@Article{Yao2019,
  author      = {Zhewei Yao and Amir Gholami and Kurt Keutzer and Michael Mahoney},
  title       = {PyHessian: Neural Networks Through the Lens of the Hessian},
  date        = {2019-12-16},
  eprint      = {1912.07145},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {We present PyHessian, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. This framework is developed in Pytorch, and it enables distributed-memory execution on cloud or supercomputer systems. PyHessian enables fast computations of the top Hessian eigenvalue, the Hessian trace, and the full Hessian eigenvalue density. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. To illustrate this, we apply PyHessian to analyze the effect of residual connections and Batch Normalization layers on the smoothness of the loss landscape. One recent claim, based on simpler first-order analysis, is that residual connections and batch normalization make the loss landscape ``smoother'', thus making it easier for Stochastic Gradient Descent to converge to a good solution. We perform an extensive analysis by measuring directly the Hessian spectrum using PyHessian. This analysis leads to finer-scale insight, demonstrating that while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that batch normalization layers do not necessarily make the loss landscape smoother, especially for shallow networks. Instead, the claimed smoother loss landscape only becomes evident for deep neural networks. We perform extensive experiments on four residual networks (ResNet20/32/38/56) on Cifar-10/100 dataset. We have open-sourced our PyHessian framework for Hessian spectrum computation.},
  file        = {:http\://arxiv.org/pdf/1912.07145v1:PDF},
  keywords    = {cs.LG, cs.NA, math.NA},
}

@Article{Doikov2019,
  author       = {Nikita Doikov and Yurii Nesterov},
  title        = {Contracting Proximal Methods for Smooth Convex Optimization},
  journaltitle = {CORE Discussion Papers ; 2019/27 (2019) 24 pages http://hdl.handle.net/2078.1/223949},
  date         = {2019-12-17},
  eprint       = {1912.07972},
  eprinttype   = {arXiv},
  eprintclass  = {math.OC},
  abstract     = {In this paper, we propose new accelerated methods for smooth Convex Optimization, called Contracting Proximal Methods. At every step of these methods, we need to minimize a contracted version of the objective function augmented by a regularization term in the form of Bregman divergence. We provide global convergence analysis for a general scheme admitting inexactness in solving the auxiliary subproblem. In the case of using for this purpose high-order Tensor Methods, we demonstrate an acceleration effect for both convex and uniformly convex composite objective function. Thus, our construction explains acceleration for methods of any order starting from one. The augmentation of the number of calls of oracle due to computing the contracted proximal steps, is limited by the logarithmic factor in the worst-case complexity bound.},
  file         = {:http\://arxiv.org/pdf/1912.07972v1:PDF},
  keywords     = {math.OC},
}

@Article{Zhang2019e,
  author      = {Junyu Zhang and Mingyi Hong and Shuzhong Zhang},
  title       = {On Lower Iteration Complexity Bounds for the Saddle Point Problems},
  date        = {2019-12-16},
  eprint      = {1912.07481},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we study the lower iteration complexity bounds for finding the saddle point of a strongly convex and strongly concave saddle point problem: $\min_x\max_yF(x,y)$. We restrict the classes of algorithms in our investigation to be either pure first-order methods or methods using proximal mappings. The existing lower bound result for this type of problems is obtained via the framework of strongly monotone variational inequality problems, which corresponds to the case where the gradient Lipschitz constants ($L_x, L_y$ and $L_{xy}$) and strong convexity/concavity constants ($\mu_x$ and $\mu_y$) are uniform with respect to variables $x$ and $y$. However, specific to the min-max saddle point problem these parameters are naturally different. Therefore, one is led to finding the best possible lower iteration complexity bounds, specific to the min-max saddle point models. In this paper we present the following results. For the class of pure first-order algorithms, our lower iteration complexity bound is $\Omega\left(\sqrt{\frac{L_x}{\mu_x}+\frac{L_{xy}^2}{\mu_x\mu_y}+\frac{L_y}{\mu_y}}\cdot\ln\left(\frac{1}{\epsilon}\right)\right)$, where the term $\frac{L_{xy}^2}{\mu_x\mu_y}$ explains how the coupling influences the iteration complexity. Under several special parameter regimes, this lower bound has been achieved by corresponding optimal algorithms. However, whether or not the bound under the general parameter regime is optimal remains open. Additionally, for the special case of bilinear coupling problems, given the availability of certain proximal operators, a lower bound of $\Omega\left(\sqrt{\frac{L_{xy}^2}{\mu_x\mu_y}+1}\cdot\ln(\frac{1}{\epsilon})\right)$ is established in this paper, and optimal algorithms have already been developed in the literature.},
  file        = {:http\://arxiv.org/pdf/1912.07481v1:PDF},
  keywords    = {math.OC},
}

@InProceedings{Wongpanich2020,
  author    = {Wongpanich, Arissa and You, Yang and Demmel, James},
  title     = {Rethinking the Value of Asynchronous Solvers for Distributed Deep Learning},
  booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
  year      = {2020},
  series    = {HPCAsia2020},
  publisher = {ACM},
  location  = {Fukuoka, Japan},
  isbn      = {978-1-4503-7236-7},
  pages     = {52--60},
  doi       = {10.1145/3368474.3368498},
  url       = {http://doi.acm.org/10.1145/3368474.3368498},
  abstract  = {In recent years, the field of machine learning has seen significant advances as data becomes more abundant and deep learning models become larger and more complex. However, these improvements in accuracy [2] have come at the cost of longer training time. As a result, state-of-the-art models like OpenAI's GPT-2 [18] or AlphaZero [20] require the use of distributed systems or clusters in order to speed up training. Currently, there exist both asynchronous and synchronous solvers for distributed training. In this paper, we implement state-of-the-art asynchronous and synchronous solvers, then conduct a comparison between them to help readers pick the most appropriate solver for their own applications. We address three main challenges: (1) implementing asynchronous solvers that can outperform six common algorithm variants, (2) achieving state-of-the-art distributed performance for various applications with different computational patterns, and (3) maintaining accuracy for large-batch asynchronous training. For asynchronous algorithms, we implement an algorithm called EA-wild, which combines the idea of non-locking wild updates from Hogwild! [19] with EASGD. Our implementation is able to scale to 217,600 cores and finish 90 epochs of training the ResNet-50 model on ImageNet in 15 minutes (the baseline takes 29 hours on eight NVIDIA P100 GPUs). We conclude that more complex models (e.g., ResNet-50) favor synchronous methods, while our asynchronous solver outperforms the synchronous solver for models with a low computation-communication ratio. The results are documented in this paper; for more results, readers can refer to our supplemental website 1.},
  acmid     = {3368498},
  address   = {New York, NY, USA},
  numpages  = {9},
}

@Article{Thien2019,
  author       = {David Thien and Bill Zorn and Pavel Panchenka and Zachary Tatlock},
  title        = {Toward Multi-Precision, Multi-Format Numerics},
  journaltitle = {Proceedings of the Third International Workshop on Software Correctness for HPC Applications},
  year         = {2019},
  date         = {2019},
  abstract     = {Recent research has provided new, domain-specific number systems that accelerate modern workloads. Using these number systems effectively requires analyzing subtle multiformat, multi-precision (MPMF) code. Ideally, recent programming tools that automate numerical analysis tasks could help make MPMF programs both accurate and fast. However, three key challenges must be addressed: existing automated tools are difficult to compose due to subtle incompatibilities; there is no “gold standard” for correct MPMF execution; and no methodology exists for generalizing existing, IEEE-754-specialized tools to support MPMF. In this paper we report on recent work towards mitigating these related challenges. First, we extend the FPBench standard to support multi-precision, multi-format (MPMF) applications. Second, we present Titanic, a tool which provides reference results for arbitrary MPMF computations. Third, we describe our experience adapting an existing numerical tool to support MPMF programs},
}

@Article{FineLicht2019,
  author       = {Johannes de Fine Licht and Grzegorz Kwasniewski and Torsten Hoefler},
  title        = {Flexible Communication Avoiding Matrix Multiplication on {FPGA} with High-Level Synthesis},
  journaltitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  date         = {2019-12-13},
  series       = {FPGA '20},
  doi          = {10.1145/3373087.3375296},
  eprint       = {arXiv:1912.06526v1},
  eprintclass  = {cs.DC},
  eprinttype   = {arXiv},
  abstract     = {Data movement is the dominating factor affecting performance and energy in modern computing systems. Consequently, many algorithms have been developed to minimize the number of I/O operations for common computing patterns. Matrix multiplication is no exception, and lower bounds have been proven and implemented both for shared and distributed memory systems. Reconfigurable hardware platforms are a lucrative target for I/O minimizing algorithms, as they offer full control of memory accesses to the programmer. While bounds developed in the context of fixed architectures still apply to these platforms, the spatially distributed nature of their computational and memory resources requires a decentralized approach to optimize algorithms for maximum hardware utilization. We present a model to optimize matrix multiplication for FPGA platforms, simultaneously targeting maximum performance and minimum off-chip data movement, within constraints set by the hardware. We map the model to a concrete architecture using a high-level synthesis tool, maintaining a high level of abstraction, allowing us to support arbitrary data types, and enables maintainability and portability across FPGA devices. Kernels generated from our architecture are shown to offer competitive performance in practice, scaling with both compute and memory resources. We offer our design as an open source project to encourage the open development of linear algebra and I/O minimizing algorithms on reconfigurable hardware platforms.},
  file         = {:http\://arxiv.org/pdf/1912.06526v1:PDF},
  keywords     = {cs.DC, cs.CC},
}

@Article{Hein2019,
  author   = {Hein, Eric and Eswar, Srinivas and Yaşar, Abdurrahman and Li, Jiajia and Young, Jeffrey S. and Conte, Thomas M. and Çatalyürek, {\"U}mit V. and Vuduc, Rich and Riedy, Jason and Uçar, Bora},
  title    = {Programming Strategies for Irregular Algorithms on the Emu Chick},
  journal  = {ACM Transactions on Parallel Computing},
  year     = {2019},
  abstract = {The Emu Chick prototype implements migratory memory-side processing in a novel hardware system. Rather than transferring large amounts of data across the system interconnect, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each remote memory read. Previous work has characterized the performance of the Chick prototype in terms of memory bandwidth and programming differences from more typical, non-migratory platforms, but there has not yet been an analysis of algorithms on this system.  This work evaluates irregular algorithms that could benefit from the lightweight, memory-side processing of the Chick and demonstrates techniques and optimization strategies for achieving performance in sparse matrix-vector multiply operation (SpMV), breadth-first search (BFS), and graph alignment across up to eight distributed nodes encompassing 64 nodelets in the Chick system. We also define and justify relative metrics to compare prototype FPGA-based hardware with established ASIC architectures. The Chick currently supports up to 68x scaling for graph alignment, 80 MTEPS for BFS on balanced graphs, and 50\% of measured STREAM bandwidth for SpMV.},
}

@Article{Song2019,
  author      = {Lili Song and Luis Nunes Vicente},
  title       = {Modeling Hessian-vector products in nonlinear optimization: New Hessian-free methods},
  date        = {2019-12-22},
  eprint      = {1912.10523},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {In this paper, we suggest two ways of calculating interpolation models for unconstrained smooth nonlinear optimization when Hessian-vector products are available. The main idea is to interpolate the objective function using a quadratic on a set of points around the current one and concurrently using the curvature information from products of the Hessian times appropriate vectors, possibly defined by the interpolating points. These enriched interpolating conditions form then an affine space of model Hessians or model Newton directions, from which a particular one can be computed once an equilibrium or least secant principle is defined. A first approach consists of recovering the Hessian matrix satisfying the enriched interpolating conditions, from which then a Newton direction model can be computed. In a second approach we pose the recovery problem directly in the Newton direction. These techniques can lead to a significant reduction in the overall number of Hessian-vector products when compared to the inexact or truncated Newton method, although simple implementations may pay a cost in linear algebra or number of function evaluations.},
  file        = {:http\://arxiv.org/pdf/1912.10523v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Lin2019a,
  author      = {Lin Lin and Xiaojie Wu},
  title       = {Numerical solution of large scale {Hartree-Fock-Bogoliubov} equations},
  date        = {2019-12-21},
  eprint      = {1912.10157},
  eprinttype  = {arXiv},
  eprintclass = {physics.comp-ph},
  abstract    = {The Hartree-Fock-Bogoliubov (HFB) theory is the starting point for treating superconducting systems. However, the computational cost for solving large scale HFB equations can be much larger than that of the Hartree-Fock equations, particularly when the Hamiltonian matrix is sparse, and the number of electrons $N$ is relatively small compared to the matrix size $N_{b}$. We first provide a concise and relatively self-contained review of the HFB theory for general finite sized quantum systems, with special focus on the treatment of spin symmetries from a linear algebra perspective. We then demonstrate that the pole expansion and selected inversion (PEXSI) method can be particularly well suited for solving large scale HFB equations. For a Hubbard-type Hamiltonian, the cost of PEXSI is at most $O(N_b^2)$ for both gapped and gapless systems, which can be significantly faster than the standard cubic scaling diagonalization methods. We show that PEXSI can solve a two-dimensional Hubbard-Hofstadter model with $N_b$ up to $2.88\times 10^6$, and the wall clock time is less than $100$ s using $17280$ CPU cores. This enables the simulation of physical systems under experimentally realizable magnetic fields, which cannot be otherwise simulated with smaller systems.},
  file        = {:http\://arxiv.org/pdf/1912.10157v1:PDF},
  keywords    = {physics.comp-ph, cs.NA, math.NA},
}

@Article{Majumdar2020,
  author   = {Majumdar, Anirudha and Hall, Georgina and Ahmadi, Amir Ali},
  title    = {Recent Scalability Improvements for Semidefinite Programming with Applications in Machine Learning, Control, and Robotics},
  journal  = {Annual Review of Control, Robotics, and Autonomous Systems},
  year     = {2020},
  volume   = {3},
  number   = {1},
  doi      = {10.1146/annurev-control-091819-074326},
  eprint   = {https://doi.org/10.1146/annurev-control-091819-074326},
  abstract = {Historically, scalability has been a major challenge for the successful application of semidefinite programming in fields such as machine learning, control, and robotics. In this article, we survey recent approaches to this challenge, including those that exploit structure (e.g., sparsity and symmetry) in a problem, those that produce low-rank approximate solutions to semidefinite programs, those that use more scalable algorithms that rely on augmented Lagrangian techniques and the alternating-direction method of multipliers, and those that trade off scalability with conservatism (e.g., by approximating semidefinite programs with linear and second-order cone programs). For each class of approaches, we provide a high-level exposition, an entry point to the corresponding literature, and examples drawn from machine learning, control, or robotics. We also present a list of software packages that implement many of the techniques discussed in the review. Our hope is that this article will serve as a gateway to the rich and exciting literature on scalable semidefinite programming for both theorists and practitioners.},
}

@PhdThesis{Yin2019,
  author      = {Dong Yin},
  title       = {Towards More Scalable and Robust Machine Learning},
  institution = {Department of Electrical Engineering and Computer Sciences, Univerisity of California at Berkeley},
  year        = {2019},
  eprint      = {UCB/EECS-2019-175},
  url         = {https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-175.pdf},
  abstract    = {For many data-intensive real-world applications, such as recognizing objects from images, detecting spam emails, and recommending items on retail websites, the most successful current approaches involve learning rich prediction rules from large datasets. There are many challenges in these machine learning tasks. For example, as the size of the datasets and the complexity of these prediction rules increase, there is a significant challenge in designing scalable methods that can effectively exploit the availability of distributed computing units. As another example, in many machine learning applications, there can be data corruptions, communication errors, and even adversarial attacks during training and test. Therefore, to build reliable machine learning models, we also have to tackle the challenge of robustness in machine learning. \\ In this dissertation, we study several topics on the scalability and robustness in large-scale learning, with a focus of establishing solid theoretical foundations for these problems, and demonstrate recent progress towards the ambitious goal of building more scalable and robust machine learning models. We start with the speedup saturation problem in distributed stochastic gradient descent (SGD) algorithms with large mini-batches. We introduce the notion of gradient diversity, a metric of the dissimilarity between concurrent gradient updates, and show its key role in the convergence and generalization performance of mini-batch SGD. We then move forward to Byzantine distributed learning, a topic that involves both scalability and robustness in distributed learning. In the Byzantine setting that we consider, a fraction of distributed worker machines can have arbitrary or even adversarial behavior. We design statistically and computationally efficient algorithms to defend against Byzantine failures in distributed optimization with convex and non-convex objectives. Lastly, we discuss the adversarial example phenomenon. We provide theoretical analysis of the adversarially robust generalization properties of machine learning models through the lens of Radamacher complexity.},
}

@Article{Sundar2020,
  author      = {Kaarthik Sundar and Harsha Nagarajan and Site Wang and Jeff Linderoth and Russell Bent},
  title       = {Piecewise Polyhedral Formulations for a Multilinear Term},
  date        = {2020-01-02},
  eprint      = {2001.00514},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we present a mixed-integer linear programming formulation of a piecewise, polyhedral relaxation (PPR) of a multilinear term using it's convex hull representation. Based on the solution of the PPR, we also present a MIP-based piecewise formulation which restricts the solutions to be feasible for the multilinear term. We then present computational results showing the effectiveness of proposed formulations on instances from the standard Mixed-Integer Nonlinear Programming Library (MINLPLib) and compare the proposed formulation with a formulation that is built by recursively relaxing bilinear groupings of the multilinear term, typically applied in the literature.},
  file        = {:http\://arxiv.org/pdf/2001.00514v1:PDF},
  keywords    = {math.OC, cs.DM},
}

@Article{Wang2019b,
  author   = {Akang Wang and Chrysanthos E. Gounaris},
  title    = {On Tackling Reverse Convex Constraints for Non-overlapping of Circles},
  year     = {2019},
  abstract = {We study the circle-circle non-overlapping constraints, a form of reverse convex constraints that often arise in optimization models for cutting and packing applications. The feasible region induced by the intersection of circle-circle non-overlapping constraints is highly non-convex, and standard approaches to construct convex relaxations for spatial branch-and-bound global optimization of such models typically yield unsatisfactory loose relaxations. Consequently, solving such non-convex models to guaranteed optimality remains extremely challenging even for the state-of-the-art codes. In this paper, we apply a purpose-built branching scheme on non-overlapping constraints and utilize strengthened intersection cuts and various feasibility-based tightening techniques to further tighten the model relaxation. We embed these techniques into a branch-and-bound code and test them on two variants of circle packing problems. Our computational studies on a suite of 75 benchmark instances yielded, for the first time in the open literature, a total of 54 provably optimal solutions, and it was demonstrated to be competitive over the use of the state-of-the-art general-purpose global optimization solvers.},
}

@TechReport{Witzig2019,
  author      = {Jakob Witzig and Timo Berthold and Stefan Heinz},
  title       = {Computational Aspects of Infeasibility Analysis in Mixed Integer Programming},
  institution = {Zuse Institute Berlin},
  year        = {2019},
  number      = {19--54},
  abstract    = {The analysis of infeasible subproblems plays an important role in solving mixed integer programs (MIPs) and is implemented in most major MIP solvers. There are two fundamentally different concepts to generate valid global constraints from infeasible subproblems. The first is to analyze the sequence of implications, obtained by domain propagation, that led to infeasibility. The result of this analysis is one or more sets of contradicting variable bounds from which so-called conflict constraints can be generated. This concept is called conflict graph analysis and has its origin in solving satisfiability problems and is similarly used in constraint programming. The second concept is to analyze infeasible linear programming (LP) relaxations. Every ray of the dual LP provides a set of multipliers that can be used to generate a single new globally valid linear constraint. This method is called dual proof analysis. The main contribution of this paper is twofold. Firstly, we present three enhancements of dual proof analysis: presolving via variable cancellation, strengthening by applying mixed integer rounding functions, and a filtering mechanism. Further, we provide an intense computational study evaluating the impact of every presented component regarding dual proof analysis. Secondly, this paper presents the first integrated approach to use both conflict graph and dual proof analysis simultaneously within a single MIP solution process. All experiments are carried out on general MIP instances from the standard public test set MIPLIB 2017; the presented algorithms have been implemented within the non-commercial MIP solver SCIP and the commercial MIP solver FICO Xpress.},
}

@TechReport{Witzig2019a,
  author      = {Jakob Witzig and Timo Berthold},
  institution = {Zuse Institute Berlin},
  title       = {Conflict-Free Learning for Mixed Integer Programming},
  number      = {19--59},
  abstract    = {Conflict learning plays an important role in solving mixed integer programs (MIPs) and is implemented in most major MIP solvers. A major step for MIP conflict learning is to aggregate the LP relaxation of an infeasible subproblem to a single globally valid constraint, the dual proof, that proves infeasibility within the local bounds. Among others, one way of learning is to add these constraints to the problem formulation for the remainder of the search. We suggest to not restrict this procedure to infeasible subproblems, but to also use global proof constraints from subproblems that are not (yet) infeasible, but can be expected to be pruned soon. As a special case, we also consider learning from integer feasible LP solutions. First experiments of this conflict-free learning strategy show promising results on the MIPLIB2017 benchmark set.},
  year        = {2019},
}

@Article{Bonami2019,
  author   = {Pierre Bonami and Domenico Salvagnin and Andrea Tramontani},
  title    = {Implementing Automatic Benders Decomposition in a Modern {MIP} Solver},
  year     = {2019},
  abstract = {We describe the automatic Benders decomposition implemented in the commercial solver IBM CPLEX. We propose several improvements to the state-of-the-art along two lines: making a numerically robust method able to deal with the general case and improving the efficiency of the method on models amenable to decomposition. For the former, we deal with: unboundedness, failures in generating cuts and scaling of the artificial variable representing the objective. For the latter, we propose a new technique to handle so-called generalized bound constraints and we use different types of normalization conditions in the Cut Generating LPs. We present computational experiments aimed at assessing the importance of the various enhancements. In particular, on our test bed of models amenable to a decomposition, our implementation is approximately 5 times faster than CPLEX default branch-and-cut. A remarkable result is that, on the same test bed, default branch-and-cut is faster than a Benders decomposition that doesn't implement our improvements.},
}

@TechReport{Gleixner2019,
  author      = {Ambros Gleixner and Daniel E. Steffy},
  title       = {Linear Programming using Limited-Precision Oracles},
  institution = {Zuse Institute Berlin},
  year        = {2019},
  number      = {19--57},
  abstract    = {Since the elimination algorithm of Fourier and Motzkin, many different methods have been developed for solving linear programs. When analyzing the time complexity of LP algorithms, it is typically either assumed that calculations are performed exactly and bounds are derived on the number of elementary arithmetic operations necessary, or the cost of all arithmetic operations is considered through a bit-complexity analysis. Yet in practice, implementations typically use limited-precision arithmetic. In this paper we introduce the idea of a limited-precision LP oracle and study how such an oracle could be used within a larger framework to compute exact precision solutions to LPs. Under mild assumptions, it is shown that a polynomial number of calls to such an oracle and a polynomial number of bit operations, is sufficient to compute an exact solution to an LP. This work provides a foundation for understanding and analyzing the behavior of the methods that are currently most effective in practice for solving LPs exactly.},
}
%%
@Article{Fagnon2019,
  author      = {Vincent Fagnon and Imed Kacem and Giorgio Lucarelli and Bertrand Simon},
  title       = {Scheduling on Hybrid Platforms: Improved Approximability Window},
  date        = {2019-12-06},
  eprint      = {1912.03088},
  eprinttype  = {arXiv},
  eprintclass = {cs.DS},
  abstract    = {Modern platforms are using accelerators in conjunction with standard processing units in order to reduce the running time of specific operations, such as matrix operations, and improve their performance. Scheduling on such hybrid platforms is a challenging problem since the algorithms used for the case of homogeneous resources do not adapt well. In this paper we consider the problem of scheduling a set of tasks subject to precedence constraints on hybrid platforms, composed of two types of processing units. We propose a $(3+2\sqrt{2})$-approximation algorithm and a conditional lower bound of 3 on the approximation ratio. These results improve upon the 6-approximation algorithm proposed by Kedad-Sidhoum et al. as well as the lower bound of 2 due to Svensson for identical machines. Our algorithm is inspired by the former one and distinguishes the allocation and the scheduling phases. However, we propose a different allocation procedure which, although is less efficient for the allocation sub-problem, leads to an improved approximation ratio for the whole scheduling problem. This approximation ratio actually decreases when the number of processing units of each type is close and matches the conditional lower bound when they are equal.},
  file        = {:http\://arxiv.org/pdf/1912.03088v1:PDF},
  keywords    = {cs.DS},
}

@TechReport{Berry2019,
  author      = {Jonathan W. Berry and Neil Butcher and {\"U}mit V. Çatalyürek and Simon D. Hammond and Peter Kogge and Paul Lin and Stephen L. Olivier and Cynthia A. Phillips and Siva Rajamanickam and George M. Slota and Gwen R. Voskuilen and Abdurrahman Yaşar and Jefrey S Young},
  title       = {Multi-Level Memory Algorithmics for Large, Sparse Problems},
  institution = {Sandia National Laboratories},
  year        = {2019},
  number      = {SAND2019--13871},
  url         = {https://www.osti.gov/servlets/purl/1574408},
  abstract    = {In this report, we abstract eleven papers published during the project and describe preliminary unpublished results that warrant follow-up work. The topic is multi-level memory algorithmics, or how to efectively use multiple layers of main memory. Modern compute nodes all have this feature in some form.},
}

@Article{Zhang2019f,
  author      = {Thomas Zhang},
  title       = {Sparse Optimization on General Atomic Sets: Greedy and Forward-Backward Algorithms},
  date        = {2019-12-26},
  eprint      = {1912.11931},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We consider the problem of sparse atomic optimization, where the notion of "sparsity" is generalized to meaning some linear combination of few atoms. The definition of atomic set is very broad; popular examples include the standard basis, low-rank matrices, overcomplete dictionaries, permutation matrices, orthogonal matrices, etc. The model of sparse atomic optimization therefore includes problems coming from many fields, including statistics, signal processing, machine learning, computer vision and so on. Specifically, we consider the problem of maximizing a restricted strongly convex (or concave), smooth function restricted to a sparse linear combination of atoms. We extend recent work that establish linear convergence rates of greedy algorithms on restricted strongly concave, smooth functions on sparse vectors to the realm of general atomic sets, where the convergence rate involves a novel quantity: the "sparse atomic condition number". This leads to the strongest known multiplicative approximation guarantees for various flavors of greedy algorithms for sparse atomic optimization; in particular, we show that in many settings of interest the greedy algorithm can attain strong approximation guarantees while maintaining sparsity. Furthermore, we introduce a scheme for forward-backward algorithms that achieves the same approximation guarantees. Secondly, we define an alternate notion of weak submodularity, which we show is tightly related to the more familiar version that has been used to prove earlier linear convergence rates. We prove analogous multiplicative approximation guarantees using this alternate weak submodularity, and establish its distinct identity and applications.},
  file        = {:http\://arxiv.org/pdf/1912.11931v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@InProceedings{Thuerck2019,
  author    = {D. Thuerck},
  title     = {Stretching Jacobi: Two-Stage Pivoting in Block-Based Factorization},
  booktitle = {Proceedings of the 9th IEEE/ACM Workshop on Irregular Applications: Architectures and Algorithms},
  year      = {2019},
  series    = {IA3 '19},
  month     = {11},
  pages     = {51--58},
  doi       = {10.1109/IA349570.2019.00014},
  abstract  = {Solving numerically tough matrices often requires full pivoting and aggressive iterative refinement even with modern direct solvers. Iterative solvers and preconditioners, preferred in parallel computing often cannot keep up, especially novel, massively-parallel fixed-point methods. We show that even for tough, indefinite matrices, these methods can be an alternative by (a) using a blocked version and (b) introducing a data structure an algorithms for two level, global pivoting. Our approach allows register-based pivoting for high performance, batched CUDA kernels and, for the first time on GPUs, also flexible permutations on the block level. Our experiments show that these modifications help to mitigate the irregular computation stemming from pivoting. Our implementation generates fixed-point style preconditioners that can keep up with traditional, more accurate and static preconditioners - even for tough, indefinite systems.},
  issn      = {null},
}

@Article{Ye2019,
  author      = {Haishan Ye and Shusen Wang and Zhihua Zhang and Tong Zhang},
  title       = {Fast Generalized Matrix Regression with Applications in Machine Learning},
  date        = {2019-12-27},
  eprint      = {1912.12008},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {Fast matrix algorithms have become the fundamental tools of machine learning in big data era. The generalized matrix regression problem is widely used in the matrix approximation such as CUR decomposition, kernel matrix approximation, and stream singular value decomposition (SVD), etc. In this paper, we propose a fast generalized matrix regression algorithm (Fast GMR) which utilizes sketching technique to solve the GMR problem efficiently. Given error parameter $0 < \epsilon < 1$, the Fast GMR algorithm can achieve a $O(1+\epsilon)$ relative error with the sketching sizes being of order $O(\epsilon^{-1/2})$ for a large group of GMR problems. We apply the Fast GMR algorithm to the symmetric positive definite matrix approximation and single pass singular value decomposition and they achieve a better performance than conventional algorithms. Our empirical study also validates the effectiveness and efficiency of our proposed algorithms.},
  file        = {:http\://arxiv.org/pdf/1912.12008v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Slak2019,
  author      = {Jure Slak and Gregor Kosec},
  title       = {Medusa: A {C++} Library for solving {PDEs} using Strong Form Mesh-Free methods},
  date        = {2019-12-31},
  eprint      = {1912.13282},
  eprinttype  = {arXiv},
  eprintclass = {cs.MS},
  abstract    = {Medusa, a novel library for implementation of strong form mesh-free methods, is described. We identify and present common parts and patterns among many such methods reported in the literature, such as node positioning, stencil selection and stencil weight computation. Many different algorithms exist for each part and the possible combinations offer a plethora of possibilities for improvements of solution procedures that are far from fully understood. As a consequence there are still many unanswered questions in mesh-free community resulting in vivid ongoing research in the field. Medusa implements the core mesh-free elements as independent blocks, which offers users great flexibility in experimenting with the method they are developing, as well as easily comparing it with other existing methods. The paper describes the chosen abstractions and their usage, illustrates aspects of the philosophy and design, offers some executions time benchmarks and demonstrates the application of the library on cases from linear elasticity and fluid flow in irregular 2D and 3D domains.},
  file        = {:http\://arxiv.org/pdf/1912.13282v1:PDF},
  keywords    = {cs.MS, cs.NA, math.NA, 65M99, G.4},
}

@Article{ValeroLara2020,
  author   = {Pedro Valero-Lara and Sandra Catalán and Xavier Martorell and Tetsuzo Usui and Jesús Labarta},
  title    = {{sLASs}: A fully automatic auto-tuned linear algebra library based on OpenMP extensions implemented in {OmpSs} (LASs Library)},
  journal  = {Journal of Parallel and Distributed Computing},
  year     = {2020},
  issn     = {0743-7315},
  doi      = {https://doi.org/10.1016/j.jpdc.2019.12.002},
  url      = {http://www.sciencedirect.com/science/article/pii/S0743731519303417},
  abstract = {In this work we have implemented a novel Linear Algebra Library on top of the task-based runtime OmpSs-2. We have used some of the most advanced OmpSs-2 features; weak dependencies and regions, together with the final clause for the implementation of auto-tunable code for the BLAS-3 trsm routine and the LAPACK routines npgetrf and npgesv. All these implementations are part of the first prototype of sLASs library, a novel library for auto-tunable codes for linear algebra operations based on LASs library. In all these cases, the use of the OmpSs-2 features presents an improvement in terms of execution time against other reference libraries such as, the original LASs library, PLASMA, ATLAS and Intel MKL. These codes are able to reduce the execution time in about 18\% on big matrices, by increasing the IPC on gemm and reducing the time of task instantiation. For a few medium matrices, benefits are also seen. For small matrices and a subset of medium matrices, specific optimizations that allow to increase the degree of parallelism in both, gemm and trsm tasks, are applied. This strategy achieves an increment in performance of up to 40\%.},
  keywords = {LASs, OmpSs, Linear algebra, Auto-tuning},
}

@Article{Madani2020,
  author   = {Ramtin Madani and Mohsen Kheirandishfard and Javad Lavaei and Alper Atamturk},
  title    = {Penalized Semidefinite Programming for QuadraticallyConstrained Quadratic Optimization},
  year     = {2020},
  url      = {https://www.ocf.berkeley.edu/~madani/paper/penalized_sdp.pdf},
  abstract = {In this paper, we give a new penalized semidefinite programming approach for non-convex quadratically-constrained quadratic programs (QCQPs). We incorporate penalty terms into the objective of convex relaxations in order to retrieve feasible and near-optimal solutions for non-convex QCQPs. We introduce a generalized linear independence constraint qualification (GLICQ) criterion and prove that any GLICQ regular point that is sufficiently close to the feasible set can be used to construct an appropriate penalty term and recover a feasible solution. As a consequence, we describe a heuristic sequential procedure that preserves feasibility and aims to improve the  bjective value at each iteration. Numerical experiments on large-scale system identification problems as well as benchmark instances from the library of quadratic programming (QPLIB) demonstrate the ability of the proposed penalized semidefinite programs in finding near-optimal solutions for non-convex QCQP.},
}

@InProceedings{Ellis2019,
  author    = {Marquita Ellis and Giulia Guidi and Aydın Buluç and Leonid Oliker and Katherine Yelick},
  title     = {{diBELLA}: Distributed Long Read to Long Read Alignment},
  booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
  year      = {2019},
  series    = {ICPP 2019},
  publisher = {{ACM} Press},
  doi       = {10.1145/3337821.3337919},
  abstract  = {We present a parallel algorithm and scalable implementation for genome analysis, specifically the problem of finding overlaps and alignments for data from “third generation” long read sequencers [29]. While long sequences of DNA offer enormous advantages for biological analysis and insight, current long read sequencing instruments have high error rates and therefore require different approaches to analysis than their short read counterparts. Our work focuses on an efficient distributed-memory parallelization of an accurate single-node algorithm for overlapping and aligning long reads. We achieve scalability of this irregular algorithm by addressing the competing issues of increasing parallelism, minimizing communication, constraining the memory footprint, and ensuring good load balance. The resulting application, diBELLA, is the first distributed memory overlapper and aligner specifically designed for long reads and parallel scalability. We describe and present analyses for high level design trade-offs and conduct an extensive empirical analysis that compares performance characteristics across state-of-the-art HPC systems as well as a commercial cloud architectures, highlighting the advantages of state-of-the-art network technologies.},
}

@Article{Allman2020,
  author      = {Andrew Allman and Qi Zhang},
  title       = {Branch-and-Price for a Class of Nonconvex Mixed-Integer Nonlinear Programs},
  date        = {2020-01-06},
  eprint      = {2001.01794},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This work attempts to combine the strengths of two major technologies that have matured over the last three decades: global mixed-integer nonlinear optimization and branch-and-price. We consider a class of generally nonconvex mixed-integer nonlinear programs (MINLPs) with linear complicating constraints and integer linking variables. If the complicating constraints are removed, the problem becomes easy to solve, e.g. due to decomposable structure. Integrality of the linking variables allows us to apply a discretization approach to derive a Dantzig-Wolfe reformulation and solve the problem to global optimality using branch-and-price. It is a remarkably simple idea; but to our surprise, it has barely found any application in the literature. In this work, we show that many relevant problems directly fall or can be reformulated into this class of MINLPs. We present the branch-and-price algorithm and demonstrate its effectiveness (and sometimes ineffectiveness) in an extensive computational study considering multiple large-scale problems of practical relevance, showing that, in many cases, orders-of-magnitude reductions in solution time can be achieved.},
  file        = {:http\://arxiv.org/pdf/2001.01794v1:PDF},
  keywords    = {math.OC},
}

@Article{Li2020,
  author      = {Jiajia Li and Mahesh Lakshminarasimhan and Xiaolong Wu and Ang Li and Catherine Olschanowsky and Kevin Barker},
  title       = {A Parallel Sparse Tensor Benchmark Suite on {CPUs} and {GPUs}},
  date        = {2020-01-02},
  eprint      = {2001.00660},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Tensor computations present significant performance challenges that impact a wide spectrum of applications ranging from machine learning, healthcare analytics, social network analysis, data mining to quantum chemistry and signal processing. Efforts to improve the performance of tensor computations include exploring data layout, execution scheduling, and parallelism in common tensor kernels. This work presents a benchmark suite for arbitrary-order sparse tensor kernels using state-of-the-art tensor formats: coordinate (COO) and hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of reference tensor kernel implementations that are compatible with real-world tensors and power law tensors extended from synthetic graph generation techniques. We also propose Roofline performance models for these kernels to provide insights of computer platforms from sparse tensor view.},
  file        = {:http\://arxiv.org/pdf/2001.00660v1:PDF},
  keywords    = {cs.DC, cs.PF},
}

@InProceedings{Kedward2020,
  author    = {Laurence Kedward and Christian B. Allen and T. Rendall},
  title     = {Comparing Matrix-based and Matrix-free Discrete Adjoint Approaches to the Euler Equations},
  booktitle = {{AIAA} Scitech 2020 Forum},
  year      = {2020},
  publisher = {American Institute of Aeronautics and Astronautics},
  month     = {1},
  doi       = {10.2514/6.2020-1294},
  abstract  = {Detail is presented on the implementation of numerical derivatives with focus given to the discrete adjoint equations. Two approaches are considered: a hybrid matrix-based scheme where the convective Jacobian is constructed explicitly; and a matrix-free method using reverse-mode automatic differentiation. The hybrid matrix-based scheme exploits a compact convective stencil using graph colouring to evaluate the convective Jacobian terms in $O(10)$ residual evaluations. Jacobian terms, grouped by colours, are evaluated using the complex step tangent model; this approach requires no external libraries or tools, minimal code modification and provides derivatives accurate to machine precision. The remaining artificial dissipation terms are trivial to differentiate by hand where the sensor coefficients are held constant. The hybrid matrix-based methodology is validated and compared with the `traditional` matrix-free approach using reverse-mode automatic differentiation. The adjoint equations using both approaches are solved using the same fixed-point Runge-Kutta iteration accelerated by agglomeration multigrid. No loss in accuracy is seen between the matrix-based and the matrix-free methods when validated with the complex step tangent model. The hybrid matrix-based approach demonstrates a notable runtime performance advantage over the traditional matrix-free approach due to the prior calculation of Jacobian terms. Moreover, the convective Jacobian calculation takes less than $5\%$ of primal runtime due to the compact stencil used. A critical analysis of the results and methodology is consequently presented, focusing on the general applicability of the hybrid approach to more complex problems.},
}

@Article{Bergamaschi2020,
  author      = {Luca Bergamaschi and Jose Marin and Angeles Martinez},
  title       = {Compact Quasi-Newton preconditioners for {SPD} linear systems},
  date        = {2020-01-04},
  eprint      = {2001.01062},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {In this paper preconditioners for the Conjugate Gradient method are studied to solve the Newton system with symmetric positive definite Jacobian. In particular, we define a sequence of preconditioners built by means of SR1 and BFGS low-rank updates. We develop conditions under which the SR1 update maintains the preconditioner SPD. Spectral analysis of the SR1 preconditioned Jacobians shows an improved eigenvalue distribution as the Newton iteration proceeds. A compact matrix formulation of the preconditioner update is developed which reduces the cost of its application and is more suitable for parallel implementation. Some notes on the implementation of the corresponding Inexact Newton method are given and numerical results on a number of model problems illustrate the efficiency of the proposed preconditioners.},
  file        = {:http\://arxiv.org/pdf/2001.01062v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Morinigo2020,
  author    = {José A. Moríñigo and Pablo Garcı́a-Muller and Antonio J. Rubio-Montero and Antonio Gómez-Iglesias and Norbert Meyer and Rafael Mayo-Garcı́a},
  title     = {Performance drop at executing communication-intensive parallel algorithms},
  journal   = {The Journal of Supercomputing},
  year      = {2020},
  month     = {1},
  doi       = {10.1007/s11227-019-03142-8},
  abstract  = {This work summarizes the results of a set of executions completed on three fat-tree network supercomputers: Stampede at TACC (USA), Helios at IFERC (Japan) and Eagle at PSNC (Poland). Three MPI-based, communication-intensive scientific applications compiled for CPUs have been executed under weak-scaling tests: the molecular dynamics solver LAMMPS; the finite element-based mini-kernel miniFE of NERSC (USA); and the three-dimensional fast Fourier transform mini-kernel bigFFT of LLNL (USA). The design of the experiments focuses on the sensitivity of the applications to rather different patterns of task location, to assess the impact on the cluster performance. The accomplished weak-scaling tests stress the effect of the MPI-based application mappings (concentrated vs. distributed patterns of MPI tasks over the nodes) on the cluster. Results reveal that highly distributed task patterns may imply a much larger execution time in scale, when several hundreds or thousands of MPI tasks are involved in the experiments. Such a characterization serves users to carry out further, more efficient executions. Also researchers may use these experiments to improve their scalability simulators. In addition, these results are useful from the clusters administration standpoint since tasks mapping has an impact on the cluster throughput.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Mukunoki2020,
  author    = {Daichi Mukunoki and Takeshi Ogita},
  title     = {Performance and energy consumption of accurate and mixed-precision linear algebra kernels on {GPUs}},
  journal   = {Journal of Computational and Applied Mathematics},
  year      = {2020},
  month     = {1},
  pages     = {112701},
  doi       = {10.1016/j.cam.2019.112701},
  abstract  = {This paper presents the implementation, performance, and energy consumption of accurate and mixed-precision linear algebra kernels, including inner-product (DOT), dense matrix–vector multiplication (GEMV), dense matrix multiplication (GEMM), and sparse matrix–vector multiplication (SpMV) for the compressed sparse row (CSR) format (CSRMV), on graphics processing units (GPUs). We employ a mixed-precision design in our implementation, which makes it possible to perform internal floating-point operations with at least 2-fold the precision of the input and output data precision: for binary32 data, the computation is performed on binary64, and for binary64 data, the computation is performed on 2-fold the precision with an accurate inner product algorithm referred to as Dot2. We developed highly optimized implementations which can achieve performance close to the upper bound performance. From our evaluation on Titan V, a Volta architecture GPU, we made the following observations: as the Dot2 operation consumes 11 times binary64 instructions, GEMM requires the corresponding overheads (in terms of both execution time and energy consumption), compared to the standard binary64 implementation. On the other hand, the accuracy of DOT, GEMV, and CSRMV is improved with a very small overhead to the execution time and up to roughly 30\% overhead to the energy requirement.},
  publisher = {Elsevier {BV}},
}

@InCollection{Anikin2020,
  author    = {Anton Anikin and Yuriy Dorn and Yurii Nesterov},
  title     = {Computational Methods for the Stable Dynamic Model},
  booktitle = {Communications in Computer and Information Science},
  year      = {2020},
  publisher = {Springer International Publishing},
  pages     = {280--294},
  doi       = {10.1007/978-3-030-38603-0_21},
  abstract  = {Traffic assignment problem is one of the central problems in transportation science. Various model assumptions lead to different setups corresponding to nonlinear optimization problems.\\ In this work, we focus on the stable dynamic model and its generalizations. We propose new equivalent representation for stable dynamic model [Nesterov and de Palma, 2003]. We use smoothing technique to derive new model, which can be interpreted as a stochastic equilibrium model.},
}

@Article{Liers2020,
  author   = {Frauke Liers and Alexander Martin and Maximilian Merkert and Nick Mertens Dennis Michaels},
  title    = {Towards the Solution of Mixed-Integer Nonlinear Optimization Problems using Simultaneous Convexification},
  year     = {2020},
  abstract = {Solving mixed-integer nonlinear optimization problems (MINLPs) to global optimality is extremely challenging. An important step for enabling their solution consists in the design of convex relaxations of the feasible set. Known solution approaches based on spatial branch-and-bound become more effective the tighter the used relaxations are. Relaxations are commonly established by convex underestimators, where each constraint function is considered separately. Instead, a considerably tighter relaxation can be found via so-called simultaneous convexification, where convex underestimators are derived for more than one constraint at a time. In this work, we present a global solution approach for solving mixed-integer nonlinear problems that uses simultaneous convexification. We introduce a separation method for the convex hull of constrained sets. It relies on determining the convex envelope of linear combinations of the constraints and on solving a nonsmooth convex problem. In particular, we apply the method to quadratic absolute value functions and derive their convex envelopes. The practicality of the proposed solution approach is demonstrated on several test instances from gas network optimization, where the method outperforms standard approaches that use separate convex relaxations.},
}

@InProceedings{Laberge2020,
  author    = {Laberge, G. and Shirzad, S. and Diehl, P. and Kaiser, H. and Prudhomme, S. and Lemoine, A. S.},
  title     = {Scheduling Optimization of Parallel Linear Algebra Algorithms Using Supervised Learning},
  booktitle = {Proceedings of the 2019 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments},
  year      = {2019},
  series    = {MLHPC '19},
  month     = {11},
  pages     = {31--43},
  doi       = {10.1109/MLHPC49564.2019.00009},
  abstract  = {Linear algebra algorithms are used widely in a variety of domains, e.g machine learning, numerical physics and video games graphics. For all these applications, loop-level parallelism is required to achieve high performance. However, finding the optimal way to schedule the workload between threads is a non-trivial problem because it depends on the structure of the algorithm being parallelized and the hardware the executable is run on. In the realm of Asynchronous Many Task runtime systems, a key aspect of the scheduling problem is predicting the proper chunk-size, where the chunk-size is defined as the number of iterations of a for-loop are assigned to a thread as one task. In this paper, we study the applications of supervised learning models to predict the chunk-size which yields maximum performance on multiple parallel linear algebra operations using the HPX backend of Blaze's linear algebra library. More precisely, we generate our training and tests sets by measuring performance of the application with different chunk-sizes for multiple linear algebra operations; vector-addition, matrix-vector-multiplication, matrix-matrix addition and matrix-matrix-multiplication. We compare the use of logistic regression, neural networks and decision trees with a newly developed decision tree based model in order to predict the optimal value for chunk-size. Our results show that classical decision trees and our custom decision tree model are able to forecast a chunk-size which results in good performance for the linear algebra operations.},
}

@Article{Iakymchuk2020,
  author     = {Roman Iakymchuk and Maria Barreda and Stef Graillat and José Aliaga and Enrique Quintana-Ortí},
  title      = {Reproducibility of Parallel Preconditioned Conjugate Gradient in Hybrid Programming Environments},
  eprint     = {02427795},
  eprinttype = {HAL},
  url        = {https://hal.archives-ouvertes.fr/hal-02427795},
  abstract   = {The Preconditioned Conjugate Gradient method is often employed for the solution of linear systems of equations arising in numerical simulations of physical phenomena. While being widely used, the solver is also known for its lack of accuracy while computing the residual. In this article, we propose two algorithmic solutions that originate from the ExBLAS project to enhance the accuracy of the solver as well as to ensure its reproducibility in a hybrid MPI + OpenMP tasks programming environment. One is based on ExBLAS and preserves every bit of information until the final rounding, while the other relies upon floating-point expansions and, hence, expands the intermediate precision. Instead of converting the entire solver into its ExBLAS-related implementation, we identify those parts that violate reproducibility/non-associativity, secure them, and combine this with the sequential executions. These algorithmic strategies are reinforced with programmability suggestions to assure deterministic executions. Finally, we verify these approaches on two modern HPC systems: both versions deliver reproducible number of iterations, residuals, direct errors, and vector-solutions for the overhead of less than 37.7 \% on 768 cores.},
  year       = {2020},
}

@Article{Chou2020,
  author      = {Stephen Chou and Fredrik Kjolstad and Saman Amarasinghe},
  title       = {Automatic Generation of Efficient Sparse Tensor Format Conversion Routines},
  date        = {2020-01-08},
  eprint      = {2001.02609},
  eprinttype  = {arXiv},
  eprintclass = {cs.MS},
  abstract    = {This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) like CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor's nonzeros in memory. This enables a compiler to emit code that performs complex reorderings (remappings) of nonzeros when converting between formats. We additionally develop a query language that can extract complex statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of a wide range of formats without hard-coding for any specific one. Our evaluation shows that our technique generates sparse tensor conversion routines with performance between 0.99 and 2.2$\times$ that of hand-optimized implementations in two widely used sparse linear algebra libraries, SPARSKIT and Intel MKL. By emitting code that avoids materializing temporaries, our technique also outperforms both libraries by between 1.4 and 3.4$\times$ for CSC/COO to DIA/ELL conversion.},
  file        = {:http\://arxiv.org/pdf/2001.02609v1:PDF},
  keywords    = {cs.MS, cs.PL},
}

@InProceedings{Antonakopoulos2020,
  author    = {Kimon Antonakopoulos and E. Veronica Belmega and Panayotis Mertikopoulos},
  title     = {Online and Stochastic Optimization beyond Lipschitz Continuity: A Riemannian Approach},
  booktitle = {Proceedings of the 8th International Conference on Learning Representations},
  year      = {2020},
  series    = {ICLR 2020},
  abstract  = {Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann–Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem’s loss functions. In this way, we are able to tackle cases beyond the Lipschitz framework provided by a global norm, and we derive optimal regret bounds and last iterate convergence results through the use of regularized learning methods (such as online mirror descent). These results are subsequently validated in a class of stochastic Poisson inverse problems that arise in imaging science.},
}

@Article{Attouch2020,
  author   = {Hedy Attouch and Zaki Chbani and Hassan Riahi},
  title    = {Fast Convex Optimization Via a Third-Order In Time Evolution Equation},
  year     = {2020},
  abstract = {In a Hilbert space H, we develop fast convex optimization methods, which are based on a third order in time evolution system. The function to minimize $f : \mathcal{H} \rightarrow \mathbb{R}$ is convex, continuously differentiable, with $\text{argmin} f \neq \emptyset$, and enters the dynamic via its gradient. On the basis of Lyapunov’s analysis and temporal scaling techniques, we show a convergence rate of the values of the order $\nicefrac{1}{t^3}$ , and obtain the convergence of the trajectories towards optimal solutions. When $f$ is strongly convex, an exponential rate of convergence is obtained. We complete the study of the continuous dynamic by introducing a damping term induced by the Hessian of $f$. This allows the oscillations to be controlled and attenuated. Then, we analyze the convergence of the proximal-based algorithms obtained by temporal discretization of this system, and obtain similar convergence rates. The algorithmic results are valid for a general convex, lower semicontinuous, and proper function $f : \mathcal{H} \rightarrow \mathbb{R} \cup \{ +\infty \}$.},
}

@Article{Mo2020,
  author    = {Tieqiang Mo and Renfa Li},
  title     = {Iteratively solving sparse linear system based on {PaRSEC} task scheduling},
  journal   = {The International Journal of High Performance Computing Applications},
  year      = {2020},
  month     = {1},
  pages     = {109434201989999},
  doi       = {10.1177/1094342019899997},
  abstract  = {With the new architecture and new programming paradigms such as task-based scheduling emerging in the parallel high performance computing area, it is of great importance to utilize these features to tune the monolithic computing codes. In this article, the classical conjugate gradient algorithms targeting at sparse linear system $Ax = b$ in Krylov subspace are pipelining to execute interdependent tasks on Parallel Runtime Scheduling and Execution Controller (PaRSEC) runtime. Firstly, the sparse matrix $A$ is split in rows to unfold more coarse-grained parallelism. Secondly, the partitioned sub-vectors are not assembled into one full vector in RAM to run sparse matrix--vector product (SpMV) operations for eliminating the communication overhead. Moreover, in the SpMV computation, if all elements of one column in the split sub-matrix are zeros, the corresponding product operations of these elements may be removed by reorganizing sub-vectors. Finally, the latency of migrating sub-vector is partially overlapped by the duration of performing SpMV operations through the further splitting in columns of sparse matrix on GPUs. In experiments, a series of tests demonstrate that optimal speedup and higher pipelining efficiency has been achieved for the pipelined task scheduling on PaRSEC runtime. Fusing SpMV concurrency and dot product pipelining can achieve higher speedup and efficiency.},
  publisher = {{SAGE} Publications},
}

@Article{Porcelli2020,
  author      = {Margherita Porcelli and Philippe L. Toint},
  title       = {Global and local information in structured derivative free optimization with {BFO}},
  date        = {2020-01-14},
  eprint      = {2001.04801},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {A structured version of derivative-free random pattern search optimization algorithms is introduced which is able to exploit coordinate partially separable structure (typically associated with sparsity) often present in unconstrained and bound-constrained optimization problems. This technique improves performance by orders of magnitude and makes it possible to solve large problems that otherwise are totally intractable by other derivative-free methods. A library of interpolation-based modelling tools is also described, which can be associated to the structured or unstructured versions of the initial BFO pattern search algorithm. The use of the library further enhances performance, especially when associated with structure. The significant gains in performance associated with these two techniques are illustrated using a new freely-available release of BFO which incorporates them. A interesting conclusion of the results presented is that providing global structural information on a problem can result in significantly less evaluations of the objective function than attempting to building local Taylor-like models.},
  file        = {:http\://arxiv.org/pdf/2001.04801v1:PDF},
  keywords    = {math.OC, 65K05, 90C56, 90C90},
}

@Article{Bramas2020,
  author    = {Bérenger Bramas and Alain Ketterlin},
  title     = {Improving parallel executions by increasing task granularity in task-based runtime systems using acyclic {DAG} clustering},
  journal   = {{PeerJ} Computer Science},
  year      = {2020},
  volume    = {6},
  month     = {1},
  pages     = {e247},
  doi       = {10.7717/peerj-cs.247},
  abstract  = {The task-based approach is a parallelization paradigm in which an algorithm is transformed into a direct acyclic graph of tasks: the vertices are computational elements extracted from the original algorithm and the edges are dependencies between those. During the execution, the management of the dependencies adds an overhead that can become significant when the computational cost of the tasks is low. A possibility to reduce the makespan is to aggregate the tasks to make them heavier, while having fewer of them, with the objective of mitigating the importance of the overhead. In this paper, we study an existing clustering/partitioning strategy to speed up the parallel execution of a task-based application. We provide two additional heuristics to this algorithm and perform an in-depth study on a large graph set. In addition, we propose a new model to estimate the execution duration and use it to choose the proper granularity. We show that this strategy allows speeding up a real numerical application by a factor of 7 on a multi-core system.},
  publisher = {{PeerJ}},
}

@Article{Chen2020,
  author    = {Yongyong Chen and Shuqin Wang and Fangying Zheng and Yigang Cen},
  title     = {Graph-regularized least squares regression for multi-view subspace clustering},
  journal   = {Knowledge-Based Systems},
  year      = {2020},
  month     = {1},
  pages     = {105482},
  doi       = {10.1016/j.knosys.2020.105482},
  abstract  = {Many works have proven that the consistency and differences in multi-view subspace clustering make the clustering results better than the single-view clustering. Therefore, this paper studies the multi-view clustering problem, which aims to divide data points into several groups using multiple features. However, existing multi-view clustering methods fail to capturing the grouping effect and local geometrical structure of the multiple features. In order to solve these problems, this paper proposes a novel multi-view subspace clustering model called graph-regularized least squares regression (GLSR), which uses not only the least squares regression instead of the nuclear norm to generate grouping effect, but also the manifold constraint to preserve the local geometrical structure of multiple features. Specifically, the proposed GLSR method adopts the least squares regression to learn the globally consensus information shared by multiple views and the column-sparsity norm to measure the residual information. Under the alternating direction method of multipliers framework, an effective method is developed by iteratively update all variables. Numerical studies on eight real databases demonstrate the effectiveness and superior performance of the proposed GLSR over eleven state-of-the-art methods.},
  publisher = {Elsevier {BV}},
}

@Article{Jakovetic2020,
  author      = {Dusan Jakovetic and Natasa Krejic and Natasa Krklec Jerinkic and Greta Malaspina and Alessandra Micheletti},
  title       = {Distributed Fixed Point Method for Solving Systems of Linear Algebraic Equations},
  date        = {2020-01-12},
  eprint      = {2001.03968},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We present a class of iterative fully distributed fixed point methods to solve a system of linear equations, such that each agent in the network holds one of the equations of the system. Under a generic directed, strongly connected network, we prove a convergence result analogous to the one for fixed point methods in the classical, centralized, framework: the proposed method converges to the solution of the system of linear equations at a linear rate. We further explicitly quantify the rate in terms of the linear system and the network parameters. Next, we show that the algorithm provably works under time-varying directed networks provided that the underlying graph is connected over bounded iteration intervals, and we establish a linear convergence rate for this setting as well. A set of numerical results is presented, demonstrating practical benefits of the method over existing alternatives.},
  file        = {:http\://arxiv.org/pdf/2001.03968v1:PDF},
  keywords    = {math.NA, cs.NA, math.OC},
}

@InProceedings{Cao2019,
  author    = {Quinglei Cao and Yu Pei and Thomas Herauldt and Kadir Akbudak and Aleksandr Mikhalev and George Bosilca and Hatem Ltaief and David Keyes and Jack Dongarra},
  title     = {Performance Analysis of Tile Low-Rank Cholesky Factorization Using {PaRSEC} Instrumentation Tools},
  booktitle = {Proceedings of the 2019 {IEEE}/{ACM} International Workshop on Programming and Performance Visualization Tools},
  year      = {2019},
  series    = {ProTools '19},
  publisher = {{IEEE}},
  month     = {11},
  doi       = {10.1109/protools49597.2019.00009},
  abstract  = {This paper highlights the necessary development of new instrumentation tools within the PaRSE task-based runtime system to leverage the performance of low-rank matrix computations. In particular, the tile low-rank (TLR) Cholesky factorization represents one of the most critical matrix operations toward solving challenging large-scale scientific applications. The challenge resides in the heterogeneous arithmetic intensity of the various computational kernels, which stresses PaRSE's dynamic engine when orchestrating the task executions at runtime. Such irregular workload imposes the deployment of new scheduling heuristics to privilege the critical path, while exposing task parallelism to maximize hardware occupancy. To measure the effectiveness of PaRSE's engine and its various scheduling strategies for tackling such workloads, it becomes paramount to implement adequate performance analysis and profiling tools tailored to fine-grained and heterogeneous task execution. This permits us not only to provide insights from PaRSE, but also to identify potential applications' performance bottlenecks. These instrumentation tools may actually foster synergism between applications and PaRSE developers for productivity as well as high-performance computing purposes. We demonstrate the benefits of these amenable tools, while assessing the performance of TLR Cholesky factorization from data distribution, communication-reducing and synchronization-reducing perspectives. This tool-assisted performance analysis results in three major contributions: a new hybrid data distribution, a new hierarchical TLR Cholesky algorithm, and a new performance model for tuning the tile size. The new TLR Cholesky factorization achieves an 8X performance speedup over existing implementations on massively parallel supercomputers, toward solving large-scale 3D climate and weather prediction applications.},
}

@Article{Chen2019,
  author      = {Long Chen and Hao Luo},
  title       = {First order optimization methods based on Hessian-driven Nesterov accelerated gradient flow},
  date        = {2019-12-19},
  eprint      = {1912.09276},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {A novel dynamical inertial Newton system, which is called Hessian-driven Nesterov accelerated gradient (H-NAG) flow is proposed. Convergence of the continuous trajectory are established via tailored Lyapunov function, and new first-order accelerated optimization methods are proposed from ODE solvers. It is shown that (semi-)implicit schemes can always achieve linear rate and explicit schemes have the optimal(accelerated) rates for convex and strongly convex objectives. In particular, Nesterov's optimal method is recovered from an explicit scheme for our H-NAG flow. Furthermore, accelerated splitting algorithms for composite optimization problems are also developed.},
  file        = {:http\://arxiv.org/pdf/1912.09276v2:PDF},
  keywords    = {math.OC},
}

@Article{Constantinides2020,
  author    = {G. A. Constantinides},
  title     = {Rethinking arithmetic for deep neural networks},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190051},
  doi       = {10.1098/rsta.2019.0051},
  abstract  = {We consider efficiency in the implementation of deep neural networks. Hardware accelerators are gaining interest as machine learning becomes one of the drivers of high-performance computing. In these accelerators, the directed graph describing a neural network can be implemented as a directed graph describing a Boolean circuit. We make this observation precise, leading naturally to an understanding of practical neural networks as discrete functions, and show that the so-called binarized neural networks are functionally complete. In general, our results suggest that it is valuable to consider Boolean circuits as neural networks, leading to the question of which circuit topologies are promising. We argue that continuity is central to generalization in learning, explore the interaction between data coding, network topology, and node functionality for continuity and pose some open questions for future research. As a first step to bridging the gap between continuous and Boolean views of neural network accelerators, we present some recent results from our work on LUTNet, a novel Field-Programmable Gate Array inference approach. Finally, we conclude with additional possible fruitful avenues for research bridging the continuous and discrete views of neural networks. \\This article is part of a discussion meeting issue "Numerical algorithms for high-performance computational science".},
  publisher = {The Royal Society},
}

@Article{Hey2020,
  author    = {Tony Hey and Keith Butler and Sam Jackson and Jeyarajan Thiyagalingam},
  title     = {Machine learning and big scientific data},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190054},
  doi       = {10.1098/rsta.2019.0054},
  abstract  = {This paper reviews some of the challenges posed by the huge growth of experimental data generated by the new generation of large-scale experiments at UK national facilities at the Rutherford Appleton Laboratory (RAL) site at Harwell near Oxford. Such ‘Big Scientific Data’ comes from the Diamond Light Source and Electron Microscopy Facilities, the ISIS Neutron and Muon Facility and the UK's Central Laser Facility. Increasingly, scientists are now required to use advanced machine learning and other AI technologies both to automate parts of the data pipeline and to help find new scientific discoveries in the analysis of their data. For commercially important applications, such as object recognition, natural language processing and automatic translation, deep learning has made dramatic breakthroughs. Google's DeepMind has now used the deep learning technology to develop their AlphaFold tool to make predictions for protein folding. Remarkably, it has been able to achieve some spectacular results for this specific scientific problem. Can deep learning be similarly transformative for other scientific problems? After a brief review of some initial applications of machine learning at the RAL, we focus on challenges and opportunities for AI in advancing materials science. Finally, we discuss the importance of developing some realistic machine learning benchmarks using Big Scientific Data coming from several different scientific domains. We conclude with some initial examples of our ‘scientific machine learning’ benchmark suite and of the research challenges these benchmarks will enable. \\ This article is part of a discussion meeting issue ‘Numerical algorithms for high-performance computational science’.},
  publisher = {The Royal Society},
}

@Article{Carson2020,
  author    = {Carson, Erin and Strakoš, Zdeněk},
  title     = {On the cost of iterative computations},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190050},
  doi       = {10.1098/rsta.2019.0050},
  abstract  = {With exascale-level computation on the horizon, the art of predicting the cost of computations has acquired a renewed focus. This task is especially challenging in the case of iterative methods, for which convergence behaviour often cannot be determined with certainty a priori (unless we are satisfied with potentially outrageous overestimates) and which typically suffer from performance bottlenecks at scale due to synchronization cost. Moreover, the amplification of rounding errors can substantially affect the practical performance, in particular for methods with short recurrences. In this article, we focus on what we consider to be key points which are crucial to understanding the cost of iteratively solving linear algebraic systems. This naturally leads us to questions on the place of numerical analysis in relation to mathematics, computer science and sciences, in general.},
  publisher = {The Royal Society},
}

@Article{Navarro2020,
  author      = {Cristóbal A. Navarro and Roberto Carrasco and Ricardo J. Barrientos and Javier A. Riquelme and Raimundo Vega},
  title       = {{GPU} Tensor Cores for fast Arithmetic Reductions},
  date        = {2020-01-15},
  eprint      = {2001.05585},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {This work proposes a GPU tensor core approach that encodes the arithmetic reduction of $n$ numbers as a set of chained $m \times m$ matrix multiply accumulate (MMA) operations executed in parallel by GPU tensor cores. The asymptotic running time of the proposed chained tensor core approach is $T(n)=5 log_{m^2}{n}$ and its speedup is $S=\dfrac{4}{5} log_{2}{m^2}$ over the classic $O(n \log n)$ parallel reduction algorithm. Experimental performance results show that the proposed reduction method is $\sim 3.2 \times$ faster than a conventional GPU reduction implementation, and preserves the numerical precision because the sub-results of each chain of $R$ MMAs is kept as a 32-bit floating point value, before being all reduced into as a final 32-bit result. The chained MMA design allows a flexible configuration of thread-blocks; small thread-blocks of 32 or 128 threads can still achieve maximum performance using a chain of $R=4,5$ MMAs per block, while large thread-blocks work best with $R=1$. The results obtained in this work show that tensor cores can indeed provide a significant performance improvement to non-Machine Learning applications such as the arithmetic reduction, which is an integration tool for studying many scientific phenomena.},
  file        = {:http\://arxiv.org/pdf/2001.05585v1:PDF},
  keywords    = {cs.DC},
}

@Article{Domingos2020,
  author      = {João Domingos and José M. F. Moura},
  title       = {Graph Fourier Transform: A Stable Approximation},
  date        = {2020-01-14},
  eprint      = {2001.05042},
  eprinttype  = {arXiv},
  eprintclass = {eess.SP},
  abstract    = {In Graph Signal Processing (GSP), data dependencies are represented by a graph whose nodes label the data and the edges capture dependencies among nodes. The graph is represented by a weighted adjacency matrix $A$ that, in GSP, generalizes the Discrete Signal Processing (DSP) shift operator $z^{-1}$. The (right) eigenvectors of the shift $A$ (graph spectral components) diagonalize $A$ and lead to a graph Fourier basis $F$ that provides a graph spectral representation of the graph signal. The inverse of the (matrix of the) graph Fourier basis $F$ is the Graph Fourier transform (GFT), $F^{-1}$. Often, including in real world examples, this diagonalization is numerically unstable. This paper develops an approach to compute an accurate approximation to $F$ and $F^{-1}$, while insuring their numerical stability, by means of solving a non convex optimization problem. To address the non-convexity, we propose an algorithm, the stable graph Fourier basis algorithm (SGFA) that we prove to exponentially increase the accuracy of the approximating $F$ per iteration. Likewise, we can apply SGFA to $A^H$ and, hence, approximate the stable left eigenvectors for the graph shift $A$ and directly compute the GFT. We evaluate empirically the quality of SGFA by applying it to graph shifts $A$ drawn from two real world problems, the 2004 US political blogs graph and the Manhattan road map, carrying out a comprehensive study on tradeoffs between different SGFA parameters. We also confirm our conclusions by applying SGFA on very sparse and very dense directed Erd\H os-R\'enyi graphs.},
  file        = {:http\://arxiv.org/pdf/2001.05042v1:PDF},
  keywords    = {eess.SP},
}

@Article{Tan2020,
  author       = {Conghui Tan and Yuqiu Qian and Shiqian Ma and Tong Zhang},
  title        = {Accelerated Dual-Averaging Primal-Dual Method for Composite Convex Minimization},
  journaltitle = {Optimization Methods and Software 2020},
  date         = {2020-01-15},
  doi          = {10.1080/10556788.2020.1713779},
  eprint       = {arXiv:2001.05537v1},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  abstract     = {Dual averaging-type methods are widely used in industrial machine learning applications due to their ability to promoting solution structure (e.g., sparsity) efficiently. In this paper, we propose a novel accelerated dual-averaging primal-dual algorithm for minimizing a composite convex function. We also derive a stochastic version of the proposed method which solves empirical risk minimization, and its advantages on handling sparse data are demonstrated both theoretically and empirically.},
  file         = {:http\://arxiv.org/pdf/2001.05537v1:PDF},
  keywords     = {math.OC, cs.LG, stat.ML},
}

@Article{Bomze2019,
  author      = {Immanuel M. Bomze and Francesco Rinaldi and Damiano Zeffiro},
  title       = {Active set complexity of the Away-step Frank-Wolfe Algorithm},
  date        = {2019-12-24},
  eprint      = {1912.11492},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we study active set identification results for the away-step Frank-Wolfe algorithm in different settings. We first prove a local identification property that we apply, in combination with a convergence hypothesis, to get an active set identification result. We then prove, in the nonconvex case, a novel $O(1/\sqrt{k})$ convergence rate result and active set identification for different stepsizes (under suitable assumptions on the set of stationary points). By exploiting those results, we also give explicit active set complexity bounds for both strongly convex and nonconvex objectives. While we initially consider the probability simplex as feasible set, in the appendix we show how to adapt some of our results to generic polytopes.},
  file        = {:http\://arxiv.org/pdf/1912.11492v1:PDF},
  keywords    = {math.OC, 65K05, 90C06, 90C30},
}

@Article{Gao2020,
  author      = {Tianxiang Gao and Songtao Lu and Jia Liu and Chris Chu},
  title       = {Randomized Bregman Coordinate Descent Methods for Non-Lipschitz Optimization},
  date        = {2020-01-15},
  eprint      = {2001.05202},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We propose a new \textit{randomized Bregman (block) coordinate descent} (RBCD) method for minimizing a composite problem, where the objective function could be either convex or nonconvex, and the smooth part are freed from the global Lipschitz-continuous (partial) gradient assumption. Under the notion of relative smoothness based on the Bregman distance, we prove that every limit point of the generated sequence is a stationary point. Further, we show that the iteration complexity of the proposed method is $O(n\varepsilon^{-2})$ to achieve $\epsilon$-stationary point, where $n$ is the number of blocks of coordinates. If the objective is assumed to be convex, the iteration complexity is improved to $O(n\epsilon^{-1} )$. If, in addition, the objective is strongly convex (relative to the reference function), the global linear convergence rate is recovered. We also present the accelerated version of the RBCD method, which attains an $O(n\varepsilon^{-1/\gamma} )$ iteration complexity for the convex case, where the scalar $\gamma\in [1,2]$ is determined by the \textit{generalized translation variant} of the Bregman distance. Convergence analysis without assuming the global Lipschitz-continuous (partial) gradient sets our results apart from the existing works in the composite problems.},
  file        = {:http\://arxiv.org/pdf/2001.05202v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Sharma2019,
  author   = {Meenarli Sharma and Prashant Palkar and Ashutosh Mahajan},
  title    = {Linearization and Parallelization Schemes for Convex {MINLPs}},
  year     = {2019},
  abstract = {We present parallelization and linearization schemes for improving state-of-the-art algorithms for convex mixed-integer nonlinear programs (MINLPs). Recently, shared-memory multicore computing systems are quite prevalent and one can effectively harness the available computing resources. On this front, we first present parallel extensions of the tree based algorithms in the MINLP solver MINOTAUR, the nonlinear branch-and-bound (NLP-BB) and the LP/NLP based branch-andbound (QG). Next, we deploy several linearization techniques to obtain tighter relaxations in QG. Adding cuts only at integer feasible points in the branch-and-cut framework of QG may lead to large trees. First, we describe methods based on primal and dual information at the nodes to detect appropriate conditions for adding more inequalities. Next, we describe methods to generate tight inequalities by exploiting specific structures in the nonlinear constraint functions using iterative solution of LP and NLP relaxations in some neighbourhood of the root NLP solution. Third, we present line search based methods for finding good points for generating linear inequalities when this structure is missing.\\ We benchmark our improvised algorithms with two parallel extensions of outer-approximation (OA) algorithm implemented in MINOTAUR that deploy a multi-threaded mixed-integer linear programming (MILP) solver. The first extension executes the MILPs within OA in parallel. Recently, some MILP solvers allow the calling programs to pass problem specific information like cuts, solutions etc. during their tree-search through callback functions. Using this (lazy cuts callback) functionality, we implement a second variant, which can also be viewed as a parallel QG algorithm primarily managed by the MILP solver (LSTOA). We present extensive computational experiments to show the encouraging individual and combined effects of parallelization and linearization techniques on the tree-based algorithms and analyze their performance alongside MILP based algorithms.},
}

@Article{Palleschi2020,
  author    = {Alessandro Palleschi and Riccardo Mengacci and Franco Angelini and Danilo Caporale and Lucia Pallottino and Alessandro De Luca and Manolo Garabini},
  title     = {Time-Optimal Trajectory Planning for Flexible Joint Robots},
  journal   = {{IEEE} Robotics and Automation Letters},
  year      = {2020},
  pages     = {1--1},
  doi       = {10.1109/lra.2020.2965861},
  abstract  = {In this paper, a new approach is proposed to optimally plan the motion along a parametrized path for flexible joint robots, i.e., robots whose structure is purposefully provided with compliant elements. State-of-the-art methods efficiently solve the problem in case of torque-controlled rigid robots via a translation of the optimal control problem into a convex optimization problem. Recently, we showed that, for jerk-controlled rigid robots, the problem could be recast into a non-convex optimization problem. The non-convexity is given by bilinear constraints that can be efficiently handled through McCormick relaxations and spatial Branch-and-Bound techniques. In this paper, we show that, even in case of robots with flexible joints, the time-optimal trajectory planning problem can be recast into a non-convex problem in which the non-convexity is still given by bilinear constraints. We performed experimental tests on a planar 2R elastic manipulator to validate the benefits of the proposed approach. The scalability of the method for robots with multiple degrees of freedom is also discussed.},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Ding2020,
  author   = {Nan Ding and Samuel Williams and Yang Liu and Xiaoye S. Li},
  title    = {Leveraging One-Sided Communication for Sparse Triangular Solvers},
  year     = {2020},
  abstract = {In this paper, we implement and evaluate a one-sided communication-based distributed-memory sparse triangular solve (SpTRSV). SpTRSV is used in conjunction with Sparse LU to affect preconditioning in linear solvers. One-sided communication paradigms enjoy higher effective network bandwidth and lower synchronization costs compared to their two-sided counterparts. We use a passive target mode in one-sided communication to implement a synchronizationfree task queue to manage the messaging between producerconsumer pairs. Whereas some numerical methods lend themselves to simple performance analysis, the DAG-based computational graph of SpTRSV demands we construct a critical path performance model in order to assess our observed performance relative to machine capabilities. In alignment with our model, our foMPI-based one-sided implementation of SpTRSV reduces communication time by 1.5$\times$ to 2.5$\times$ and improves SpTRSV solver performance by up to 2.4$\times$ compared to the SuperLU DIST’s two-sided MPI implementation running on 64 to 4,096 processes on Cray supercomputers.},
}

@Article{Curtis2020,
  author      = {Frank E. Curtis and Katya Scheinberg},
  title       = {Adaptive Stochastic Optimization},
  date        = {2020-01-18},
  eprint      = {2001.06699},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Optimization lies at the heart of machine learning and signal processing. Contemporary approaches based on the stochastic gradient method are non-adaptive in the sense that their implementation employs prescribed parameter values that need to be tuned for each application. This article summarizes recent research and motivates future work on adaptive stochastic optimization methods, which have the potential to offer significant computational savings when training large-scale systems.},
  file        = {:http\://arxiv.org/pdf/2001.06699v1:PDF},
  keywords    = {math.OC, cs.LG, stat.ML},
}

@Article{Higham2020,
  author     = {Higham, Nicholas J. and Mary, Theo},
  title      = {Sharper Probabilistic Backward Error Analysis for Basic Linear Algebra Kernels with Random Data},
  date       = {2020},
  issn       = {1749-9097},
  eprint     = {MIMS EPrint:2020.4},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2743/1/paper.pdf},
  abstract   = {Standard backward error analyses for numerical linear algebra algorithms provide worst-case bounds that can significantly overestimate the backward error. Our recent probabilistic error analysis, which assumes rounding errors to be independent random variables [SIAM J. Sci. Comput., 41 (2019), pp. A2815–A2835], contains smaller constants but its bounds can still be pessimistic. We perform a new probabilistic error analysis that assumes both the data and the rounding errors to be random variables and assumes only mean independence. We prove that for data with zero or small mean we can relax the existing probabilistic bounds of order $sqrt{n}u$ to much sharper bounds of order $u$, which are independent of $n$. Our fundamental result is for summation and we use it to derive results for inner products, matrix–vector products, and matrix–matrix products. The analysis answers the open question of why random data distributed on $[-1, 1]$ leads to smaller error growth for these kernels than random data distributed on $[0, 1]$. We also propose a new algorithm for multiplying two matrices that transforms the rows of the first matrix to have zero mean and we show that it can achieve significantly more accurate results than standard matrix multiplication.},
}

@Article{Tao2020,
  author       = {Wei Tao and Zhisong Pan and Gaowei Wu and Qing Tao},
  title        = {The Strength of Nesterov’s Extrapolation in the Individual Convergence of Nonsmooth Optimization},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  date         = {2020},
  abstract     = {The extrapolation strategy raised by Nesterov, which can accelerate the convergence rate of gradient descent methods by orders of magnitude when dealing with smooth convex objective, has led to tremendous success in training machine learning tasks. In this article, the convergence of individual iterates of projected subgradient (PSG) methods for nonsmooth convex optimization problems is theoretically studied based on Nesterov’s extrapolation, which we name individual convergence. We prove that Nesterov’s extrapolation has the strength to make the individual convergence of PSG optimal for nonsmooth problems. In light of this consideration, a direct modification of the subgradient evaluation suffices to achieve optimal individual convergence for strongly convex problems, which can be regarded as making an interesting step toward the open question about stochastic gradient descent (SGD) posed by Shamir. Furthermore, we give an extension of the derived algorithms to solve regularized learning tasks with nonsmooth losses in stochastic settings. Compared with other state-of-theart nonsmooth methods, the derived algorithms can serve as an alternative to the basic SGD especially in coping with machine learning problems, where an individual output is needed to guarantee the regularization structure while keeping an optimal rate of convergence. Typically, our method is applicable as an efficient tool for solving large-scale l1-regularized hinge-loss learning problems. Several comparison experiments demonstrate that our individual output not only achieves an optimal convergence rate but also guarantees better sparsity than the averaged solution.},
}

@Article{Anzt2020,
  author    = {Hartwig Anzt and Erik Boman and Rob Falgout and Pieter Ghysels and Michael Heroux and Xiaoye Li and Lois Curfman McInnes and Richard Tran Mills and Sivasankaran Rajamanickam and Karl Rupp and Barry Smith and Ichitaro Yamazaki and Ulrike Meier Yang},
  title     = {Preparing sparse solvers for exascale computing},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190053},
  doi       = {10.1098/rsta.2019.0053},
  abstract  = {Sparse solvers provide essential functionality for a wide variety of scientific applications. Highly parallel sparse solvers are essential for continuing advances in high-fidelity, multi-physics and multi-scale simulations, especially as we target exascale platforms. This paper describes the challenges, strategies and progress of the US Department of Energy Exascale Computing project towards providing sparse solvers for exascale computing platforms. We address the demands of systems with thousands of high-performance node devices where exposing concurrency, hiding latency and creating alternative algorithms become essential. The efforts described here are works in progress, highlighting current success and upcoming challenges.},
  publisher = {The Royal Society},
}

@Article{Yelick2020,
  author    = {Katherine Yelick and Aydın Buluç and Muaaz Awan and Ariful Azad and Benjamin Brock and Rob Egan and Saliya Ekanayake and Marquita Ellis and Evangelos Georganas and Giulia Guidi and Steven Hofmeyr and Oguz Selvitopi and Cristina Teodoropol and Leonid Oliker},
  title     = {The parallelism motifs of genomic data analysis},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190394},
  doi       = {10.1098/rsta.2019.0394},
  abstract  = {Genomic datasets are growing dramatically as the cost of sequencing continues to decline and small sequencing devices become available. Enormous community databases store and share these data with the research community, but some of these genomic data analysis problems require large-scale computational platforms to meet both the memory and computational requirements. These applications differ from scientific simulations that dominate the workload on high-end parallel systems today and place different requirements on programming support, software libraries and parallel architectural design. For example, they involve irregular communication patterns such as asynchronous updates to shared data structures. We consider several problems in high-performance genomics analysis, including alignment, profiling, clustering and assembly for both single genomes and metagenomes. We identify some of the common computational patterns or ‘motifs’ that help inform parallelization strategies and compare our motifs to some of the established lists, arguing that at least two key patterns, sorting and hashing, are missing.},
  publisher = {The Royal Society},
}

@Article{Keyes2020,
  author    = {D. E. Keyes and H. Ltaief and G. Turkiyyah},
  title     = {Hierarchical algorithms on hierarchical architectures},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  year      = {2020},
  volume    = {378},
  number    = {2166},
  month     = {1},
  pages     = {20190055},
  doi       = {10.1098/rsta.2019.0055},
  abstract  = {A traditional goal of algorithmic optimality, squeezing out flops, has been superseded by evolution in architecture. Flops no longer serve as a reasonable proxy for all aspects of complexity. Instead, algorithms must now squeeze memory, data transfers, and synchronizations, while extra flops on locally cached data represent only small costs in time and energy. Hierarchically low-rank matrices realize a rarely achieved combination of optimal storage complexity and high-computational intensity for a wide class of formally dense linear operators that arise in applications for which exascale computers are being constructed. They may be regarded as algebraic generalizations of the fast multipole method. Methods based on these hierarchical data structures and their simpler cousins, tile low-rank matrices, are well proportioned for early exascale computer architectures, which are provisioned for high processing power relative to memory capacity and memory bandwidth. They are ushering in a renaissance of computational linear algebra. A challenge is that emerging hardware architecture possesses hierarchies of its own that do not generally align with those of the algorithm. We describe modules of a software toolkit, hierarchical computations on manycore architectures, that illustrate these features and are intended as building blocks of applications, such as matrix-free higher-order methods in optimization and large-scale spatial statistics. Some modules of this open-source project have been adopted in the software libraries of major vendors.},
  publisher = {The Royal Society},
}

@Article{Huo2020,
  author    = {Zenan Huo and Gang Mei and Giampaolo Casolla and Fabio Giampaolo},
  title     = {Designing an efficient parallel spectral clustering algorithm on multi-core processors in Julia},
  journal   = {Journal of Parallel and Distributed Computing},
  year      = {2020},
  volume    = {138},
  month     = {4},
  pages     = {211--221},
  doi       = {10.1016/j.jpdc.2020.01.003},
  abstract  = {Spectral clustering is widely used in data mining, machine learning and other fields. It can identify the arbitrary shape of a sample space and converge to the global optimal solution. Compared with the traditional k-means algorithm, the spectral clustering algorithm has stronger adaptability to data and better clustering results. However, the computation of the algorithm is quite expensive. In this paper, an efficient parallel spectral clustering algorithm on multi-core processors in the Julia language is proposed, and we refer to it as juPSC. The Julia language is a high-performance, open-source programming language. The juPSC is composed of three procedures: (1) calculating the affinity matrix, (2) calculating the eigenvectors, and (3) conducting k-means clustering. Procedures (1) and (3) are computed by the efficient parallel algorithm, and the COO format is used to compress the affinity matrix. Two groups of experiments are conducted to verify the accuracy and efficiency of the juPSC. Experimental results indicate that (1) the juPSC achieves speedups of approximately $14 \times \tilde 18 \times$ on a 24-core CPU and that (2) the serial version of the juPSC is faster than the Python version of scikit-learn. Moreover, the structure and functions of the juPSC are designed considering modularity, which is convenient for combination and further optimization with other parallel computing platforms.},
  publisher = {Elsevier {BV}},
}

@InProceedings{Goncalves2019,
  author    = {Goncalves, M. and Lamb, I. and Brum, R. M. and Azambuja, J. R.},
  title     = {Evaluating the Impact of Accuracy Relaxation in the Reliability of GPU Register Files},
  booktitle = {Proceedings of the 26th IEEE International Conference on Electronics, Circuits and Systems},
  year      = {2019},
  series    = {ICECS '19},
  month     = {11},
  pages     = {205--208},
  doi       = {10.1109/ICECS46596.2019.8964908},
  abstract  = {Thanks to the high computing power, GPUs have joined application domains where reliability is a major concern. Faults on electronic components are mainly caused by energized particles, which may cause malfunction and make them result in incorrect output. Errors may be unacceptable for most applications but a small margin of error can be considered safe in some cases. This work uses an approximate computing perspective to analyze the influence of application accuracy relaxation in GPU register files reliability. We perform a fault injection campaign in a Kepler GPU to identify registers' vulnerability and the impact on resulting data. Results show increase in register file reliability in an average of 71.6\% for 1\% of application accuracy relaxation.},
  issn      = {null},
  keywords  = {Approximate computing;GPUs;radiation effects},
}

@Article{Berger2020,
  author      = {Guillaume O. Berger and P. -A. Absil and Raphaël M. Jungers and Yurii Nesterov},
  title       = {On the Quality of First-Order Approximation of Functions with Hölder Continuous Gradient},
  date        = {2020-01-22},
  eprint      = {2001.07946},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We show that Hölder continuity of the gradient is not only a sufficient condition, but also a necessary condition for the existence of a global upper bound on the error of the first-order Taylor approximation. We also relate this global upper bound to the Hölder constant of the gradient. This relation is expressed as an interval, depending on the Hölder constant, in which the error of the first-order Taylor approximation is guaranteed to be. We show that, for the Lipschitz continuous case, the interval cannot be reduced. An application to the norms of quadratic forms is proposed, which allows us to derive a novel characterization of Euclidean norms.},
  file        = {:http\://arxiv.org/pdf/2001.07946v1:PDF},
  keywords    = {math.OC, 68Q25, 90C30, 90C48},
}

@Article{Alimo2020,
  author   = {Alimo, Ryan and Beyhaghi, Pooriya and Bewley, Thomas R.},
  title    = {Delaunay-based derivative-free optimization via global surrogates. Part III: nonconvex constraints},
  journal  = {Journal of Global Optimization},
  year     = {2020},
  month    = {1},
  issn     = {1573-2916},
  doi      = {10.1007/s10898-019-00854-2},
  abstract = {This paper introduces a Delaunay-based derivative-free optimization algorithm, dubbed $\Delta- -DOGS(\Omega)$, for problems with both (a) a nonconvex, computationally expensive objective function $f(x)$, and (b) nonlinear, computationally expensive constraint functions $c_\mathcal{l}(x)$  which, taken together, define a nonconvex, possibly even disconnected feasible domain $\Omega$ which is assumed to lie within a known rectangular search domain   $\Omega_s$, everywhere within which the $f(x)$ and $c_\mathcal{l}(x)$ may be evaluated. Approximations of both the objective function $f(x)$ as well as the feasible domain $\Omega$ are developed and refined as the iterations proceed. The approach is practically limited to the problems with less than about ten adjustable parameters. The work is an extension of our original Delaunay-based optimization algorithm (see JOGO DOI: 10.1007/s10898-015-0384-2), and inherits many of the constructions and strengths of that algorithm, including: (1) a surrogate function $p(x)$ interpolating all existing function evaluations and summarizing their trends, (2) a synthetic, piecewise-quadratic uncertainty function $e(x)$ built on the framework of a Delaunay triangulation amongst existing datapoints, (3) a tunable balance between global exploration (large $K$) and local refinement (small $K$), (4) provable global convergence for a sufficiently large $K$, under the assumption that the objective and constraint functions are twice differentiable with bounded Hessians, (5) an Adaptive-K variant of the algorithm that efficiently tunes $K$ automatically based on a target value of the objective function, and (6) remarkably fast global convergence on a variety of benchmark problems.},
  day      = {23},
}

@Article{Baayen2020,
  author      = {Jorn Baayen and Jakub Marecek},
  title       = {Mixed-Integer Path-Stable Optimisation, with Applications in Model-Predictive Control of Water Systems},
  date        = {2020-01-22},
  eprint      = {2001.08121},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Many systems exhibit a mixture of continuous and discrete dynamics. We consider a family of mixed-integer non-convex non-linear optimisation problems obtained in discretisations of optimal control of such systems. For this family, a branch-and-bound algorithm solves the discretised problem to global optimality. As an example, we consider water systems, where variations in flow and variations in water levels are continuous, while decisions related to fixed-speed pumps and whether gates that may be opened and closed are discrete. We show that the related optimal-control problems come from the family we introduce -- and implement deterministic solvers with global convergence guarantees.},
  file        = {:http\://arxiv.org/pdf/2001.08121v1:PDF},
  keywords    = {math.OC, cs.SY, eess.SY},
}

@Article{Ginsbach2020,
  author      = {Philip Ginsbach and Bruce Collie and Michael F. P. O'Boyle},
  title       = {Automatically Harnessing Sparse Acceleration},
  date        = {2020-01-22},
  doi         = {10.1145/3377555.3377893},
  eprint      = {2001.07938},
  eprinttype  = {arXiv},
  eprintclass = {cs.PF},
  abstract    = {Sparse linear algebra is central to many scientific programs, yet compilers fail to optimize it well. High-performance libraries are available, but adoption costs are significant. Moreover, libraries tie programs into vendor-specific software and hardware ecosystems, creating non-portable code. In this paper, we develop a new approach based on our specification Language for implementers of Linear Algebra Computations (LiLAC). Rather than requiring the application developer to (re)write every program for a given library, the burden is shifted to a one-off description by the library implementer. The LiLAC-enabled compiler uses this to insert appropriate library routines without source code changes. LiLAC provides automatic data marshaling, maintaining state between calls and minimizing data transfers. Appropriate places for library insertion are detected in compiler intermediate representation, independent of source languages. We evaluated on large-scale scientific applications written in FORTRAN; standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across heterogeneous platforms, applications and data sets we show speedups of 1.1$\times$ to over 10$\times$ without user intervention.},
  file        = {:http\://arxiv.org/pdf/2001.07938v1:PDF},
  keywords    = {cs.PF, cs.MS},
}

@Article{Dong2020,
  author      = {Jing Dong and Xin T. Tong},
  title       = {Replica Exchange for Non-Convex Optimization},
  date        = {2020-01-23},
  eprint      = {2001.08356},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Gradient descent (GD) is known to converge quickly for convex objective functions, but it can be trapped at local minimums. On the other hand, Langevin dynamics (LD) can explore the state space and find global minimums, but in order to give accurate estimates, LD needs to run with small discretization stepsize and weak stochastic force, which in general slow down its convergence. This paper shows that these two algorithms can "collaborate" through a simple exchange mechanism, in which they swap their current positions if LD yields a lower objective function. This idea can be seen as the singular limit of the replica exchange technique from the sampling literature. We show that this new algorithm converges to the global minimum linearly with high probability, assuming the objective function is strongly convex in a neighborhood of the unique global minimum. By replacing gradients with stochastic gradients, and adding a proper threshold to the exchange mechanism, our algorithm can also be used in online settings. We further verify our theoretical results through some numerical experiments, and observe superior performance of the proposed algorithm over running GD or LD alone.},
  file        = {:http\://arxiv.org/pdf/2001.08356v1:PDF},
  keywords    = {math.OC, math.PR, stat.ML},
}

@Article{Bolte2020,
  author      = {Jerome Bolte and Edouard Pauwels},
  title       = {Curiosities and counterexamples in smooth convex optimization},
  date        = {2020-01-22},
  eprint      = {2001.07999},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Counterexamples to some old-standing optimization problems in the smooth convex coercive setting are provided. We show that block-coordinate, steepest descent with exact search or Bregman descent methods do not generally converge. Other failures of various desirable features are established: directional convergence of Cauchy's gradient curves, convergence of Newton's flow, finite length of Tikhonov path, convergence of central paths, or smooth Kurdyka-Lojasiewicz inequality. All examples are planar. These examples are based on general smooth convex interpolation results. Given a decreasing sequence of positively curved C k convex compact sets in the plane, we provide a level set interpolation of a C k smooth convex function where k $\ge$ 2 is arbitrary. If the intersection is reduced to one point our interpolant has positive definite Hessian, otherwise it is positive definite out of the solution set. Furthermore , given a sequence of decreasing polygons we provide an interpolant agreeing with the vertices and whose gradients coincide with prescribed normals.},
  file        = {:http\://arxiv.org/pdf/2001.07999v1:PDF},
  keywords    = {math.OC},
}

@Article{Mukhopadhyay2020,
  author    = {Samrat Mukhopadhyay},
  title     = {Stochastic Gradient Descent For Linear Systems With Sequential Matrix Entry Accumulation},
  journal   = {Signal Processing},
  year      = {2020},
  month     = {1},
  pages     = {107494},
  doi       = {10.1016/j.sigpro.2020.107494},
  abstract  = {Conventional stochastic iterative methods are often employed for solving linear systems of equations involving large matrix sizes using low memory footprint. However, their performances are often limited by the unavailability of all the matrix entries, which is often termed as the problem of missing data. Although Ma and Needell [1] has recently proposed a method, termed as mSGD, assuming a model for data missing that results in improved convergence, their result is also affected by constant large variance of the stochastic gradient. In this paper we propose a SGD type method termed as cumulative information SGD (CISGD) for solving a linear system with missing data with an additional provision to accumulate a very small number of matrix entries sequentially per iteration, termed as the sequential matrix entry accumulation (SEMEA) mechanism. CISGD uses the data collected by SEMEA mechanism along with the prior model for data missing mechanism of [1] to gradually reduce variance of the stochastic gradient. The convergence of the proposed CISGD is theoretically analyzed and some interesting implications of the result are investigated under a specific SEMEA mechanism. Finally, numerical experiments are performed along with simulations that corroborate the theoretical findings regarding the efficacy of the proposed CISGD method.},
  publisher = {Elsevier {BV}},
}

@InCollection{Li2020a,
  author    = {Ruipeng Li and Chaoyu Zhang},
  title     = {Efficient Parallel Implementations of Sparse Triangular Solves for {GPU} Architectures},
  booktitle = {Proceedings of the 2020 {SIAM} Conference on Parallel Processing for Scientific Computing},
  year      = {2020},
  publisher = {Society for Industrial and Applied Mathematics},
  pages     = {106--117},
  doi       = {10.1137/1.9781611976137.10},
  abstract  = {The sparse triangular matrix solve (SpTrSV) is an important computation kernel that is demanded by a variety of numerical methods such as the Gauss-Seidel iterations. However, developing efficient parallel algorithms for SpTrSV that are suitable for GPUs remains a challenging task due to the inherently sequential nature in the solve. In this paper, we revisit this problem by reviewing several parallel algorithms based on different task scheduling and different sparse matrix storage schemes, proposing modifications to the existing methods that can greatly improve the performance, and describing the implementations in detail. Numerical results of Gauss-Seidel iterations with structured and unstructured matrices make evident the superiority of the proposed algorithms and implementations comparing with state-of-the-art methods in the literature.},
  month     = {1},
}

@InCollection{Loe2020,
  author    = {Jennifer A. Loe and Heidi K. Thornquist and Erik G. Boman},
  title     = {Polynomial Preconditioned {GMRES} in Trilinos: Practical Considerations for High-Performance Computing},
  booktitle = {Proceedings of the 2020 {SIAM} Conference on Parallel Processing for Scientific Computing},
  year      = {2020},
  publisher = {Society for Industrial and Applied Mathematics},
  pages     = {35--45},
  doi       = {10.1137/1.9781611976137.4},
  abstract  = {Polynomial preconditioners for GMRES and other Krylov solvers are well-known but are infrequently used in large-scale software libraries or applications. This may be due to stability problems or complicated algorithms. We implement the GMRES polynomial as a preconditioner in the software library Trilinos and demonstrate that it is stable and effective for parallel computing. Trade-offs when selecting a polynomial degree and combining with other preconditioners are analyzed. We also discuss communication-avoiding (CA) properties of the polynomial and relate these to current CA-GMRES methods.},
  month     = {1},
}

@Article{Stonyakin2020,
  author      = {Fedor Stonyakin and Alexander Tyurin and Alexander Gasnikov and Pavel Dvurechensky and Artem Agafonov and Darina Dvinskikh and Dmitry Pasechnyuk and Sergei Artamonov and Victorya Piskunova},
  title       = {Inexact Relative Smoothness and Strong Convexity for Optimization and Variational Inequalities by Inexact Model},
  date        = {2020-01-23},
  eprint      = {2001.09013},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper we propose a general algorithmic framework for first-order methods in optimization in a broad sense, including minimization problems, saddle-point problems and variational inequalities. This framework allows to obtain many known methods as a special case, the list including accelerated gradient method, composite optimization methods, level-set methods, Bregman proximal methods. The idea of the framework is based on constructing an inexact model of the main problem component, i.e. objective function in optimization or operator in variational inequalities. Besides reproducing known results, our framework allows to construct new methods, which we illustrate by constructing a universal conditional gradient method and universal method for variational inequalities with composite structure. These method works for smooth and non-smooth problems with optimal complexity without a priori knowledge of the problem smoothness. As a particular case of our general framework, we introduce relative smoothness for operators and propose an algorithm for VIs with such operator. We also generalize our framework for relatively strongly convex objectives and strongly monotone variational inequalities. This paper is an extended and updated version of [arXiv:1902.00990]. In particular, we add an extension of relative strong convexity for optimization and variational inequalities.},
  file        = {:http\://arxiv.org/pdf/2001.09013v1:PDF},
  keywords    = {math.OC},
}

@Book{Kaner1999,
  author    = {Kaner, Cem and Falk, Jack and Nguyen, Hung Q.},
  title     = {Testing Computer Software},
  year      = {1999},
  date      = {1999-04-12},
  publisher = {John Wiley \& Sons Inc},
  isbn      = {0471358460},
  pagetotal = {496},
  url       = {https://www.ebook.de/de/product/3241729/cem_kaner_jack_falk_hung_q_nguyen_testing_computer_software.html},
  ean       = {9780471358466},
}

@InCollection{Paulin-Mohring2012,
  author    = {Christine Paulin-Mohring},
  booktitle = {Lecture Notes in Computer Science},
  title     = {Introduction to the Coq Proof-Assistant for Practical Software Verification},
  doi       = {10.1007/978-3-642-35746-6_3},
  pages     = {45--95},
  publisher = {Springer Berlin Heidelberg},
  volume    = {7682},
  abstract  = {This paper is a tutorial on using the Coq proof-assistant for reasoning on software correctness. It illustrates features of Coq like inductive definitions and proof automation on a few examples including arithmetic, algorithms on functional and imperative lists and cryptographic protocols.\\ Coq is not a tool dedicated to software verification but a general purpose environment for developing mathematical proofs. However, it is based on a powerful language including basic functional programming and high-level specifications. As such it offers modern ways to literally program proofs in a structured way with advanced data-types, proofs by computation, and general purpose libraries of definitions and lemmas. \\ Coq is well suited for software verification of programs involving advanced specifications like language semantics and real numbers. The Coq architecture is also based on a small trusted kernel, making possible to use third-party libraries while being sure that proofs are not compromised.},
  year      = {2012},
}

@TechReport{Gopalakrishnan2017,
  author    = {Ganesh Gopalakrishnan and Paul D. Hovland and Costin Iancu and Sriram Krishnamoorthy and Ignacio Laguna and Richard A. Lethin and Koushik Sen and Stephen F. Siegel and Armando Solar-Lezama},
  title     = {Report of the {HPC} Correctness Summit, January 25-26, 2017, Washington, {DC}},
  year      = {2017},
  month     = oct,
  doi       = {10.2172/1470989},
  abstract  = {Technologies for verification and debugging have made significant strides in the context of general systems software. An investment in such technologies to make them applicable for High Performance Computing (HPC) could lead to substantial improvements in the productivity and sustainability of HPC software development. Such improvements will be essential to fully exploit new exascale computer architectures. Without such investment, there is the possibility of a substantial crisis in our ability to advance the field of HPC, as the complexity of our architectures, algorithms, and applications is moving beyond the ability of our developers. As HPC is of strategic importance to our nation, forming the bedrock of its scientific and technological capabilities, such investment is highly warranted.},
  publisher = {Office of Scientific and Technical Information ({OSTI})},
}

@InProceedings{Fonseca2017,
  author    = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
  title     = {An Empirical Study on the Correctness of Formally Verified Distributed Systems},
  booktitle = {Proceedings of the 12th European Conference on Computer Systems},
  year      = {2017},
  series    = {EuroSys '17},
  publisher = {ACM},
  location  = {Belgrade, Serbia},
  isbn      = {978-1-4503-4938-3},
  pages     = {328--343},
  doi       = {10.1145/3064176.3064183},
  url       = {http://doi.acm.org/10.1145/3064176.3064183},
  abstract  = {Recent advances in formal verification techniques enabled the implementation of distributed systems with machine-checked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and verification practices.\\ This paper thoroughly analyzes three state-of-the-art, formally verified implementations of distributed systems: Iron-Fleet, Verdi, and Chapar. Through code review and testing, we found a total of 16 bugs, many of which produce serious consequences, including crashing servers, returning incorrect results to clients, and invalidating verification guarantees. These bugs were caused by violations of a wide-range of assumptions on which the verified components relied. Our results revealed that these assumptions referred to a small fraction of the trusted computing base, mostly at the interface of verified and unverified components. Based on our observations, we have built a testing toolkit called PK, which focuses on testing these parts and is able to automate the detection of 13 (out of 16) bugs.},
  acmid     = {3064183},
  address   = {New York, NY, USA},
  numpages  = {16},
}

@InProceedings{Franco2017,
  author    = {Anthony Di Franco and Hui Guo and Cindy Rubio-Gonzalez},
  title     = {A comprehensive study of real-world numerical bug characteristics},
  booktitle = {Proceedings of the 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering},
  year      = {2017},
  series    = {ASE '17},
  publisher = {{IEEE}},
  month     = {10},
  doi       = {10.1109/ase.2017.8115662},
  abstract  = {Numerical software is used in a wide variety of applications including safety-critical systems, which have stringent correctness requirements, and whose failures have catastrophic consequences that endanger human life. Numerical bugs are known to be particularly difficult to diagnose and fix, largely due to the use of approximate representations of numbers such as floating point. Understanding the characteristics of numerical bugs is the first step to combat them more effectively. In this paper, we present the first comprehensive study of real-world numerical bugs. Specifically, we identify and carefully examine 269 numerical bugs from five widely-used numerical software libraries: NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental. We propose a categorization of numerical bugs, and discuss their frequency, symptoms and fixes. Our study opens new directions in the areas of program analysis, testing, and automated program repair of numerical software, and provides a collection of real-world numerical bugs.},
}

@Article{Solovyev2018,
  author     = {Solovyev, Alexey and Baranowski, Marek S. and Briggs, Ian and Jacobsen, Charles and Rakamariundefined, Zvonimir and Gopalakrishnan, Ganesh},
  title      = {Rigorous Estimation of Floating-Point Round-Off Errors with Symbolic Taylor Expansions},
  journal    = {ACM Transaction Programming Language Systems},
  year       = {2018},
  volume     = {41},
  number     = {1},
  month      = dec,
  issn       = {0164-0925},
  doi        = {10.1145/3230733},
  url        = {https://doi.org/10.1145/3230733},
  abstract   = {Rigorous estimation of maximum floating-point round-off errors is an important capability central to many formal verification tools. Unfortunately, available techniques for this task often provide overestimates. Also, there are no available rigorous approaches that handle transcendental functions. We have developed a new approach called Symbolic Taylor Expansionsthat avoids this difficulty, and implemented a new tool called FPTaylor embodying this approach. Key to our approach is the use of rigorous global optimization, instead of the more familiar interval arithmetic, affine arithmetic, and/or SMT solvers. In addition to providing far tighter upper bounds of round-off error in a vast majority of cases, FPTaylor also emits analysis certificates in the form of HOL Light proofs. We release FPTaylor along with our benchmarks for evaluation.},
  address    = {New York, NY, USA},
  articleno  = {Article 2},
  issue_date = {March 2019},
  keywords   = {mixed-precision arithmetic, global optimization, formal verification, Floating-point arithmetic, IEEE floating-point standard, round-off error},
  numpages   = {39},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{Nelson2019,
  author    = {Nelson, Luke and Bornholt, James and Gu, Ronghui and Baumann, Andrew and Torlak, Emina and Wang, Xi},
  title     = {Scaling Symbolic Evaluation for Automated Verification of Systems Code with {Serval}},
  booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  year      = {2019},
  series    = {SOSP '19},
  publisher = {ACM},
  location  = {Huntsville, Ontario, Canada},
  isbn      = {978-1-4503-6873-5},
  pages     = {225--242},
  doi       = {10.1145/3341301.3359641},
  url       = {http://doi.acm.org/10.1145/3341301.3359641},
  abstract  = {This paper presents Serval, a framework for developing automated verifiers for systems software. Serval provides an extensible infrastructure for creating verifiers by lifting interpreters under symbolic evaluation, and a systematic approach to identifying and repairing verification performance bottlenecks using symbolic profiling and optimizations.\\ Using Serval, we build automated verifiers for the RISC-V, x86--32, LLVM, and BPF instruction sets. We report our experience of retrofitting CertiKOS and Komodo, two systems previously verified using Coq and Dafny, respectively, for automated verification using Serval, and discuss trade-offs of different verification methodologies. In addition, we apply Serval to the Keystone security monitor and the BPF compilers in the Linux kernel, and uncover 18 new bugs through verification, all confirmed and fixed by developers.},
  acmid     = {3359641},
  address   = {New York, NY, USA},
  numpages  = {18},
}

@InProceedings{Nurminen2019,
  author    = {Jukka K. Nurminen and Tuomas Halvari and Juha Harviainen and Juha Mylläri and Antti Röyskö and Juuso Silvennoinen and Tommi Mikkonen},
  title     = {Software Framework for Data Fault Injection to Test Machine Learning Systems},
  booktitle = {Proceedings of the 30th International Symposium on Software Reliability Engineering},
  year      = {2019},
  series    = {ISSRE '19},
  abstract  = {Data-intensive systems are sensitive to the quality of data. Data often has problems due to faulty sensors or network problems, for instance. In this work, we develop a software framework to emulate faults in data and use it to study how machine learning (ML) systems work when the data has problems. We aim for flexibility: users can use predefined or their own dedicated fault models. Likewise, different kind of data (e.g. text, time series, video) can be used and the system under test can vary from a single ML model to a complicated software system. Our goal is to show how data faults can be emulated and how that can be used in the study and development of ML solutions.},
}

@Article{Konnov2019,
  author     = {Konnov, Igor and Kukovec, Jure and Tran, Thanh-Hai},
  title      = {{TLA+} Model Checking Made Symbolic},
  journal    = {Proceedings of the ACM on Programming Languages},
  year       = {2019},
  volume     = {3},
  number     = {123},
  month      = {10},
  pages      = {1--30},
  issn       = {2475-1421},
  doi        = {10.1145/3360549},
  abstract   = {TLA+ is a language for formal specification of all kinds of computer systems. System designers use this language to specify concurrent, distributed, and fault-tolerant protocols, which are traditionally presented in pseudo-code. TLA+ is extremely concise yet expressive: The language primitives include Booleans, integers, functions, tuples, records, sequences, and sets thereof, which can be also nested. This is probably why the only model checker for TLA+ (called TLC) relies on explicit enumeration of values and states. \\ In this paper, we present APALACHE -- a first symbolic model checker for TLA+. Like TLC, it assumes that all specification parameters are fixed and all states are finite structures. Unlike TLC, APALACHE translates the underlying transition relation into quantifier-free SMT constraints, which allows us to exploit the power of SMT solvers. Designing this translation is the central challenge that we address in this paper. Our experiments show that APALACHE outperforms TLC on examples with large state spaces.},
  acmid      = {3360549},
  address    = {New York, NY, USA},
  articleno  = {123},
  issue_date = {October 2019},
  keywords   = {Model checking, SMT, TLA+},
  numpages   = {30},
  publisher  = {ACM},
}

@InProceedings{Wei2019,
  author    = {X. {Wei} and R. {Zhang} and Y. {Liu} and H. {Yue} and J. {Tan}},
  title     = {Evaluating the Soft Error Resilience of Instructions for GPU Applications},
  booktitle = {Proceedings of the IEEE International Conference on Computational Science and Engineering and IEEE International Conference on Embedded and Ubiquitous Computing},
  year      = {2019},
  series    = {CSE-EUC '19},
  month     = {8},
  pages     = {459--464},
  doi       = {10.1109/CSE/EUC.2019.00091},
  abstract  = {Graphics Processing Units (GPUs) are widely used in a range of High Performance Computing fields because of high parallelism. As the technology scaling down, GPUs are more susceptible to soft errors which dramatically impact the applications output qualities. Silent Data Corruption (SDC) is one of the most concerned reliability issues, which require efficient protection mechanisms to eliminate it. Software-directed instruction replication has been a flexible technique to solve SDCs. However, this method requires a trade-off between reliability and overhead. To this end, it is imperative to explore the SDC criticality of the instructions. In this paper, we carry out fine-grained analysis on instruction error behavior of 11 benchmarks, while previous work focused on the error resilience of the entire application. Combining the error resilience of instructions with the dynamic data flow of applications, we find potential protection opportunities for the instructions.},
  issn      = {null},
  keywords  = {Benchmark testing;Graphics processing units;Resilience;Instruction sets;Kernel;Matrix decomposition;reliability;high performance computing;soft errors;fault injection},
}

@InProceedings{Fevotte2019,
  author    = {François Févotte and Bruno Lathuilière},
  title     = {Debugging and optimization of {HPC} programs with the {Verrou} tool},
  booktitle = {Proceedings ot the International Workshop on Software Correctness for HPC Applications},
  year      = {2019},
  language  = {English},
  series    = {Correctness '19},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  month     = {9},
  abstract  = {The analysis of Floating-Point-related issues in HPC codes is becoming a topic of major interest: parallel computing and code optimization often break the reproducibility of numerical results across machines, compilers and even executions of the same program. \\ This paper presents how the Verrou tool can help during all stages of the Floating-Point analysis of HPC codes: diagnostic, debugging and optimization. Recent developments of Verrou are presented, along with examples illustrating the interest of these new features for industrial codes such as code aster. \\ More specifically, the Verrou arithmetic back-ends now allow analyzing or emulating mixed-precision programs. Interlibm, an interposition layer for the mathematical library, is introduced to mitigate long-standing issues with algorithms from the libm. Finally, debugging algorithms are extended in order to produce useful information as soon as it is available. All these features are available in released version 2.1.0 and upcoming version 2.2.0.},
  address   = {United States},
  day       = {25},
  keywords  = {performance portability, productivity, mini-app, programming models},
}

@InProceedings{Zhu2020,
  author    = {Qianqian Zhu, Andy Zaidman},
  title     = {Massively Parallel, Highly Efficient, but What About the Test Suite Quality? Applying Mutation Testing to {GPU} Programs},
  booktitle = {Proceedings of the International Conference on Software Testing, Verification, and Validation},
  year      = {2020},
  series    = {ICST '20},
  abstract  = {Thanks to rapid advances in programmability and performance, GPUs have been widely applied in HighPerformance Computing (HPC) and safety-critical domains. As such, quality assurance of GPU applications has gained increasing attention. This brings us to mutation testing, a fault-based testing technique that assesses the test suite quality by systematically introducing small artificial faults. It has been shown to perform well in exposing faults. In this paper, we investigate whether GPU programming can benefit from mutation testing. In addition to conventional mutation operators, we propose nine GPU-specific mutation operators based on the core syntax differences between CPU and GPU programming. We conduct a preliminary study on six CUDA systems. The results show that mutation testing can effectively evaluate the test quality of GPU programs: conventional mutation operators can guide the engineers to write simple direct tests, while GPU-specific mutation operators can lead to more intricate test cases which are better at revealing GPU-specific weaknesses.},
}

@Article{Titolo2020,
  author      = {Laura Titolo and Mariano Moscato and Cesar A. Muñoz},
  title       = {Automatic generation and verification of test-stable floating-point code},
  date        = {2020-01-07},
  eprint      = {2001.02981},
  eprinttype  = {arXiv},
  eprintclass = {cs.PL},
  abstract    = {Test instability in a floating-point program occurs when the control flow of the program diverges from its ideal execution assuming real arithmetic. This phenomenon is caused by the presence of round-off errors that affect the evaluation of arithmetic expressions occurring in conditional statements. Unstable tests may lead to significant errors in safety-critical applications that depend on numerical computations. Writing programs that take into consideration test instability is a difficult task that requires expertise on finite precision computations and rounding errors. This paper presents a toolchain to automatically generate and verify a provably correct test-stable floating-point program from a functional specification in real arithmetic. The input is a real-valued program written in the Prototype Verification System (PVS) specification language and the output is a transformed floating-point C program annotated with ANSI/ISO C Specification Language (ACSL) contracts. These contracts relate the floating-point program to its functional specification in real arithmetic. The transformed program detects if unstable tests may occur and, in these cases, issues a warning and terminate. An approach that combines the Frama-C analyzer, the PRECiSA round-off error estimator, and PVS is proposed to automatically verify that the generated program code is correct in the sense that, if the program terminates without a warning, it follows the same computational path as its real-valued functional specification.},
  file        = {:http\://arxiv.org/pdf/2001.02981v1:PDF},
  keywords    = {cs.PL, cs.NA, math.NA},
}

@Article{Cartis2020,
  author      = {Coralia Cartis and Nick Gould and Philippe L. Toint},
  title       = {Strong Evaluation Complexity Bounds for Arbitrary-Order Optimization of Nonconvex Nonsmooth Composite Functions},
  date        = {2020-01-29},
  eprint      = {2001.10802},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We introduce the concept of strong high-order approximate minimizers for nonconvex optimization problems. These apply in both standard smooth and composite non-smooth settings, and additionally allow convex or inexpensive constraints. An adaptive regularization algorithm is then proposed to find such approximate minimizers. Under suitable Lipschitz continuity assumptions, whenever the feasible set is convex, it is shown that using a model of degree $p$, this algorithm will find a strong approximate q-th-order minimizer in at most ${\cal O}\left(\max_{1\leq j\leq q}\epsilon_j^{-(p+1)/(p-j+1)}\right)$ evaluations of the problem's functions and their derivatives, where $\epsilon_j$ is the $j$-th order accuracy tolerance; this bound applies when either $q=1$ or the problem is not composite with $q \leq 2$. For general non-composite problems, even when the feasible set is nonconvex, the bound becomes ${\cal O}\left(\max_{1\leq j\leq q}\epsilon_j^{-q(p+1)/p}\right)$ evaluations. If the problem is composite, and either $q > 1$ or the feasible set is not convex, the bound is then ${\cal O}\left(\max_{1\leq j\leq q}\epsilon_j^{-(q+1)}\right)$ evaluations. These results not only provide, to our knowledge, the first known bound for (unconstrained or inexpensively-constrained) composite problems for optimality orders exceeding one, but also give the first sharp bounds for high-order strong approximate $q$-th order minimizers of standard (unconstrained and inexpensively constrained) smooth problems, thereby complementing known results for weak minimizers.},
  file        = {:http\://arxiv.org/pdf/2001.10802v1:PDF},
  keywords    = {math.OC, 90C60, 90C46, 90C30, 90C26, 65K10, 49M37, F.2.1; G.1.6},
}

@Article{Bellavia2020,
  author      = {Stefania Bellavia and Gianmarco Gurioli},
  title       = {Complexity Analysis of a Stochastic Cubic Regularisation Method under Inexact Gradient Evaluations and Dynamic Hessian Accuracy},
  date        = {2020-01-29},
  eprint      = {2001.10827},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {We here adapt an extended version of the adaptive cubic regularisation method with dynamic inexact Hessian information for nonconvex optimisation in [2] to the stochastic optimisation setting. While exact function evaluations are still considered, this novel variant inherits the innovative use of adaptive accuracy requirements for Hessian approximations introduced in [2] and additionally employs inexact computations of the gradient. Without restrictions on the variance of the errors, we assume that these approximations are available within a sufficiently large, but fixed, probability and we extend, in the spirit of [13], the deterministic analysis of the framework to its stochastic counterpart, showing that the expected number of iterations to reach a first-order stationary point matches the well known worst-case optimal complexity. This is, in fact, still given by $O(epsilon^(-3/2))$, with respect to the first-order epsilon tolerance.},
  file        = {:http\://arxiv.org/pdf/2001.10827v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Feng2020,
  author   = {Z. {Feng}},
  title    = {{GRASS}: {GRAph} Spectral Sparsification Leveraging Scalable Spectral Perturbation Analysis},
  journal  = {{IEEE} Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year     = {2020},
  pages    = {1-1},
  issn     = {1937-4151},
  doi      = {10.1109/TCAD.2020.2968543},
  abstract = {Spectral graph sparsification aims to find ultra-sparse subgraphs whose Laplacian matrix can well approximate the original Laplacian eigenvalues and eigenvectors. In recent years, spectral sparsification techniques have been extensively studied for accelerating various numerical and graph-related applications. Prior nearly-linear-time spectral sparsification methods first extract low-stretch spanning tree from the original graph to form the backbone of the sparsifier, and then recover small portions of spectrally-critical off-tree edges to the spanning tree to significantly improve the approximation quality. However, it is not clear how many off-tree edges should be recovered for achieving a desired spectral similarity level within the sparsifier. Motivated by recent graph signal processing techniques, this paper proposes a similarity-aware spectral graph sparsification framework that leverages efficient spectral off-tree edge embedding and filtering schemes to construct spectral sparsifiers with guaranteed spectral similarity (relative condition number) level. An iterative graph densification scheme is also introduced to facilitate efficient and effective filtering of off-tree edges for highly ill-conditioned problems. The proposed method has been validated using various kinds of graphs obtained from public domain sparse matrix collections relevant to VLSI CAD, finite element analysis, as well as social and data networks frequently studied in many machine learning and data mining applications. For instance, a sparse SDD matrix with 40 million unknowns and 180 million nonzeros can be solved (1E-3 accuracy level) within two minutes using a single CPU core and about 6GB memory.},
  keywords = {Spectral graph theory;iterative matrix solver;graph partitioning;circuit analysis;perturbation analysis.},
}

@Article{Kardos2020,
  author      = {Juraj Kardos and Drosos Kourounis and Olaf Schenk},
  title       = {Reduced-Space Interior Point Methods in Power Grid Problems},
  date        = {2020-01-29},
  eprint      = {2001.10815},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Due to critical environmental issues, the power systems have to accommodate a significant level of penetration of renewable generation which requires smart approaches to the power grid control. Associated optimal control problems are large-scale nonlinear optimization problems with up to hundreds of millions of variables and constraints. The interior point methods become computationally intractable, mainly due to the solution of large linear systems. This document addresses the computational bottlenecks of the interior point method during the solution of the security constrained optimal power flow problems by applying reduced space quasi-Newton IPM, which could utilize high-performance computers due to the inherent parallelism in the adjoint method. Reduced space IPM approach and the adjoint method is a novel approach when it comes to solving the (security constrained) optimal power flow problems. These were previously used in the PDE-constrained optimization. The presented methodology is suitable for high-performance architectures due to inherent parallelism in the adjoint method during the gradient evaluation, since the individual contingency scenarios are modeled by independent set of the constraints. Preliminary evaluation of the performance and convergence is performed to study the reduced space approach.},
  file        = {:http\://arxiv.org/pdf/2001.10815v1:PDF},
  keywords    = {math.OC, cs.CE, cs.DC},
}

@Article{Sun2020,
  author   = {J. {Sun} and G. {Sun} and S. {Zhan} and J. {Zhang} and Y. {Chen}},
  title    = {Automated Performance Modeling of {HPC} Applications Using Machine Learning},
  journal  = {{IEEE} Transactions on Computers},
  year     = {2020},
  issn     = {2326-3814},
  doi      = {10.1109/TC.2020.2964767},
  abstract = {Automated performance modeling and performance prediction of parallel programs are highly valuable in many use cases, such as in guiding task management and job scheduling, offering insights of application behaviors, and assisting resource requirement estimation. The performance of parallel programs is affected by numerous factors, including but not limited to hardware, applications, algorithms, and input parameters, thus an accurate performance prediction is often a challenging task. In this study, we focus on automatically predicting the execution time of parallel programs with different inputs, at different scale, and without domain knowledge. We model the correlation between the execution time and domain-independent runtime features. These features include values of variables, counters of branches, loops, and MPI communications. After collecting data from executions with different inputs, a random forest machine learning approach is used to build an empirical performance model, which can predict the execution time of the program given an input. An instance-transfer learning method is used to reuse an existing model and improve the prediction on a new platform that lacks historical execution data. Our experiments and analyses of three parallel applications on three different systems confirm that our method performs well, with less than $20\%$ prediction error on average.},
  keywords = {Parallel computing;performance modeling;machine learning;model transferring},
}

@Article{Carson2020a,
  author     = {Carson, Erin and Higham, Nicholas J. and Pranesh, Srikara},
  title      = {Three-Precision {GMRES}-Based Iterative Refinement for Least Squares Problems},
  date       = {2020},
  issn       = {1749-9097},
  eprint     = {MIMS EPrint:2020.5},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2745/1/paper.pdf},
  abstract   = {The standard iterative refinement procedure for improving an approximate solution to the least squares problem $\text{min}_x ||b - Ax||_2$, where $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ has full rank, is based on solving the $(m + n) \times (m + n)$ augmented system with the aid of a QR factorization. In order to exploit multiprecision arithmetic, iterative refinement can be formulated to use three precisions, but the resulting algorithm converges only for a limited range of problems. We build an iterative refinement algorithm called GMRES-LSIR, analogous to the GMRES-IR algorithm developed for linear systems [SIAM J. Sci. Comput., 40 (2019), pp. A817-A847], that solves the augmented system using GMRES preconditioned by a matrix based on the computed QR factors. We explore two left preconditioners; the first has full off-diagonal blocks and the second is block diagonal and can be applied in either left-sided or split form. We prove that for a wide range of problems the first preconditioner yields backward and forward errors for the augmented system of order the working precision under suitable assumptions on the precisions and the problem conditioning. Our proof does not extend to the block diagonal preconditioner, but our numerical experiments show that with this preconditioner the algorithm performs about as well in practice.},
}

@PhdThesis{Alghunaim2020,
  author      = {Alghunaim, Sulaiman A.},
  title       = {On the Performance and Linear Convergence of Decentralized Primal-Dual Methods},
  institution = {University of California Los Angeles},
  date        = {2020},
  abstract    = {This dissertation studies the performance and linear convergence properties of primal-dual methods for the solution of decentralized multi-agent optimization problems. Decentralized multi-agent optimization is a powerful paradigm that finds applications in diverse fields in learning and engineering design. In these setups, a network of agents is connected through some topology and agents are allowed to share information only locally. Their overall goal is to seek the minimizer of a global optimization problem through localized interactions. In decentralized consensus problems, the agents are coupled through a common consensus variable that they need to agree upon. While in decentralized resource allocation problems, the agents are coupled through global affine constraints.\\ Various decentralized consensus optimization algorithms already exist in the literature. Some methods are derived from a primal-dual perspective, while other methods are derived as gradient tracking mechanisms meant to track the average of local gradients. Among the gradient tracking methods are the adapt-then-combine implementations motivated by diffusion strategies, which have been observed to perform better than other implementations. In this dissertation, we develop a novel adapt-then-combine primal-dual algorithmic framework that captures most state-of-the-art gradient based methods as special cases including all the variations of the gradient-tracking methods. We also develop a concise and novel analysis technique that establishes the linear convergence of this general framework under stronglyii convex objectives. Due to our unified framework, the analysis reveals important characteristics for these methods such as their convergence rates and step-size stability ranges. Moreover, the analysis reveals how the augmented Lagrangian penalty term, which is utilized in most of these methods, affects the performance of decentralized algorithms.\\ Another important question that we answer is whether decentralized proximal gradient methods can achieve global linear convergence for non-smooth composite optimization. For centralized algorithms, linear convergence has been established in the presence of a nonsmooth composite term. In this dissertation, we close the gap between centralized and decentralized proximal gradient algorithms and show that decentralized proximal algorithms can also achieve linear convergence in the presence of a non-smooth term. Furthermore, we show that when each agent possesses a different local non-smooth term then global linear convergence cannot be established in the worst case.\\ Most works that study decentralized optimization problems assume that all agents are involved in computing all variables. However, in many applications the coupling across agents is sparse in the sense that only a few agents are involved in computing certain variables. We show how to design decentralized algorithms in sparsely coupled consensus and resource allocation problems. More importantly, we establish analytically the importance of exploiting the sparsity structure in coupled large-scale networks.},
}

@InProceedings{Liew2019,
  author    = {Daniel Liew and Cristian Cadar and Alastair F. Donaldson and J. Ryan Stinnett},
  title     = {Just fuzz it: solving floating-point constraints using coverage-guided fuzzing},
  booktitle = {Proceedings of the 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  year      = {2019},
  series    = {ESEC/FSE 2019},
  publisher = {{ACM} Press},
  doi       = {10.1145/3338906.3338921},
  abstract  = {We investigate the use of coverage-guided fuzzing as a means of proving satisfiability of SMT formulas over finite variable domains, with specific application to floating-point constraints. We show how an SMT formula can be encoded as a program containing a location that is reachable if and only if the program's input corresponds to a satisfying assignment to the formula. A coverage-guided fuzzer can then be used to search for an input that reaches the location, yielding a satisfying assignment. We have implemented this idea in a tool, Just Fuzz-it Solver (JFS), and we present a large experimental evaluation showing that JFS is both competitive with and complementary to state-of-the-art SMT solvers with respect to solving floating-point constraints, and that the coverage-guided approach of JFS provides significant benefit over naive fuzzing in the floating-point domain. Applied in a portfolio manner, the JFS approach thus has the potential to complement traditional SMT solvers for program analysis tasks that involve reasoning about floating-point constraints.},
}

@Article{Brust2020,
  author   = {J. J. Brust and S. Leyffer and C. G. Petra},
  title    = {Compact Representations of Structured {BFGS} Matrices},
  year     = {2020},
  eprint   = {ANL/MCS-P9279-0120},
  abstract = {For general large-scale optimization problems compact representations exist in which recursive quasi-Newton update formulas are represented as compact matrix factorizations. For problems in which the objective function contains additional structure, so-called structured quasiNewton methods exploit available second-derivative information and approximate unavailable second derivatives. This article develops the compact representations of two structured Broyden-FletcherGoldfarb-Shanno update formulas. The compact representations enable efficient limited memory and initialization strategies. Two limited memory line search algorithms are described and tested on a collection of problems.},
}

@Article{Kamzolov2020,
  author      = {Dmitry Kamzolov and Alexander Gasnikov and Pavel Dvurechensky},
  title       = {On the Optimal Combination of Tensor Optimization Methods},
  date        = {2020-02-03},
  eprint      = {2002.01004},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We consider the minimization problem of a sum of a number of functions having Lipshitz $p$-th order derivatives with different Lipschitz constants. In this case, to accelerate optimization, we propose a general framework allowing to obtain near-optimal oracle complexity for each function in the sum separately, meaning, in particular, that the oracle for a function with lower Lipschitz constant is called a smaller number of times. As a building block, we extend the current theory of tensor methods and show how to generalize near-optimal tensor methods to work with inexact tensor step. Further, we investigate the situation when the functions in the sum have Lipschitz derivatives of a different order. For this situation, we propose a generic way to separate the oracle complexity between the parts of the sum. Our method is not optimal, which leads to an open problem of the optimal combination of oracles of a different order.},
  file        = {:http\://arxiv.org/pdf/2002.01004v1:PDF},
  keywords    = {math.OC},
}

@Article{Zheng2020,
  author      = {Qingqing Zheng and Yuanzhe Xi and Yousef Saad},
  title       = {A power Schur complement Low-Rank correction preconditioner for general sparse linear systems},
  date        = {2020-02-03},
  eprint      = {2002.00917},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {An effective power based parallel preconditioner is proposed for general large sparse linear systems. The preconditioner combines a power series expansion method with some low-rank correction techniques, where the Sherman-Morrison-Woodbury formula is utilized. A matrix splitting of the Schur complement is proposed to expand the power series. The number of terms used in the power series expansion can control the approximation accuracy of the preconditioner to the inverse of the Schur complement. To construct the preconditioner, graph partitioning is invoked to reorder the original coefficient matrix, leading to a special block two-by-two matrix whose two off-diagonal submatrices are block diagonal. Variables corresponding to interface variables are obtained by solving a linear system with the coeffcient matrix being the Schur complement. For the variables related to the interior variables, one only needs to solve a block diagonal linear system. This can be performed efficiently in parallel. Various numerical examples are provided to illustrate that the efficiency of the proposed preconditioner.},
  file        = {:http\://arxiv.org/pdf/2002.00917v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Martinsson2020,
  author      = {Per-Gunnar Martinsson and Joel Tropp},
  title       = {Randomized Numerical Linear Algebra: Foundations \& Algorithms},
  date        = {2020-02-04},
  eprint      = {2002.01387},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues.\\ Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nyström approximation of positive-semidefinite matrices; single view ("streaming") algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
  file        = {:http\://arxiv.org/pdf/2002.01387v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Rodomanov2020,
  author      = {Anton Rodomanov and Yurii Nesterov},
  title       = {Greedy Quasi-Newton Methods with Explicit Superlinear Convergence},
  date        = {2020-02-03},
  eprint      = {2002.00657},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we study greedy variants of quasi-Newton methods. They are based on the updating formulas from a certain subclass of the Broyden family. In particular, this subclass includes the well-known DFP, BFGS and SR1 updates. However, in contrast to the classical quasi-Newton methods, which use the difference of successive iterates for updating the Hessian approximations, our methods apply basis vectors, greedily selected so as to maximize a certain measure of progress. For greedy quasi-Newton methods, we establish an explicit non-asymptotic bound on their rate of local superlinear convergence, which contains a contraction factor, depending on the square of the iteration counter. We also show that these methods produce Hessian approximations whose deviation from the exact Hessians linearly convergences to zero.},
  file        = {:http\://arxiv.org/pdf/2002.00657v1:PDF},
  keywords    = {math.OC},
}

@PhdThesis{Huang2019,
  author      = {Ruihao Huang},
  title       = {Novel COmputational Methods for Eigenvalue Problems},
  institution = {Michigan Technological University},
  year        = {2019},
  url         = {https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2090&context=etdr},
  abstract    = {This dissertation focuses on novel computational method for eigenvalue problems. In Chapter 1, preliminaries of functional analysis related to eigenvalue problems are presented. Some classical methods for matrix eigenvalue problems are discussed. Several PDE eigenvalue problems are covered. The chapter is concluded with a summary of the contributions. In Chapter 2, a novel recursive contour integral method (RIM) for matrix eigenvalue problem is proposed. This method can effectively find all eigenvalues in a region on the complex plane with no a priori spectrum information. Regions that contain eigenvalues are subdivided and tested recursively until the size of region reaches specified precision. The method is robust, which is demonstrated using various examples. In Chapter 3, we propose an improved version of RIM for non-Hermitian eigenvalue problems, called SIM-M. By incorporating Cayley transformation and Arnoldi’s method, the main computation cost of solving linear systems is reduced significantly. The numerical experiments demonstrate that RIM-M gains significant speed-up over RIM.\\ In Chapter 4, we propose a multilevel spectral indicator method (SIM-M) to address the memory requirement for large sparse matrices. We modify the indicator of RIM-M such that it requires much less memory. Matrices from University of Florida Sparse Matrix Collection are tested, suggesting that a parallel version of SIM-M has the potential to be efficient.\\ In Chapter 5, we develop a novel method to solve the elliptic PDE eigenvalue problem. We construct a multi-wavelet basis with Riesz stability in H1 0 ($\Omega$). By incorporating multi-grid discretization scheme and sparse grids, the method retains the optimal convergence rate for the smallest eigenvalue with much less computational cost.},
}

@Article{Lin2020,
  author      = {Tianyi Lin and Chi Jin and Michael. I. Jordan},
  title       = {Near-Optimal Algorithms for Minimax Optimization},
  date        = {2020-02-05},
  eprint      = {2002.02417},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This paper resolves a longstanding open question pertaining to the design of near-optimal first-order algorithms for smooth and strongly-convex-strongly-concave minimax problems. Current state-of-the-art first-order algorithms find an approximate Nash equilibrium using $\tilde{O}(\kappa_{\mathbf x}+\kappa_{\mathbf y})$ or $\tilde{O}(\min\{\kappa_{\mathbf x}\sqrt{\kappa_{\mathbf y}}, \sqrt{\kappa_{\mathbf x}}\kappa_{\mathbf y}\})$ gradient evaluations, where $\kappa_{\mathbf x}$ and $\kappa_{\mathbf y}$ are the condition numbers for the strong-convexity and strong-concavity assumptions. A gap remains between these results and the best existing lower bound $\tilde{\Omega}(\sqrt{\kappa_{\mathbf x}\kappa_{\mathbf y}})$. This paper presents the first algorithm with $\tilde{O}(\sqrt{\kappa_{\mathbf x}\kappa_{\mathbf y}})$ gradient complexity, matching the lower bound up to logarithmic factors. Our new algorithm is designed based on an accelerated proximal point method and an accelerated solver for minimax proximal steps. It can be easily extended to the settings of strongly-convex-concave, convex-concave, nonconvex-strongly-concave, and nonconvex-concave functions. This paper also presents algorithms that match or outperform all existing methods in these settings in terms of gradient complexity, up to logarithmic factors.},
  file        = {:http\://arxiv.org/pdf/2002.02417v1:PDF},
  keywords    = {math.OC, cs.LG, stat.ML},
}

@Article{Rahimian2020,
  author   = {Hamed Rahimian and Sanjay Mehrotra},
  title    = {Sequential Convexification of a Bilinear Set},
  url      = {http://www.optimization-online.org/DB_FILE/2020/01/7595.pdf},
  year     = {2020},
  abstract = {We present a sequential convexification procedure to derive, in the limit, a set arbitrary close to the convex hull of $\epsilon$-feasible solutions to a general nonconvex continuous bilinear set. Recognizing that bilinear terms can be represented with a finite number nonlinear nonconvex constraints in the lifted matrix space, our procedure performs a sequential convexification with respect to all nonlinear nonconvex constraints. Moreover, our approach relies on generating liftand-project cuts using simple 0-1 disjunctions, where cuts are generated at all fractional extreme point solutions of the current relaxation. An implication of our convexification procedure is that the constraints describing the convex hull can be used in a cutting plane algorithm to solve a linear optimization problem over the bilinear set to $\epsilon$-optimality},
}

@InProceedings{BenKhalifa2019,
  author    = {Dorra Ben Khalifa and Matthieu Martel and Assalé Adjé},
  title     = {{POP:} A Tuning Assistant for Mixed-Precision Floating-Point Computations},
  booktitle = {Proceedings of the 7th International Workshop on Formal Techniques for Safety-Critical Systems},
  year      = {2019},
  series    = {FTSCS '19},
  abstract  = {In this article, we describe a static program analysis to determine the lowest floating-point precisions on inputs and intermediate results that guarantees a desired accuracy of the output values. A common practice used by developers without advanced training in computer arithmetic consists in using the highest precision available in hardware (double precision on most CPU's) which can be exorbitant in terms of energy consumption, memory traffic, and bandwidth capacity. To overcome this difficulty, we propose a new precision tuning tool for the floatingpoint programs integrating a static forward and backward analysis, done by abstract interpretation. Next, our analysis will be expressed as a set of linear constraints easily checked by an SMT solver.},
}

@Article{Solomonik2020,
  author    = {Edgar Solomonik and James Demmel},
  title     = {Fast Bilinear Algorithms for Symmetric Tensor Contractions},
  journal   = {Computational Methods in Applied Mathematics},
  year      = {2020},
  month     = {2},
  doi       = {10.1515/cmam-2019-0075},
  abstract  = {In matrix-vector multiplication, matrix symmetry does not permit a straightforward reduction in computational cost. More generally, in contractions of symmetric tensors, the symmetries are not preserved in the usual algebraic form of contraction algorithms. We introduce an algorithm that reduces the bilinear complexity (number of computed elementwise products) for most types of symmetric tensor contractions. In particular, it lowers the bilinear complexity of symmetrized contractions of symmetric tensors of order $s+v$ and $v+t$ by a factor of $\frac{(s+t+v)!}{s!t!v!}$ to leading order. The algorithm computes a symmetric tensor of bilinear products, then subtracts unwanted parts of its partial sums. Special cases of this algorithm provide improvements to the bilinear complexity of the multiplication of a symmetric matrix and a vector, the symmetrized vector outer product, and the symmetrized product of symmetric matrices. While the algorithm requires more additions for each elementwise product, the total number of operations is in some cases less than classical algorithms, for tensors of any size. We provide a round-off error analysis of the algorithm and demonstrate that the error is not too large in practice. Finally, we provide an optimized implementation for one variant of the symmetry-preserving algorithm, which achieves speedups of up to 4.58$\times$ for a particular tensor contraction, relative to a classical approach that casts the problem as a matrix-matrix multiplication.},
  publisher = {Walter de Gruyter {GmbH}},
}

@Article{Guettel2020,
  author      = {Stefan Güttel and Daniel Kressner and Kathryn Lund},
  title       = {Limited-memory polynomial methods for large-scale matrix functions},
  date        = {2020-02-05},
  eprint      = {2002.01682},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {Matrix functions are a central topic of linear algebra, and problems requiring their numerical approximation appear increasingly often in scientific computing. We review various limited-memory methods for the approximation of the action of a large-scale matrix function on a vector. Emphasis is put on polynomial methods, whose memory requirements are known or prescribed a priori. Methods based on explicit polynomial approximation or interpolation, as well as restarted Arnoldi methods, are treated in detail. An overview of existing software is also given, as well as a discussion of challenging open problems.},
  file        = {:http\://arxiv.org/pdf/2002.01682v2:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Bauch2020,
  author      = {Jonathan Bauch and Boaz Nadler},
  title       = {Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries},
  date        = {2020-02-05},
  eprint      = {2002.01849},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We present a new, simple and computationally efficient iterative method for low rank matrix completion. Our method is inspired by the class of factorization-type iterative algorithms, but substantially differs from them in the way the problem is cast. Precisely, given a target rank $r$, instead of optimizing on the manifold of rank $r$ matrices, we allow our interim estimated matrix to have a specific over-parametrized rank $2r$ structure. Our algorithm, denoted \texttt{R2RILS}, for rank $2r$ iterative least squares, thus has low memory requirements, and at each iteration it solves a computationally cheap sparse least-squares problem. We motivate our algorithm by its theoretical analysis for the simplified case of a rank-1 matrix. Empirically, \texttt{R2RILS} is able to recover, with machine precision, ill conditioned low rank matrices from very few observations -- near the information limit. Finally, \texttt{R2RILS} is stable to corruption of the observed entries by additive zero mean Gaussian noise.},
  file        = {:http\://arxiv.org/pdf/2002.01849v1:PDF},
  keywords    = {math.OC},
}

@Article{Kressner2020,
  author      = {Daniel Kressner and Kathryn Lund and Stefano Massei and Davide Palitta},
  title       = {Compress-and-restart block Krylov subspace methods for Sylvester matrix equations},
  date        = {2020-02-04},
  eprint      = {2002.01506},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {Block Krylov subspace methods (KSMs) comprise building blocks in many state-of-the-art solvers for large-scale matrix equations as they arise, e.g., from the discretization of partial differential equations. While extended and rational block Krylov subspace methods provide a major reduction in iteration counts over polynomial block KSMs, they also require reliable solvers for the coefficient matrices, and these solvers are often iterative methods themselves. It is not hard to devise scenarios in which the available memory, and consequently the dimension of the Krylov subspace, is limited. In such scenarios for linear systems and eigenvalue problems, restarting is a well explored technique for mitigating memory constraints. In this work, such restarting techniques are applied to polynomial KSMs for matrix equations with a compression step to control the growing rank of the residual. An error analysis is also performed, leading to heuristics for dynamically adjusting the basis size in each restart cycle. A panel of numerical experiments demonstrates the effectiveness of the new method with respect to extended block KSMs.},
  file        = {:http\://arxiv.org/pdf/2002.01506v1:PDF},
  keywords    = {math.NA, cs.NA, 65F10, 65N22, 65J10, 65F30, 65F50},
}

@Article{Awwal2020,
  author      = {Aliyu Muhammed Awwal and Poom Kumam and Hassan Mohammad},
  title       = {Iterative algorithm with structured diagonal Hessian approximation for solving nonlinear least squares problems},
  date        = {2020-02-05},
  eprint      = {2002.01871},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Nonlinear least-squares problems are a special class of unconstrained optimization problems in which their gradient and Hessian have special structures. In this paper, we exploit these structures and proposed a matrix-free algorithm with a diagonal Hessian approximation for solving nonlinear least-squares problems. We devise appropriate safeguarding strategies to ensure the Hessian matrix is positive definite throughout the iteration process. The proposed algorithm generates descent direction and is globally convergent. Preliminary numerical experiments show that the proposed method is competitive with a recently developed similar method.},
  file        = {:http\://arxiv.org/pdf/2002.01871v1:PDF},
  keywords    = {math.OC, 90C30, 65K05, 49M37},
}

@Article{Dvurechensky2020,
  author      = {Pavel Dvurechensky and Shimrit Shtern and Mathias Staudigl and Petr Ostroukhov and Kamil Safin},
  title       = {Self-concordant analysis of Frank-Wolfe algorithms},
  date        = {2020-02-11},
  eprint      = {2002.04320},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Projection-free optimization via different variants of the Frank-Wolfe (FW) method has become one of the cornerstones in optimization for machine learning since in many cases the linear minimization oracle is much cheaper to implement than projections and some sparsity needs to be preserved. In a number of applications, e.g. Poisson inverse problems or quantum state tomography, the loss is given by a self-concordant (SC) function having unbounded curvature, implying absence of theoretical guarantees for the existing FW methods. We use the theory of SC functions to provide a new adaptive step size for FW methods and prove global convergence rate $O(\frac{1}{k})$, $k$ being the iteration counter. If the problem can be represented by a local linear minimization oracle, we are the first to propose a FW method with linear convergence rate without assuming neither strong convexity nor a Lipschitz continuous gradient.},
  file        = {:http\://arxiv.org/pdf/2002.04320v1:PDF},
  keywords    = {math.OC, cs.LG, stat.CO, 65K05, 90C25,},
}

@Article{Alimisis2020,
  author      = {Foivos Alimisis and Antonio Orvieto and Gary Bécigneul and Aurelien Lucchi},
  title       = {Practical Accelerated Optimization on Riemannian Manifolds},
  date        = {2020-02-11},
  eprint      = {2002.04144},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We develop a new Riemannian descent algorithm with an accelerated rate of convergence. We focus on functions that are geodesically convex or weakly-quasi-convex, which are weaker function classes compared to prior work that has considered geodesically strongly convex functions. Our proof of convergence relies on a novel estimate sequence which allows to demonstrate the dependency of the convergence rate on the curvature of the manifold. We validate our theoretical results empirically on several optimization problems defined on a sphere and on the manifold of positive definite matrices.},
  file        = {:http\://arxiv.org/pdf/2002.04144v1:PDF},
  keywords    = {math.OC},
}

@Article{Zhang2020,
  author      = {Jingzhao Zhang and Hongzhou Lin and Suvrit Sra and Ali Jadbabaie},
  title       = {On Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions},
  date        = {2020-02-10},
  eprint      = {2002.04130},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {We provide the first \emph{non-asymptotic} analysis for finding stationary points of nonsmooth, nonconvex functions. In particular, we study the class of Hadamard semi-differentiable functions, perhaps the largest class of nonsmooth functions for which the chain rule of calculus holds. This class contains important examples such as ReLU neural networks and others with non-differentiable activation functions. First, we show that finding an $\epsilon$-stationary point with first-order methods is impossible in finite time. Therefore, we introduce the notion of \emph{$(\delta, \epsilon)$-stationarity}, a generalization that allows for a point to be within distance $\delta$ of an $\epsilon$-stationary point and reduces to $\epsilon$-stationarity for smooth functions. We propose a series of randomized first-order methods and analyze their complexity of finding a $(\delta, \epsilon)$-stationary point. Furthermore, we provide a lower bound and show that our stochastic algorithm has min-max optimal dependence on $\delta$. Empirically, our methods perform well for training ReLU neural networks.},
  file        = {:http\://arxiv.org/pdf/2002.04130v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Zhang2020a,
  author    = {Yi Zhang and Nikolaos V. Sahinidis and Carlos Nohra and Gang Rong},
  title     = {Optimality-based domain reduction for inequality-constrained {NLP} and {MINLP} problems},
  journal   = {Journal of Global Optimization},
  year      = {2020},
  month     = {2},
  doi       = {10.1007/s10898-020-00886-z},
  abstract  = {In spatial branch-and-bound algorithms, optimality-based domain reduction is normally performed after solving a node and relies on duality information to reduce ranges of variables. In this work, we propose novel optimality conditions for NLP and MINLP problems and apply them for domain reduction prior to solving a node in branch-and-bound. The conditions apply to nonconvex inequality-constrained problems for which we exploit monotonicity properties of objectives and constraints. We develop three separate reduction algorithms for unconstrained, one-constraint, and multi-constraint problems. We use the optimality conditions to reduce ranges of variables through forward and backward bound propagation of gradients respective to each decision variable. We describe an efficient implementation of these techniques in the branch-and-bound solver BARON. The implementation dynamically recognizes and ignores inactive constraints at each node of the search tree. Our computations demonstrate that the proposed techniques often reduce the solution time and total number of nodes for continuous problems; they are less effective for mixed-integer programs.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Mai2020,
  author      = {Ngoc Hoang Anh Mai and Victor Magron and Jean-Bernard Lasserre},
  title       = {A sparse version of Reznick's Positivstellensatz},
  date        = {2020-02-12},
  eprint      = {2002.05101},
  eprinttype  = {arXiv},
  eprintclass = {math.AG},
  abstract    = {If $f$ is a positive definite form, Reznick's Positivstellensatz [Mathematische Zeitschrift. 220 (1995), pp. 75--97] states that there exists $k\in\mathbf{N}$ such that ${\| x \|^{2k}_2}f$ is a sum of squares of polynomials. Assuming that $f$ can be written as a sum of forms $\sum_{l=1}^p f_l$, where each $f_l$ depends on a subset of the initial variables, and assuming that these subsets satisfy the so-called running intersection property, we provide a sparse version of Reznick's Positivstellensatz. Namely, there exists $k \in \mathbf{N}$ such that $f=\sum_{l = 1}^p {{\sigma_l}/{H_l^{k}}}$, where $\sigma_l$ is a sum of squares of polynomials, $H_l$ is a uniform polynomial denominator, and both polynomials $\sigma_l,H_l$ involve the same variables as $f_l$, for each $l=1,\dots,p$. In other words, the sparsity pattern of $f$ is also reflected in this sparse version of Reznick's certificate of positivity. We next use this result to also obtain positivity certificates for (i) polynomials nonnegative on the whole space and (ii) polynomials nonnegative on a (possibly non-compact) basic semialgebraic set, assuming that the input data satisfy the running intersection property. Both are sparse versions of a positivity certificate due to Putinar and Vasilescu.},
  file        = {:http\://arxiv.org/pdf/2002.05101v2:PDF},
  keywords    = {math.AG},
}

@TechReport{Nesterov2020,
  author      = {Yurii Nesterov},
  title       = {Superfast second-order methods for Unconstrained Convex Optimization},
  institution = {UCL - SSH/LIDAM/CORE - Center for operations research and econometrics},
  year        = {2020},
  eprint      = {2020/07},
  eprinttype  = {CORE Discussion Papers},
  url         = {https://dial.uclouvain.be/pr/boreal/object/boreal%3A227146/datastream/PDF_01/view},
  abstract    = {In this paper, we present new second-order methods with converge rate $O(k^{-4})$, where $k$ is the iteration counter. This is faster that the existing lower bound for this type of schemes [1, 2], which is $O(k^{-7/2})$. Our progress can be explained by a finer specification of the problem class. The main idea of this approach consists in implementation of the third-order scheme from [15] using the second-order oracle. At each iteration of our method, we solve a nontrivial auxiliary problem by a linearly convergent scheme based on the relative non-degeneracy condition [3, 10]. During this process, the Hessian of the objective function is computed once, and the gradient is computed $O(\text{ln}\frac{1}{\epsilon})$ times, where $\epsilon$ is the desired accuracy of the solution for our problem.},
}

@TechReport{Nesterov2020a,
  author      = {Yurii Nesterov},
  title       = {Inexact Accelerated High-Order Proximal-Point Methods},
  institution = {UCL - SSH/LIDAM/CORE - Center for operations research and econometrics},
  year        = {2020},
  eprint      = {2020/08},
  eprinttype  = {CORE Discussion Papers},
  url         = {http://hdl.handle.net/2078.1/227219},
  abstract    = {In this paper, we present a new framework of Bi-Level Unconstrained Minimization (BLUM) for development of accelerated methods in Convex Programming. These methods use approximations of the high-order proximal points, which are solutions of some auxiliary parametric optimization problems. For computing these points, we can use different methods, and, in particular, the lower-order schemes. This opens a possibility for the latter methods to overpass traditional limits of the Complexity Theory. As an example, we obtain a new second-order method with the convergence rate $O(k^{-4})$ , where $k$ is the iteration counter. This rate is better than the maximal possible rate of convergence for this type of methods, as applied to functions with Lipschitz continuous Hessian. We also present new methods with the exact auxiliary search procedure, which have the rate of convergence $O(k^{-(3p+1)/2})$, where $p \ge 1$ is the order of the proximal operator. The auxiliary problem at each iteration of these schemes is convex.},
}

@Article{Legat2020,
  author      = {Benoit Legat and Oscar Dowson and Joaquim Dias Garcia and Miles Lubin},
  title       = {MathOptInterface: a data structure for mathematical optimization problems},
  date        = {2020-02-09},
  eprint      = {2002.03447},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {JuMP is an open-source algebraic modeling language in the Julia language. In this work, we discuss a complete re-write of JuMP based on a novel abstract data structure, which we call MathOptInterface, for representing instances of mathematical optimization problems. MathOptInterface is significantly more general than existing data structures in the literature, encompassing, for example, a spectrum of problems classes from integer programming with indicator constraints to bilinear semidefinite programming. We highlight the challenges that arise from this generality, and how we overcame them in the re-write of JuMP.},
  file        = {:http\://arxiv.org/pdf/2002.03447v1:PDF},
  keywords    = {math.OC},
}

@Article{Chen2020a,
  author      = {Tong Chen and Jean-Bernard Lasserre and Victor Magron and Edouard Pauwels},
  title       = {Polynomial Optimization for Bounding Lipschitz Constants of Deep Networks},
  date        = {2020-02-10},
  eprint      = {2002.03657},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {The Lipschitz constant of a network plays an important role in many applications of deep learning, such as robustness certification and Wasserstein Generative Adversarial Network. We introduce a semidefinite programming hierarchy to estimate the global and local Lipschitz constant of a multiple layer deep neural network. The novelty is to combine a polynomial lifting for ReLU functions derivatives with a weak generalization of Putinar's positivity certificate. This idea could also apply to other, nearly sparse, polynomial optimization problems in machine learning. We empirically demonstrate that our method not only runs faster than state-of-the-art linear programming based method, but also provides sharper bounds.},
  file        = {:http\://arxiv.org/pdf/2002.03657v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Huang2020,
  author      = {Xunpeng Huang and Xianfeng Liang and Zhengyang Liu and Yue Yu and Lei Li},
  title       = {SPAN: A Stochastic Projected Approximate Newton Method},
  date        = {2020-02-10},
  eprint      = {2002.03687},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {Second-order optimization methods have desirable convergence properties. However, the exact Newton method requires expensive computation for the Hessian and its inverse. In this paper, we propose SPAN, a novel approximate and fast Newton method. SPAN computes the inverse of the Hessian matrix via low-rank approximation and stochastic Hessian-vector products. Our experiments on multiple benchmark datasets demonstrate that SPAN outperforms existing first-order and second-order optimization methods in terms of the convergence wall-clock time. Furthermore, we provide a theoretical analysis of the per-iteration complexity, the approximation error, and the convergence rate. Both the theoretical analysis and experimental results show that our proposed method achieves a better trade-off between the convergence rate and the per-iteration efficiency.},
  file        = {:http\://arxiv.org/pdf/2002.03687v1:PDF},
  keywords    = {math.OC, cs.LG, 90C15, 90C25},
}

@Article{Muehlebach2020,
  author      = {Michael Muehlebach and Michael I. Jordan},
  title       = {Continuous-time Lower Bounds for Gradient-based Algorithms},
  date        = {2020-02-10},
  eprint      = {2002.03546},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {This article derives lower bounds on the convergence rate of continuous-time gradient-based optimization algorithms. The algorithms are subjected to a time-normalization constraint that avoids a reparametrization of time in order to make the discussion of continuous-time convergence rates meaningful. We reduce the multi-dimensional problem to a single dimension, recover well-known lower bounds from the discrete-time setting, and provide insights into why these lower bounds occur. We further explicitly provide algorithms that achieve the proposed lower bounds, even when the function class under consideration includes certain non-convex functions.},
  file        = {:http\://arxiv.org/pdf/2002.03546v1:PDF},
  keywords    = {math.OC, cs.SY, eess.SY},
}

@Article{Tutunov2020,
  author      = {Rasul Tutunov and Minne Li and Jun Wang and Haitham Bou-Ammar},
  title       = {Compositional ADAM: An Adaptive Compositional Solver},
  date        = {2020-02-10},
  eprint      = {2002.03755},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {In this paper, we present C-ADAM, the first adaptive solver for compositional problems involving a non-linear functional nesting of expected values. We proof that C-ADAM converges to a stationary point in $\mathcal{O}(\delta^{-2.25})$ with $\delta$ being a precision parameter. Moreover, we demonstrate the importance of our results by bridging, for the first time, model-agnostic meta-learning (MAML) and compositional optimisation showing fastest known rates for deep network adaptation to-date. Finally, we validate our findings in a set of experiments from portfolio optimisation and meta-learning. Our results manifest significant sample complexity reductions compared to both standard and compositional solvers.},
  file        = {:http\://arxiv.org/pdf/2002.03755v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Article{Song2020,
  author      = {Yang Song and Chenlin Meng and Renjie Liao and Stefano Ermon},
  title       = {Nonlinear Equation Solving: A Faster Alternative to Feedforward Computation},
  date        = {2020-02-10},
  eprint      = {2002.03629},
  eprinttype  = {arXiv},
  eprintclass = {cs.LG},
  abstract    = {Feedforward computations, such as evaluating a neural network or sampling from an autoregressive model, are ubiquitous in machine learning. The sequential nature of feedforward computation, however, requires a strict order of execution and cannot be easily accelerated with parallel computing. To enable parrallelization, we frame the task of feedforward computation as solving a system of nonlinear equations. We then propose to find the solution using a Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods of both. Crucially, Jacobi updates operate independently on each equation and can be executed in parallel. Our method is guaranteed to give exactly the same values as the original feedforward computation with a reduced (or equal) number of parallel iterations. Experimentally, we demonstrate the effectiveness of our approach in accelerating 1) the evaluation of DenseNets on ImageNet and 2) autoregressive sampling of MADE and PixelCNN. We are able to achieve between 1.2 and 33 speedup factors under various conditions and computation models.},
  file        = {:http\://arxiv.org/pdf/2002.03629v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Keriven2020,
  author      = {Nicolas Keriven and Samuel Vaiter},
  title       = {Sparse and Smooth: improved guarantees for Spectral Clustering in the Dynamic Stochastic Block Model},
  date        = {2020-02-07},
  eprint      = {2002.02892},
  eprinttype  = {arXiv},
  eprintclass = {stat.ML},
  abstract    = {In this paper, we analyse classical variants of the Spectral Clustering (SC) algorithm in the Dynamic Stochastic Block Model (DSBM). Existing results show that, in the relatively sparse case where the expected degree grows logarithmically with the number of nodes, guarantees in the static case can be extended to the dynamic case and yield improved error bounds when the DSBM is sufficiently smooth in time, that is, the communities do not change too much between two time steps. We improve over these results by drawing a new link between the sparsity and the smoothness of the DSBM: the more regular the DSBM is, the more sparse it can be, while still guaranteeing consistent recovery. In particular, a mild condition on the smoothness allows to treat the sparse case with bounded degree. We also extend these guarantees to the normalized Laplacian, and as a by-product of our analysis, we obtain to our knowledge the best spectral concentration bound available for the normalized Laplacian of matrices with independent Bernoulli entries.},
  file        = {:http\://arxiv.org/pdf/2002.02892v2:PDF},
  keywords    = {stat.ML, cs.LG, math.ST, stat.TH},
}

@InProceedings{Zeni2020,
  author       = {Alberto Zeni and Giulia Guidi and Marquita Ellis and Nan Ding and Marco D. Santambrogio and Steven Hofmeyr and Aydın Buluç and Leonid Oliker and Katherine Yelick},
  title        = {{LOGAN}: High-Performance GPU-Based X-Drop Long-Read Alignment},
  date         = {2020-02-12},
  series       = {IPDPS 2020},
  eprint       = {2002.05200},
  eprinttype   = {arXiv},
  eprintclass  = {q-bio.GN},
  abstract     = {Pairwise sequence alignment is one of the most computationally intensive kernels in genomic data analysis, accounting for more than 90\% of the runtime for key bioinformatics applications. This method is particularly expensive for third-generation sequences due to the high computational cost of analyzing sequences of length between 1Kb and 1Mb. Given the quadratic overhead of exact pairwise algorithms for long alignments, the community primarily relies on approximate algorithms that search only for high-quality alignments and stop early when one is not found. In this work, we present the first GPU optimization of the popular X-drop alignment algorithm, that we named LOGAN. Results show that our high-performance multi-GPU implementation achieves up to 181.6 GCUPS and speed-ups up to 6.6$\times$ and 30.7$\times$ using 1 and 6 NVIDIA Tesla V100, respectively, over the state-of-the-art software running on two IBM Power9 processors using 168 CPU threads, with equivalent accuracy. We also demonstrate a 2.3$\times$ LOGAN speed-up versus ksw2, a state-of-art vectorized algorithm for sequence alignment implemented in minimap2, a long-read mapping software. To highlight the impact of our work on a real-world application, we couple LOGAN with a many-to-many long-read alignment software called BELLA, and demonstrate that our implementation improves the overall BELLA runtime by up to 10.6$\times$. Finally, we adapt the Roofline model for LOGAN and demonstrate that our implementation is near-optimal on the NVIDIA Tesla V100s.},  
  file         = {:http\://arxiv.org/pdf/2002.05200v1:PDF},
  journaltitle = {Proceedings of the 34th IEEE International Parallel and Distributed Processing Symposium},
  keywords     = {q-bio.GN, cs.DC},
}

@InProceedings{Mutlu2019,
  author    = {B. O. {Mutlu} and G. {Kestor} and A. {Cristal} and O. {Unsal} and S. {Krishnamoorthy}},
  title     = {Ground-Truth Prediction to Accelerate Soft-Error Impact Analysis for Iterative Methods},
  booktitle = {Proceedings of the 26th IEEE International Conference on High Performance Computing, Data, and Analytics},
  year      = {2019},
  series    = {HiPC '19},
  month     = dec,
  pages     = {333-344},
  doi       = {10.1109/HiPC.2019.00048},
  abstract  = {Understanding the impact of soft errors on applications can be expensive. Often, it requires an extensive error injection campaign involving numerous runs of the full application in the presence of errors. In this paper, we present a novel approach to arriving at the ground truth-the true impact of an error on the final output-for iterative methods by observing a small number of iterations to learn deviations between normal and error-impacted execution. We develop a machine learning based predictor for three iterative methods to generate ground-truth results without running them to completion for every error injected. We demonstrate that this approach achieves greater accuracy than alternative prediction strategies, including three existing soft error detection strategies. We demonstrate the effectiveness of the ground truth prediction model in evaluating vulnerability and the effectiveness of soft error detection strategies in the context of iterative methods.},
  issn      = {1094-7256},
  keywords  = {Iterative Solvers;Failure Prediction;Machine Leraning;Soft Errors;Detectors},
}

@InProceedings{Afibuzzaman2019,
  author    = {M. {Afibuzzaman} and F. {Rabbi} and M. Y. {Özkaya} and H. M. {Aktulga} and U. V. {Çatalyürek}},
  title     = {DeepSparse: A Task-Parallel Framework for SparseSolvers on Deep Memory Architectures},
  booktitle = {2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)},
  year      = {2019},
  month     = dec,
  pages     = {373-382},
  doi       = {10.1109/HiPC.2019.00052},
  abstract  = {Data movement is an important bottleneck against efficiency and energy consumption in large-scale sparse matrix computations that are commonly used in linear solvers, eigensolvers and graph analytics. We introduce a novel task-parallel sparse solver framework, named DeepSparse, which adopts a fully integrated task-parallel approach. DeepSparse framework differs from existing work in that it adopts a holistic approach that targets all computational steps in a sparse solver rather than narrowing the problem into small kernels (e.g., SpMM, SpMV). We present the implementation details of DeepSparse and demonstrate its merit in two popular eigensolvers, LOBPCG and Lanczos algorithms. We observe that DeepSparse achieves 2$\times$ - 16$\times$ fewer cache misses across different cache layers (L1, L2 and L3) over implementations of the same solvers based on optimized library function calls. We also achieve 2$\times$ - 3.9$\times$ improvement in execution time when using DeepSparse over the same library versions.},
  issn      = {1094-7256},
  keywords  = {Sparse solvers, task parallelism, data dependency graphs, performance optimization.},
}

@InProceedings{Botoeva2020,
  author    = {lena Botoeva and Panagiotis Kouvaros and Jan Kronqvist and Alessio Lomuscio and Ruth Misener},
  title     = {Efficient Verification of ReLU-based Neural Networks via Dependency Analysis},
  booktitle = {Proceedings of the 34th AAAI Conference on Artificial Intelligence},
  series    = {AAI20},
  year      = {2020},
  abstract  = {We introduce an efficient method for the verification of ReLU-based feed-forward neural networks. We derive an automated procedure that exploits dependency relations between the ReLU nodes, thereby pruning the search tree that needs to be considered by MILP-based formulations of the verification problem. We augment the resulting algorithm with methods for input domain splitting and symbolic interval propagation. We present Venus, the resulting verification toolkit, and evaluate it on the ACAS collision avoidance networks and models trained on the MNIST and CIFAR-10 datasets. The experimental results obtained indicate considerable gains over the present state-of-the-art tools.},
}

@Article{Moutafis2020,
  author    = {Byron E Moutafis and George A Gravvanis and Christos K Filelis-Papadopoulos},
  title     = {Hybrid multi-projection method using sparse approximate inverses on {GPU} clusters},
  journal   = {The International Journal of High Performance Computing Applications},
  year      = {2020},
  month     = feb,
  pages     = {109434202090563},
  doi       = {10.1177/1094342020905637},
  abstract  = {The state-of-the-art supercomputing infrastructures are equipped with accelerators, such as graphics processing units (GPUs), that operate as coprocessors for each workstation of the distributed memory system. The multi-projection type methods are a class of algebraic domain decomposition methods based on semi-aggregation techniques. The multi-projection type methods have improved convergence behavior, as the number of subdomains increases, due to the corresponding augmentation of the semi-aggregated local linear systems with more coarse components, while the number of fine components is reduced. Moreover, limited amount of communications among the workstations is required by the proposed method. The utilization of the available GPUs allows an increase in the number of subdomains along with finer-grained parallelism, leading to improved performance. A load-balancing algorithm that ensures the concurrency of the computations on multicore processors and GPUs is proposed. Flexible parallel preconditioned Krylov subspace iterative methods enhanced with multi-projection type methods have been designed appropriately in order to have improved performance, compared to CPU-only or GPU-only executions, by exploiting the available CPUs and GPUs of the distributed memory system concurrently. The unsymmetric local linear systems are solved by the preconditioned Bi-Conjugate Gradient STABilized (BiCGSTAB) method enhanced with the modified generic factored approximate sparse inverse preconditioner, whereas the preconditioned conjugate gradient (CG) method along with the symmetric factored approximate sparse inverse preconditioner is used for the symmetric positive definite local coefficient matrices. Numerical results regarding the convergence behavior, the performance, and the scalability of the proposed method for several problems are given.},
  publisher = {{SAGE} Publications},
}

@Article{Shi2020,
  author      = {Ziqiang Shi},
  title       = {{SingCubic}: Cyclic Incremental Newton-type Gradient Descent with Cubic Regularization for Non-Convex Optimization},
  date        = {2020-02-17},
  eprint      = {2002.06848},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this work, we generalized and unified two recent completely different works of~\cite{shi2015large} and~\cite{cartis2012adaptive} respectively into one by proposing the cyclic incremental Newton-type gradient descent with cubic regularization (SingCubic) method for optimizing non-convex functions. Through the iterations of SingCubic, a cubic regularized global quadratic approximation using Hessian information is kept and solved. Preliminary numerical experiments show the encouraging performance of the SingCubic algorithm when compared to basic incremental or stochastic Newton-type implementations. The results and technique can be served as an initiate for the research on the incremental Newton-type gradient descent methods that employ cubic regularization. The methods and principles proposed in this paper can be used to do logistic regression, autoencoder training, independent components analysis, Ising model/Hopfield network training, multilayer perceptron, deep convolutional network training and so on. We will open-source parts of our implementations soon.},
  file        = {:http\://arxiv.org/pdf/2002.06848v1:PDF},
  keywords    = {math.OC},
}

@Article{Moutafis2020a,
  author    = {B. E. Moutafis and G. A. Gravvanis and C. K. Filelis-Papadopoulos},
  title     = {On the design of two-stage multiprojection methods for distributed memory systems},
  journal   = {The Journal of Supercomputing},
  year      = {2020},
  month     = {2},
  doi       = {10.1007/s11227-020-03201-5},
  abstract  = {Solving large sparse linear systems, efficiently, on supercomputing infrastructures is a time-consuming component for a wide variety of simulation processes. An effective parallel solver should meet the required specifications, concerning both convergence behavior and scalability. Herewith, a class of two-stage algebraic domain decomposition preconditioning schemes based on the upper Schur complement method is proposed, in order to exploit appropriately distributed memory systems with multicore processors. The design of the method has been focused on homogeneous hybrid parallel systems, i.e., distributed and shared memory systems. However, the proposed method can also be applied to heterogeneous systems, such as cloud infrastructures, or hybrid parallel systems with accelerators, by modifying the workload distribution algorithm and taking into account the different network latencies and bandwidths. The first stage of the proposed schemes is related to the assignment of the subdomains among the workstations of the distributed system, whereas the second stage concerns the further redistribution of the subdomains to each core of a processor. The proposed method utilizes multiprojection techniques, based on semi-aggregated subdomains, leading to improved convergence behavior as the number of subdomains increases. Moreover, a subspace compression technique is used, in order to improve the performance of the preprocessing phase and reduce the memory requirements of the proposed scheme. The preconditioning schemes were combined with a parallel Krylov subspace method, i.e., the parallel preconditioned GMRES(m) method. The convergence behavior, the performance and the scalability of the proposed preconditioning schemes are examined and compared to existing state-of-the-art methods, by conducting several numerical experiments on supercomputing infrastructures.},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Sao2020,
  author    = {Piyush Sao and Ramakrishnan Kannan and Prasun Gera and Richard Vuduc},
  title     = {A supernodal all-pairs shortest path algorithm},
  booktitle = {Proceedings of the 25th {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming},
  year      = {2020},
  series    = {PPoPP '20},
  publisher = {{ACM}},
  month     = {2},
  doi       = {10.1145/3332466.3374533},
  abstract  = {We show how to exploit graph sparsity in the Floyd-Warshall algorithm for the all-pairs shortest path (Apsp) problem. Floyd-Warshall is an attractive choice for Apsp on high-performing systems due to its structural similarity to solving dense linear systems and matrix multiplication. However, if sparsity of the input graph is not properly exploited, Floyd-Warshall will perform unnecessary asymptotic work and thus may not be a suitable choice for many input graphs. To overcome this limitation, the key idea in our approach is to use the known algebraic relationship between Floyd-Warshall and Gaussian elimination, and import several algorithmic techniques from sparse Cholesky factorization, namely, fill-in reducing ordering, symbolic analysis, supernodal traversal, and elimination tree parallelism. When combined, these techniques reduce computation, improve locality and enhance parallelism. We implement these ideas in an efficient shared memory parallel prototype that is orders of magnitude faster than an efficient multi-threaded baseline Floyd-Warshall that does not exploit sparsity. Our experiments suggest that the Floyd-Warshall algorithm can compete with Dijkstra's algorithm (the algorithmic core of Johnson's algorithm) for several classes sparse graphs.},
}

@Article{Zhang2020b,
  author      = {Zhekai Zhang and Hanrui Wang and Song Han and William J. Dally},
  title       = {{SpArch}: Efficient Architecture for Sparse Matrix Multiplication},
  date        = {2020-02-20},
  eprint      = {2002.08947},
  eprinttype  = {arXiv},
  eprintclass = {cs.AR},
  abstract    = {Generalized Sparse Matri$\times$-Matri$\times$ Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGENN introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to e$\times$tensive and e$\times$pensive DRAM access. To address this problem, this paper proposes an efficient sparse matri$\times$ multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matri$\times$ representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4$\times$. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8$\times$. We also resolve the increased input matri$\times$ read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5$\times$. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8$\times$ over previous state-of-the-art. On average, SpArch achieves 4$\times$, 19$\times$, 18$\times$, 17$\times$, 1285$\times$ speedup and 6$\times$, 164$\times$, 435$\times$, 307$\times$, 62$\times$ energy savings over OuterSPACE, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively.},
  file        = {:http\://arxiv.org/pdf/2002.08947v1:PDF},
  keywords    = {cs.AR, cs.DC},
}

@InProceedings{Parger2020,
  author    = {Mathias Parger and Martin Winter and Daniel Mlakar and Markus Steinberger},
  title     = {{spECK}: accelerating GPU sparse matrix-matrix multiplication through lightweight analysis},
  booktitle = {Proceedings of the 25th {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming},
  year      = {2020},
  series    = {PPoPP '20},
  publisher = {{ACM}},
  month     = {2},
  doi       = {10.1145/3332466.3374521},
  abstract  = {parse general matrix-matrix multiplication on GPUs is challenging due to the varying sparsity patterns of sparse matrices. Existing solutions achieve good performance for certain types of matrices, but fail to accelerate all kinds of matrices in the same manner. Our approach combines multiple strategies with dynamic parameter selection to dynamically choose and tune the best fitting algorithm for each row of the matrix. This choice is supported by a lightweight, multi-level matrix analysis, which carefully balances analysis cost and expected performance gains. Our evaluation on thousands of matrices with various characteristics shows that we outperform all currently available solutions in 79\% over all matrices with >15k products and that we achieve the second best performance in 15\%. For these matrices, our solution is on average 83\% faster than the second best approach and up to 25$\times$ faster than other state-of-the-art GPU implementations. Using our approach, applications can expect great performance independent of the matrices they work on.},
}

@InProceedings{DeFreez2020,
  author    = {Daniel DeFreez and Antara Bhowmick and Ignacio Laguna and Cindy Rubio-González},
  title     = {Detecting and reproducing error-code propagation bugs in {MPI} implementations},
  booktitle = {Proceedings of the 25th {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming},
  year      = {2020},
  series    = {PPoPP '20},
  publisher = {{ACM}},
  month     = {2},
  doi       = {10.1145/3332466.3374515},
  abstract  = {We present an approach to automatically detect and reproduce error code propagation bugs in MPI implementations. Specifically, we combine static analysis and program repair for bug detection, and apply fault injection to reproduce error propagation bugs found in MPI libraries written in C. We demonstrate our approach on the MPICH library, one of the most popular implementations of MPI, and the MPICH-based implementation MVAPICH, uncovering 447 previously unknown bugs. We discovered that 31 of these bugs result in program crashes, and 60\% of the MPICH test suite is susceptible to crashing due to failures to propagate error codes. Moreover, 95 bugs produce undesirable behavior that has been confirmed dynamically, causing tests to fail, hanging processes, or simply dropping error codes before reaching user applications.},
}

@InProceedings{Thayer2020,
  author    = {Samuel Thayer and Ganesh L. Gopalakrishnan and Ian Briggs and Michael Bentley and Dong H. Ahn and Ignacio Laguna and Gregory L. Lee},
  title     = {{ArcherGear}},
  booktitle = {Proceedings of the 25th {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming},
  year      = {2020},
  series    = {PPoPP '20},
  publisher = {{ACM}},
  month     = {2},
  doi       = {10.1145/3332466.3374504},
  abstract  = {There is growing uptake of shared memory parallelism in high performance computing, and this has increased the need for data race checking during the creation of new parallel codes or parallelizing existing sequential codes. While race checking concepts and implementations have been around for many concurrency models, including tasking models such as Cilk and PThreads (e.g., the Thread Sanitizer tool), practically usable race checkers for other APIs such as OpenMP have been lagging. For example, the OpenMP parallelization of an important library (namely Hypre) was initially unsuccessful due to inexplicable nondeterminism introduced when the code was optimized, and later root-caused to a race by the then recently developed OpenMP race checker Archer [2]. The open-source Archer now enjoys significant traction within several organizations.},
}

@InProceedings{Zhou2020,
  author    = {Keren Zhou and Mark Krentel and John Mellor-Crummey},
  title     = {A tool for top-down performance analysis of {GPU}-accelerated applications},
  booktitle = {Proceedings of the 25th {ACM} {SIGPLAN} Symposium on Principles and Practice of Parallel Programming},
  year      = {2020},
  series    = {PPoPP '20},
  publisher = {{ACM}},
  month     = {2},
  doi       = {10.1145/3332466.3374534},
  abstract  = {To support performance measurement and analysis of GPU-accelerated applications, we extended the HPCToolkit performance tools with several novel features. To support efficient monitoring of accelerated applications, HPCToolkit employs a new wait-free data structure to coordinate measurement and attribution between each application thread and a GPU monitor thread. To help developers understand the performance of accelerated applications, HPCToolkit attributes metrics to heterogeneous calling contexts that span both CPUs and GPUs. To support fine-grain analysis and tuning of GPU-accelerated code, HPCToolkit collects PC samples of both CPU and GPU activity to derive and attribute metrics at all levels in a heterogeneous calling context.},
}

@Article{Kamzolov2020a,
  author      = {Dmitry Kamzolov and Alexander Gasnikov},
  title       = {Near-Optimal Hyperfast Second-Order Method for convex optimization and its Sliding},
  date        = {2020-02-20},
  eprint      = {2002.09050},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we present a new Hyperfast Second-Order Method with convergence rate $O(N^{-5})$ up to a logarithmic factor for the convex function with Lipshitz $3$rd derivative. This method based on two ideas. The first comes from the superfast second-order scheme of Yu. Nesterov (CORE Discussion Paper 2020/07, 2020). It allows implementing the third-order scheme by solving subproblem using only the second-order oracle. This method converges with rate $O(N^{-4})$. The second idea comes from the work of Kamzolov et al. (arXiv:2002.01004). It is the inexact near-optimal third-order method. In this work, we improve its convergence and merge it with the scheme of solving subproblem using only the second-order oracle. As a result, we get convergence rate $O(N^{-5})$ up to a logarithmic factor. This convergence rate is near-optimal and the best known up to this moment. Further, we investigate the situation when there is a sum of two functions and improve the sliding framework from Kamzolov et al. (arXiv:2002.01004) for the second-order methods.},
  file        = {:http\://arxiv.org/pdf/2002.09050v1:PDF},
  keywords    = {math.OC},
}

@Article{Doikov2020,
  author      = {Nikita Doikov and Yurii Nesterov},
  title       = {Inexact Tensor Methods with Dynamic Accuracies},
  date        = {2020-02-21},
  eprint      = {2002.09403},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we study inexact high-order Tensor Methods for solving convex optimization problems with composite objective. At every step of such methods, we use approximate solution of the auxiliary problem, defined by the bound for the residual in function value. We propose two dynamic strategies for choosing the inner accuracy: the first one is decreasing as $1/k^{p + 1}$, where $p \geq 1$ is the order of the method and $k$ is the iteration counter, and the second approach is using for the inner accuracy the last progress in the target objective. We show that inexact Tensor Methods with these strategies achieve the same global convergence rate as in the error-free case. For the second approach we also establish local superlinear rates (for $p \geq 2$), and propose the accelerated scheme. Lastly, we present computational results on a variety of machine learning problems for several methods and different accuracy policies.},
  file        = {:http\://arxiv.org/pdf/2002.09403v1:PDF},
  keywords    = {math.OC},
}

@Article{Acebron2020,
  author    = {Juan A. Acebrón},
  title     = {A Probabilistic Linear Solver Based on a Multilevel Monte Carlo Method},
  journal   = {Journal of Scientific Computing},
  year      = {2020},
  volume    = {82},
  number    = {3},
  month     = {2},
  doi       = {10.1007/s10915-020-01168-2},
  abstract  = {We describe a new Monte Carlo method based on a multilevel method for computing the action of the resolvent matrix over a vector. The method is based on the numerical evaluation of the Laplace transform of the matrix exponential, which is computed efficiently using a multilevel Monte Carlo method. Essentially, it requires generating suitable random paths which evolve through the indices of the matrix according to the probability law of a continuous-time Markov chain governed by the associated Laplacian matrix. The convergence of the proposed multilevel method has been discussed, and several numerical examples were run to test the performance of the algorithm. These examples concern the computation of some metrics of interest in the analysis of complex networks, and the numerical solution of a boundary-value problem for an elliptic partial differential equation. In addition, the algorithm was conveniently parallelized, and the scalability analyzed and compared with the results of other existing Monte Carlo method for solving linear algebra systems.},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Jyothi2020,
  author      = {R. Jyothi and P. Babu},
  title       = {PIANO: A Fast Parallel Iterative Algorithm for Multinomial and Sparse Multinomial Logistic Regression},
  date        = {2020-02-21},
  eprint      = {2002.09133},
  eprinttype  = {arXiv},
  eprintclass = {stat.ML},
  abstract    = {Multinomial Logistic Regression is a well-studied tool for classification and has been widely used in fields like image processing, computer vision and, bioinformatics, to name a few. Under a supervised classification scenario, a Multinomial Logistic Regression model learns a weight vector to differentiate between any two classes by optimizing over the likelihood objective. With the advent of big data, the inundation of data has resulted in large dimensional weight vector and has also given rise to a huge number of classes, which makes the classical methods applicable for model estimation not computationally viable. To handle this issue, we here propose a parallel iterative algorithm: Parallel Iterative Algorithm for MultiNomial LOgistic Regression (PIANO) which is based on the Majorization Minimization procedure, and can parallely update each element of the weight vectors. Further, we also show that PIANO can be easily extended to solve the Sparse Multinomial Logistic Regression problem - an extensively studied problem because of its attractive feature selection property. In particular, we work out the extension of PIANO to solve the Sparse Multinomial Logistic Regression problem with l1 and l0 regularizations. We also prove that PIANO converges to a stationary point of the Multinomial and the Sparse Multinomial Logistic Regression problems. Simulations were conducted to compare PIANO with the existing methods, and it was found that the proposed algorithm performs better than the existing methods in terms of speed of convergence.},
  file        = {:http\://arxiv.org/pdf/2002.09133v1:PDF},
  keywords    = {stat.ML, cs.LG, math.OC},
}

@Article{Hanzely2020,
  author      = {Filip Hanzely and Nikita Doikov and Peter Richtárik and Yurii Nesterov},
  title       = {Stochastic Subspace Cubic Newton Method},
  date        = {2020-02-21},
  eprint      = {2002.09526},
  eprinttype  = {arXiv},
  eprintclass = {math.OC},
  abstract    = {In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional convex function $f$. Our method can be seen both as a {\em stochastic} extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a {\em second-order} enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function $\frac12 (x-x^*)^\top \nabla^2f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.},
  file        = {:http\://arxiv.org/pdf/2002.09526v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Selvitopi2020,
  author       = {Oguz Selvitopi and Md Taufique Hussain and Ariful Azad and Aydın Buluç},
  title        = {Optimizing High Performance Markov Clustering for Pre-Exascale Architectures},
  journaltitle = {34th IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2020},
  date         = {2020-02-24},
  eprint       = {2002.10083},
  eprinttype   = {arXiv},
  eprintclass  = {cs.DC},
  abstract     = {HipMCL is a high-performance distributed memory implementation of the popular Markov Cluster Algorithm (MCL) and can cluster large-scale networks within hours using a few thousand CPU-equipped nodes. It relies on sparse matrix computations and heavily makes use of the sparse matrix-sparse matrix multiplication kernel (SpGEMM). The existing parallel algorithms in HipMCL are not scalable to Exascale architectures, both due to their communication costs dominating the runtime at large concurrencies and also due to their inability to take advantage of accelerators that are increasingly popular. In this work, we systematically remove scalability and performance bottlenecks of HipMCL. We enable GPUs by performing the expensive expansion phase of the MCL algorithm on GPU. We propose a CPU-GPU joint distributed SpGEMM algorithm called pipelined Sparse SUMMA and integrate a probabilistic memory requirement estimator that is fast and accurate. We develop a new merging algorithm for the incremental processing of partial results produced by the GPUs, which improves the overlap efficiency and the peak memory usage. We also integrate a recent and faster algorithm for performing SpGEMM on CPUs. We validate our new algorithms and optimizations with extensive evaluations. With the enabling of the GPUs and integration of new algorithms, HipMCL is up to 12.4x faster, being able to cluster a network with 70 million proteins and 68 billion connections just under 15 minutes using 1024 nodes of ORNL's Summit supercomputer.},
  file         = {:http\://arxiv.org/pdf/2002.10083v1:PDF},
  keywords     = {cs.DC},
}

@TechReport{Carratala2020,
  author      = {Rocío Carratalá-Sáez and Mathieu Faverge and Grégoire Pichon and Guillaume Sylvand and Enrique S. Quintana-Ortí},
  title       = {Tiled Algorithms for Efficient Task-Parallel H-Matrix Solvers},
  institution = {INRIA},
  year        = {2020},
  eprint      = {hal-02489269},
  url         = {https://hal.inria.fr/hal-02489269},
  abstract    = {In this paper, we describe and evaluate an extension of the Chameleon library to operate with hierarchical matrices (H-Matrices) and hierarchical arithmetic (H-Arithmetic), producing efficient solvers for linear systems arising in Boundary Element Methods (BEM). Our approach builds upon an open-source H-Matrices library from Airbus, named Hmat-oss, that collects sequential numerical kernels for both hierarchical and low-rank structures; the tiled algorithms and task-parallel decompositions available in Chameleon for the solution of linear systems; and the StarPU runtime system to orchestrate an efficient task-parallel (multi-threaded) execution on a multicore architecture. Using an application producing matrices with features close to real industrial applications, we present shared-memory results that demonstrate a fair level of performance, close to (and sometimes better than) the one offered by a pure H-Matrix approach, as proposed by Airbus Hmat proprietary (and non open-source) library. Hence, this combination Chameleon + Hmat-oss proposes the most efficient fully open-source software stack to solve dense compressible linear systems on shared memory architectures (distributed memory is under development).},
}

@Article{Gu2020,
  author      = {Zhixiang Gu and Jose Moreira and David Edelsohn and Ariful Azad},
  title       = {Bandwidth-Optimized Parallel Algorithms for Sparse Matrix-Matrix Multiplication using Propagation Blocking},
  date        = {2020-02-26},
  eprint      = {2002.11302},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {Sparse matrix-matrix multiplication (SpGEMM) is a widely used kernel in various graph, scientific computing and machine learning algorithms. It is well known that SpGEMM is a memory-bound operation, and its peak performance is expected to be bound by the memory bandwidth. Yet, existing algorithms fail to saturate the memory bandwidth, resulting in suboptimal performance under the Roofline model. In this paper we characterize existing SpGEMM algorithms based on their memory access patterns and develop practical lower and upper bounds for SpGEMM performance. We then develop an SpGEMM algorithm based on outer product matrix multiplication. The newly developed algorithm called PB-SpGEMM saturates memory bandwidth by using the propagation blocking technique and by performing in-cache sorting and merging. For many practical matrices, PB-SpGEMM runs 20\%-50\% faster than the state-of-the-art heap and hash SpGEMM algorithms on modern multicore processors. Most importantly, PB-SpGEMM attains performance predicted by the Roofline model, and its performance remains stable with respect to matrix size and sparsity.},
  file        = {:http\://arxiv.org/pdf/2002.11302v1:PDF},
  keywords    = {cs.DC},
}

@Article{Gao2020a,
  author      = {Jianhua Gao and Weixing Ji and Zhaonian Tan and Yueyan Zhao},
  title       = {A Systematic Survey of General Sparse Matrix-Matrix Multiplication},
  date        = {2020-02-26},
  eprint      = {2002.11273},
  eprinttype  = {arXiv},
  eprintclass = {cs.DC},
  abstract    = {SpGEMM (General Sparse Matrix-Matrix Multiplication) has attracted much attention from researchers in fields of multigrid methods and graph analysis. Many optimization techniques have been developed for certain application fields and computing architecture over the decades. The objective of this paper is to provide a structured and comprehensive overview of the research on SpGEMM. Existing optimization techniques have been grouped into different categories based on their target problems and architectures. Covered topics include SpGEMM applications, size prediction of result matrix, matrix partitioning and load balancing, result accumulating, and target architecture-oriented optimization. The rationales of different algorithms in each category are analyzed, and a wide range of SpGEMM algorithms are summarized. This survey sufficiently reveals the latest progress and research status of SpGEMM optimization from 1977 to 2019. More specifically, an experimentally comparative study of existing implementations on CPU and GPU is presented. Based on our findings, we highlight future research directions and how future studies can leverage our findings to encourage better design and implementation.},
  file        = {:http\://arxiv.org/pdf/2002.11273v1:PDF},
  keywords    = {cs.DC, 68-02, 68W10, 65F50, A.1; D.1.3; G.1.3},
}

@TechReport{Agullo2020,
  author      = {Emmanuel Agullo and Siegfried Cools and Emrullah Fatih-Yetkin and Luc Giraud and Nick Schenkel and Wim Vanroose},
  institution = {INRIA},
  title       = {On soft errors in the Conjugate Gradient method: sensitivity and robust numerical detection},
  eprint      = {hal-02495301},
  eprinttype  = {HAL},
  url         = {https://hal.inria.fr/hal-02495301},
  abstract    = {The conjugate gradient (CG) method is the most widely used iterative scheme for the solution of large sparse systems of linear equations when the matrix is symmetric positive definite. Although more than sixty year old, it is still a serious candidate for extreme-scale computation on large computing platforms. On the technological side, the continuous shrinking of transistor geometry and the increasing complexity of these devices affect dramatically their sensitivity to natural radiation, and thus diminish their reliability. One of the most common effects produced by natural radiation is the single event upset which consists in a bit-flip in a memory cell producing unexpected results at application level. Consequently, the future computing facilities at extreme scale might be more prone to errors of any kind including bit-flip during calculation. These numerical and technological observations are the main motivations for this work, where we first investigate through extensive numerical experiments the sensitivity of CG to bit-flips in its main computationally intensive kernels, namely the matrix-vector product and the preconditioner application. We further propose numerical criteria to detect the occurrence of such soft errors; we assess their robustness through extensive numerical experiments.},
  year        = {2020},
}

@InCollection{Chevalier2020,
  author    = {Cédric Chevalier and Franck Ledoux and Sébastien Morais},
  title     = {A Multilevel Mesh Partitioning Algorithm Driven by Memory Constraints},
  booktitle = {Proceedings of the {SIAM} Workshop on Combinatorial Scientific Computing},
  year      = {2020},
  publisher = {Society for Industrial and Applied Mathematics},
  pages     = {85--95},
  doi       = {10.1137/1.9781611976229.9},
  abstract  = {Running numerical simulations on HPC architectures requires distributing data to be processed over the various available processing units. This task is usually done by partitioning tools, whose primary goal is to balance the workload while minimizing inter-process communication. However, they do not take the memory load and memory capacity of the processing units into account. As this can lead to memory overflow, we propose a new approach to address mesh partitioning by including ghost cells in the memory usage and by considering memory capacity as a strong constraint to abide. We model the problem using a bipartite graph and present a new greedy algorithm that aims at producing a partition according to the memory capacity. This algorithm focuses on memory consumption, and we use it in a multi-level approach to improving the quality of the returned solutions during the refinement phase. The experimental results obtained from our benchmarks show that our approach can yield solutions respecting memory constraints for instances where traditional partitioning tools fail.},
  month     = {1},
}

@Article{Boley2020,
  author      = {Daniel Boley},
  title       = {On Fast Computation of Directed Graph Laplacian Pseudo-Inverse},
  date        = {2020-02-28},
  eprint      = {2002.12773},
  eprinttype  = {arXiv},
  eprintclass = {math.NA},
  abstract    = {The Laplacian matrix and its pseudo-inverse for a strongly connected directed graph is fundamental in computing many properties of a directed graph. Examples include random-walk centrality and betweenness measures, average hitting and commute times, and other connectivity measures. These measures arise in the analysis of many social and computer networks. In this short paper, we show how a linear system involving the Laplacian may be solved in time linear in the number of edges, times a factor depending on the separability of the graph. This leads directly to the column-by-column computation of the entire Laplacian pseudo-inverse in time quadratic in the number of nodes, i.e., constant time per matrix entry. The approach is based on "off-the-shelf" iterative methods for which global linear convergence is guaranteed, without recourse to any matrix elimination algorithm.},
  file        = {:http\://arxiv.org/pdf/2002.12773v1:PDF},
  keywords    = {math.NA, cs.NA, 65F20, 65F10},
}

@InCollection{Kolodziej2020,
  author    = {Scott P. Kolodziej and Timothy A. Davis},
  title     = {Generalized Gains for Hybrid Vertex Separator Algorithms},
  booktitle = {Proceedings of the {SIAM} Workshop on Combinatorial Scientific Computing},
  year      = {2020},
  publisher = {Society for Industrial and Applied Mathematics},
  pages     = {96--105},
  doi       = {10.1137/1.9781611976229.10},
  abstract  = {In this paper, we derive generalized vertex gains for computing vertex separators, greatly improving the efficiency and data reuse in hybrid graph partitioning contexts. Using these generalized gains, we design a novel algorithm for computing vertex separators in arbitrary graphs and compare our approach to METIS, a popular graph partitioning library. In general, our hybrid algorithm scales well to very large graphs with the increased information sharing that the generalized gains afford.},
  month     = {1},
}

@Article{Zhu2020a,
  author   = {Y. {Zhu} and Y. {Liu} and G. {Zhang}},
  title    = {{FT-PBLAS}: {PBLAS}-based Fault-tolerant Linear Algebra Computation on High-performance Computing Systems},
  journal  = {IEEE Access},
  year     = {2020},
  pages    = {1-1},
  issn     = {2169-3536},
  doi      = {10.1109/ACCESS.2020.2975832},
  abstract = {As high-performance computing (HPC) systems have scaled up, resilience has become a great challenge. To guarantee resilience, various kinds of hardware and software techniques have been proposed. However, among popular software fault-tolerant techniques, both the checkpoint-restart approach and the replication technique face challenges of scalability in the era of peta- and exa-scale systems due to their numerous processes. In this situation, algorithm-based approaches, or algorithm-based fault tolerance (ABFT) mechanisms, have become attractive because they are efficient and lightweight. Although the ABFT technique is algorithm-dependent, it is possible to implement it at a low level (e.g., in libraries for basic numerical algorithms) and make it application-independent. However, previous ABFT approaches have mainly aimed at achieving fault tolerance in integrated circuits (ICs) or at the architecture level and are therefore not suitable for HPC systems; e.g., they use checksums of rows and columns of matrices rather than checksums of blocks to detect errors. Furthermore, they cannot deal with errors caused by node failure, which are common in current HPC systems. To solve these problems, this paper proposes FT-PBLAS, a PBLAS-based library for fault-tolerant parallel linear algebra computations that can be regarded as a fault-tolerant version of the parallel basic linear algebra subprograms (PBLAS), because it provides a series of fault-tolerant versions of interfaces in PBLAS. To support the underlying error detection and recovery mechanisms in the library, we propose a block-checksum approach for non-fatal errors and a scheme for addressing node failure, respectively. We evaluate two fault-tolerant mechanisms and FT-PBLAS on HPC systems, and the experimental results demonstrate the performance of our library.},
  keywords = {Fault tolerance;Fault tolerant systems;Libraries;Linear algebra;Computational modeling;Software;Integrated circuits;Algorithm-based fault tolerance;HPC systems;Node failure;Matrix multiplication;Linear algebra computations},
}

@Article{Dinh2020,
  author      = {Grace Dinh and James Demmel},
  title       = {Communication-Optimal Tilings for Projective Nested Loops with Arbitrary Bounds},
  date        = {2020-02-28},
  eprint      = {2003.00119},
  eprinttype  = {arXiv},
  eprintclass = {cs.DS},
  abstract    = {Reducing communication - either between levels of a memory hierarchy or between processors over a network - is a key component of performance optimization (in both time and energy) for many problems, including dense linear algebra, particle interactions, and machine learning. For these problems, which can be represented as nested-loop computations, previous tiling based approaches have been used to find both lower bounds on the communication required to execute them and optimal rearrangements, or blockings, to attain such lower bounds. However, such general approaches have typically assumed the problem sizes are large, an assumption that is often not met in practice. For instance, the classical $(\text{num arithmetic operations})/(\text{cache size})^{1/2}$ lower bound for matrix multiplication is not tight for matrix-vector multiplications, which must read in at least $O(\text{num arithmetic operations})$ words of memory; similar issues occur for almost all convolutions in machine learning applications, which use extremely small filter sizes (and therefore, loop bounds). In this paper, we provide an efficient way to both find and obtain, via an appropriate, efficiently constructible blocking, communication lower bounds and matching tilings which attain these lower bounds for nested loop programs with arbitrary loop bounds that operate on multidimensional arrays in the projective case, where the array indices are subsets of the loop indices. Our approach works on all such problems, regardless of dimensionality, size, memory access patterns, or number of arrays, and directly applies to (among other examples) matrix multiplication and similar dense linear algebra operations, tensor contractions, n-body pairwise interactions, pointwise convolutions, and fully connected layers.},
  keywords    = {cs.DS},
}

@InProceedings{Menon2020,
  author    = {Harshitha Menon and Abhinav Bhatele and Todd Gamblin},
  title     = {Auto-tuning Parameter Choices in HPC Applications using Bayesian Optimization},
  booktitle = {Proceedings of the 2020 IEEE International Parallel and Distributed Processing Symposium},
  date      = {2020},
  series    = {IPDPS '20},
  abstract  = {High performance computing applications, runtimes, and platforms are becoming more configurable to enable applications to obtain better performance. As a result, users are increasingly presented with a multitude of options to configure application-specific as well as platform-level parameters. The combined effect of different parameter choices on application performance is difficult to predict, and an exhaustive evaluation of this combinatorial parameter space is practically infeasible. One approach to parameter selection is a user-guided exploration of a part of the space. However, such an ad hoc exploration of the parameter space can result in suboptimal choices. Therefore, an automatic approach that can efficiently explore the parameter space is needed. In this paper, we propose HiPerBOt, a Bayesian optimization based configuration selection framework to identify application and platform-level parameters that result in high performing configurations. We demonstrate the effectiveness of HiPerBOt in tuning parameters that include compiler flags, runtime settings, and application-level options for several parallel codes, including, Kripke, Hypre, LULESH, and OpenAtom.},
}

@InProceedings{Vlaski2019,
  author    = {S. Vlaski and A. H. Sayed},
  title     = {Polynomial Escape-Time from Saddle Points in Distributed Non-Convex Optimization},
  booktitle = {Proceedings of the 8th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing},
  year      = {2019},
  series    = {CAMSAP '19},
  month     = {12},
  pages     = {171-175},
  doi       = {10.1109/CAMSAP45676.2019.9022458},
  abstract  = {The diffusion strategy for distributed learning from streaming data employs local stochastic gradient updates along with exchange of iterates over neighborhoods. In this work we establish that agents cluster around a network centroid in the mean-fourth sense and proceeded to study the dynamics of this point. We establish expected descent in non-convex environments in the large-gradient regime and introduce a short-term model to examine the dynamics over finite-time horizons. Using this model, we establish that the diffusion strategy is able to escape from strict saddle-points in $O(1/\mu)$ iterations, where $\mu$ denotes the step-size; it is also able to return approximately second-order stationary points in a polynomial number of iterations. Relative to prior works on the polynomial escape from saddle-points, most of which focus on centralized perturbed or stochastic gradient descent, our approach requires less restrictive conditions on the gradient noise process.},
  keywords  = {Perturbation methods;Stochastic processes;Optimization;Eigenvalues and eigenfunctions;Approximation algorithms;Topology;Annealing;Stochastic optimization;adaptation;nonconvex costs;saddle point;escape time;gradient noise;stationary points;distributed optimization;diffusion learning},
}

@TechReport{Nesterov2020b,
  author      = {Yurii Nesterov},
  title       = {Inexact high-order proximal-point methods with auxiliary search procedure},
  institution = {UCL - SSH/LIDAM/CORE - Center for operations research and econometrics},
  year        = {2020},
  eprint      = {2020/10},
  eprinttype  = {CORE Discussion Papers},
  url         = {http://hdl.handle.net/2078.1/227954},
  abstract    = {In this paper, we complement the framework of Bi-Level Unconstrained Minimization (BLUM)[21] by a new pth-order proximal-point method convergent as $O(k^{-(3p+1)/2})$, where k is the iteration counter. As compared with [21], we replace the auxiliary line search by a convex segment search. This allows us to bound its complexity of by a logarithm of the desired accuracy. Each step in this search needs an approximate computation of the proximal-point operator. Under assumption on boundedness of $(p+1)$st derivative of the objective function, this can be done by one step of the pth-order augmented tensor method. In this way, for $p = 2$, we get a new second-order method with the rate of convergence $O(^{k-7/2})$ and logarithmic complexity of the auxiliary search at each iteration. Another possibility is to compute the proximal-point operator by lower-order minimization methods. As an example, for $p = 3$, we consider the upper-level process convergent as $O^{(k-5)}$. Assuming the boundedness of fourth derivative, an appropriate approximation of the proximal-point operator can be computed by a second-order method in logarithmic number of iterations. This combination gives a second-order scheme with much better complexity than the existing theoretical limits.},
}

@Article{Gruetzmacher2020,
  author    = {Thomas Grützmacher and Terry Cojean and Goran Flegar and Hartwig Anzt and Enrique S. Quintana-Ortı́},
  title     = {Acceleration of {PageRank} with Customized Precision Based on Mantissa Segmentation},
  doi       = {10.1145/3380934},
  number    = {1},
  pages     = {1--19},
  volume    = {7},
  abstract  = {We describe the application of a communication-reduction technique for the PageRank algorithm that dynamically adapts the precision of the data access to the numerical requirements of the algorithm as the iteration converges. Our variable-precision strategy, using a customized precision format based on mantissa segmentation (CPMS), abandons the IEEE 754 single- and double-precision number representation formats employed in the standard implementation of PageRank, and instead handles the data in memory using a customized floating-point format. The customized format enables fast data access in different accuracy, prevents overflow/underflow by preserving the IEEE 754 double-precision exponent, and efficiently avoids data duplication, since all bits of the original IEEE 754 double-precision mantissa are preserved in memory, but re-organized for efficient reduced precision access. With this approach, the truncated values (omitting significand bits), as well as the original IEEE double-precision values, can be retrieved without duplicating the data in different formats.\\ Our numerical experiments on an NVIDIA V100 GPU (Volta architecture) and a server equipped with two Intel Xeon Platinum 8168 CPUs (48 cores in total) expose that, compared with a standard IEEE double-precision implementation, the CPMS-based PageRank completes about 10\% faster if high-accuracy output is needed, and about 30\% faster if reduced output accuracy is acceptable.},
  journal   = {{ACM} Transactions on Parallel Computing},
  month     = {3},
  publisher = {Association for Computing Machinery ({ACM})},
  year      = {2020},
}

@Article{Kahl2020,
  author      = {Karsten Kahl and Bruno Lang},
  date        = {2020-03-06},
  title       = {On the equivalence of the Hermitian eigenvalue problem and hypergraph edge elimination},
  eprint      = {2003.03145},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {It is customary to identify sparse matrices with the corresponding adjacency or incidence graph. For the solution of linear systems of equations using Gaussian elimination, the representation by its adjacency graph allows a symbolic computation that can be used to predict memory footprints and enables the determination of near-optimal elimination orderings based on heuristics. The Hermitian eigenvalue problem on the other hand seems to evade such treatment at first glance due to its inherent iterative nature. In this paper we prove this assertion wrong by showing the equivalence of the Hermitian eigenvalue problem with a symbolic edge elimination procedure. A symbolic calculation based on the incidence graph of the matrix can be used in analogy to the symbolic phase of Gaussian elimination to develop heuristics which reduce memory footprint and computations. Yet, we also show that the question of an optimal elimination strategy remains NP-hard, in analogy to the linear systems case.},
  file        = {:http\://arxiv.org/pdf/2003.03145v1:PDF},
  keywords    = {math.NA, cs.NA, math.CO, 65F15, 05C50, 05C65, 68R10},
}

@Article{Thebelt2020,
  author      = {Alexander Thebelt and Jan Kronqvist and Miten Mistry and Robert M. Lee and Nathan Sudermann-Merx and Ruth Misener},
  date        = {2020-03-10},
  title       = {ENTMOOT: A Framework for Optimization over Ensemble Tree Models},
  eprint      = {2003.04774},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {Gradient boosted trees and other regression tree models perform well in a wide range of real-world, industrial applications. These tree models (i) offer insight into important prediction features, (ii) effectively manage sparse data, and (iii) have excellent prediction capabilities. Despite their advantages, they are generally unpopular for decision-making tasks and black-box optimization, which is due to their difficult-to-optimize structure and the lack of a reliable uncertainty measure. ENTMOOT is our new framework for integrating (already trained) tree models into larger optimization problems. The contributions of ENTMOOT include: (i) explicitly introducing a reliable uncertainty measure that is compatible with tree models, (ii) solving the larger optimization problems that incorporate these uncertainty aware tree models, (iii) proving that the solutions are globally optimal, i.e. no better solution exists. In particular, we show how the ENTMOOT approach allows a simple integration of tree models into decision-making and black-box optimization, where it proves as a strong competitor to commonly-used frameworks.},
  file        = {:http\://arxiv.org/pdf/2003.04774v1:PDF},
  keywords    = {stat.ML, cs.AI, cs.LG, math.OC},
}

@Article{Carmon2020,
  author      = {Yair Carmon and John C. Duchi},
  date        = {2020-03-10},
  title       = {First-Order Methods for Nonconvex Quadratic Minimization},
  eprint      = {2003.04546},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We consider minimization of indefinite quadratics with either trust-region (norm) constraints or cubic regularization. Despite the nonconvexity of these problems we prove that, under mild assumptions, gradient descent converges to their global solutions, and give a non-asymptotic rate of convergence for the cubic variant. We also consider Krylov subspace solutions and establish sharp convergence guarantees to the solutions of both trust-region and cubic-regularized problems. Our rates mirror the behavior of these methods on convex quadratics and eigenvector problems, highlighting their scalability. When we use Krylov subspace solutions to approximate the cubic-regularized Newton step, our results recover the strongest known convergence guarantees to approximate second-order stationary points of general smooth nonconvex functions.},
  file        = {:http\://arxiv.org/pdf/2003.04546v1:PDF},
  keywords    = {math.OC},
}

@Article{Duff2020,
  author    = {Iain Duff and Jonathan Hogg and Florent Lopez},
  title     = {A New Sparse $LDL^T$ Solver Using A Posteriori Threshold Pivoting},
  doi       = {10.1137/18m1225963},
  number    = {2},
  pages     = {C23--C42},
  volume    = {42},
  abstract  = {The factorization of sparse symmetric indefinite systems is particularly challenging since pivoting is required to maintain stability of the factorization. Pivoting techniques generally offer limited parallelism and are associated with significant data movement hindering the scalability of these methods. Variants of the threshold partial pivoting (TPP) algorithm, for example, have often been used because of its numerical robustness but standard implementations exhibit poor parallel performance. On the other hand, some methods trade stability for performance on parallel architectures such as the supernode Bunch--Kaufman used in the PARDISO solver. In this case, however, the factors obtained might not be used to accurately compute the solution of the system. For this reason we have designed a task-based $LDL^{T}$ factorization algorithm based on a new pivoting strategy called a posteriori threshold pivoting (APTP) that is much more suitable for modern multicore architectures and has the same numerical robustness as the TPP strategy. We implemented our algorithm in a new version of the SPRAL sparse symmetric indefinite direct solver, which initially supported GPU-only factorization. We have used OpenMP 4 task features to implement a multifrontal algorithm with dense factorizations using the novel APTP, and we show that it performs favorably compared to the state-of-the-art solvers HSL\_MA86, HSL\_MA97 and PARDISO both in terms of performance on a multicore machine and in terms of numerical robustness. Finally we show that this new solver is able to make use of GPU devices for accelerating the factorization on heterogeneous architectures.},
  journal   = {{SIAM} Journal on Scientific Computing},
  month     = {1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year      = {2020},
}

@Article{Tanaka2020,
  author      = {Yuichi Tanaka and Yonina C. Eldar and Antonio Ortega and Gene Cheung},
  date        = {2020-03-09},
  title       = {Sampling on Graphs: From Theory to Applications},
  eprint      = {2003.03957},
  eprintclass = {eess.SP},
  eprinttype  = {arXiv},
  abstract    = {The study of sampling signals on graphs, with the goal of building an analog of sampling for standard signals in the time and spatial domains, has attracted considerable attention recently. Beyond adding to the growing theory on graph signal processing (GSP), sampling on graphs has various promising applications. In this article, we review current progress on sampling over graphs focusing on theory and potential applications. Most methodologies used in graph signal sampling are designed to parallel those used in sampling for standard signals, however, sampling theory for graph signals significantly differs from that for Shannon--Nyquist and shift invariant signals. This is due in part to the fact that the definitions of several important properties, such as shift invariance and bandlimitedness, are different in GSP systems. Throughout, we discuss similarities and differences between standard and graph sampling and highlight open problems and challenges.},
  file        = {:http\://arxiv.org/pdf/2003.03957v2:PDF},
  keywords    = {eess.SP, cs.LG},
}

@Article{Miasnikof2020,
  author      = {Pierre Miasnikof and Seo Hong and Yuri Lawryshyn},
  date        = {2020-03-09},
  title       = {Graph Clustering Via QUBO and Digital Annealing},
  eprint      = {2003.03872},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  abstract    = {This article empirically examines the computational cost of solving a known hard problem, graph clustering, using novel purpose-built computer hardware. We express the graph clustering problem as an intra-cluster distance or dissimilarity minimization problem. We formulate our poblem as a quadratic unconstrained binary optimization problem and employ a novel computer architecture to obtain a numerical solution. Our starting point is a clustering formulation from the literature. This formulation is then converted to a quadratic unconstrained binary optimization formulation. Finally, we use a novel purpose-built computer architecture to obtain numerical solutions. For benchmarking purposes, we also compare computational performances to those obtained using a commercial solver, Gurobi, running on conventional hardware. Our initial results indicate the purpose-built hardware provides equivalent solutions to the commercial solver, but in a very small fraction of the time required.},
  file        = {:http\://arxiv.org/pdf/2003.03872v1:PDF},
  keywords    = {cs.SI, cs.DM},
}

@TechReport{Brock2020,
  author      = {Benjamin Brock and Aydın Buluç and Timothy G. Mattson and Scott McMillan and Jose E. Moreira},
  date        = {2020},
  institution = {U.C. Berkeley},
  title       = {A Roadmap for the {GraphBLAS} {C++} {API}},
  abstract    = {The GraphBLAS is an API for graph algorithms expressed in terms of linear algebra. The current GraphBLAS specification is for the C Programming Language. Implementations of the GraphBLAS exposed a number of limitations due to C that restrict both the expressiveness and the performance of the GraphBLAS. The C++ language’s first-class support for generics, including template metaprogramming, addresses these limitations, yielding a simpler GraphBLAS API that should deliver better performance especially for methods based on user-defined types and operators. When combined with the pervasiveness of C++ across many domains as well as within largescale distributed codes, we see a compelling argument to define a GraphBLAS C++ API. This paper presents a roadmap for the development of a GraphBLAS C++ API with a focus on the open issues we must resolve before completing the specification. Our goal is to foster discussion within the GraphBLAS user community and receive feedback on the directions we are taking with the GraphBLAS C++ API.},
}

@Article{Nayak2020,
  author      = {Pratik Nayak and Terry Cojean and Hartwig Anzt},
  date        = {2020-03-11},
  title       = {Evaluating Abstract Asynchronous Schwarz solvers},
  eprint      = {2003.05361v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {With the commencement of the exascale computing era, we realize that the majority of the leadership supercomputers are heterogeneous and massively parallel even on a single node with multiple co-processors such as GPUs and multiple cores on each node. For example, ORNLs Summit accumulates six NVIDIA Tesla V100s and 42 core IBM Power9s on each node. Synchronizing across all these compute resources in a single node or even across multiple nodes is prohibitively expensive. Hence it is necessary to develop and study asynchronous algorithms that circumvent this issue of bulk-synchronous computing for massive parallelism. In this study, we examine the asynchronous version of the abstract Restricted Additive Schwarz method as a solver where we do not explicitly synchronize, but allow for communication of the data between the sub-domains to be completely asynchronous thereby removing the bulk synchronous nature of the algorithm. We accomplish this by using the onesided RMA functions of the MPI standard. We study the benefits of using such an asynchronous solver over its synchronous counterpart on both multi-core architectures and on multiple GPUs. We also study the communication patterns and local solvers and their effect on the global solver. Finally, we show that this concept can render attractive runtime benefits over the synchronous counterparts.},
  file        = {:http\://arxiv.org/pdf/2003.05361v1:PDF},
  keywords    = {cs.DC, cs.MS},
}

@Article{Yu2020,
  author      = {Tong Yu and Hong Zhu},
  date        = {2020-03-12},
  title       = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
  eprint      = {2003.05689v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  file        = {:http\://arxiv.org/pdf/2003.05689v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Chen2020b,
  author    = {Yuedan Chen and Guoqing Xiao and Fan Wu and Zhuo Tang and Keqin Li},
  title     = {{tpSpMV}: A Two-Phase Large-scale Sparse Matrix-Vector Multiplication Kernel for Manycore Architectures},
  doi       = {10.1016/j.ins.2020.03.020},
  abstract  = {Sparse matrix-vector multiplication (SpMV) is one of the important subroutines in numerical linear algebras widely used in lots of large-scale applications. Accelerating SpMV on multicore and manycore architectures based on Compressed Sparse Row (CSR) format via row-wise parallelization is one of the most popular directions. However, there are three main challenges in optimizing parallel CSR-based SpMV: (a) limited local memory of each computing unit can be overwhelmed by assignments to long rows of large-scale sparse matrices; (b) irregular accesses to the input vector result in expensive memory access latency; (c) sparse data structure leads to low bandwidth usage. This paper proposes a two-phase large-scale SpMV, called tpSpMV, based on the memory structure and computing architecture of multicore and manycore architectures to alleviate the three main difficulties. First, we propose the two-phase parallel execution technique for tpSpMV that performs parallel CSR-based SpMV into two separate phases to overcome the computational scale limitation. Second, we respectively propose the adaptive partitioning methods and parallelization designs using the local memory caching technique for the two phases to exploit the architectural advantages of the high-performance computing platforms and alleviate the problem of high memory access latency. Third, we design several optimizations, such as data reduction, aligned memory accessing, and pipeline technique, to improve bandwidth usage and optimize tpSpMV’s performance. Experimental results on SW26010 CPUs of the Sunway TaihuLight supercomputer prove that tpSpMV achieves up to 28.61 speedups and yields the performance improvement of 13.16\% over the state-of-the-art work on average.},
  journal   = {Information Sciences},
  month     = mar,
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Acer2020,
  author    = {Seher Acer and Cevdet Aykanat},
  title     = {Reordering sparse matrices into block-diagonal column-overlapped form},
  doi       = {10.1016/j.jpdc.2020.03.002},
  abstract  = {Many scientific and engineering applications necessitate computing the minimum norm solution of a sparse underdetermined linear system of equations. The minimum 2-norm solution of such systems can be obtained by a recent parallel algorithm, whose numerical effectiveness and parallel scalability are validated in both shared- and distributed-memory architectures. This parallel algorithm assumes the coefficient matrix in a block-diagonal column-overlapped (BDCO) form, which is a variant of the block-diagonal form where the successive diagonal blocks may overlap along their columns. The total overlap size of the BDCO form is an important metric in the performance of the subject parallel algorithm since it determines the size of the reduced system, solution of which is a bottleneck operation in the parallel algorithm. In this work, we propose a hypergraph partitioning model for reordering sparse matrices into BDCO form with the objective of minimizing the total overlap size and the constraint of maintaining balance on the number of nonzeros of the diagonal blocks. Our model makes use of existing partitioning tools that support fixed vertices in the recursive bipartitioning paradigm. Experimental results validate the use of our model as it achieves small overlap size and balanced diagonal blocks.},
  journal   = {Journal of Parallel and Distributed Computing},
  month     = {3},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Chilukuri2020,
  author      = {Aditya Chilukuri and Josh Milthorpe and Beau Johnston},
  date        = {2020-03-12},
  title       = {Characterizing Optimizations to Memory Access Patterns using Architecture-Independent Program Features},
  eprint      = {2003.06064v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {High-performance computing developers are faced with the challenge of optimizing the performance of OpenCL workloads on diverse architectures. The Architecture-Independent Workload Characterization (AIWC) tool is a plugin for the Oclgrind OpenCL simulator that gathers metrics of OpenCL programs that can be used to understand and predict program performance on an arbitrary given hardware architecture. However, AIWC metrics are not always easily interpreted and do not reflect some important memory access patterns affecting efficiency across architectures. We propose a new metric of parallel spatial locality -- the closeness of memory accesses simultaneously issued by OpenCL work-items (threads). We implement the parallel spatial locality metric in the AIWC framework, and analyse gathered results on matrix multiply and the Extended OpenDwarfs OpenCL benchmarks. The differences in the observed parallel spatial locality metric across implementations of matrix multiply reflect the optimizations performed. The new metric can be used to distinguish between the OpenDwarfs benchmarks based on the memory access patterns affecting their performance on various architectures. The improvements suggested to AIWC will help HPC developers better understand memory access patterns of complex codes and guide optimization of codes for arbitrary hardware targets.},
  file        = {:http\://arxiv.org/pdf/2003.06064v1:PDF},
  keywords    = {cs.DC},
}

@Article{Chowdhury2020,
  author      = {Agniva Chowdhury and Palma London and Haim Avron and Petros Drineas},
  date        = {2020-03-18},
  title       = {Speeding up Linear Programming using Randomized Linear Algebra},
  eprint      = {2003.08072v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Linear programming (LP) is an extremely useful tool and has been successfully applied to solve various problems in a wide range of areas, including operations research, engineering, economics, or even more abstract mathematical areas such as combinatorics. It is also used in many machine learning applications, such as $\mathcal{l}_1$-regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider \emph{infeasible} IPMs for the special case where the number of variables is much larger than the number of constraints. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, provably guarantees that infeasible IPM algorithms (suitably modified to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real-world and synthetic data.},
  file        = {:http\://arxiv.org/pdf/2003.08072v1:PDF},
  keywords    = {cs.DS, math.OC},
}

@InProceedings{Gu2019,
  author    = {Ruidong Gu and Paul Beata and Michela Becchi},
  booktitle = {Proceedings of the 2019 {IEEE} International Symposium on Workload Characterization},
  title     = {Characterizing the Performance/Accuracy Tradeoff of High-Precision Applications via Auto-tuning},
  doi       = {10.1109/iiswc47752.2019.9042137},
  publisher = {{IEEE}},
  series    = {IISWC '19},
  abstract  = {Many scientific applications (e.g., molecular dynamics, climate modeling and astrophysical simulations) rely on floating-point arithmetic. Floating-point representation is by definition a finite approximation of real numbers, and thus it can lead to inaccuracy and reproducibility issues. To overcome these issues, existing work has proposed high-precision floating-point libraries to be used in scientific simulations, but they come at the cost of significant additional execution time. In this work we analyze performance and accuracy effects from tuning down groups of variables and operations guided by compile-time considerations. The goal of our tuning approach is to convert existing floating-point programs to mixed precision while balancing accuracy and performance. To this end, the tuner starts by maximizing accuracy through the use of a high-precision library and then achieves performance gains under a given error bound by incrementally tuning down groups of variables and operations from higher to lower precision (e.g., double precision). The approach provides input-data independence in its results by defining tuning strategies based on loop structures and the investigation of floating-point computation patterns. In addition, it has a smaller search space than exhaustive or bitonic search algorithms, leading to a significant reduction in tuning time, especially on larger, long-running applications. We tested our tuning on a computational fluid dynamics (CFD) application.},
  month     = {11},
  year      = {2019},
}

@Article{Anaissi2020,
  author      = {Ali Anaissi and Basem Suleiman and Seid Miad Zandavi},
  date        = {2020-03-18},
  title       = {NeCPD: An Online Tensor Decomposition with Optimal Stochastic Gradient Descent},
  eprint      = {2003.08844v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Multi-way data analysis has become an essential tool for capturing underlying structures in higher-order datasets stored in tensor $\mathcal{X} \in \mathbb{R} ^{I_1 \times \dots \times I_N}$. CANDECOMP/PARAFAC (CP) decomposition has been extensively studied and applied to approximate $\mathcal{X}$ by $N$ loading matrices $A^{(1)}, \dots, A^{(N)}$ where $N$ represents the order of the tensor. We propose a new efficient CP decomposition solver named NeCPD for non-convex problem in multi-way online data based on stochastic gradient descent (SGD) algorithm. SGD is very useful in online setting since it allows us to update $\mathcal{X}^{(t+1)}$ in one single step. In terms of global convergence, it is well known that SGD stuck in many saddle points when it deals with non-convex problems. We study the Hessian matrix to identify theses saddle points, and then try to escape them using the perturbation approach which adds little noise to the gradient update step. We further apply Nesterov's Accelerated Gradient (NAG) method in SGD algorithm to optimally accelerate the convergence rate and compensate Hessian computational delay time per epoch. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets show that our method provides more accurate results compared with existing online tensor analysis methods.},
  file        = {:http\://arxiv.org/pdf/2003.08844v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Rodomanov2020a,
  author      = {Anton Rodomanov and Yurii Nesterov},
  date        = {2020-03-20},
  title       = {Rates of Superlinear Convergence for Classical Quasi-Newton Methods},
  eprint      = {2003.09174v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We study the local convergence of classical quasi-Newton methods for nonlinear optimization. Although it was well established a long time ago that asymptotically these methods converge superlinearly, the corresponding rates of convergence still remain unknown. In this paper, we address this problem. We obtain first explicit non-asymptotic rates of superlinear convergence for the standard quasi-Newton methods, which are based on the updating formulas from the convex Broyden class. In particular, for the well-known DFP and BFGS methods, we obtain the rates of the form $(\frac{n L^2}{\mu^2 k})^{k/2}$ and $(\frac{n L}{\mu k})^{k/2}$ respectively, where $k$ is the iteration counter, $n$ is the dimension of the problem, $\mu$ is the strong convexity parameter, and $L$ is the Lipschitz constant of the gradient.},
  file        = {:http\://arxiv.org/pdf/2003.09174v1:PDF},
  keywords    = {math.OC},
}

@Article{Fegaras2020,
  author      = {Leonidas Fegaras and Md Hasanuzzaman Noor},
  date        = {2020-03-21},
  title       = {Translation of Array-Based Loops to Distributed Data-Parallel Programs},
  eprint      = {2003.09769v1},
  eprintclass = {cs.DB},
  eprinttype  = {arXiv},
  abstract    = {Large volumes of data generated by scientific experiments and simulations come in the form of arrays, while programs that analyze these data are frequently expressed in terms of array operations in an imperative, loop-based language. But, as datasets grow larger, new frameworks in distributed Big Data analytics have become essential tools to large-scale scientific computing. Scientists, who are typically comfortable with numerical analysis tools but are not familiar with the intricacies of Big Data analytics, must now learn to convert their loop-based programs to distributed data-parallel programs. We present a novel framework for translating programs expressed as array-based loops to distributed data parallel programs that is more general and efficient than related work. Although our translations are over sparse arrays, we extend our framework to handle packed arrays, such as tiled matrices, without sacrificing performance. We report on a prototype implementation on top of Spark and evaluate the performance of our system relative to hand-written programs.},
  file        = {:http\://arxiv.org/pdf/2003.09769v1:PDF},
  keywords    = {cs.DB, cs.PL},
}

@Article{Cummins2020,
  author      = {Chris Cummins and Zacharias V. Fisches and Tal Ben-Nun and Torsten Hoefler and Hugh Leather},
  date        = {2020-03-23},
  title       = {{ProGraML}: Graph-based Deep Learning for Program Optimization and Analysis},
  eprint      = {2003.10536v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. We introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. ProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.},
  file        = {:http\://arxiv.org/pdf/2003.10536v1:PDF},
  keywords    = {cs.LG, cs.PF, cs.PL, stat.ML},
}

@Article{Cai2020,
  author      = {Yunfeng Cai and Ping Li},
  date        = {2020-03-24},
  title       = {Solving the Robust Matrix Completion Problem via a System of Nonlinear Equations},
  eprint      = {2003.10992v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {We consider the problem of robust matrix completion, which aims to recover a low rank matrix $L_*$ and a sparse matrix $S_*$ from incomplete observations of their sum $M=L_*+S_*\in\mathbb{R}^{m\times n}$. Algorithmically, the robust matrix completion problem is transformed into a problem of solving a system of nonlinear equations, and the alternative direction method is then used to solve the nonlinear equations. In addition, the algorithm is highly parallelizable and suitable for large scale problems. Theoretically, we characterize the sufficient conditions for when $L_*$ can be approximated by a low rank approximation of the observed $M_*$. And under proper assumptions, it is shown that the algorithm converges to the true solution linearly. Numerical simulations show that the simple method works as expected and is comparable with state-of-the-art methods.},
  file        = {:http\://arxiv.org/pdf/2003.10992v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Srinivasa2020,
  author      = {Rakshith S Srinivasa and Mark A Davenport and Justin Romberg},
  date        = {2020-03-20},
  title       = {Localized sketching for matrix multiplication and ridge regression},
  eprint      = {2003.09097v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {We consider sketched approximate matrix multiplication and ridge regression in the novel setting of localized sketching, where at any given point, only part of the data matrix is available. This corresponds to a block diagonal structure on the sketching matrix. We show that, under mild conditions, block diagonal sketching matrices require only $O(\frac{sr}{\epsilon^2})$ and $O(\frac{sd_\lambda}{\epsilon})$ total sample complexity for matrix multiplication and ridge regression, respectively. This matches the state-of-the-art bounds that are obtained using global sketching matrices. The localized nature of sketching considered allows for different parts of the data matrix to be sketched independently and hence is more amenable to computation in distributed and streaming settings and results in a smaller memory and computational footprint.},
  file        = {:http\://arxiv.org/pdf/2003.09097v1:PDF},
  keywords    = {stat.ML, cs.IT, cs.LG, eess.SP, math.IT},
}

@Article{Li2020b,
  author      = {Shuang Li and Qiuwei Li and Zhihui Zhu and Gongguo Tang and Michael B. Wakin},
  date        = {2020-03-24},
  title       = {The Global Geometry of Centralized and Distributed Low-rank Matrix Recovery without Regularization},
  eprint      = {2003.10981v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Low-rank matrix recovery is a fundamental problem in signal processing and machine learning. A recent very popular approach to recovering a low-rank matrix $X$ is to factorize it as a product of two smaller matrices, i.e., $X=UV^T$, and then optimize over $U$, $V$ instead of $X$. Despite the resulting non-convexity, recent results have shown that the factorized objective functions have benign global geometry---with no spurious local minima and satisfying the so-called strict saddle property---ensuring convergence to a global minimum for many local-search algorithms. However, most of these results actually consider a modified cost function that includes a balancing regularizer. While useful for deriving theory, this balancing regularizer does not appear to be necessary in practice. In this work, we close this theory-practice gap by proving that the original factorized non-convex problem, without the balancing regularizer, also has similar benign global geometry. Moreover, we also extend our theoretical results to the field of distributed optimization.},
  file        = {:http\://arxiv.org/pdf/2003.10981v1:PDF},
  keywords    = {math.OC},
}

@Article{Booth2020,
  author    = {Joshua Dennis Booth and Gregory Bolet},
  title     = {An On-Node Scalable Sparse Incomplete {LU} Factorization for a Many-Core Iterative Solver with Javelin},
  doi       = {10.1016/j.parco.2020.102622},
  pages     = {102622},
  abstract  = {We present a scalable incomplete LU factorization to be used as a preconditioner for solving sparse linear systems with iterative methods in the package called Javelin. Javelin allows for improved parallel factorization on shared-memory many-core systems by packaging the coefficient matrix into a format that allows for high performance sparse matrix-vector multiplication and sparse triangular solves with minimal overheads. The framework achieves these goals by using a collection of traditional permutations, point-to-point thread synchronizations, tasking, and segmented prefix scans in a conventional compressed sparse row (CSR) format. Moreover, this framework stresses the importance of co-designing dependent tasks, such as sparse factorization and triangular solves, on highly-threaded architectures. We compare our method to the past distributed methods for incomplete factorization (Aztec) and current multithreaded packages (WSMP) in order to demonstrate the importance of having highly threaded factorizations on many-core systems. Using these changes, traditional fill-in and drop tolerance methods can be used, while still being able to have observed speedups of up to $\sim 42 \times$ on 68 Intel Knights Landing cores and $\sim 12 \times$  on 14 Intel Haswell cores. Moreover, this work provides insight into how the new data-structure impacts iteration counts, and provides insight into future improvements, such as point to GPUs.},
  journal   = {Parallel Computing},
  month     = {3},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@InCollection{Favaro2020,
  author    = {Federico Favaro and Ernesto Dufrechou and Pablo Ezzatti and Juan P. Oliver},
  booktitle = {Applied Reconfigurable Computing. Architectures, Tools, and Applications},
  title     = {Exploring {FPGA} Optimizations to Compute Sparse Numerical Linear Algebra Kernels},
  doi       = {10.1007/978-3-030-44534-8_20},
  pages     = {258--268},
  publisher = {Springer International Publishing},
  abstract  = {The solution of sparse triangular linear systems (sptrsv) is the bottleneck of many numerical methods. Thus, it is crucial to count with efficient implementations of such kernel, at least for commonly used platforms. In this sense, Field–Programmable Gate Arrays (FPGAs) have evolved greatly in the last years, entering the HPC hardware ecosystem largely due to their superior energy–efficiency relative to more established accelerators. Up until recently, the design for FPGAs implied the use of low–level Hardware Description Languages (HDL) such as VHDL or Verilog. Nowadays, manufacturers are making a large effort to adopt High–Level Synthesis languages like C/C++ or OpenCL, but the gap between their performance and that of HDLs is not yet fully studied. This work focuses on the performance offered by FPGAs to compute the sptrsv using OpenCL. For this purpose, we implement different parallel variants of this kernel and experimentally evaluate several setups, varying among others the work–group size, the number of compute units, the unroll–factor and the vectorization–factor.},
  year      = {2020},
}

@Article{Deng2020,
  author    = {Xiaoge Deng and Tao Sun and Peibing Du and Dongsheng Li},
  title     = {A Nonconvex Implementation of Sparse Subspace Clustering: Algorithm and Convergence Analysis},
  doi       = {10.1109/access.2020.2981740},
  pages     = {54741--54750},
  volume    = {8},
  abstract  = {Subspace clustering has been widely applied to detect meaningful clusters in high-dimensional data spaces. And the sparse subspace clustering (SSC) obtains superior clustering performance by solving a relaxed $\epsilon_0$-minimization problem with  $\epsilon_1$-norm. Although the use of  $\epsilon_1$-norm instead of the  $\epsilon_0$ one can make the object function convex, it causes large errors on large coefficients in some cases. In this paper, we study the sparse subspace clustering algorithm based on a nonconvex modeling formulation. Specifically, we introduce a nonconvex pseudo-norm that makes a better approximation to the  $\epsilon_0$-minimization than the traditional  $\epsilon_1$-minimization framework and consequently finds a better affinity matrix. However, this formulation makes the optimization task challenging due to that the traditional alternating direction method of multipliers (ADMM) encounters troubles in solving the nonconvex subproblems. In view of this, the reweighted techniques are employed in making these subproblems convex and easily solvable. We provide several guarantees to derive the convergence results, which proves that the nonconvex algorithm is globally convergent to a critical point. Experiments on two real-world problems of motion segmentation and face clustering show that our method outperforms state-of-the-art techniques.},
  journal   = {{IEEE} Access},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Wei2020,
  author    = {Xiaohui Wei and Hengshan Yue and Shang Gao and Lina Li and Ruyu Zhang and Jingweijia Tan},
  title     = {G-{SEAP}: Analyzing and characterizing soft-error aware approximation in {GPGPUs}},
  doi       = {10.1016/j.future.2020.03.040},
  abstract  = {As General-Purpose Graphics Processing Units (GPGPUs) become pervasive for the High-Performance Computing (HPC), ensuring that programs can be protected from soft errors has become increasingly important. Soft errors may cause Silent Data Corruptions (SDCs), which produces erroneous execution results silently. Due to the massive parallelism of GPGPUs, fully protecting them against soft errors introduces nontrivial overhead. Fortunately, imprecise execution outcomes are inherently tolerable for some HPC programs due to the nature of these applications. Leveraging the feature, selective soft error protection can be applied to reduce energy consumptions.\\ In this work, we first propose a GPGPU-based Soft-Error aware APproximation analysis framework (G-SEAP) to characterize the approximation characteristics of soft errors. Based on G-SEAP, we perform an exhaustive analysis for 17 representative HPC benchmarks and observe 72.7\% of SDCs on average are approximable. We also observe that the dataflow of application, kernel function reliability requirement, instruction-type, and data bit-location are all important factors for program's correctness. Lastly, according to observations, we further design an approximate Error Correction Codes (ECCs) mechanism and an approximate instruction duplication technique to illustrate how G-SEAP provides useful guidance for energy-efficient soft-error elimination in GPGPUs.},
  journal   = {Future Generation Computer Systems},
  month     = {3},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Fasi2020,
  author     = {Fasi, Massimiliano and Higham, Nicholas},
  date       = {2020},
  title      = {Generating extreme-scale matrices with specified singular values or condition numbers},
  eprint     = {MIMS EPrint:2020.8},
  eprinttype = {MIMS EPrint},
  issn       = {1749-9097},
  url        = {http://eprints.maths.manchester.ac.uk/2755/1/fahi20.pdf},
  abstract   = {A widely used form of test matrix is the randsvd matrix constructed as the product $A = U \Sigma V^*$, where $U$ and $V$ are random orthogonal or unitary matrices from the Haar distribution and $\Sigma$ is a diagonal matrix of singular values. Such matrices are random but have a specified singular value distribution. The cost of forming an $m \times n$ randsvd matrix is $m^3 + n ^3$ flops, which is prohibitively expensive at extreme scale; moreover, the randsvd construction requires a significant amount of communication, making it unsuitable for distributed memory environments. By dropping the requirement that $U$ and $V$ be Haar distributed and that both be random, we derive new algorithms for forming $A$ that have cost linear in the number of matrix elements and require a low amount of communication and synchronization. We specialize these algorithms to generating matrices with specified 2-norm condition number. Numerical experiments show that the algorithms have excellent efficiency and scalability.},
}

@Article{Anzt2020a,
  author    = {Hartwig Anzt and Terry Cojean and Chen Yen-Chen and Jack Dongarra and Goran Flegar and Pratik Nayak and Stanimire Tomov and Yuhsiang M. Tsai and Weichung Wang},
  title     = {Load-balancing Sparse Matrix Vector Product Kernels on {GPUs}},
  doi       = {10.1145/3380930},
  number    = {1},
  pages     = {1--26},
  volume    = {7},
  abstract  = {Efficient processing of Irregular Matrices on Single Instruction, Multiple Data (SIMD)-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for Irregular Matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, in this article, we address the challenge of designing high performance sparse matrix-vector product (SpMV) kernels designed for Nvidia Graphics Processing Units (GPUs). We present a compressed sparse row (CSR) format suitable for unbalanced matrices. We also provide a load-balancing kernel for the coordinate (COO) matrix format and extend it to a hybrid algorithm that stores part of the matrix in SIMD-friendly Ellpack format (ELL) format. The ratio between the ELL- and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against SpMV kernels provided by NVIDIA’s cuSPARSE library and a heavily-tuned sliced ELL (SELL-P) kernel that prevents unnecessary padding by considering the irregular matrices as a combination of matrix blocks stored in ELL format.},
  journal   = {{ACM} Transactions on Parallel Computing},
  month     = {3},
  publisher = {Association for Computing Machinery ({ACM})},
  year      = {2020},
}

@Article{Hamilton2020,
  author    = {Kathleen E. Hamilton and Catherine D. Schuman and Steven R. Young and Ryan S. Bennink and Neena Imam and Travis S. Humble},
  title     = {Accelerating Scientific Computing in the Post-Moore's Era},
  doi       = {10.1145/3380940},
  number    = {1},
  pages     = {1--31},
  volume    = {7},
  abstract  = {Novel uses of graphical processing units for accelerated computation revolutionized the field of high-performance scientific computing by providing specialized workflows tailored to algorithmic requirements. As the era of Moore’s law draws to a close, many new non–von Neumann processors are emerging as potential computational accelerators, including those based on the principles of neuromorphic computing, tensor algebra, and quantum information. While development of these new processors is continuing to mature, the potential impact on accelerated computing is anticipated to be profound. We discuss how different processing models can advance computing in key scientific paradigms: machine learning and constraint satisfaction. Significantly, each of these new processor types utilizes a fundamentally different model of computation, and this raises questions about how to best use such processors in the design and implementation of applications. While many processors are being developed with a specific domain target, the ubiquity of spin-glass models and neural networks provides an avenue for multi-functional applications. This also hints at the infrastructure needed to integrate next-generation processing units into future high-performance computing systems.},
  journal   = {{ACM} Transactions on Parallel Computing},
  month     = {3},
  publisher = {Association for Computing Machinery ({ACM})},
  year      = {2020},
}

@Article{Vlaski2020,
  author      = {Stefan Vlaski and Ali H. Sayed},
  date        = {2020-03-31},
  title       = {Second-Order Guarantees in Centralized, Federated and Decentralized Nonconvex Optimization},
  eprint      = {2003.14366v1},
  eprintclass = {cs.MA},
  eprinttype  = {arXiv},
  abstract    = {Rapid advances in data collection and processing capabilities have allowed for the use of increasingly complex models that give rise to nonconvex optimization problems. These formulations, however, can be arbitrarily difficult to solve in general, in the sense that even simply verifying that a given point is a local minimum can be NP-hard [1]. Still, some relatively simple algorithms have been shown to lead to surprisingly good empirical results in many contexts of interest. Perhaps the most prominent example is the success of the backpropagation algorithm for training neural networks. Several recent works have pursued rigorous analytical justification for this phenomenon by studying the structure of the nonconvex optimization problems and establishing that simple algorithms, such as gradient descent and its variations, perform well in converging towards local minima and avoiding saddle-points. A key insight in these analyses is that gradient perturbations play a critical role in allowing local descent algorithms to efficiently distinguish desirable from undesirable stationary points and escape from the latter. In this article, we cover recent results on second-order guarantees for stochastic first-order optimization algorithms in centralized, federated, and decentralized architectures.},
  file        = {:http\://arxiv.org/pdf/2003.14366v1:PDF},
  keywords    = {cs.MA, cs.LG, eess.SP, math.OC, stat.ML},
}

@Article{Jin2020,
  author      = {Qiujiang Jin and Aryan Mokhtari},
  date        = {2020-03-30},
  title       = {Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods},
  eprint      = {2003.13607v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we study the non-asymptotic superlinear convergence rate of DFP and BFGS, which are two well-known quasi-Newton methods. The asymptotic superlinear convergence rate of these quasi-Newton methods has been extensively studied, but their explicit finite time local convergence rate has not been established yet. In this paper, we provide a finite time (non-asymptotic) convergence analysis for BFGS and DFP methods under the assumptions that the objective function is strongly convex, its gradient is Lipschitz continuous, and its Hessian is Lipschitz continuous only in the direction of the optimal solution. We show that in a local neighborhood of the optimal solution, the iterates generated by both DFP and BFGS converge to the optimal solution at a superlinear rate of $\mathcal{O}((\frac{1}{ {k}})^{k/2})$, where $k$ is the number of iterations. In particular, for a specific choice of the local neighborhood, both DFP and BFGS converge to the optimal solution at the rate of $(\frac{0.85}{k})^{k/2}$. Our theoretical guarantee is one of the first results that provide a non-asymptotic superlinear convergence rate for DFP and BFGS quasi-Newton methods.},
  file        = {:http\://arxiv.org/pdf/2003.13607v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Goncalves2020,
  author   = {M. M. {Goncalves} and I. P. {Lamb} and P. {Rech} and R. M. {Brum} and J. R. {Azambuja}},
  title    = {Improving Selective Fault Tolerance in {GPU} Register Files by Relaxing Application Accuracy},
  doi      = {10.1109/TNS.2020.2982162},
  issn     = {1558-1578},
  pages    = {1-1},
  abstract = {The high computing power of GPUs makes them attractive for safety-critical applications, where reliability is a major concern. This paper uses an approximate computing perspective to relax application accuracy in order to improve selective fault tolerance techniques. Our approach first assesses the vulnerability of a Kepler GPU to transient effects through a neutron beam experiment. Then, it performs a fault injection campaign to identify the most critical registers and relaxes result accuracy. Finally, it uses acquired data to improve selective fault tolerance techniques in terms of occupation and performance. Results show that it was possible to improve the GPU register file’s reliability in an average of 71.6\% by relaxing application accuracy and, when compared to selective hardening techniques, it was able to reduce replicated registers by an average of 41.4\%, while maintaining 100\% fault coverage.},
  journal  = {IEEE Transactions on Nuclear Science},
  keywords = {Approximate computing;graphics processing units;radiation effects;selective fault tolerance},
  year     = {2020},
}

@Article{Serafino2020,
  author      = {Daniela di Serafino and Gerardo Toraldo and Marco Viola},
  date        = {2020-04-02},
  title       = {Using gradient directions to get global convergence of Newton-type methods},
  eprint      = {2004.00968v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {The renewed interest in Steepest Descent (SD) methods following the work of Barzilai and Borwein [IMA Journal of Numerical Analysis, 8 (1988)] has driven us to consider a globalization strategy based on SD, which is applicable to any line-search method. In particular, we combine Newton-type directions with scaled SD steps to have suitable descent directions. Scaling the SD directions with a suitable step length makes a significant difference with respect to similar globalization approaches, in terms of both theoretical features and computational behavior. We apply our strategy to Newton's method and the BFGS method, with computational results that appear interesting compared with the results of well-established globalization strategies devised ad hoc for those methods.},
  file        = {:http\://arxiv.org/pdf/2004.00968v1:PDF},
  keywords    = {math.OC, cs.NA, math.NA, 65K05, 90C30, 49M15, G.1.6},
}

@Article{Steinerberger2020,
  author      = {Stefan Steinerberger},
  date        = {2020-04-02},
  title       = {A Spectral Approach to the Shortest Path Problem},
  eprint      = {2004.01163v1},
  eprintclass = {math.CO},
  eprinttype  = {arXiv},
  abstract    = {Let $G=(V,E)$ be a simple, connected graph. One is often interested in a short path between two vertices $u,v$. We propose a spectral algorithm: construct the function $\phi:V \rightarrow \mathbb{R}_{\geq 0}$ $$ \phi = \arg\min_{f:V \rightarrow \mathbb{R} \atop f(u) = 0, f \not\equiv 0} \frac{\sum_{(w_1, w_2) \in E}{(f(w_1)-f(w_2))^2}}{\sum_{w \in V}{f(w)^2}}.$$ $\phi$ can also be understood as the smallest eigenvector of the Laplacian Matrix $L=D-A$ after the $u-$th row and column have been removed. We start in the point $v$ and construct a path from $v$ to $u$: at each step, we move to the neighbor for which $\phi$ is the smallest. This algorithm provably terminates and results in a short path from $v$ to $u$, often the shortest. The efficiency of this method is due to a discrete analogue of a phenomenon in Partial Differential Equations that is not well understood. We prove optimality for trees and discuss a number of open questions.},
  file        = {:http\://arxiv.org/pdf/2004.01163v1:PDF},
  keywords    = {math.CO, cs.CG, cs.DM, math.SP},
}

@TechReport{Buttari2020,
  author      = {Alfredo Buttari and Markus Huber and Philippe Leleux and Theo Mary and Ulrich Rüde and Barbara Wohlmuth},
  date        = {2020},
  institution = {Institut de recherche en informatique de Toulouse (IRIT)},
  title       = {Block Low Rank Single Precision Coarse Grid Solvers forExtreme Scale Multigrid Methods},
  eprint      = {02528532},
  eprinttype  = {HAL},
  url         = {https://hal.archives-ouvertes.fr/hal-02528532},
  abstract    = {Extreme scale simulation requires fast and scalable algorithms, such as multigrid methods. To achieve asymptotically optimal complexity it is essential to employ a hierarchy of grids. The cost to solve the coarsest grid system can often be neglected in sequential computings, but cannot be ignored in massively parallel executions. In this case, the coarsest grid can be large and its efficient solution becomes a challenging task. We propose solving the coarse grid system using modern, approximate sparse direct methods and investigate the expected gains compared with traditional iterative methods. Since the coarse grid system only requires an approximate solution, we show that we can leverage block low-rank techniques, combined with the use of single precision arithmetic, to significantly reduce the computational requirements of the direct solver. In the case of extreme scale computing, the coarse grid system is too large for a sequential solution, but too small to permit massively parallel efficiency. We show that the agglomeration of the coarse grid system to a subset of processors is necessary for the sparse direct solver to achieve performance. We demonstrate the efficiency of the proposed method on a Stokes-type saddle point system. We employ a monolithic Uzawa multigrid method. In particular, we show that the use of an approximate sparse direct solver for the coarse grid system can outperform that of a preconditioned minimal residual iterative method. This is demonstrated for the multigrid solution of systems of order up to 1+e11 degrees of freedom on a petascale supercomputer using 43 200 processes.},
}

@Article{Altschuler2020,
  author      = {Jason M. Altschuler and Pablo A. Parrilo},
  date        = {2020-04-06},
  title       = {Random Osborne: a simple, practical algorithm for Matrix Balancing in near-linear time},
  eprint      = {2004.02837v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We revisit Matrix Balancing, a pre-conditioning task used ubiquitously for computing eigenvalues and matrix exponentials. Since 1960, Osborne's algorithm has been the practitioners' algorithm of choice, and is now implemented in most numerical software packages. However, the theoretical properties of Osborne's algorithm are not well understood. Here, we show that a simple random variant of Osborne's algorithm converges in near-linear time in the input sparsity. Specifically, it balances $K\in\mathbb{R}_{\geq 0}^{n\times n}$ after $O(m\epsilon^{-2}\log\kappa)$ arithmetic operations, where $m$ is the number of nonzeros in $K$, $\epsilon$ is the $\ell_1$ accuracy, and $\kappa=\sum_{ij}K_{ij}/(\min_{ij:K_{ij}\neq 0}K_{ij})$ measures the conditioning of $K$. Previous work had established near-linear runtimes either only for $\ell_2$ accuracy (a weaker criterion which is less relevant for applications), or through an entirely different algorithm based on (currently) impractical Laplacian solvers. We further show that if the graph with adjacency matrix $K$ is moderately connected--e.g., if $K$ has at least one positive row/column pair--then Osborne's algorithm initially converges exponentially fast, yielding an improved runtime $O(m\epsilon^{-1}\log\kappa)$. We also address numerical precision issues by showing that these runtime bounds still hold when using $O(\log(n\kappa/\epsilon))$-bit numbers. Our results are established through a potential argument that leverages a convex optimization perspective of Osborne's algorithm, and relates the per-iteration progress to the current imbalance as measured in Hellinger distance. Unlike previous analyses, we critically exploit log-convexity of the potential. Our analysis extends to other variants of Osborne's algorithm: along the way, we establish significantly improved runtime bounds for cyclic, greedy, and parallelized variants.},
  file        = {:http\://arxiv.org/pdf/2004.02837v1:PDF},
  keywords    = {math.OC, cs.DS, cs.NA, math.NA},
}

@Article{Altschuler2020a,
  author      = {Jason M. Altschuler and Pablo A. Parrilo},
  date        = {2020-04-07},
  title       = {Approximating Min-Mean-Cycle for low-diameter graphs in near-optimal time and memory},
  eprint      = {2004.03114v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {We revisit Min-Mean-Cycle, the classical problem of finding a cycle in a weighted directed graph with minimum mean weight. Despite an extensive algorithmic literature, previous work falls short of a near-linear runtime in the number of edges $m$--in fact, there is a natural barrier which precludes such a runtime for solving Min-Mean-Cycle exactly. Here, we give a much faster approximation algorithm that, for graphs with polylogarithmic diameter, has near-linear runtime. In particular, this is the first algorithm whose runtime for the complete graph scales in the number of vertices $n$ as $\tilde{O}(n^2)$. Moreover--unconditionally on the diameter--the algorithm uses only $O(n)$ memory beyond reading the input, making it "memory-optimal". The algorithm is also simple to implement and has remarkable practical performance. Our approach is based on solving a linear programming (LP) relaxation using entropic regularization, which effectively reduces the LP to a Matrix Balancing problem--a la the popular reduction of Optimal Transport to Matrix Scaling. We then round the fractional LP solution using a variant of the classical Cycle-Cancelling algorithm that is sped up to near-linear runtime at the expense of being approximate, and implemented in a memory-optimal manner. We also provide an alternative algorithm with slightly faster theoretical runtime, albeit worse memory usage and practicality. This algorithm uses the same rounding procedure, but solves the LP relaxation by leveraging recent developments in area-convexity regularization. Its runtime scales inversely in the approximation accuracy, which we show is optimal--barring a major breakthrough in algorithmic graph theory, namely faster Shortest Paths algorithms.},
  file        = {:http\://arxiv.org/pdf/2004.03114v1:PDF},
  keywords    = {cs.DS, math.OC},
}

@Article{Wang2020,
  author      = {Hanzhi Wang and Zhewei Wei and Ye Yuan and Xiaoyong Du and Ji-Rong Wen},
  date        = {2020-04-07},
  title       = {Exact Single-Source SimRank Computation on Large Graphs},
  doi         = {10.1145/3318464.3389781},
  eprint      = {2004.03493v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {SimRank is a popular measurement for evaluating the node-to-node similarities based on the graph topology. In recent years, single-source and top-$k$ SimRank queries have received increasing attention due to their applications in web mining, social network analysis, and spam detection. However, a fundamental obstacle in studying SimRank has been the lack of ground truths. The only exact algorithm, Power Method, is computationally infeasible on graphs with more than $10^6$ nodes. Consequently, no existing work has evaluated the actual trade-offs between query time and accuracy on large real-world graphs. In this paper, we present ExactSim, the first algorithm that computes the exact single-source and top-$k$ SimRank results on large graphs. With high probability, this algorithm produces ground truths with a rigorous theoretical guarantee. We conduct extensive experiments on real-world datasets to demonstrate the efficiency of ExactSim. The results show that ExactSim provides the ground truth for any single-source SimRank query with a precision up to 7 decimal places within a reasonable query time.},
  file        = {:http\://arxiv.org/pdf/2004.03493v1:PDF},
  keywords    = {cs.DS},
}

@Article{Tang2020,
  author      = {Wentao Tang and Prodromos Daoutidis},
  date        = {2020-04-04},
  title       = {Fast and Stable Nonconvex Constrained Distributed Optimization: The ELLADA Algorithm},
  eprint      = {2004.01977v1},
  eprintclass = {eess.SY},
  eprinttype  = {arXiv},
  abstract    = {Distributed optimization, where the computations are performed in a localized and coordinated manner using multiple agents, is a promising approach for solving large-scale optimization problems, e.g., those arising in model predictive control (MPC) of large-scale plants. However, a distributed optimization algorithm that is computationally efficient, globally convergent, amenable to nonconvex constraints and general inter-subsystem interactions remains an open problem. In this paper, we combine three important modifications to the classical alternating direction method of multipliers (ADMM) for distributed optimization. Specifically, (i) an extra-layer architecture is adopted to accommodate nonconvexity and handle inequality constraints, (ii) equality-constrained nonlinear programming (NLP) problems are allowed to be solved approximately, and (iii) a modified Anderson acceleration is employed for reducing the number of iterations. Theoretical convergence towards stationary solutions and computational complexity of the proposed algorithm, named ELLADA, is established. Its application to distributed nonlinear MPC is also described and illustrated through a benchmark process system.},
  file        = {:http\://arxiv.org/pdf/2004.01977v1:PDF},
  keywords    = {eess.SY, cs.SY},
}

@Article{Bergou2020,
  author       = {E. Bergou and Y. Diouane and V. Kungurtsev},
  date         = {2020-04-06},
  journaltitle = {JOTA, 2020},
  title        = {Convergence and Complexity Analysis of a Levenberg-Marquardt Algorithm for Inverse Problems},
  eprint       = {2004.03005v1},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  abstract     = {The Levenberg-Marquardt algorithm is one of the most popular algorithms for finding the solution of nonlinear least squares problems. Across different modified variations of the basic procedure, the algorithm enjoys global convergence, a competitive worst case iteration complexity rate, and a guaranteed rate of local convergence for both zero and nonzero small residual problems, under suitable assumptions. We introduce a novel Levenberg-Marquardt method that matches, simultaneously, the state of the art in all of these convergence properties with a single seamless algorithm. Numerical experiments confirm the theoretical behavior of our proposed algorithm.},
  file         = {:http\://arxiv.org/pdf/2004.03005v1:PDF},
  keywords     = {math.OC},
}

@Article{Luo2020,
  author      = {Xiaopeng Luo and Xin Xu},
  date        = {2020-04-05},
  title       = {Regularized asymptotic descents for nonconvex optimization},
  eprint      = {2004.02210v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this paper we propose regularized asymptotic descent (RAD) methods for solving nonconvex optimization problems. Our motivation is first to apply the regularized iteration and then to use an explicit asymptotic formula to approximate the solution of each regularized minimization. We consider a class of possibly nonconvex, nonsmooth, or even discontinuous objectives extended from strongly convex functions with Lipschitz-continuous gradients, in each of which has a unique global minima and is continuously differentiable at the global minimizer. The main theoretical result shows that the RAD method enjoys the global linear convergence with high probability for such a class of nonconvex objectives, i.e., the method will not be trapped in saddle points, local minima, or even discontinuities. Besides, the method is derivative-free and its per-iteration cost, i.e., the number of function evaluations, is bounded, so that it has a complexity bound $\mathcal{O}(\log\frac{1}{\epsilon})$ for finding a point such that the optimality gap at this point is less than $\epsilon>0$.},
  file        = {:http\://arxiv.org/pdf/2004.02210v1:PDF},
  keywords    = {math.OC, cs.NA, math.NA, 65K05, 68Q25, 90C26, 90C56},
}

@Article{Lu2020,
  author   = {Yuechao Lu and Ichitaro Yamazaki and Fumihiko Ino and Yasuyuki Matsushita and StanimireTomov and Jack Dongarra},
  date     = {2020},
  title    = {Reducing the Amount of Out-of-Core Data Access for {GPU}-Accelerated Randomized {SVD}},
  abstract = {We propose two acceleration methods, namely Fused and Gram, for reducing outof-core data access when performing randomized singular value decomposition (RSVD) on graphics processing units (GPUs). Out-of-core data here are data that are too large to fit into the GPU memory at once. Both methods accelerate GPU-enabled RSVD using the following three schemes: (1) a highly tuned general matrix-matrix multiplication (GEMM) scheme for processing out-of-core data on GPUs; (2) a data-access reduction scheme based on one-dimensional (1D) data partition; and (3) a first-in, first-out (FIFO) scheme that reduces CPU-GPU data transfer using the reverse iteration. The Fused method further reduces the amount of out-of-core data access by merging two GEMM operations into a single operation. In contrast, the Gram method reduces both in-core and out-of-core data access by explicitly forming the Gram matrix. According to our experimental results, the Fused and Gram methods improved the RSVD performance by up to 1.7$\times$ and 5.2$\times$, respectively, compared with a straightforward method that deploys schemes (1) and (2) on the GPU. In addition, we present a case study of deploying the Gram method for accelerating robust principal component analysis (RPCA), a convex optimization problem in machine learning.},
}

@Article{Ek2020,
  author      = {David Ek and Anders Forsgren},
  date        = {2020-04-08},
  title       = {Approximate solution of system of equations arising in interior-point methods for bound-constrained optimization},
  eprint      = {2004.04057v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {The focus in this paper is interior-point methods for bound-constrained nonlinear optimization where the system of nonlinear equations that arise are solved with Newton's method. There is a trade-off between solving Newton systems directly, which give high quality solutions, and solving many approximate Newton systems which are computationally less expensive but give lower quality solutions. We propose partial and full approximate solutions to the Newton systems, which in general involves solving a reduced system of linear equations. The specific approximate solution and the size of the reduced system that needs to be solved at each iteration are determined by estimates of the active and inactive constraints at the solution. These sets are at each iteration estimated by a simple heuristic. In addition, we motivate and suggest two modified-Newton approaches which are based on an intermediate step that consists of the partial approximate solutions. The theoretical setting is introduced and asymptotic error bounds are given along with numerical results for bound-constrained convex quadratic optimization problems, both random and from the CUTEst test collection.},
  file        = {:http\://arxiv.org/pdf/2004.04057v1:PDF},
  keywords    = {math.OC},
}

@Article{Champion2020,
  author      = {Camille Champion and Blazère Mélanie and Burcelin Rémy and Loubes Jean-Michel and Risser Laurent},
  date        = {2020-04-08},
  title       = {Robust spectral clustering using LASSO regularization},
  eprint      = {2004.03845v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {Cluster structure detection is a fundamental task for the analysis of graphs, in order to understand and to visualize their functional characteristics. Among the different cluster structure detection methods, spectral clustering is currently one of the most widely used due to its speed and simplicity. Yet, there are few theoretical guarantee to recover the underlying partitions of the graph for general models. This paper therefore presents a variant of spectral clustering, called 1-spectral clustering, performed on a new random model closely related to stochastic block model. Its goal is to promote a sparse eigenbasis solution of a 1 minimization problem revealing the natural structure of the graph. The effectiveness and the robustness to small noise perturbations of our technique is confirmed through a collection of simulated and real data examples.},
  file        = {:http\://arxiv.org/pdf/2004.03845v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Murray2020,
  author      = {Michael Murray and Jared Tanner},
  date        = {2020-04-10},
  title       = {The Permuted Striped Block Model and its Factorization -- Algorithms with Recovery Guarantees},
  eprint      = {2004.05094v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {We introduce a novel class of matrices which are defined by the factorization $\textbf{Y} :=\textbf{A}\textbf{X}$, where $\textbf{A}$ is an $m \times n$ wide sparse binary matrix with a fixed number $d$ nonzeros per column and $\textbf{X}$ is an $n \times N$ sparse real matrix whose columns have at most $k$ nonzeros and are $\textit{dissociated}$. Matrices defined by this factorization can be expressed as a sum of $n$ rank one sparse matrices, whose nonzero entries, under the appropriate permutations, form striped blocks - we therefore refer to them as Permuted Striped Block (PSB) matrices. We define the $\textit{PSB data model}$ as a particular distribution over this class of matrices, motivated by its implications for community detection, provable binary dictionary learning with real valued sparse coding, and blind combinatorial compressed sensing. For data matrices drawn from the PSB data model, we provide computationally efficient factorization algorithms which recover the generating factors with high probability from as few as $N =O\left(\frac{n}{k}\log^2(n)\right)$ data vectors, where $k$, $m$ and $n$ scale proportionally. Notably, these algorithms achieve optimal sample complexity up to logarithmic factors.},
  file        = {:http\://arxiv.org/pdf/2004.05094v1:PDF},
  keywords    = {cs.LG, cs.DM, eess.SP, stat.ML},
}

@Article{Duff2020a,
  author    = {Duff, Iain and Leleux, Philippe and Ruiz, Daniel and Torun, F. Sukru},
  title     = {Improving the Scalability of the {ABCD} Solver with a Combination of New Load Balancing and Communication Minimization Techniques},
  doi       = {10.3233/APC200052},
  issn      = {0927-5452},
  number    = {Parallel Computing: Technology Trends},
  pages     = {277--286},
  volume    = {36},
  abstract  = {The hybrid scheme block row-projection method implemented in the ABCD Solver is designed for solving large sparse unsymmetric systems of equations on distributed memory parallel computers. The method implements a block Cimmino iterative scheme, accelerated with a stabilized block conjugate gradient algorithm. An augmented pseudo-direct variant has also been developed to overcome convergence issues. Both methods are included in the ABCD solver with a hybrid parallelization scheme. The parallel performance of the ABCD Solver is improved in the first non-beta release, version 1.0, which we present in this paper. Novel algorithms for the distribution of partitions to processes are introduced to minimize communication as well as to balance the workload. Furthermore, the master-slave approach on each subsystem is also improved in order to achieve higher scalability through run-time placement of processes. We illustrate the improved parallel scalability of the ABCD Solver on a distributed memory architecture by solving several problems from the SuiteSparse Matrix Collection.},
  journal   = {Advances in Parallel Computing},
  publisher = {IOS Press},
  year      = {2020},
}

@Article{Zhang2020c,
  author      = {Qingsong Zhang and Feihu Huang and Cheng Deng and Heng Huang},
  date        = {2020-04-12},
  title       = {Faster Stochastic Quasi-Newton Methods},
  eprint      = {2004.06479v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Recently, stochastic optimization methods are a class of powerful optimization tools in machine learning. Stochastic gradient descent (SGD) is one of the representative stochastic methods and is widely used for many machine learning problems. However, SGD only uses the first-order information of problems to optimize them, which results in its some limitations such as its solutions without high accuracy. Thus, stochastic quasi-Newton methods recently have been widely concerned due to utilizing approximate Hessian information, which is more robust and can achieve better accuracy than stochastic first-order methods. Considering that existing stochastic quasi-Newton methods still do not reach the best known stochastic first-order oracle (SFO) complexity, thus, we propose a novel faster stochastic quasi-Newton method (SpiderSQN) based on the variance reduced technique of SIPDER. Moreover, we prove that our SpiderSQN method reach the best known SFO complexity of $\mathcal{O}(n+n^{1/2}\epsilon^{-2})$ in the finite-sum setting to obtain an $\epsilon$-first-order stationary point. To further improve its practical performance, we incorporate SpiderSQN with different effective momentum schemes. Moreover, the proposed algorithms are generalized to the online setting, and the corresponding SFO complexity of $\mathcal{O}(\epsilon^{-3})$ is developed, which matches the existing best result. Extensive experiments on benchmark datasets demonstrate that the proposed SpiderSQN-type of algorithms outperform state-of-the-art algorithms for nonconvex optimization.},
  file        = {:http\://arxiv.org/pdf/2004.06479v1:PDF},
  keywords    = {math.OC},
}

@Article{Bergamaschi2020a,
  author    = {Luca Bergamaschi},
  title     = {A Survey of Low-Rank Updates of Preconditioners for Sequences of Symmetric Linear Systems},
  doi       = {10.3390/a13040100},
  number    = {4},
  pages     = {100},
  volume    = {13},
  abstract  = {The aim of this survey is to review some recent developments in devising efficient preconditioners for sequences of symmetric positive definite (SPD) linear systems $A_k x_k = b_k,\, k = 1, \dots$ arising in many scientific applications, such as discretization of transient Partial Differential Equations (PDEs), solution of eigenvalue problems, (Inexact) Newton methods applied to nonlinear systems, rational Krylov methods for computing a function of a matrix. In this paper, we will analyze a number of techniques of updating a given initial preconditioner by a low-rank matrix with the aim of improving the clustering of eigenvalues around 1, in order to speed-up the convergence of the Preconditioned Conjugate Gradient (PCG) method. We will also review some techniques to efficiently approximate the linearly independent vectors which constitute the low-rank corrections and whose choice is crucial for the effectiveness of the approach. Numerical results on real-life applications show that the performance of a given iterative solver can be very much enhanced by the use of low-rank updates.},
  journal   = {Algorithms},
  month     = {4},
  publisher = {{MDPI} {AG}},
  year      = {2020},
}

@InCollection{Cheramangalath2020,
  author    = {Unnikrishnan Cheramangalath and Rupesh Nasre and Y. N. Srikant},
  booktitle = {Distributed Graph Analytics},
  title     = {Graph Analytics Frameworks},
  doi       = {10.1007/978-3-030-41886-1_4},
  pages     = {99--122},
  publisher = {Springer International Publishing},
  abstract  = {Frameworks take away the drudgery of routine tasks in programming graph analytic applications. This chapter describes in some detail, the different models of execution that are used in graph analytics, such as BSP, Map-Reduce, asynchronous execution, GAS, Inspector-Executor, and Advance-Filter-Compute. It also provides a glimpse of different existing frameworks on multi-core CPUs, GPUs, and distributed systems.},
  year      = {2020},
}

@Article{Karunakaran2020,
  author    = {Sivakumar Karunakaran and Lavanya Selvaganesh},
  title     = {A novel graph matrix representation: sequence of neighbourhood matrices with an application},
  doi       = {10.1007/s42452-020-2635-1},
  number    = {5},
  volume    = {2},
  abstract  = {In the study of network optimization, fnding the shortest path minimizing time/distance/cost from a source node to a destination node is one of the fundamental problems. Our focus here is to fnd the shortest path between any pair of nodes in a given undirected unweighted simple graph with the help of the sequence of powers of neighbourhood matrices. The authors recently introduced the concept of neighbourhood matrix as a novel representation of graphs using the neighbourhood sets of the vertices. In this article, an extension of the above work is presented by introducing a sequence of matrices, referred to as the sequence of powers of $\mathcal{N}\mathcal{M}(G)$. It is denoted it by $\mathcal{N}\mathcal{M}^{(l)} (G)=\left[ \eta^{l}_{ij}\right], 1 \le l \le k(G)$, where $k(G)$ is called the iteration number, $k(G) = \lceil \text{log}_2 \text{diameter}(G) \rceil$. As this sequence of matrices captures the distance between the nodes profoundly, we further develop the technique and present several characterizations. Based on the theoretical results, we present an algorithm to fnd the shortest path between any pair of nodes in a given graph. The proposed algorithm and the claims therein are formally validated through simulations on synthetic data and the real network data from Facebook. The empirical results are quite promising with our algorithm having best running time among all the existing well-known shortest path algorithms for the considered graph classes.},
  journal   = {{SN} Applied Sciences},
  month     = {4},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@InCollection{Khalifa2020,
  author    = {Dorra Ben Khalifa and Matthieu Martel and Assal{\'{e}} Adj{\'{e}}},
  booktitle = {Communications in Computer and Information Science},
  title     = {{POP}: A Tuning Assistant for Mixed-Precision Floating-Point Computations},
  doi       = {10.1007/978-3-030-46902-3_5},
  pages     = {77--94},
  publisher = {Springer International Publishing},
  abstract  = {In this article, we describe a static program analysis to determine the lowest floating-point precisions on inputs and intermediate results that guarantees a desired accuracy of the output values. A common practice used by developers without advanced training in computer arithmetic consists in using the highest precision available in hardware (double precision on most CPU’s) which can be exorbitant in terms of energy consumption, memory traffic, and bandwidth capacity. To overcome this difficulty, we propose a new precision tuning tool for the floating-point programs integrating a static forward and backward analysis, done by abstract interpretation. Next, our analysis will be expressed as a set of linear constraints easily checked by an SMT solver.},
  year      = {2020},
}

@Article{Kanagasabapathi2020,
  author    = {Somasundaram Kanagasabapathi and MG Thushara},
  title     = {{FORWARD} {AND} {BACKWARD} {STATIC} {ANALYSIS} {FOR} {CRITICAL} {NUMERICAL} {ACCURACY} {IN} {FLOATING} {POINT} {PROGRAMS}},
  doi       = {10.7494/csci.2020.21.2.3421},
  number    = {2},
  volume    = {21},
  abstract  = {In this article, we introduce a new static analysis for numerical accuracy. We address the problem of determining the minimal accuracy on the inputs and on the intermediary results of a program containing floating-point computations in order to ensure the desired accuracy of the outputs. The main approach is to combine a forward and backward static analysis, done by abstract interpretation. The backward analysis computes the minimal accuracy needed for the inputs and intermediary results of the program in order to ensure the desired accuracy of the results (as specified by the user). In practice, the information collected by our analysis may help optimize the formats used to represent the values stored in the variables of the program or to select the appropriate sensors. To illustrate our analysis, we have shown a prototype example with experimental results.},
  journal   = {Computer Science},
  month     = {4},
  publisher = {{AGHU} University of Science and Technology Press},
  year      = {2020},
}

@Article{Fasi2020a,
  author     = {Fasi, Massimiliano and Higham, Nicholas J. and Mikaitis, Mantas and Pranesh, Srikara},
  date       = {2020},
  title      = {Numerical Behavior of the {NVIDIA} Tensor Cores},
  eprint     = {MIMS EPrint:2020.10},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2761/1/fhms20.pdf},
  abstract   = {We explore the floating-point arithmetic used by the NVIDIA Volta tensor cores, which are hardware accelerators for mixed-precision matrix multiplication. We investigate what precision is used for intermediate results, whether subnormal numbers are supported, what rounding mode is used, in which order the operations in the dot products arising in the matrix multiplication are performed, and whether partial sums are normalized. These aspects are not documented by NVIDIA, and we gain insight by running carefully designed numerical experiments on these hardware accelerators. Knowing the answers to these questions is important if one wishes to: 1) build hardware that computes a matrix-matrix product matching the results of NVIDIA tensor cores; 2) achieve bit-reproducible results when designing on conventional hardware with IEEE 754 floating point arithmetic code meant to run on NVIDIA tensor cores; and 3) understand the differences between results produced by code that utilizes tensor cores and code that uses only IEEE 754-compliant arithmetic operations. As an additional result, we point out a non-monotonicity issue that arises in floating-point multi-operand addition without the normalization of the intermediate results.},
}

@Article{Zhang2020d,
  author    = {Zhang, Guanglu and Allaire, Douglas and Cagan, Jonathan},
  title     = {An Initial Guess Free Method for Least Squares Parameter Estimation in Nonlinear Models},
  doi       = {10.13140/RG.2.2.32573.82402},
  language  = {en},
  abstract  = {Fitting models to data is critical in many science and engineering fields. A major task in fitting models to data is to estimate the value of each parameter in a given model. Iterative methods, such as the Gauss-Newton method and the Levenberg-Marquardt method, are often employed for parameter estimation in nonlinear models. However, practitioners must guess the initial value for each parameter in order to initialize these iterative methods. A poor initial guess can contribute to non-convergence of these methods or lead these methods to converge to a wrong solution. In this paper, an initial guess free method is introduced to find the optimal parameter estimators in a nonlinear model that minimizes the squared error of the fit. The method includes three algorithms that require different level of computational power to find the optimal parameter estimators. The method constructs a solution interval for each parameter in the model. These solution intervals significantly reduce the search space for optimal parameter estimators. The method also provides an empirical probability distribution for each parameter, which is valuable for parameter uncertainty assessment. The initial guess free method is validated through a case study in which Fick's second law is fit to an experimental data set. This case study shows that the initial guess free method can find the optimal parameter estimators efficiently. A four-step procedure for implementing the initial guess free method in practice is also outlined.},
  publisher = {Unpublished},
  year      = {2020},
}

@Article{Connolly2020,
  author     = {Connolly, Michael P. and Higham, Nicholas J. and Mary, Theo},
  date       = {2020},
  title      = {Stochastic Rounding and its Probabilistic Backward Error Analysis},
  eprint     = {MIMS EPrint:2020.12},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2763/1/paper.pdf},
  abstract   = {Stochastic rounding rounds a real number to the next larger or smaller floatingpoint number with probabilities 1 minus the relative distances to those numbers. It is gaining attention in deep learning because it can improve the accuracy of the computations. We compare basic properties of stochastic rounding with those for round to nearest, finding properties in common as well as significant differences. We prove that for stochastic rounding the rounding errors are mean independent random variables with zero mean. We derive a new version of our probabilistic error analysis theorem from [SIAM J. Sci. Comput., 41 (2019), pp. A2815–A2835], weakening the assumption of independence of the random variables to mean independence. These results imply that for a wide range of linear algebra computations the backward error for stochastic rounding is unconditionally bounded by a multiple of $\sqrt{n}u$ to first order, with a certain probability, where $n$ is the problem size and $u$ is the unit roundoff. This is the first scenario where the rule of thumb that one can replace nu by $\sqrt{n}u$ in a rounding error bound has been shown to hold without any additional assumptions on the rounding errors. We also explain how stochastic rounding avoids the phenomenon of stagnation in sums, whereby small addends are obliterated by round to nearest when they are too small relative to the sum.},
}

@Article{Das2020,
  author      = {Arnab Das and Ian Briggs and Ganesh Gopalakrishnan and Sriram Krishnamoorthy},
  date        = {2020-04-24},
  title       = {An Abstraction-guided Approach to Scalable and Rigorous Floating-Point Error Analysis},
  eprint      = {2004.11960v1},
  eprintclass = {cs.PL},
  eprinttype  = {arXiv},
  abstract    = {Automated techniques for rigorous floating-point round-off error analysis are important in areas including formal verification of correctness and precision tuning. Existing tools and techniques, while providing tight bounds, fail to analyze expressions with more than a few hundred operators, thus unable to cover important practical problems. In this work, we present Satire, a new tool that sheds light on how scalability and bound-tightness can be attained through a combination of incremental analysis, abstraction, and judicious use of concrete and symbolic evaluation. Satire has handled problems exceeding 200K operators. We present Satire's underlying error analysis approach, information-theoretic abstraction heuristics, and a wide range of case studies, with evaluation covering FFT, Lorenz system of equations, and various PDE stencil types. Our results demonstrate the tightness of Satire's bounds, its acceptable runtime, and valuable insights provided.},
  file        = {:http\://arxiv.org/pdf/2004.11960v1:PDF},
  keywords    = {cs.PL, cs.NA, cs.SC, math.NA},
}

@InProceedings{Said2020,
  author    = {Noureddine Ait Said and Mounir Benabdenbi and Katell Morin-Allory},
  booktitle = {2020 15th Design {\&} Technology of Integrated Systems in Nanoscale Era ({DTIS})},
  title     = {{FPU} Bit-Width Optimization for Approximate Computing: A Non-Intrusive Approach},
  doi       = {10.1109/dtis48698.2020.9080931},
  publisher = {{IEEE}},
  abstract  = {F1oating-Point Units (FPUs) count as a significant part of computing resources in modern general-purpose and application-specific processors. Full-precision FPUs can be a source of extensive hardware overhead (power consumption, area, memory footprint etc.). On the other hand, several applications feature the inherent ability to tolerate precision loss. This has lead to the development of a new computing paradigm: Transprecision Computing (TC), where variable and arbitrary precision hardware FPUs have been introduced. Many tools and libraries have been proposed to simulate the effects of approximation on applications, to help designers to select the most optimized FPU architecture adequate for a given application.However, existing techniques require developers to rewrite part or all of their existing software stacks (applications, libraries, operating systems …), which is often infeasible, complex or at least a very time-consuming development effort.This work proposes a non-intrusive approach, which does not need source code modification, by introducing approximations at the low-level in assembly. This allows approximating virtually all kinds of executable binaries (bare-metal applications, single- /multi-threaded user applications, OS/RTOS, etc.).We implement the approach on top of the well known QEMU dynamic binary translator. We perform experiments on a set of benchmarks from the literature, and we demonstrate how the approach further simplifies evaluating the impact of FP approximations on numerical applications outputs, without being intrusive to the source code.},
  month     = {4},
  year      = {2020},
}

@Article{Rodomanov2020b,
  author       = {Anton Rodomanov and Yurii Nesterov},
  date         = {2020-04-29},
  journaltitle = {CORE Discussion Papers ; 2020/13 (2020) 24 pages http://hdl.handle.net/2078.1/229640},
  title        = {New Results on Superlinear Convergence of Classical Quasi-Newton Methods},
  eprint       = {2004.14866v1},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  abstract     = {We present a new theoretical analysis of local superlinear convergence of the classical quasi-Newton methods from the convex Broyden class. Our analysis is based on the potential function involving the logarithm of determinant of Hessian approximation and the trace of inverse Hessian approximation. For the well-known DFP and BFGS methods, we obtain the rates of the form $\left[\frac{L}{\mu} \left(\exp\left\{\frac{n}{k} \ln \frac{L}{\mu}\right\} - 1\right)\right]^{k/2}$ and $\left[\exp\left\{\frac{n}{k} \ln \frac{L}{\mu}\right\} - 1\right]^{k/2}$ respectively, where $k$ is the iteration counter, $n$ is the dimension of the problem, $\mu$ is the strong convexity parameter, and $L$ is the Lipschitz constant of the gradient. Currently, these are the best known superlinear convergence rates for these methods. In particular, our results show that the starting moment of superlinear convergence of BFGS method depends on the logarithm of the condition number $\frac{L}{\mu}$ in the worst case.},
  file         = {:http\://arxiv.org/pdf/2004.14866v1:PDF},
  keywords     = {math.OC},
}

@Article{Schaefer2020,
  author      = {Florian Schäfer and Matthias Katzfuss and Houman Owhadi},
  date        = {2020-04-29},
  title       = {Sparse Cholesky factorization by Kullback-Leibler minimization},
  eprint      = {2004.14455v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {We propose to compute a sparse approximate inverse Cholesky factor $L$ of a dense covariance matrix $\Theta$ by minimizing the Kullback-Leibler divergence between the Gaussian distributions $\mathcal{N}(0, \Theta)$ and $\mathcal{N}(0, L^{-\top} L^{-1})$, subject to a sparsity constraint. Surprisingly, this problem has a closed-form solution that can be computed efficiently, recovering the popular Vecchia approximation in spatial statistics. Based on recent results on the approximate sparsity of inverse Cholesky factors of $\Theta$ obtained from pairwise evaluation of Green's functions of elliptic boundary-value problems at points $\{x_{i}\}_{1 \leq i \leq N} \subset \mathbb{R}^{d}$, we propose an elimination ordering and sparsity pattern that allows us to compute $\epsilon$-approximate inverse Cholesky factors of such $\Theta$ in computational complexity $\mathcal{O}(N \log(N/\epsilon)^d)$ in space and $\mathcal{O}(N \log(N/\epsilon)^{2d})$ in time. To the best of our knowledge, this is the best asymptotic complexity for this class of problems. Furthermore, our method is embarrassingly parallel, automatically exploits low-dimensional structure in the data, and can perform Gaussian-process regression in linear (in $N$) space complexity. Motivated by the optimality properties of our methods, we propose methods for applying it to the joint covariance of training and prediction points in Gaussian-process regression, greatly improving stability and computational cost. Finally, we show how to apply our method to the important setting of Gaussian processes with additive noise, sacrificing neither accuracy nor computational complexity.},
  file        = {:http\://arxiv.org/pdf/2004.14455v1:PDF},
  keywords    = {math.NA, cs.NA, math.OC, math.ST, stat.CO, stat.TH},
}

@Article{Liu2020,
  author    = {Hongbin Liu and Hu Ren and Hanfeng Gu and Fei Gao and Guangwen Yang},
  title     = {{UNAT}: {UNstructured} Acceleration Toolkit on {SW}26010 many-core processor},
  doi       = {10.1108/ec-09-2019-0401},
  number    = {ahead-of-print},
  volume    = {ahead-of-print},
  abstract  = {The purpose of this paper is to provide an automatic parallelization toolkit for unstructured mesh-based computation. Among all kinds of mesh types, unstructured meshes are dominant in engineering simulation scenarios and play an essential role in scientific computations for their geometrical flexibility. However, the high-fidelity applications based on unstructured grids are still time-consuming, no matter for programming or running.\\ This study develops an efficient UNstructured Acceleration Toolkit (UNAT), which provides friendly high-level programming interfaces and elaborates lower level implementation on the target hardware to get nearly hand-optimized performance. At the present state, two efficient strategies, a multi-level blocks method and a row-subsections method, are designed and implemented on Sunway architecture. Random memory access and write–write conflict issues of unstructured meshes have been handled by partitioning, coloring and other hardware-specific techniques. Moreover, a data-reuse mechanism is developed to increase the computational intensity and alleviate the memory bandwidth bottleneck.\\ The authors select sparse matrix-vector multiplication as a performance benchmark of UNAT across different data layouts and different matrix formats. Experimental results show that the speed-ups reach up to 26$\times$ compared to single management processing element, and the utilization ratio tests indicate the capability of achieving nearly hand-optimized performance. Finally, the authors adopt UNAT to accelerate a well-tuned unstructured solver and obtain speed-ups of 19$\times$ and 10$\times$ on average for main kernels and overall solver, respectively.\\ The authors design an unstructured mesh toolkit, UNAT, to link the hardware and numerical algorithm, and then, engineers can focus on the algorithms and solvers rather than the parallel implementation. For the many-core processor SW26010 of the fastest supercomputer in China, UNAT yields up to 26$\times$ speed-ups and achieves nearly hand-optimized performance.},
  journal   = {Engineering Computations},
  month     = {5},
  publisher = {Emerald},
  year      = {2020},
}

@Article{Perez2020,
  author    = {Alberto Cabrera Perez and Alejandro Acosta and Francisco Almeida and Vicente Blanco},
  title     = {A dynamic Multi-Objective approach for dynamic load balancing in heterogeneous systems},
  doi       = {10.1109/tpds.2020.2989869},
  pages     = {1--1},
  abstract  = {Modern standards in High Performance Computing (HPC) have started to consider energy consumption and power draw as a limiting factor. New and more complex architectures have been introduced in HPC systems to afford these new restrictions, and include coprocessors such as GPGPUs for intensive computational tasks. As systems increase in heterogeneity, workload distribution becomes a more core problem to achieve the maximum efficiency in every computational component. We present a Multi-Objective Dynamic Load Balancing (DLB) approach where several objectives can be applied to tune an application. These objectives can be dynamically exchanged during the execution of an algorithm to better adapt to the resources available in a system. We have implemented the Multi-Objective DLB together with a generic heuristic engine, designed to perform multiple strategies for DLB in iterative problems. We also present Ull Multiobjective Framework (UllMF), an open-source tool that implements the Multi-Objective generic approach. UllMF separates metric gathering, objective functions to be optimized and load balancing algorithms, and improves code portability using a simple interface to reduce the costs of new implementations. We illustrate how performance and energy consumption are improved for the implemented techniques, and analyze their quality using different DLB techniques from the literature.},
  journal   = {{IEEE} Transactions on Parallel and Distributed Systems},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Bonifati2020,
  author      = {Angela Bonifati and Stefania Dumbrava and Haridimos Kondylakis},
  date        = {2020-04-30},
  title       = {Graph Summarization},
  eprint      = {2004.14794v2},
  eprintclass = {cs.DB},
  eprinttype  = {arXiv},
  abstract    = {The continuous and rapid growth of highly interconnected datasets, which are both voluminous and complex, calls for the development of adequate processing and analytical techniques. One method for condensing and simplifying such datasets is graph summarization. It denotes a series of application-specific algorithms designed to transform graphs into more compact representations while preserving structural patterns, query answers, or specific property distributions. As this problem is common to several areas studying graph topologies, different approaches, such as clustering, compression, sampling, or influence detection, have been proposed, primarily based on statistical and optimization methods. The focus of our chapter is to pinpoint the main graph summarization methods, but especially to focus on the most recent approaches and novel research trends on this topic, not yet covered by previous surveys.},
  file        = {:http\://arxiv.org/pdf/2004.14794v2:PDF},
  keywords    = {cs.DB},
}

@Article{Cao2020,
  author    = {Xuanyu Cao and K. J. Ray Liu},
  title     = {Distributed Newton's Method for Network Cost Minimization},
  doi       = {10.1109/tac.2020.2989266},
  pages     = {1--1},
  abstract  = {In this work, we examine a novel generic network cost minimization problem, in which every node has a local decision vector to optimize. Each node incurs a cost associated with its decision vector while each link incurs a cost related to the decision vectors of its two end nodes. All nodes collaborate to minimize the overall network cost. The formulated network cost minimization problem has broad applications in distributed signal processing and control, in which the notion of link costs often arises. To solve this problem in a decentralized manner, we develop a distributed variant of the Newton's method, which possesses faster convergence than alternative first order optimization methods such as gradient descent and alternating direction method of multipliers. The proposed method is based on an appropriate splitting of the Hessian matrix and an approximation of its inverse, which is used to determine the Newton step. Global linear convergence of the proposed algorithm is established under several standard technical assumptions on the local cost functions. Furthermore, analogous to classical centralized Newton's method, a quadratic convergence phase of the algorithm over a certain time interval is identified. Finally, numerical simulations are conducted to validate the effectiveness of the proposed algorithm and its superiority over other first order methods, especially when the cost functions are ill-conditioned. Complexity issues of the proposed distributed Newton's method and alternative first order methods are also discussed.},
  journal   = {{IEEE} Transactions on Automatic Control},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Soltaniyeh2020,
  author      = {Mohammadreza Soltaniyeh and Richard P. Martin and Santosh Nagarakatte},
  date        = {2020-04-29},
  title       = {Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra},
  eprint      = {2004.13907v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {This paper describes REAP, a software-hardware approach that enables high performance sparse linear algebra computations on a cooperative CPU-FPGA platform. REAP carefully separates the task of organizing the matrix elements from the computation phase. It uses the CPU to provide a first-pass re-organization of the matrix elements, allowing the FPGA to focus on the computation. We introduce a new intermediate representation that allows the CPU to communicate the sparse data and the scheduling decisions to the FPGA. The computation is optimized on the FPGA for effective resource utilization with pipelining. REAP improves the performance of Sparse General Matrix Multiplication (SpGEMM) and Sparse Cholesky Factorization by 3.2X and 1.85X compared to widely used sparse libraries for them on the CPU, respectively.},
  file        = {:http\://arxiv.org/pdf/2004.13907v1:PDF},
  keywords    = {cs.DC, cs.MS, cs.PL},
}

@Article{Chen2020c,
  author    = {Yuedan Chen and Guoqing Xiao and M. Tamer Ozsu and Chubo Liu and Albert Zomaya and Tao Li},
  title     = {{aeSpTV}: An Adaptive and Efficient Framework for Sparse Tensor-Vector Product Kernel on a High-Performance Computing Platform},
  doi       = {10.1109/tpds.2020.2990429},
  pages     = {1--1},
  abstract  = {Multi-dimensional, large-scale, and sparse data, which can be neatly represented by sparse tensors, are increasingly used in various applications such as data analysis and machine learning. A high-performance sparse tensor-vector product (SpTV), one of the most fundamental operations of processing sparse tensors, is necessary for improving efficiency of related applications. In this paper, we propose aeSpTV, an adaptive and efficient SpTV framework on Sunway TaihuLight supercomputer, to solve several challenges of optimizing SpTV on high-performance computing platforms. First, to map SpTV to Sunway architecture and tame expensive memory access latency and parallel writing conflict due to the intrinsic irregularity of SpTV, we introduce an adaptive SpTV parallelization. Second, to co-execute with the parallelization design while still ensuring high efficiency, we design a sparse tensor data structure named CSSoCR. Third, based on the adaptive SpTV parallelization with the novel tensor data structure, we present an auto-tuner that chooses the most befitting tensor partitioning method for aeSpTV using the variance analysis theory of mathematical statistics to achieve load balance. Fourth, to further leverage the computing power of Sunway, we propose customized optimizations for aeSpTV. Experimental results show that aeSpTV yields good sacalability on both thread-level and process-level parallelism of Sunway. It achieves a maximum GFLOPS of 195.69 on 128 processes. Additionally, it is proved that optimization effects of the partitioning auto-tuner and optimization techniques are remarkable.},
  journal   = {{IEEE} Transactions on Parallel and Distributed Systems},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@TechReport{Regev2020,
  author      = {Shaked Regev and Michael A. Saunders},
  date        = {2020},
  institution = {Stanford University},
  title       = {{SSAI}: A Symmetric Sparse Approximate Inverse Preconditioner for the Conjugate Gradient Methods {PCG} and {PCGLS}},
  eprint      = {M130639},
  url         = {https://web.stanford.edu/group/SOL/reports/M130639.pdf},
  abstract    = {We propose a method for solving a Hermitian positive definite linear system $Ax = b$, where $A$ is an explicit sparse matrix (real or complex). A sparse approximate right inverse $M$ is computed and replaced by its symmetrization $\tilde{M}$, which is used as a left-right preconditioner in a modified version of the preconditioned conjugategradient method (PCG), where $M$ is modified occasionally, if necessary, to make it more positive definite. $M$ is formed column by column and can therefore be computed in parallel. PCG requires only matrix-vector multiplications with $A$ and $\tilde{M}$ (not solving a linear system with $\tilde{M}$), and so too can be carried out in parallel. We compare it with incomplete Cholesky factorization (the gold standard for PCG) and with MATLAB’s backslash operator (sparse Cholesky) on matrices from various applications. For least-squares problems, we implement an analogous form of preconditioned Conjugate Gradient Least-Squares (PCGLS) which is also shown to be robust.},
}

@Article{Tripathy2020,
  author      = {Alok Tripathy and Katherine Yelick and Aydin Buluc},
  date        = {2020-05-07},
  title       = {Reducing Communication in Graph Neural Network Training},
  eprint      = {2005.03300v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Graph Neural Networks (GNNs) are powerful and flexible neural networks that use the naturally sparse connectivity information of the data. GNNs represent this connectivity as sparse matrices, which have lower arithmetic intensity and thus higher communication costs compared to dense matrices, making GNNs harder to scale to high concurrencies than convolutional or fully-connected neural networks. We present a family of parallel algorithms for training GNNs. These algorithms are based on their counterparts in dense and sparse linear algebra, but they had not been previously applied to GNN training. We show that they can asymptotically reduce communication compared to existing parallel GNN training methods. We implement a promising and practical version that is based on 2D sparse-dense matrix multiplication using torch.distributed. Our implementation parallelizes over GPU-equipped clusters. We train GNNs on up to a hundred GPUs on datasets that include a protein network with over a billion edges.},
  file        = {:http\://arxiv.org/pdf/2005.03300v1:PDF},
  keywords    = {cs.LG, cs.DC, stat.ML},
}

@Article{Derezinski2020,
  author      = {Michał Dereziński and Michael W. Mahoney},
  date        = {2020-05-07},
  title       = {Determinantal Point Processes in Randomized Numerical Linear Algebra},
  eprint      = {2005.03185v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Randomized Numerical Linear Algebra (RandNLA) uses randomness to develop improved algorithms for matrix problems that arise in scientific computing, data science, machine learning, etc. Determinantal Point Processes (DPPs), a seemingly unrelated topic in pure and applied mathematics, is a class of stochastic point processes with probability distribution characterized by sub-determinants of a kernel matrix. Recent work has uncovered deep and fruitful connections between DPPs and RandNLA which lead to new guarantees and improved algorithms that are of interest to both areas. We provide an overview of this exciting new line of research, including brief introductions to RandNLA and DPPs, as well as applications of DPPs to classical linear algebra tasks such as least squares regression, low-rank approximation and the Nystr\"om method. For example, random sampling with a DPP leads to new kinds of unbiased estimators for least squares, enabling more refined statistical and inferential understanding of these algorithms; a DPP is, in some sense, an optimal randomized algorithm for the Nystr\"om method; and a RandNLA technique called leverage score sampling can be derived as the marginal distribution of a DPP. We also discuss recent algorithmic developments, illustrating that, while not quite as efficient as standard RandNLA techniques, DPP-based algorithms are only moderately more expensive.},
  file        = {:http\://arxiv.org/pdf/2005.03185v1:PDF},
  keywords    = {cs.DS, cs.LG},
}

@Article{Cornelis2020,
  author      = {Jeffrey Cornelis and Wim Vanroose},
  date        = {2020-05-06},
  title       = {Projected Newton method for noise constrained $\ell_p$ regularization},
  eprint      = {2005.02687v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {Choosing an appropriate regularization term is necessary to obtain a meaningful solution to an ill-posed linear inverse problem contaminated with measurement errors or noise. A regularization term in the the $\ell_p$ norm with $p\geq 1$ covers a wide range of choices since its behavior critically depends on the choice of $p$ and since it can easily be combined with a suitable regularization matrix. We develop an efficient algorithm that simultaneously determines the regularization parameter and corresponding $\ell_p$ regularized solution such that the discrepancy principle is satisfied. We project the problem on a low-dimensional Generalized Krylov subspace and compute the Newton direction for this much smaller problem. We illustrate some interesting properties of the algorithm and compare its performance with other state-of-the-art approaches using a number of numerical experiments, with a special focus of the sparsity inducing $\ell_1$ norm and edge-preserving total variation regularization.},
  file        = {:http\://arxiv.org/pdf/2005.02687v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Kochurov2020,
  author      = {Max Kochurov and Rasul Karimov and Sergei Kozlukov},
  date        = {2020-05-06},
  title       = {Geoopt: Riemannian Optimization in PyTorch},
  eprint      = {2005.02819v1},
  eprintclass = {cs.CG},
  eprinttype  = {arXiv},
  abstract    = {Geoopt is a research-oriented modular open-source package for Riemannian Optimization in PyTorch. The core of Geoopt is a standard Manifold interface which allows for the generic implementation of optimization algorithms. Geoopt supports basic Riemannian SGD as well as adaptive optimization algorithms. Geoopt also provides several algorithms and arithmetic methods for supported manifolds, which allow composing geometry-aware neural network layers that can be integrated with existing models.},
  file        = {:http\://arxiv.org/pdf/2005.02819v1:PDF},
  keywords    = {cs.CG, cs.LG, 53-04, G.4},
}

@Article{Ahrens2020,
  author    = {Ahrens, Peter and Demmel, James and Nguyen, Hong},
  date      = {2020},
  title     = {Algorithms for Efficient Reproducible Floating Point Summation},
  doi       = {10.1145/3389360},
  issn      = {0098-3500},
  number    = {ja},
  url       = {https://dl.acm.org/doi/abs/10.1145/3389360},
  volume    = {0},
  abstract  = {We define reproducibility to mean getting bitwise identical results from multiple runs of the same program, perhaps with different hardware resources or other changes that should not change the answer. Many users depend on reproducibility for debugging or correctness. However, dynamic scheduling of parallel computing resources, with nonassociativity of floating-point addition, makes reproducibility a challenge even for summing a vector of numbers, or the Basic Linear Algebra Subprograms (BLAS). We describe an algorithm that computes a reproducible floating point sum, independent of summation order. The algorithm uses only a subset of IEEE Standard 754-2008. It is communication-optimal, i.e. it does just one pass over the data in the sequential case, or one reduction operation in parallel, requiring an \emph{accumulator} of just 6 words (higher precision is possible). The arithmetic cost is $7n$ additions to sum $n$ words, and the error bound can be up to $10^(-8)$ times smaller than for conventional summation. We describe the algorithm, the software infrastructure for reproducible BLAS (ReproBLAS), and performance results. For example, for the dot product of 4096 doubles, we get a 4$\times$ slowdown compared to Intel MKL on an Intel Core i7-2600 CPU operating at 3.4 GHz and 256 KB L2 Cache.},
  address   = {New York, NY, USA},
  journal   = {ACM Transactions on Mathematical Software},
  numpages  = {1},
  publisher = {Association for Computing Machinery},
}

@Article{Ma2020,
  author      = {Linjian Ma and Jiayu Ye and Edgar Solomonik},
  date        = {2020-05-10},
  title       = {AutoHOOT: Automatic High-Order Optimization for Tensors},
  eprint      = {2005.04540v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {High-order optimization methods, including Newton's method and its variants as well as alternating minimization methods, dominate the optimization algorithms for tensor decompositions and tensor networks. These tensor methods are used for data analysis and simulation of quantum systems. In this work, we introduce AutoHOOT, the first automatic differentiation (AD) framework targeting at high-order optimization for tensor computations. AutoHOOT takes input tensor computation expressions and generates optimized derivative expressions. In particular, AutoHOOT contains a new explicit Jacobian / Hessian expression generation kernel whose outputs maintain the input tensors' granularity and are easy to optimize. The expressions are then optimized by both the traditional compiler optimization techniques and specific tensor algebra transformations. Experimental results show that AutoHOOT achieves competitive performance for both tensor decomposition and tensor network applications compared to existing AD software and other tensor computation libraries with manually written kernels, both on CPU and GPU architectures. The scalability of the generated kernels is as good as other well-known high-order numerical algorithms so that it can be executed efficiently on distributed parallel systems.},
  file        = {:http\://arxiv.org/pdf/2005.04540v1:PDF},
  keywords    = {cs.MS, cs.NA, math.NA},
}

@Article{Bertsimas2020,
  author      = {Dimitris Bertsimas and Ryan Cory-Wright and Jean Pauphilet},
  date        = {2020-05-11},
  title       = {Solving Large-Scale Sparse PCA to Certifiable (Near) Optimality},
  eprint      = {2005.05195v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Sparse principal component analysis (PCA) is a popular dimensionality reduction technique for obtaining principal components which are linear combinations of a small subset of the original features. Existing approaches cannot supply certifiably optimal principal components with more than $p=100s$ covariates. By reformulating sparse PCA as a convex mixed-integer semidefinite optimization problem, we design a cutting-plane method which solves the problem to certifiable optimality at the scale of selecting k=10s covariates from p=300 variables, and provides small bound gaps at a larger scale. We also propose two convex relaxations and randomized rounding schemes that provide certifiably near-exact solutions within minutes for p=100s or hours for p=1,000s. Using real-world financial and medical datasets, we illustrate our approach's ability to derive interpretable principal components tractably at scale.},
  file        = {:http\://arxiv.org/pdf/2005.05195v1:PDF},
  keywords    = {math.OC, cs.LG, math.ST, stat.CO, stat.TH},
}

@Article{Fang2020,
  author      = {Jianbin Fang and Chun Huang and Tao Tang and Zheng Wang},
  date        = {2020-05-05},
  title       = {Parallel Programming Models for Heterogeneous Many-Cores : A Survey},
  eprint      = {2005.04094v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Heterogeneous many-cores are now an integral part of modern computing systems ranging from embedding systems to supercomputers. While heterogeneous many-core design offers the potential for energy-efficient high-performance, such potential can only be unlocked if the application programs are suitably parallel and can be made to match the underlying heterogeneous platform. In this article, we provide a comprehensive survey for parallel programming models for heterogeneous many-core architectures and review the compiling techniques of improving programmability and portability. We examine various software optimization techniques for minimizing the communicating overhead between heterogeneous computing devices. We provide a road map for a wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This article provides both an accessible introduction to the fast-moving area of heterogeneous programming and a detailed bibliography of its main achievements.},
  file        = {:http\://arxiv.org/pdf/2005.04094v1:PDF},
  keywords    = {cs.DC, cs.PL},
}

@Article{Jung2020,
  author    = {Yoon Mo Jung and Joyce Jiyoung Whang and Sangwoon Yun},
  title     = {Sparse probabilistic K-means},
  doi       = {10.1016/j.amc.2020.125328},
  pages     = {125328},
  volume    = {382},
  abstract  = {The goal of clustering is to partition a set of data points into groups of similar data points, called clusters. Clustering algorithms can be classified into two categories: hard and soft clustering. Hard clustering assigns each data point to one cluster exclusively. On the other hand, soft clustering allows probabilistic assignments to clusters. In this paper, we propose a new model which combines the benefits of these two models: clarity of hard clustering and probabilistic assignments of soft clustering. Since the majority of data usually have a clear association, only a few points may require a probabilistic interpretation. Thus, we apply the $\mathcal{l}_1$ norm constraint to impose sparsity on probabilistic assignments. Moreover, we also incorporate outlier detection in our clustering model to simultaneously detect outliers which can cause serious problems in statistical analyses. To optimize the model, we introduce an alternating minimization method and prove its convergence. Numerical experiments and comparisons with existing models show the soundness and effectiveness of the proposed model.},
  journal   = {Applied Mathematics and Computation},
  month     = {10},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Melo2020,
  author   = {Melo, Jefferson G. and Monteiro, Renato D. C. and Wang, Hairong},
  date     = {2020},
  title    = {Iteration-complexity of an inexact proximal acceleratedaugmented Lagrangian method for solving linearly constrainedsmooth nonconvex composite optimization problems},
  abstract = {This paper proposes and establishes the iteration-complexity of an inexact proximal accelerated augmented Lagrangian (IPAAL) method for solving linearly constrained smooth nonconvex composite optimization problems. Each IPAAL iteration consists of inexactly solving a proximal augmented Lagrangian subproblem by an accelerated composite gradient (ACG) method followed by a suitable Lagrange multiplier update. It is shown that IPAAL generates an approximate stationary solution in at most $O(\text{log}(1/\rho)/\rho^3)$ ACG iterations, where $\rho > 0$ is the given tolerance. It is also shown that the previous complexity bound can be sharpened to $O(\text{log}(1/\rho)/\rho^{2.5})$ under additional mildly stronger assumptions. The above bounds are derived assuming that the initial point is neither feasible nor the domain of the composite term of the objective function is bounded. Some preliminary numerical results are presented to illustrate the performance of the IPAAL method.},
}

@Article{Mei2020,
  author      = {Jincheng Mei and Chenjun Xiao and Csaba Szepesvari and Dale Schuurmans},
  date        = {2020-05-13},
  title       = {On the Global Convergence Rates of Softmax Policy Gradient Methods},
  eprint      = {2005.06392v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {We make three contributions toward better understanding policy gradient methods in the tabular setting. First, we show that with the true gradient, policy gradient with a softmax parametrization converges at a $O(1/t)$ rate, with constants depending on the problem and initialization. This result significantly expands the recent asymptotic convergence results. The analysis relies on two findings: that the softmax policy gradient satisfies a \L{}ojasiewicz inequality, and the minimum probability of an optimal action during optimization can be bounded in terms of its initial value. Second, we analyze entropy regularized policy gradient and show that it enjoys a significantly faster linear convergence rate $O(e^{-t})$ toward softmax optimal policy. This result resolves an open question in the recent literature. Finally, combining the above two results and additional new $\Omega(1/t)$ lower bound results, we explain how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate. The separation of rates is further explained using the notion of non-uniform \L{}ojasiewicz degree. These results provide a theoretical understanding of the impact of entropy and corroborate existing empirical studies.},
  file        = {:http\://arxiv.org/pdf/2005.06392v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@InProceedings{Wang2020a,
  author    = {Cheng-Long Wang and Feiping Nie and Rong Wang and Xuelong Li},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  title     = {Revisiting Fast Spectral Clustering with Anchor Graph},
  doi       = {10.1109/icassp40776.2020.9053271},
  publisher = {{IEEE}},
  series    = {ICASSP 2020},
  abstract  = {Many anchor-graph-based spectral clustering methods have been proposed to accelerate spectral clustering for large scale problems. In this paper, we revisit the popular large-scale spectral clustering method based on the anchor graph which is equivalent to the spectral decomposition on a similar matrix obtained using a second-order transition probability. However, due to the special structure of the bipartite graph, there is no stable distribution of the random walk process. The even-order transition probabilities may only a side view of the bipartite structure, resulting in breaking the independence of data points and leading to undesired artifacts for boundary samples. Therefore, we propose a Fast Spectral Clustering based on the Random Walk Laplacian (FRWL) method. The random walk Laplacian balances explicitly the popularity of anchors and the independence of data points, which keeps the structure of boundary samples. The experimental results demonstrate the efficiency and effectiveness of our method.},
  month     = {5},
  year      = {2020},
}

@InCollection{Aggarwal2020,
  author    = {Charu C. Aggarwal},
  booktitle = {Linear Algebra and Optimization for Machine Learning},
  title     = {The Linear Algebra of Similarity},
  doi       = {10.1007/978-3-030-40344-7_9},
  pages     = {379--410},
  publisher = {Springer International Publishing},
  abstract  = {A dot-product similarity matrix is an alternative way to represent a multidimensional data set. In other words, one can convert an $n \times d$ data matrix $D$ into an $n \times n$ similarity matrix $S = DD^T$ (which contains $n^2$ pairwise dot products between points). One can use $S$ instead of $D$ for machine learning algorithms. The reason is that the similarity matrix contains almost the same information about the data as the original matrix. This equivalence is the genesis of a large class of methods in machine learning, referred to as kernel methods. This chapter builds the linear algebra framework required for understanding this important class of methods in machine learning. The real utility of such methods arises when the similarity matrix is chosen differently from the use of dot products (and the data matrix is sometimes not even available).},
  year      = {2020},
}

@Article{Higham2020a,
  author     = {Higham, Desmond J. and Higham, Nicholas J. and Pranesh, Srikara},
  date       = {2020},
  title      = {Random Matrices Generating Large Growth in {LU} Factorization with Pivoting},
  eprint     = {MIMS EPrint:2020.13},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2764/1/paper.pdf},
  abstract   = {We identify a class of random, dense $n \times n$ matrices for which LU factorization with any form of pivoting produces a growth factor of at least $n/(4 \text{log} n)$ for large n with high probability. The condition number of the matrices can be arbitrarily chosen and large growth also happens for the transpose. No previous matrices with all these properties were known. The matrices can be generated by the MATLAB function \texttt{gallery('randsvd',..)}, and they are formed as the product of two random orthogonal matrices from the Haar distribution with a diagonal matrix having only one diagonal entry different from 1, which lies between 0 and 1 (the "one small singular value" case). Our explanation for the large growth uses the fact that the maximum absolute value of any element of a Haar distributed orthogonal matrix tends to be relatively small for large $n$. We verify the behavior numerically, finding that for partial pivoting the actual growth is significantly larger than the lower bound, and much larger than the growth observed for random matrices with elements from the uniform $[0, 1]$ or standard normal distributions. We show more generally that a rank-1 perturbation to an orthogonal matrix producing large growth for any form of pivoting also generates large growth under reasonable assumptions. Finally, we demonstrate that GMRES-based iterative refinement can provide stable solutions to $Ax = b$ when large growth occurs in low precision LU factors, even when standard iterative refinement cannot.},
}

@Article{Haidar2020,
  author    = {Azzam Haidar and Harun Bayraktar and Stanimire Tomov and Jack Dongarra and Nicholas J. Higham},
  title     = {Mixed-Precision IterativeRefinement using TensorCores on {GPUs} to AccelerateSolution of Linear Systems},
  abstract  = {Double-precision floating-point arithmetic (FP64) has been the de facto standard for engineering and scientific simulations for several decades. Problem complexity and the sheer volume of data coming from various instruments and sensors motivate researchers to mix and match various approaches to optimize compute resources, including different levels of floating-point precision. In recent years, machine learning has motivated hardware support for halfprecision floating-point arithmetic. A primary challenge in high-performance computing is to leverage reducedprecision and mixed-precision hardware. We show how the FP16/FP32 Tensor Cores on NVIDIA GPUs can be exploited to accelerate the solution of linear systems of equations $Ax = b$ without sacrificing numerical stability. On the NVIDIA Quadro GV100 (Volta) GPU, we achieve a $4\times$ -- $5\times$ performance increase and $5\times$ better energy efficiency versus the standard FP64 implementation while maintaining an FP64 level of numerical stability.},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  publisher = {The Royal Society},
  year      = {2020},
}

@Article{Zhang2020e,
  author    = {Yongzhe Zhang and Ariful Azad and Ayd{\i}n Bulu{\c{c}}},
  title     = {Parallel algorithms for finding connected components using linear algebra},
  doi       = {10.1016/j.jpdc.2020.04.009},
  abstract  = {Finding connected components is one of the most widely used operations on a graph. Optimal serial algorithms for the problem have been known for half a century, and many competing parallel algorithms have been proposed over the last several decades under various different models of parallel computation. This paper presents a class of parallel connected-component algorithms designed using linear-algebraic primitives. These algorithms are based on a PRAM algorithm by Shiloach and Vishkin and can be designed using standard GraphBLAS operations. We demonstrate two algorithms of this class, one named LACC for Linear Algebraic Connected Components, and the other named FastSV which can be regarded as LACC’s simplification. With the support of the highly-scalable Combinatorial BLAS library, LACC and FastSV outperform the previous state-of-the-art algorithm by a factor of up to 12x for small to medium scale graphs. For large graphs with more than 50B edges, LACC and FastSV scale to 4K nodes (262K cores) of a Cray XC40 supercomputer and outperform previous algorithms by a significant margin. This remarkable performance is accomplished by (1) exploiting sparsity that was not present in the original PRAM algorithm formulation, (2) using high-performance primitives of Combinatorial BLAS, and (3) identifying hot spots and optimizing them away by exploiting algorithmic insights.},
  journal   = {Journal of Parallel and Distributed Computing},
  month     = {5},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Li2020c,
  author    = {Pei Heng Li and Taeho Lee and Hee Yong Youn},
  title     = {Dimensionality Reduction with Sparse Locality for Principal Component Analysis},
  doi       = {10.1155/2020/9723279},
  pages     = {1--12},
  volume    = {2020},
  abstract  = {Various dimensionality reduction (DR) schemes have been developed for projecting high-dimensional data into low-dimensional representation. The existing schemes usually preserve either only the global structure or local structure of the original data, but not both. To resolve this issue, a scheme called sparse locality for principal component analysis (SLPCA) is proposed. In order to effectively consider the trade-off between the complexity and efficiency, a robust $L_{2,p}$-norm-based principal component analysis (R2P-PCA) is introduced for global DR, while sparse representation-based locality preserving projection (SR-LPP) is used for local DR. Sparse representation is also employed to construct the weighted matrix of the samples. Being parameter-free, this allows the construction of an intrinsic graph more robust against the noise. In addition, simultaneous learning of projection matrix and sparse similarity matrix is possible. Experimental results demonstrate that the proposed scheme consistently outperforms the existing schemes in terms of clustering accuracy and data reconstruction error.},
  journal   = {Mathematical Problems in Engineering},
  month     = {5},
  publisher = {Hindawi Limited},
  year      = {2020},
}

@Article{Ahrens2020a,
  author      = {Peter Ahrens and Erik G. Boman},
  date        = {2020-05-25},
  title       = {On Optimal Partitioning For Sparse Matrices In Variable Block Row Format},
  eprint      = {2005.12414v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {The Variable Block Row (VBR) format is an influential blocked sparse matrix format designed to represent shared sparsity structure between adjacent rows and columns. VBR consists of groups of adjacent rows and columns, storing the resulting blocks that contain nonzeros in a dense format. This reduces the memory footprint and enables optimizations such as register blocking and instruction-level parallelism. Existing approaches use heuristics to determine which rows and columns should be grouped together. We adapt and optimize a dynamic programming algorithm for sequential hypergraph partitioning to produce a linear time algorithm which can determine the optimal partition of rows under an expressive cost model, assuming the column partition remains fixed. Furthermore, we show that the problem of determining an optimal partition for the rows and columns simultaneously is NP-Hard under a simple linear cost model. To evaluate our algorithm empirically against existing heuristics, we introduce the 1D-VBR format, a specialization of VBR format where columns are left ungrouped. We evaluate our algorithms on all 1626 real-valued matrices in the SuiteSparse Matrix Collection. When asked to minimize an empirically derived cost model for a sparse matrix-vector multiplication kernel, our algorithm produced partitions whose 1D-VBR realizations achieve a speedup of at least 1.18 over an unblocked kernel on 25\% of the matrices, and a speedup of at least 1.59 on 12.5\% of the matrices. The 1D-VBR representation produced by our algorithm had faster SpMVs than the 1D-VBR representations produced by any existing heuristics on 87.8\% of the test matrices.},
  file        = {:http\://arxiv.org/pdf/2005.12414v1:PDF},
  keywords    = {cs.DS},
}

@Article{Gao2020b,
  author      = {Weiguo Gao and Yingzhou Li and Bichen Lu},
  date        = {2020-05-25},
  title       = {Triangularized Orthogonalization-free Method for Solving Extreme Eigenvalue Problems},
  eprint      = {2005.12161v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {A novel orthogonalization-free method together with two specific algorithms are proposed to solve extreme eigenvalue problems. On top of gradient-based algorithms, the proposed algorithms modify the multi-column gradient such that earlier columns are decoupled from later ones. Global convergence to eigenvectors instead of eigenspace is guaranteed almost surely. Locally, algorithms converge linearly with convergence rate depending on eigengaps. Momentum acceleration, exact linesearch, and column locking are incorporated to further accelerate both algorithms and reduce their computational costs. We demonstrate the efficiency of both algorithms on several random matrices with different spectrum distribution and matrices from practice.},
  file        = {:http\://arxiv.org/pdf/2005.12161v1:PDF},
  keywords    = {math.NA, cs.NA, 65F15},
}

@InProceedings{Dinda2020,
  author       = {Peter Dinda and Alex Bernat and Conor Hetland},
  booktitle    = {Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing},
  date         = {2020},
  title        = {Spying on the Floating Point Behavior ofExisting, Unmodified Scientific Applications},
  series       = {HPDC' 20},
  url          = {http://pdinda.org/Papers/hpdc20.pdf},
  abstract     = {Scientific (and other) applications are critically dependent on calculations done using IEEE floating point arithmetic. A number of concerns have been raised about correctness in such applications given the numerous gotchas the IEEE standard presents for developers, as well as the complexity of its implementation at the hardware and compiler levels. The standard and its implementations do provide mechanisms for analyzing floating point arithmetic as it executes, making it possible to find and track problematic operations. However, this capability is seldom used in practice. In response, we have developed FPSpy, a tool that provides this capability when operating underneath existing, unmodified x64 application binaries on Linux, including those using thread- and process-level parallelism. FPSpy can observe application behavior without any cooperation from the application or developer, and can potentially be deployed as part of a job launch process. We present the design, implementation, and performance evaluation of FPSpy. FPSpy operates conservatively, getting out of the way if the application itself begins to use any of the OS or hardware features that FPSpy depends on. Its overhead can be throttled, allowing a tradeoff between which and how many unusual events are to be captured, and the slowdown incurred by the application, with the low point providing virtually zero slowdown. We evaluated FPSpy by using it to methodically study seven widely-used applications/frameworks from a range of domains (five of which are in the NSF XSEDE top-20), as well as the NAS and PARSEC benchmark suites. All told, these comprise about 7.5 million lines of source code in a wide range of languages, and parallelism models (including OpenMP and MPI). FPSpy was able to produce trace information for all of them. The traces show that problematic floating point events occur in both the applications and the benchmarks. Analysis of the rounding behavior captured in our traces also suggests the feasibility of an approach to adding adaptive precision underneath existing, unmodified binaries.},
}

@InProceedings{He2020,
  author    = {Xin He and Subhankar Pal and Aporva Amarnath and Siying Feng and Dong-Hyeon Park and Austin Rovinski and Haojie Ye and Yu},
  booktitle = {Proceedings of the International Conference on Supercomputing},
  date      = {2020},
  title     = {Sparse-{TPU}: Adapting Systolic Arrays for Sparse Matrices},
  series    = {ICS '20},
  url       = {https://web.eecs.umich.edu/~subh/publication/stpu-ics20/stpu-ics20.pdf},
  abstract  = {While systolic arrays are widely used for dense-matrix operations, they are seldom used for sparse-matrix operations. In this paper, we show how a systolic array of Multiply-and-Accumulate (MAC) units, similar to Google’s Tensor Processing Unit (TPU), can be adapted to efficiently handle sparse matrices. TPU-like accelerators are built upon a 2D array of MAC units and have demonstrated high throughput and efficiency for dense matrix multiplication, which is a key kernel in machine learning algorithms and is the target of the TPU. In this work, we employ a co-designed approach of first developing a packing technique to condense a sparse matrix and then propose a systolic array based system, Sparse-TPU, abbreviated to STPU, to accommodate the matrix computations for the packed denser matrix counterparts. To demonstrate the efficacy of our co-designed approach, we evaluate sparse matrix-vector multiplication on a broad set of synthetic and real-world sparse matrices. Experimental results show that STPU delivers 16.08$\times$ higher performance while consuming 4.39$\times$ and 19.79$\times$ lower energy for integer (int8) and floating point (float32) implementations, respectively, over a TPU baseline. Meanwhile, STPU has 12.93\% area overhead and an average of 4.14\% increase in dynamic energy over the TPU baseline for the float32 implementation.},
}

@Article{Cherubin2020,
  author    = {Stefano Cherubin and Daniele Cattaneo and Michele Chiari and Giovanni Agosta},
  title     = {Dynamic Precision Autotuning with {TAFFO}},
  doi       = {10.1145/3388785},
  number    = {2},
  pages     = {1--26},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3388785},
  volume    = {17},
  abstract  = {Many classes of applications, both in the embedded and high performance domains, can trade off the accuracy of the computed results for computation performance. One way to achieve such a trade-off is precision tuning—that is, to modify the data types used for the computation by reducing the bit width, or by changing the representation from floating point to fixed point. We present a methodology for high-accuracy dynamic precision tuning based on the identification of input classes (i.e., classes of input datasets that benefit from similar optimizations). When a new input region is detected, the application kernels are re-compiled on the fly with the appropriate selection of parameters. In this way, we obtain a continuous optimization approach that enables the exploitation of the reduced precision computation while progressively exploring the solution space, thus reducing the time required by compilation overheads. We provide tools to support the automation of the runtime part of the solution, leaving to the user only the task of identifying the input classes. Our approach provides a significant performance boost (up to 320\%) on the typical approximate computing benchmarks, without meaningfully affecting the accuracy of the result, since the error remains always below 3\%.},
  journal   = {{ACM} Transactions on Architecture and Code Optimization},
  month     = {5},
  publisher = {Association for Computing Machinery ({ACM})},
  year      = {2020},
}

@InProceedings{Pirkelbauer2020,
  author    = {Peter Pirkelbauer and Pei-Hung Lin and Tristan Vanderbruggen and Chunhua Liao},
  booktitle = {Proceedings of the 2020 IEEE International Parallel and Distributed Processing Symposium},
  date      = {2020},
  title     = {{XPlacer}: Automatic Analysis of Data AccessPatterns on Heterogeneous {CPU}/{GPU} Systems},
  series    = {IPDPS '20},
  url       = {https://www.osti.gov/servlets/purl/1630806},
  abstract  = {This paper presents XPlacer, a framework to automatically analyze problematic data access patterns in C++ and CUDA code. XPlacer records heap memory operations in both host and device code for later analysis. To this end, XPlacer instruments read and write operations, function calls, and kernel launches. Programmers mark points in the program execution where the recorded data is analyzed and anomalies diagnosed. XPlacer reports data access anti-patterns, including alternating CPU/GPU accesses to the same memory, memory with low access density, and unnecessary data transfers. The diagnostic also produces summative information about the recorded accesses, which aids users in identifying code that could degrade performance.\\ The paper evaluates XPlacer using LULESH, a Lawrence Livermore proxy application, Rodina benchmarks, and an implementation of the Smith-Waterman algorithm. XPlacer diagnosed several performance issues in these codes. The elimination of a performance problem in LULESH resulted in a 3$\times$ speedup on a heterogeneous platform combining Intel CPUs and Nvidia GPUs.},
}

@Article{Cortiella2020,
  author      = {Alexandre Cortiella and Kwang-Chun Park and Alireza Doostan},
  date        = {2020-05-27},
  title       = {Sparse Identification of Nonlinear Dynamical Systems via Reweighted $\ell_1$-regularized Least Squares},
  eprint      = {2005.13232v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {This work proposes an iterative sparse-regularized regression method to recover governing equations of nonlinear dynamical systems from noisy state measurements. The method is inspired by the Sparse Identification of Nonlinear Dynamics (SINDy) approach of {\it [Brunton et al., PNAS, 113 (15) (2016) 3932-3937]}, which relies on two main assumptions: the state variables are known {\it a priori} and the governing equations lend themselves to sparse, linear expansions in a (nonlinear) basis of the state variables. The aim of this work is to improve the accuracy and robustness of SINDy in the presence of state measurement noise. To this end, a reweighted $\ell_1$-regularized least squares solver is developed, wherein the regularization parameter is selected from the corner point of a Pareto curve. The idea behind using weighted $\ell_1$-norm for regularization -- instead of the standard $\ell_1$-norm -- is to better promote sparsity in the recovery of the governing equations and, in turn, mitigate the effect of noise in the state variables. We also present a method to recover single physical constraints from state measurements. Through several examples of well-known nonlinear dynamical systems, we demonstrate empirically the accuracy and robustness of the reweighted $\ell_1$-regularized least squares strategy with respect to state measurement noise, thus illustrating its viability for a wide range of potential applications.},
  file        = {:http\://arxiv.org/pdf/2005.13232v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@InCollection{Lin2020a,
  author    = {Zhouchen Lin and Huan Li and Cong Fang},
  booktitle = {Accelerated Optimization for Machine Learning},
  title     = {Accelerated Algorithms for Constrained Convex Optimization},
  doi       = {10.1007/978-981-15-2910-8_3},
  pages     = {57--108},
  publisher = {Springer Singapore},
  abstract  = {This chapter reviews the representative accelerated algorithms for deterministic constrained convex optimization. We overview the accelerated penalty method, accelerated Lagrange multiplier method, and the accelerated augmented Lagrange multiplier method. In particular, we concentrate on two widely used algorithms, namely the alternating direction method of multiplier (ADMM) and the primal-dual method. For ADMM, we study four scenarios, namely the generally convex and nonsmooth case, the strongly convex and nonsmooth case, the generally convex and smooth case, and the strongly convex and smooth case. We also introduce its non-ergodic accelerated variant. For the primal-dual method, we study three scenarios: both the two functions are generally convex, both are strongly convex, and one is generally convex, while the other is strongly convex. Finally, we introduce the Frank–Wolfe algorithm under the condition of strongly convex constraint set.},
  year      = {2020},
}

@TechReport{Hu2020,
  author      = {Jonathan Hu and Luc Berger-Vergiat and Stephen Thomas and Kasia Swirydowicz and Ichitaro Yamazaki and Paul Mullowney and Sheyas Ananthan and Sivasankaran Rajamanickam and Jay Sitaraman and Michael A. Sprague},
  date        = {2020},
  institution = {Office of Advanced Scientific Computing Research, Office of Science, US Department of Energy},
  title       = {Compare linear-system solver and preconditioner stacks with emphasis on GPU performance and propose phase-2 NGP solvervelopment pathway},
  eprint      = {ECP-Q2-FY20},
  url         = {https://www.osti.gov/servlets/purl/1630801},
  abstract    = {The goal of the ExaWind project is to enable predictive simulations of wind farms comprised of many megawatt-scale turbines situated in complex terrain. Predictive simulations will require computational fluid dynamics (CFD) simulations for which the mesh resolves the geometry of the turbines and captures the rotation and large deflections of blades. Whereas such simulations for a single turbine are arguably petascale class, multi-turbine wind farm simulations will require exascale-class resources. The primary physics codes in the ExaWind project are Nalu-Wind, which is an unstructured-grid solver for the acoustically incompressible Navier-Stokes equations, and OpenFAST, which is a whole-turbine simulation code. The Nalu-Wind model consists of the mass-continuity Poisson-type equation for pressure and a momentum equation for the velocity. For such modeling approaches, simulation times are dominated by linear-system setup and solution for the continuity and momentum systems. For the ExaWind challenge problem, the moving meshes greatly affect overall solver costs as reinitialization of matrices and recomputation of preconditioners is required at every time step.\\ In this report we evaluated GPU-performance baselines for the linear solvers in the Trilinos and hypre solver stacks using two representative Nalu-Wind simulations: an atmospheric boundary layer precursor simulation on a structured mesh, and a fixed-wing simulation using unstructured overset meshes. Both strong-scaling and weak-scaling experiments were conducted on the OLCF supercomputer Summit and similar proxy clusters. We focused on the performance of multi-threaded Gauss-Seidel and two-stage Gauss-Seidel that are extensions of classical Gauss-Seidel; of one-reduce GMRES, a communication-reducing variant of the Krylov GMRES; and algebraic multigrid methods that incorporate the afore-mentioned methods. The team has established that AMG methods are capable of solving linear systems arising from the fixed-wing overset meshes on CPU, a critical intermediate result for ExaWind FY20 Q3 and Q4 milestones. For the fixed-wing strong-scaling study (model with 3M grid-points), the team identified that Nalu-Wind simulations with the new Trilinos and hypre solvers scale to modest GPU counts, maintaining above 70\% efficiency up to 6 GPUs. However, there still remain significant bottlenecks to performance: matrix assembly (hypre), AMG setup (hypre and Trilinos) In the weak-scaling experiments (going from 0.4M to 211M gridpoints), it's shown that the solver apply phases are faster on GPUs, but that Nalu-Wind simulation times grow, primarily due to the multigrid-setup process.\\ Finally, based on the report outcomes, we propose a linear solver path-forward for the remainder of the ExaWind project. Near term, the NREL team will continue their work on GPU-based linear-system assembly. They will also investigate how the use of alternatives to the NVIDIA UVM (unified virtual memory) paradigm affects performance. Longer term, the NREL team will evaluate algorithmic performance on other types of accelerators and merge their improvements back to the main hypre repository branch. Near term, the Trilinos team will address performance bottlenecks identified in this milestone, such as implementing a GPU-based segregated momentum solve and reusing matrix graphs across linear-system assembly phases. Longer term, the Trilinos team will do detailed analysis and optimization of multigrid setup.},
}

@Article{Guler2020,
  author    = {Basak Guler and Salman Avestimehr and Antonio Ortega},
  title     = {{TACC}: Topology-Aware Coded Computing for Distributed Graph Processing},
  doi       = {10.1109/tsipn.2020.2998223},
  abstract  = {This paper proposes a coded distributed graph processing framework to alleviate the communication bottleneck in large-scale distributed graph processing. In particular, we propose a topology-aware coded computing (TACC) algorithm that has two novel salient features: (i) a topology-aware graph allocation strategy, and (ii) a coded aggregation scheme that combines the intermediate computations for graph processes while constructing coded messages. The proposed setup results in a trade-off between computation and communication, in that increasing the computation load at the distributed parties can in turn reduce the communication load. We demonstrate the effectiveness of the TACC algorithm by comparing the communication load with existing setups on both Erdos-Renyi and Barabasi-Albert type random graphs, as well as real-world Google web graph for PageRank computations. In particular, we show that the proposed coding strategy can lead to up to 82\% reduction in communication load and up to 46\% reduction overall execution time, when compared to the state-of-the-art and implemented on the Amazon EC2 cloud compute platform.},
  journal   = {{IEEE} Transactions on Signal and Information Processing over Networks},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Li2020d,
  author    = {Rui Li and Zeng-Qi Wang},
  title     = {Restrictively Preconditioned Conjugate Gradient Method for a Series of Constantly Augmented Least Squares Problems},
  doi       = {10.1137/19m1284853},
  number    = {2},
  pages     = {838--851},
  volume    = {41},
  abstract  = {In this study, we analyze the real-time solution of a series of augmented least squares problems, which are generated by adding information to an original least squares model repetitively. Instead of solving the least squares problems directly, we transform them into a batch of saddle point linear systems and subsequently solve the linear systems using restrictively preconditioned conjugate gradient (RPCG) methods. Approximation of the new Schur complement is generated effectively based on a previously approximated Schur complement. Owing to the variations of the preconditioned conjugate gradient method, the proposed methods generate convergence results similar to the conjugate gradient method and achieve a very fast convergent iterative sequence when the coefficient matrix is well preconditioned. Numerical tests show that the new methods are more effective than some standard Krylov subspace methods. Updated RPCG methods meet the requirement of real-time computing successfully for multifactor models.},
  journal   = {{SIAM} Journal on Matrix Analysis and Applications},
  month     = {1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year      = {2020},
}

@Article{Dixit2020,
  author      = {Rishabh Dixit and Waheed U. Bajwa},
  date        = {2020-06-01},
  title       = {Exit Time Analysis for Approximations of Gradient Descent Trajectories Around Saddle Points},
  eprint      = {2006.01106v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {This paper considers the problem of understanding the exit time for trajectories of gradient-related first-order methods from saddle neighborhoods under some initial boundary conditions. Given the `flat' geometry around saddle points, first-order methods can struggle in escaping these regions in a fast manner due to the small magnitudes of gradients encountered. In particular, while it is known that gradient-related first-order methods escape strict-saddle neighborhoods, existing literature does not explicitly leverage the local geometry around saddle points in order to control behavior of gradient trajectories. It is in this context that this paper puts forth a rigorous geometric analysis of the gradient-descent method around strict-saddle neighborhoods using matrix perturbation theory. In doing so, it provides a key result that can be used to generate an approximate gradient trajectory for any given initial conditions. In addition, the analysis leads to a linear exit-time solution for gradient-descent method under certain necessary initial conditions for a class of strict-saddle functions.},
  file        = {:http\://arxiv.org/pdf/2006.01106v1:PDF},
  keywords    = {math.OC, cs.LG, cs.SY, eess.SY, 90C26, 15Axx, 41A58, 65Hxx},
}

@Article{Zou2020,
  author    = {Qinmeng Zou and Frédéric Magoulès},
  title     = {Reducing the effect of global synchronization in delayed gradient methods for symmetric linear systems},
  doi       = {10.1016/j.advengsoft.2020.102837},
  pages     = {102837},
  volume    = {147},
  abstract  = {Compared with arithmetic operation, communication cost is often the bottleneck on modern computers, and thus should be paid increasing attention when choosing algorithms. Lagged gradient methods are known for their error tolerance and fast convergence. However, it appears that their parallel behavior is not well understood. In this paper, we explore the cyclic formulations of lagged gradient methods and s-dimensional methods for reducing global synchronizations. We provide parallel implementations for these methods and propose some new variants. A comparison is then reported for different gradient iterative schemes. To illustrate the performance, we run a number of experiments, from which we conclude that our formulations perform better than traditional methods in view of both iteration count and computing time.},
  journal   = {Advances in Engineering Software},
  month     = {9},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Engelmann2020,
  author      = {Alexander Engelmann and Yuning Jiang and Henrieke Benner and Ruchuan Ou and Boris Houska and Timm Faulwasser},
  date        = {2020-06-02},
  title       = {ALADIN-$\alpha$ -- An open-source MATLAB toolbox for distributed non-convex optimization},
  eprint      = {2006.01866v1},
  eprintclass = {eess.SY},
  eprinttype  = {arXiv},
  abstract    = {This paper introduces an open-source software for distributed and decentralized non-convex optimization named ALADIN-$\alpha$. ALADIN-$\alpha$ is a MATLAB implementation of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm, which is tailored towards rapid prototyping for non-convex distributed optimization. An improved version of the recently proposed bi-level variant of ALADIN is included enabling decentralized non-convex optimization. A collection of application examples from different applications fields including chemical engineering, robotics, and power systems underpins the application potential of ALADIN-$\alpha$.},
  file        = {:http\://arxiv.org/pdf/2006.01866v1:PDF},
  keywords    = {eess.SY, cs.DC, cs.MA, cs.SY, math.OC},
}

@Article{Cheng2020,
  author      = {Yu Cheng and Debmalya Panigrahi and Kevin Sun},
  date        = {2020-06-02},
  title       = {Sparsification of Balanced Directed Graphs},
  eprint      = {2006.01975v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Sparsification, where the cut values of an input graph are approximately preserved by a sparse graph (called a cut sparsifier) or a succinct data structure (called a cut sketch), has been an influential tool in graph algorithms. But, this tool is restricted to undirected graphs, because some directed graphs are known to not admit sparsification. Such examples, however, are structurally very dissimilar to undirected graphs in that they exhibit highly unbalanced cuts. This motivates us to ask: can we sparsify a balanced digraph? To make this question concrete, we define balance $\beta$ of a digraph as the maximum ratio of the cut value in the two directions (Ene et al., STOC 2016). We show the following results: For-All Sparsification: If all cut values need to be simultaneously preserved (cf. Bencz\'ur and Karger, STOC 1996), then we show that the size of the sparsifier (or even cut sketch) must scale linearly with $\beta$. The upper bound is a simple extension of sparsification of undirected graphs (formally stated recently in Ikeda and Tanigawa (WAOA 2018)), so our main contribution here is to show a matching lower bound. For-Each Sparsification: If each cut value needs to be individually preserved (Andoni et al., ITCS 2016), then the situation is more interesting. Here, we give a cut sketch whose size scales with $\sqrt{\beta}$, thereby beating the linear lower bound above. We also show that this result is tight by exhibiting a matching lower bound of $\sqrt{\beta}$ on "for-each" cut sketches. Our upper bounds work for general weighted graphs, while the lower bounds even hold for unweighted graphs with no parallel edges.},
  file        = {:http\://arxiv.org/pdf/2006.01975v1:PDF},
  keywords    = {cs.DS},
}

@Article{Crane2020,
  author      = {Rixon Crane and Fred Roosta},
  date        = {2020-06-05},
  title       = {{DINO}: Distributed Newton-Type Optimization Method},
  eprint      = {2006.03694v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We present a novel communication-efficient Newton-type algorithm for finite-sum optimization over a distributed computing environment. Our method, named DINO, overcomes both theoretical and practical shortcomings of similar existing methods. Under minimal assumptions, we guarantee global sub-linear convergence of DINO to a first-order stationary point for general non-convex functions and arbitrary data distribution over the network. Furthermore, for functions satisfying Polyak-Lojasiewicz (PL) inequality, we show that DINO enjoys a linear convergence rate. Our proposed algorithm is practically parameter free, in that it will converge regardless of the selected hyper-parameters, which are easy to tune. Additionally, its sub-problems are simple linear least-squares, for which efficient solvers exist. Numerical simulations demonstrate the efficiency of DINO as compared with similar alternatives.},
  file        = {:http\://arxiv.org/pdf/2006.03694v1:PDF},
  keywords    = {math.OC},
}

@Article{Yi2020,
  author      = {Xinlei Yi and Shengjun Zhang and Tao Yang and Tianyou Chai and Karl H. Johansson},
  date        = {2020-06-04},
  title       = {A Primal-Dual SGD Algorithm for Distributed Nonconvex Optimization},
  eprint      = {2006.03474v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {The distributed nonconvex optimization problem of minimizing a global cost function formed by a sum of $n$ local cost functions by using local information exchange is considered. This problem is an important component of many machine learning techniques with data parallelism, such as deep learning and federated learning. We propose a distributed primal-dual stochastic gradient descent (SGD) algorithm, suitable for arbitrarily connected communication networks and any smooth (possibly nonconvex) cost functions. We show that the proposed algorithm achieves the linear speedup convergence rate $\mathcal{O}(1/\sqrt{nT})$ for general nonconvex cost functions and the well known $\mathcal{O}(1/T)$ convergence rate when the global cost function satisfies the Polyak-{\L}ojasiewicz condition, where $T$ is the total number of iterations. We also show that the output of the proposed algorithm with fixed parameters linearly converges to a neighborhood of a global optimum. We demonstrate through numerical experiments the efficiency of our algorithm in comparison with the baseline centralized SGD and recently proposed distributed SGD algorithms.},
  file        = {:http\://arxiv.org/pdf/2006.03474v1:PDF},
  keywords    = {math.OC},
}

@InProceedings{Tandon2020,
  author    = {Suyash Tandon and Ivana Marincic and Henry Hoffmann and Eric Johnsen},
  booktitle = {Proceedings of the 2020 {AIAA} Aviation Forum},
  title     = {Enabling power-performance balance with transprecision calculations for extreme-scale computations of turbulent flows},
  doi       = {10.2514/6.2020-2922},
  publisher = {American Institute of Aeronautics and Astronautics},
  abstract  = {In modern scientific computing, the execution of floating-point operations emerges as a major contributor to the energy consumption of a compute-intensive application with a large dynamic range. Experimental evidence shows that over 50\% of the energy consumed by a core and its data memory is related to floating-point computations. The adoption of floating-point formats requiring lesser number of bits is an interesting opportunity to reduce the energy consumption as it allows simplification of the arithmetic circuitry and reduces the memory bandwidth required to transfer the data between memory and registers. In theory, the adoption of multiple floating-point types following the principle of transprecision computing allows fine-grained control of floating-point arithmetic while meeting the desired standards on the accuracy of the final result. In this paper, the power-performance trade-offs for computing at different precision levels are analyzed for a parallel and distributed framework based on recovery-assisted discontinuous Galerkin (RADG) methods. The recovery operator of the RADG, operates on compact support from neighboring elements and allows high-order approximation of the solution, with potential for massive parallelism. Using PoLiMEr – a power monitoring and management tool for HPC applications – fine-grained insights into the power characteristics of the RADG code on the supercomputer Theta at Argonne National Laboratory are presented. 3D benchmark tests indicate a savings of approximately 5 W per node with single precision computing. A mixed precision approach where all computations except recovery operation is performed in single precision shows promising results, however, an automated approach for tuning floating-point types and analyzing the floating-point sensitivity of variables and operations is desirable.},
  month     = {6},
  year      = {2020},
}

@InProceedings{Kim2020,
  author    = {Hongjune Kim and Jianping Zeng and Qingrui Liu and Mohammad Abdel-Majeed and Jaejin Lee and Changhee Jung},
  booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
  title     = {Compiler-directed soft error resilience for lightweight {GPU} register file protection},
  doi       = {10.1145/3385412.3386033},
  publisher = {{ACM}},
  abstract  = {This paper presents Penny, a compiler-directed resilience scheme for protecting GPU register files (RF) against soft errors. Penny replaces the conventional error correction code (ECC) based RF protection by using less expensive error detection code (EDC) along with idempotence based recovery. Compared to the ECC protection, Penny can achieve either the same level of RF resilience yet with significantly lower hardware costs or stronger resilience using the same ECC due to its ability to detect multi-bit errors when it is used solely for detection. In particular, to address the lack of store buffers in GPUs, which causes both checkpoint storage overwriting and the high cost of checkpointing stores, Penny provides several compiler optimizations such as storage coloring and checkpoint pruning. Across 25 benchmarks, Penny causes only $\approx$3\% run-time overhead on average.},
  month     = {6},
  year      = {2020},
}

@Article{Frandsen2020,
  author    = {Abraham Frandsen and Rong Ge},
  title     = {Optimization landscape of Tucker decomposition},
  doi       = {10.1007/s10107-020-01531-z},
  abstract  = {Tucker decomposition is a popular technique for many data analysis and machine learning applications. Finding a Tucker decomposition is a nonconvex optimization problem. As the scale of the problems increases, local search algorithms such as stochastic gradient descent have become popular in practice. In this paper, we characterize the optimization landscape of the Tucker decomposition problem. In particular, we show that if the tensor has an exact Tucker decomposition, for a standard nonconvex objective of Tucker decomposition, all local minima are also globally optimal. We also give a local search algorithm that can find an approximate local (and global) optimal solution in polynomial time.},
  journal   = {Mathematical Programming},
  month     = {6},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@Article{Trotter2020,
  author    = {James D. Trotter and Johannes Langguth and Xing Cai},
  title     = {Cache simulation for irregular memory traffic on multi-core {CPUs}: Case study on performance models for sparse matrix{\textendash}vector multiplication},
  doi       = {10.1016/j.jpdc.2020.05.020},
  pages     = {189--205},
  volume    = {144},
  abstract  = {Parallel computations with irregular memory access patterns are often limited by the memory subsystems of multi-core CPUs, though it can be difficult to pinpoint and quantify performance bottlenecks precisely. We present a method for estimating volumes of data traffic caused by irregular, parallel computations on multi-core CPUs with memory hierarchies containing both private and shared caches. Further, we describe a performance model based on these estimates that applies to bandwidth-limited computations. As a case study, we consider two standard algorithms for sparse matrix–vector multiplication, a widely used, irregular kernel. Using three different multi-core CPU systems and a set of matrices that induce a range of irregular memory access patterns, we demonstrate that our cache simulation combined with the proposed performance model accurately quantifies performance bottlenecks that would not be detected using standard best- or worst-case estimates of the data traffic volume.},
  journal   = {Journal of Parallel and Distributed Computing},
  month     = {10},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@InProceedings{AlHarthi2020,
  author    = {Al-Harthi, Noha and A.Alomairy, Rabab M. and Akbudak, Kadir and Chen, Rui and Ltaief, Hatem and Bagci, Hakan and Keyes, David E.},
  booktitle = {Proceedings of ISC High Performance 2020},
  date      = {2020-06-04},
  title     = {Solving Acoustic Boundary Integral Equations Using High Performance Tile Low-Rank {LU} Factorization},
  series    = {ISC '20},
  url       = {http://hdl.handle.net/10754/663212},
  abstract  = {We design and develop a new high performance implementation of a fast direct LU-based solver using low-rank approximations on massively parallel systems. The LU factorization is the most time-consuming step in solving systems of linear equations in the context of analyzing acoustic scattering from large 3D objects. The matrix equation is obtained by discretizing the boundary integral of the exterior Helmholtz problem using a higher-order Nyström scheme. The main idea is to exploit the inherent data sparsity of the matrix operator by performing local tile-centric approximations while still capturing the most significant information. In particular, the proposed LU-based solver leverages the Tile Low-Rank (TLR) data compression format as implemented in the Hierarchical Computations on Manycore Architectures (HiCMA) library to decrease the complexity of “classical” dense direct solvers from cubic to quadratic order. We taskify the underlying boundary integral kernels to expose fine-grained computations. We then employ the dynamic runtime system StarPU to orchestrate the scheduling of computational tasks on shared and distributed-memory systems. The resulting asynchronous execution permits to compensate for the load imbalance due to the heterogeneous ranks, while mitigating the overhead of data motion. We assess the robustness of our TLR LU-based solver and study the qualitative impact when using different numerical accuracies. The new TLR LU factorization outperforms the state-of-the-art dense factorizations by up to an order of magnitude on various parallel systems, for analysis of scattering from large-scale 3D synthetic and real geometries.},
}

@Article{Zanon2020,
  author    = {Mattia Zanon and Giuliano Zambonin and Gian Antonio Susto and Se{\'{a}}n McLoone},
  title     = {Sparse Logistic Regression: Comparison of Regularization and Bayesian Implementations},
  doi       = {10.3390/a13060137},
  number    = {6},
  pages     = {137},
  volume    = {13},
  abstract  = {In knowledge-based systems, besides obtaining good output prediction accuracy, it is crucial to understand the subset of input variables that have most influence on the output, with the goal of gaining deeper insight into the underlying process. These requirements call for logistic model estimation techniques that provide a sparse solution, i.e., where coefficients associated with non-important variables are set to zero. In this work we compare the performance of two methods: the first one is based on the well known Least Absolute Shrinkage and Selection Operator (LASSO) which involves regularization with an $\mathcal{l}$1 norm; the second one is the Relevance Vector Machine (RVM) which is based on a Bayesian implementation of the linear logistic model. The two methods are extensively compared in this paper, on real and simulated datasets. Results show that, in general, the two approaches are comparable in terms of prediction performance. RVM outperforms the LASSO both in term of structure recovery (estimation of the correct non-zero model coefficients) and prediction accuracy when the dimensionality of the data tends to increase. However, LASSO shows comparable performance to RVM when the dimensionality of the data is much higher than number of samples that is $p \gg n$},
  journal   = {Algorithms},
  month     = {6},
  publisher = {{MDPI} {AG}},
  year      = {2020},
}

@Article{Fang2020a,
  author      = {Zhuangyan Fang and Shengyu Zhu and Jiji Zhang and Yue Liu and Zhitang Chen and Yangbo He},
  date        = {2020-06-10},
  title       = {Low Rank Directed Acyclic Graphs and Causal Structure Learning},
  eprint      = {2006.05691v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Despite several important advances in recent years, learning causal structures represented by directed acyclic graphs (DAGs) remains a challenging task in high dimensional settings when the graphs to be learned are not sparse. In particular, the recent formulation of structure learning as a continuous optimization problem proved to have considerable advantages over the traditional combinatorial formulation, but the performance of the resulting algorithms is still wanting when the target graph is relatively large and dense. In this paper we propose a novel approach to mitigate this problem, by exploiting a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model. We establish several useful results relating interpretable graphical conditions to the low rank assumption, and show how to adapt existing methods for causal structure learning to take advantage of this assumption. We also provide empirical evidence for the utility of our low rank algorithms, especially on graphs that are not sparse. Not only do they outperform state-of-the-art algorithms when the low rank condition is satisfied, the performance on randomly generated scale-free graphs is also very competitive even though the true ranks may not be as low as is assumed.},
  file        = {:http\://arxiv.org/pdf/2006.05691v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Jalving2020,
  author      = {Jordan Jalving and Sungho Shin and Victor M. Zavala},
  date        = {2020-06-09},
  title       = {A Graph-Based Modeling Abstraction for Optimization: Concepts and Implementation in Plasmo.jl},
  eprint      = {2006.05378v2},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We present a general graph-based modeling abstraction for optimization that we call an OptiGraph. Under this abstraction, any optimization problem is treated as a hierarchical hypergraph in which nodes represent optimization subproblems and edges represent connectivity between such subproblems. The abstraction enables the modular construction of highly complex models in an intuitive manner, facilitates the use of graph analysis tools (to perform partitioning, aggregation, and visualization tasks), and facilitates communication of structures to decomposition algorithms. We provide an open-source implementation of the abstraction in the Julia-based package Plasmo.jl. We provide tutorial examples and large application case studies to illustrate the capabilities.},
  file        = {:http\://arxiv.org/pdf/2006.05378v2:PDF},
  keywords    = {math.OC},
}

@PhdThesis{Ellis2020,
  author      = {Marquita May Ellis},
  date        = {2020},
  institution = {University of California, Berkeley},
  title       = {Parallelizing Irregular Applications for Distributed Memory Scalability: Case Studies from Genomics},
  eprint      = {EECS-2020-133},
  url         = {https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-133.pdf},
  abstract    = {Generalizable approaches, models, and frameworks for irregular application scalability is an old yet open area in parallel and distributed computing research. Irregular applications are particularly hard to parallelize and distribute because, by definition, the pattern of computation is dependent upon the input data. With the proliferation of data-driven and data-intensive applications from the realm of Big Data, and the increasing demand for and availability of large-scale computing resources through HPC-Cloud convergence, the importance of generalized approaches to achieving irregular application scalability is only growing.\\ Rather than offering another software language or framework, this dissertation argues we first need to understand application scalability, especially irregular application scalability, and more closely examine patterns of computation, data sharing, and dependencies. As it stands, predominant performance models and tools from parallel and distributed computing focus on applications that are divided into distinct communication and computation phases, and ignore issues related to memory utilization. While time-tested and valuable, these models are not always sucient for understanding full application scalability, particularly, the scalability of data-intensive irregular applications. We present application case studies from genomics, highlighting the interdependencies of communication, computation, and memory capacities and performance.\\ The genomics applications we will examine offer a particularly useful and practical vantage point for this analysis, as they are data-intensive irregular application targets for both HPC and cloud computing. Further, they present an extreme for both domains. For HPC, they are less akin to traditional, well-studied and well-supported scientific simulations and more akin to text and document analysis applications. For cloud computing, they are an extreme in that they require frequent random global access to memory and data, stressing interconnection network latency and bandwidth and co-scheduled processors for tightly orchestrated computation.\\ We show how common patterns of irregular all-to-all computation can be managed eciently, comparing bulk-synchronous approaches built on collective communication and asynchronous approaches based on one-sided communication. For the former, our work is based on the popular Message Passing Interface (MPI) and makes heavy use of globally collective communication operations that exchange data across processors in a single step or, to save memory use, in a set of irregular steps. For the latter, we build on the UPC++ programming framework, which provides lightweight RPC mechanisms, to transfer both data and computational work between processors. We present performance results across multiple platforms including several modern HPC systems and, at least in one case, a cloud computing platform. With these application case studies, we seek not only to contribute to discussions around parallel algorithm and data structure design, programming systems, and performance modeling within the parallel computing community, but also to contribute to broader work in genomics through software development and analysis. Thus, we develop and present the first distributed memory scalable software for analyzing data sets from the latest generation of sequencing technologies, known as long read data sets. Specifically, we present scalable solutions to the problem of many-to-many long read overlap and alignment, the computational bottleneck to long read assembly, error correction, and direct analysis. Through cross-architectural empirical analysis, we identify the key components to ecient scalability, and highlight the priorities for any future optimization with analytical models.},
}

@Article{Boukaram2020,
  author    = {Wajih Boukaram and Marco Lucchesi and George Turkiyyah and Olivier Le Ma{\^{\i}}tre and Omar Knio and David Keyes},
  title     = {Hierarchical matrix approximations for space-fractional diffusion equations},
  doi       = {10.1016/j.cma.2020.113191},
  pages     = {113191},
  volume    = {369},
  abstract  = {Space fractional diffusion models generally lead to dense discrete matrix operators, which lead to substantial computational challenges when the system size becomes large. For a state of size $N$, full representation of a fractional diffusion matrix would require $O(N^2)$ memory storage requirement, with a similar estimate for matrix–vector products. In this work, we present $\mathcal{H}^2$ matrix representation and algorithms that are amenable to efficient implementation on GPUs, and that can reduce the cost of storing these operators to $O(N)$ asymptotically. Matrix–vector multiplications can be performed in asymptotically linear time as well. Performance of the algorithms is assessed in light of 2D simulations of space fractional diffusion equation with constant diffusivity. Attention is focused on smooth particle approximation of the governing equations, which lead to discrete operators involving explicit radial kernels. The algorithms are first tested using the fundamental solution of the unforced space fractional diffusion equation in an unbounded domain, and then for the steady, forced, fractional diffusion equation in a bounded domain. Both matrix-inverse and pseudo-transient solution approaches are considered in the latter case. Our experiments show that the construction of the fractional diffusion matrix, the matrix–vector multiplication, and the generation of an approximate inverse pre-conditioner all perform very well on a single GPU on 2D problems with $N$ in the range $10^5 -- 10^6$. In addition, the tests also showed that, for the entire range of parameters and fractional orders considered, results obtained using the $\mathcal{H}^2$ approximations were in close agreement with results obtained using dense operators, and exhibited the same spatial order of convergence. Overall, the present experiences showed that the $\mathcal{H}^2$ matrix framework promises to provide practical means to handle large-scale space fractional diffusion models in several space dimensions, at a computational cost that is asymptotically similar to the cost of handling classical diffusion equations.},
  journal   = {Computer Methods in Applied Mechanics and Engineering},
  month     = {9},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Doikov2020a,
  author       = {Nikita Doikov and Yurii Nesterov},
  date         = {2020-06-15},
  journaltitle = {CORE Discussion Papers ; 2020/23 (2020) 22 pages http://hdl.handle.net/2078.1/230370},
  title        = {Convex optimization based on global lower second-order models},
  eprint       = {2006.08518v1},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  abstract     = {In this paper, we present new second-order algorithms for composite convex optimization, called Contracting-domain Newton methods. These algorithms are affine-invariant and based on global second-order lower approximation for the smooth component of the objective. Our approach has an interpretation both as a second-order generalization of the conditional gradient method, or as a variant of trust-region scheme. Under the assumption, that the problem domain is bounded, we prove $\mathcal{O}(1/k^{2})$ global rate of convergence in functional residual, where $k$ is the iteration counter, minimizing convex functions with Lipschitz continuous Hessian. This significantly improves the previously known bound $\mathcal{O}(1/k)$ for this type of algorithms. Additionally, we propose a stochastic extension of our method, and present computational results for solving empirical risk minimization problem.},
  file         = {:http\://arxiv.org/pdf/2006.08518v1:PDF},
  keywords     = {math.OC},
}

@Article{You2020,
  author      = {Yang You and Yuhui Wang and Huan Zhang and Zhao Zhang and James Demmel and Cho-Jui Hsieh},
  date        = {2020-06-15},
  title       = {The Limit of the Batch Size},
  eprint      = {2006.08517v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
  file        = {:http\://arxiv.org/pdf/2006.08517v1:PDF},
  keywords    = {cs.LG, cs.CV, cs.DC, stat.ML},
}

@Article{Wiebe2020,
  author      = {Johannes Wiebe and Inês Cecílio and Jonathan Dunlop and Ruth Misener},
  date        = {2020-06-15},
  title       = {A robust approach to warped Gaussian process-constrained optimization},
  eprint      = {2006.08222v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Optimization problems with uncertain black-box constraints, modeled by warped Gaussian processes, have recently been considered in the Bayesian optimization setting. This work introduces a new class of constraints in which the same black-box function occurs multiple times evaluated at different domain points. Such constraints are important in applications where, e.g., safety-critical measures are aggregated over multiple time periods. Our approach, which uses robust optimization, reformulates these uncertain constraints into deterministic constraints guaranteed to be satisfied with a specified probability, i.e., deterministic approximations to a chance constraint. This approach extends robust optimization methods from parametric uncertainty to uncertain functions modeled by warped Gaussian processes. We analyze convexity conditions and propose a custom global optimization strategy for non-convex cases. A case study derived from production planning and an industrially relevant example from oil well drilling show that the approach effectively mitigates uncertainty in the learned curves. For the drill scheduling example, we develop a custom strategy for globally optimizing integer decisions.},
  file        = {:http\://arxiv.org/pdf/2006.08222v1:PDF},
  keywords    = {math.OC},
}

@Article{Simonetto2020,
  author      = {Andrea Simonetto and Emiliano Dall'Anese and Santiago Paternain and Geert Leus and Georgios B. Giannakis},
  date        = {2020-06-15},
  title       = {Time-Varying Convex Optimization: Time-Structured Algorithms and Applications},
  eprint      = {2006.08500v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Optimization underpins many of the challenges that science and technology face on a daily basis. Recent years have witnessed a major shift from traditional optimization paradigms grounded on batch algorithms for medium-scale problems to challenging dynamic, time-varying, and even huge-size settings. This is driven by technological transformations that converted infrastructural and social platforms into complex and dynamic networked systems with even pervasive sensing and computing capabilities. The present paper reviews a broad class of state-of-the-art algorithms for time-varying optimization, with an eye to both algorithmic development and performance analysis. It offers a comprehensive overview of available tools and methods, and unveils open challenges in application domains of broad interest. The real-world examples presented include smart power systems, robotics, machine learning, and data analytics, highlighting domain-specific issues and solutions. The ultimate goal is to exempify wide engineering relevance of analytical tools and pertinent theoretical foundations.},
  file        = {:http\://arxiv.org/pdf/2006.08500v1:PDF},
  keywords    = {math.OC, cs.SY, eess.SY},
}

@InCollection{Tsai2020,
  author    = {Yuhsiang M. Tsai and Terry Cojean and Hartwig Anzt},
  booktitle = {Lecture Notes in Computer Science},
  title     = {Sparse Linear Algebra on {AMD} and~{NVIDIA} {GPUs} {\textendash} The Race Is On},
  doi       = {10.1007/978-3-030-50743-5_16},
  pages     = {309--327},
  publisher = {Springer International Publishing},
  abstract  = {Efficiently processing sparse matrices is a central and performance-critical part of many scientific simulation codes. Recognizing the adoption of manycore accelerators in HPC, we evaluate in this paper the performance of the currently best sparse matrix-vector product (SpMV) implementations on high-end GPUs from AMD and NVIDIA. Specifically, we optimize SpMV kernels for the CSR, COO, ELL, and HYB format taking the hardware characteristics of the latest GPU technologies into account. We compare for 2,800 test matrices the performance of our kernels against AMD’s hipSPARSE library and NVIDIA’s cuSPARSE library, and ultimately assess how the GPU technologies from AMD and NVIDIA compare in terms of SpMV performance.},
  year      = {2020},
}

@InCollection{Madsen2020,
  author    = {Jonathan R. Madsen and Muaaz G. Awan and Hugo Brunie and Jack Deslippe and Rahul Gayatri and Leonid Oliker and Yunsong Wang and Charlene Yang and Samuel Williams},
  booktitle = {Lecture Notes in Computer Science},
  title     = {Timemory: Modular Performance Analysis for {HPC}},
  doi       = {10.1007/978-3-030-50743-5_22},
  pages     = {434--452},
  publisher = {Springer International Publishing},
  abstract  = {HPC has undergone a significant transition toward heterogeneous architectures. This transition has introduced several issues in code migration to support multiple frameworks for targeting the various architectures. In order to cope with these challenges, projects such as Kokkos and LLVM create abstractions which map a generic front-end API to the backend that supports the targeted architecture. This paper presents a complementary framework for performance measurement and analysis. Several performance measurement and analysis tools in existence provide their capabilities through various methods but the common theme among these tools are prohibitive limitations in terms of user-level extensions. For this reason, software developers commonly have to learn multiple tools and valuable analysis methods, such as the roofline model, are frequently required to be generated manually. The timemory framework provides complete modularity for performance measurement and analysis and eliminates all restrictions on user-level extensions. The timemory framework also provides a highly-efficient and intuitive method for handling multiple tools/measurements (i.e., "components") concurrently. The intersection of these characteristics provide ample evidence that timemory can serve as the common interface for existing performance measurement and analysis tools. Timemory components are developed in C++ but includes multi-language support for C, Fortran, and Python codes. Numerous components are provided by the library itself – including, but not limited to, timers, memory usage, hardware counters, and FLOP and instruction roofline models. Additionally, analysis of the intrinsic overhead demonstrates superior performance in comparison with popular tools.},
  year      = {2020},
}

@InProceedings{Chatzidimitriou2020,
  author    = {A. Chatzidimitriou and D. Gizopoulos},
  booktitle = {Proceedings of the 2020 Design, Automation Test in Europe Conference Exhibition},
  title     = {{rACE}: Reverse-Order Processor Reliability Analysis},
  doi       = {10.23919/DATE48585.2020.9116355},
  pages     = {1115--1120},
  series    = {DATE 2020},
  abstract  = {Modern microprocessors suffer from increased error rates that come along with fabrication technology scaling. Processor designs continuously become more prone to hardware faults that lead to execution errors and system failures, which raise the requirement of protection mechanisms. However, error mitigation strategies have to be applied diligently, as they impose significant power, area, and performance overheads. Early and accurate reliability estimation of a microprocessor design is essential in order to determine the most vulnerable hardware structures and the most efficient protection schemes. One of the most commonly used techniques for reliability estimation is Architecturally Correct Execution (ACE) analysis.ACE analysis can be applied at different abstraction models, including microarchitecture and RTL and often requires a single or few simulations to report the Architectural Vulnerability Factor (AVF) of the processor structures. However, ACE analysis overestimates the vulnerability of structures because of its pessimistic, worst-case nature. Moreover, it only delivers coarse-grain vulnerability reports and no details about the expected result of hardware faults (silent data corruptions, crashes). In this paper, we present reverse ACE (rACE), a methodology that (a) improves the accuracy of ACE analysis and (b) delivers fine-grain error outcome reports. Using a reverse-order tracing flow, rACE analysis associates portions of the simulated execution of a program with the actual output and the control flow, delivering finer accuracy and results classification. Our findings show that rACE reports an average 1.45$\times$ overestimation, compared to Statistical Fault Injection, for different sizes of the register file of an out-of-order CPU core (executing both ARM and x86 binaries), when a baseline ACE analysis reports 2.3$\times$ overestimation and even refined versions of ACE analysis report an average of 1.8$\times$ overestimation.},
  issn      = {1558-1101},
  month     = {3},
  year      = {2020},
}

@InProceedings{Nie2020,
  author    = {Bin Nie and Adwait Jog and Evgenia Smirni},
  booktitle = {Proceedings of the 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing},
  date      = {2020},
  title     = {Characterizing Accuracy-Aware Resilience of {GPGPU} Applications},
  pages     = {111--120},
  series    = {CCGRID 2020},
  abstract  = {Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing. In addition to achieving exascale performance at a stringent power budget, it is imperative for GPUs to provide reliable computing guarantees to the end user. In current commodity systems, such guarantees are often achieved by incurring high protection cost in terms of performance, power, and hardware resources. However, we argue that these strict guarantees are often not required (and that the associated protected overheads can be significantly reduced) because several GPGPU applications are either fault-tolerant or can accept a quantifiable loss in output quality. To this end, this paper characterizes in a hierarchical manner the accuracy-aware resilience of GPGPU applications consisting of thousands of threads. This characterization study shows that accuracy-aware error resilience exhibits several interesting patterns across threads at different hierarchies (i.e., kernel/thread-block/warp). The insights from this characterization study can be used to reduce the overheads of expensive protection or recovery mechanisms that are typically used by GPUs to ensure application reliability.},
}

@PhdThesis{Chang2020,
  author      = {Tyler H. Chang},
  date        = {2020},
  institution = {Virginia Polytechnic Institute and State University},
  title       = {Mathematical Software for Multiobjective Optimization Problems},
  abstract    = {In this thesis, two distinct problems in data-driven computational science are considered. The main problem of interest is the multiobjective optimization problem, where the tradeoff surface (called the Pareto front) between multiple conflicting objectives must be approximated in order to identify designs that balance real-world tradeoffs. In order to solve multiobjective optimization problems that are derived from computationally expensive blackbox functions, such as engineering design optimization problems, several methodologies are combined, including surrogate modeling, trust region methods, and adaptive weighting. The result is a numerical software package that finds approximately Pareto optimal solutions that are evenly distributed across the Pareto front, using minimal cost function evaluations. The second problem of interest is the closely related problem of multivariate interpolation, where an unknown response surface representing an underlying phenomenon is approximated by finding a function that exactly matches available data. To solve the interpolation problem, a novel algorithm is proposed for computing only a sparse subset of the elements in the Delaunay triangulation, as needed to compute the Delaunay interpolant. For high-dimensional data, this reduces the time and space complexity of Delaunay interpolation from exponential time to polynomial time in practice. For each of the above problems, both serial and parallel implementations are described. Additionally, both solutions are demonstrated on real-world problems in computer system performance modeling.},
}

@PhdThesis{Wang2020b,
  author      = {Leyuan Wang},
  date        = {2020},
  institution = {University of California Davis},
  title       = {Parallel Algorithms on Graph Matching},
  abstract    = {Subgraph matching is a basic task in querying graph dataset. It can also be called subgraph isomorphism search which consists to find all embeddings of a small query graph in a large data graph. It is one of the key techniques for understanding the underlying structure of graph datasets.\\ Graphs have been used to provide meaningful representations of objects and patterns, as well as more abstract descriptions. The representative power of graphs lies in their ability to characterize multiple pieces of information, as well as the relationships between them. Because of those properties, graph data structures have been leveraged in a wide spectrum of applications including social media, the World Wide Web, biological and genetic interactions, cyber network, co-author networks, citations, etc.. And at the heart of graph theory is the problem of graph matching, which attempts to find a way to map one graph onto another in such a way that both the topological structure and the node and edge labels are matched. For domains where data is noisy, an identical match may not be possible, so an inexact graph matching algorithm is used to search for the closest match, minimizing some similarity function.\\ There have been two completely different directions for supporting subgraph pattern matching. One direction is to develop specialized query processing engines, while the other direction is to develop efficient subgraph isomorphism algorithms for general, labeled graphs. Previously, both directions target distributed CPU systems. But the expensive network transfer overhead becomes a bottleneck. In order to explore the efficiency and parallel abilities of a single computer, we address latter direction of Subgraph Matching.\\ Most of previous works of subgraph matching fall into three classes of approaches: depth-first tree search, constraint propagation and graph indexing, all of which are not efficient on GPUs. Former intention to run subgraph matching on GPUs only targets a specific application and turns out to be memory-bounded. My research intends to tackle the bottleneck and further make subgraph matching meet the needs of a great spectrum of real-world applications.},
}

@InCollection{Ayala2020,
  author    = {Alan Ayala and Stanimire Tomov and Azzam Haidar and Jack Dongarra},
  booktitle = {Lecture Notes in Computer Science},
  title     = {{heFFTe}: Highly Efficient {FFT} for Exascale},
  doi       = {10.1007/978-3-030-50371-0_19},
  pages     = {262--275},
  publisher = {Springer International Publishing},
  abstract  = {Exascale computing aspires to meet the increasing demands from large scientific applications. Software targeting exascale is typically designed for heterogeneous architectures; henceforth, it is not only important to develop well-designed software, but also make it aware of the hardware architecture and efficiently exploit its power. Currently, several and diverse applications, such as those part of the Exascale Computing Project (ECP) in the United States, rely on efficient computation of the Fast Fourier Transform (FFT). In this context, we present the design and implementation of heFFTe (Highly Efficient FFT for Exascale) library, which targets the upcoming exascale supercomputers. We provide highly (linearly) scalable GPU kernels that achieve more than equation $40\times$ speedup with respect to local kernels from CPU state-of-the-art libraries, and over equation $2\times$ speedup for the whole FFT computation. A communication model for parallel FFTs is also provided to analyze the bottleneck for large-scale problems. We show experiments obtained on Summit supercomputer at Oak Ridge National Laboratory, using up to 24,576 IBM Power9 cores and 6,144 NVIDIA V-100 GPUs.},
  year      = {2020},
}

@InProceedings{Mohanamuraly2020,
  author    = {Pavanakumar Mohanamuraly and Gabriel Staffelbach},
  booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
  title     = {Hardware Locality-Aware Partitioning and Dynamic Load-Balancing of Unstructured Meshes for Large-Scale Scientific Applications},
  doi       = {10.1145/3394277.3401851},
  publisher = {{ACM}},
  abstract  = {We present an open-source topology-aware hierarchical unstructured mesh partitioning and load-balancing tool TreePart. The framework provides powerful abstractions to automatically detect and build hierarchical MPI topology resembling the hardware at runtime. Using this information it intelligently chooses between shared and distributed parallel algorithms for partitioning and loadbalancing. It provides a range of partitioning methods by interfacing with existing shared and distributed memory parallel partitioning libraries. It provides powerful and scalable abstractions like onesided distributed dictionaries and MPI3 shared memory based halo communicators for optimising HPC codes. The tool was successfully integrated into our in-house code and we present results from a large-eddy simulation of a combustion problem.},
  month     = {6},
  year      = {2020},
}

@Article{Gower2020,
  author      = {Robert M. Gower and Othmane Sebbouh and Nicolas Loizou},
  date        = {2020-06-18},
  title       = {SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation},
  eprint      = {2006.10311v2},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We provide several convergence theorems for SGD for two large classes of structured non-convex functions: (i) the Quasar (Strongly) Convex functions and (ii) the functions satisfying the Polyak-Lojasiewicz condition. Our analysis relies on the Expected Residual condition which we show is a strictly weaker assumption as compared to previously used growth conditions, expected smoothness or bounded variance assumptions. We provide theoretical guarantees for the convergence of SGD for different step size selections including constant, decreasing and the recently proposed stochastic Polyak step size. In addition, all of our analysis holds for the arbitrary sampling paradigm, and as such, we are able to give insights into the complexity of minibatching and determine an optimal minibatch size. In particular we recover the best known convergence rates of full gradient descent and single element sampling SGD as a special case. Finally, we show that for models that interpolate the training data, we can dispense of our Expected Residual condition and give state-of-the-art results in this setting.},
  file        = {:http\://arxiv.org/pdf/2006.10311v2:PDF},
  keywords    = {math.OC, cs.LG, stat.ML},
}

@TechReport{Herault2020,
  author      = {Thomas Herault and Yves Robert† and George Bosilca and Robert J. Harrison and Cannada A. Lewis and Edward F. Valeev},
  date        = {2020},
  institution = {Institut de recherche en informatique de Toulouse (IRIT)},
  title       = {Distributed-memory multi-{GPU} block-sparse tensor contraction for electronic structure},
  eprint      = {02872813},
  eprinttype  = {HAL},
  url         = {https://hal.inria.fr/hal-02872813/document},
  abstract    = {Many domains of scientific simulation (chemistry, condensed matter physics, data science) increasingly eschew dense tensors for block-sparse tensors, sometimes with additional structure (recursive hierarchy, rank sparsity, etc.). Distributed-memory parallel computation with block-sparse tensorial data is paramount to minimize the time-tosolution (e.g., to study dynamical problems or for real-time analysis) and to accommodate problems of realistic size that are too large to fit into the host/device memory of a single node equipped with accelerators. Unfortunately, computation with such irregular data structures is a poor match to the dominant imperative, bulk-synchronous parallel programming model. In this paper, we focus on the critical element of block-sparse tensor algebra, namely binary tensor contraction, and report on an efficient and scalable implementation using the task-focused PaRSEC runtime. High performance of the block-sparse tensor contraction on the Summit supercomputer is demonstrated for synthetic data as well as for real data involved in electronic structure simulations of unprecedented size.},
}

@InProceedings{Wang2020c,
  author    = {Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  booktitle = {Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics},
  date      = {2020},
  title     = {Assessing Local Generalization Capability in Deep Models},
  series    = {ASITATS 2020},
  volume    = {108},
  abstract  = {While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima, which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order “smoothness” terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of a model, as well as an algorithm that optimizes the perturbed model accordingly.},
}

@Article{Chieu2020,
  author    = {Nguyen Huy Chieu and Le Van Hien and Nguyen Thi Quynh Trang},
  title     = {Tilt Stability for Quadratic Programs with One or Two Quadratic Inequality Constraints},
  doi       = {10.1007/s40306-020-00372-4},
  number    = {2},
  pages     = {477--499},
  volume    = {45},
  abstract  = {This paper examines tilt stability for quadratic programs with one or two quadratic inequality constraints. Exploiting specific features of these problems and using some known results on tilt stability in nonlinear programming, we establish quite simple characterizations of tilt-stable local minimizers for quadratic programs with one quadratic inequality constraint under metric subregularity constraint qualification. By the same way, we also derive various tilt stability conditions for quadratic programs with two quadratic inequality constraints and satisfying certain suitable assumptions. Especially, the obtained results show that some tilt stability conditions only known to be sufficient in nonlinear programming become the necessary ones when the considered problems are quadratic programs with one or two quadratic inequality constraints.},
  journal   = {Acta Mathematica Vietnamica},
  month     = {6},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@InCollection{Goik2020,
  author    = {Damian Goik and Krzysztof Banaś},
  booktitle = {Lecture Notes in Computer Science},
  title     = {A Block Preconditioner for Scalable Large Scale Finite Element Incompressible Flow Simulations},
  doi       = {10.1007/978-3-030-50420-5_15},
  pages     = {199--211},
  publisher = {Springer International Publishing},
  abstract  = {We present a block preconditioner, based on the algebraic multigrid method, for solving systems of linear equations, that arise in incompressible flow simulations performed by the stabilized finite element method. We select a set of adjustable parameters for the preconditioner and show how to tune the parameters in order to obtain fast convergence of the standard GMRES solver in which the preconditioner is employed. Additionally, we show some details of the parallel implementation of the preconditioner and the achieved scalability of the solver in large scale parallel incompressible flow simulations.},
  year      = {2020},
}

@InCollection{Abdelfattah2020,
  author    = {Ahmad Abdelfattah and Stan Tomov and Jack Dongarra},
  booktitle = {Lecture Notes in Computer Science},
  title     = {Investigating the Benefit of {FP}16-Enabled Mixed-Precision Solvers for Symmetric Positive Definite Matrices Using {GPUs}},
  doi       = {10.1007/978-3-030-50417-5_18},
  pages     = {237--250},
  publisher = {Springer International Publishing},
  abstract  = {Half-precision computation refers to performing floating-point operations in a 16-bit format. While half-precision has been driven largely by machine learning applications, recent algorithmic advances in numerical linear algebra have discovered beneficial use cases for half precision in accelerating the solution of linear systems of equations at higher precisions. In this paper, we present a high-performance, mixed-precision linear solver ($Ax=b$) for symmetric positive definite systems in double-precision using graphics processing units (GPUs). The solver is based on a mixed-precision Cholesky factorization that utilizes the high-performance tensor core units in CUDA-enabled GPUs. Since the Cholesky factors are affected by the low precision, an iterative refinement (IR) solver is required to recover the solution back to double-precision accuracy. Two different types of IR solvers are discussed on a wide range of test matrices. A preprocessing step is also developed, which scales and shifts the matrix, if necessary, in order to preserve its positive-definiteness in lower precisions. Our experiments on the V100 GPU show that performance speedups are up to 4.7$\times$  against a direct double-precision solver. However, matrix properties such as the condition number and the eigenvalue distribution can affect the convergence rate, which would consequently affect the overall performance.},
  year      = {2020},
}

@InCollection{Moeller2020,
  author    = {Matthias Möller and Merel Schalkers},
  booktitle = {Lecture Notes in Computer Science},
  title     = {$|Lib>$ : A Cross-Platform Programming Framework for Quantum-Accelerated Scientific Computing},
  doi       = {10.1007/978-3-030-50433-5_35},
  pages     = {451--464},
  publisher = {Springer International Publishing},
  abstract  = {This paper introduces a new cross-platform programming framework for developing quantum-accelerated scientific computing applications and executing them on most of today’s cloud-based quantum computers and simulators. It makes use of C++ template meta-programming techniques to implement quantum algorithms as generic, platform-independent expressions, which get automatically synthesized into device-specific compute kernels upon execution. Our software framework supports concurrent and asynchronous execution of multiple quantum kernels via a CUDA-inspired stream concept.},
  year      = {2020},
}

@Article{Ploskas2020,
  author    = {Nikolaos Ploskas and Nikolaos V. Sahinidis and Nikolaos Samaras},
  title     = {A triangulation and fill-reducing initialization procedure for the simplex algorithm},
  doi       = {10.1007/s12532-020-00188-1},
  abstract  = {The computation of an initial basis is of great importance for simplex algorithms since it determines to a large extent the number of iterations and the computational effort needed to solve linear programs. We propose three algorithms that aim to construct an initial basis that is sparse and will reduce the fill-in and computational effort during LU factorization and updates that are utilized in modern simplex implementations. The algorithms rely on triangulation and fill-reducing ordering techniques that are invoked prior to LU factorization. We compare the performance of the CPLEX 12.6.1 primal and dual simplex algorithms using the proposed starting bases against CPLEX using its default crash procedure over a set of 95 large benchmarks (NETLIB, Kennington, Mészáros, Mittelmann). The best proposed algorithm utilizes METIS (Karypis and Kumar in SIAM J Sci Comput 20:359–392, 1998), produces remarkably sparse starting bases, and results in 5\% reduction of the geometric mean of the execution time of CPLEX’s primal simplex algorithm. Although the proposed algorithm improves CPLEX’s primal simplex algorithm across all problem types studied in this paper, it performs better on hard problems, i.e., the instances for which the CPLEX default requires over 1000 s. For these problems, the proposed algorithm results in 37\% reduction of the geometric mean of the execution time of CPLEX’s primal simplex algorithm. The proposed algorithm also reduces the execution time of CPLEX’s dual simplex on hard instances by 10\%. For the instances that are most difficult for CPLEX, and for which CPLEX experiences numerical difficulties as it approaches the optimal solution, the best proposed algorithm speeds up CPLEX by more than 10 times. Finally, the proposed algorithms lead to a natural way to parallelize CPLEX with speedups over CPLEX’s dual simplex of 1.2 and 1.3 on two and four cores, respectively.},
  journal   = {Mathematical Programming Computation},
  month     = {6},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@Article{Angriman2020,
  author      = {Eugenio Angriman and Maria Predari and Alexander van der Grinten and Henning Meyerhenke},
  date        = {2020-06-24},
  title       = {Approximation of the Diagonal of a Laplacian's Pseudoinverse for Complex Network Analysis},
  eprint      = {2006.13679v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {The ubiquity of massive graph data sets in numerous applications requires fast algorithms for extracting knowledge from these data. We are motivated here by three electrical measures for the analysis of large small-world graphs $G = (V, E)$ -- i.e., graphs with diameter in $O(\log |V|)$, which are abundant in complex network analysis. From a computational point of view, the three measures have in common that their crucial component is the diagonal of the graph Laplacian's pseudoinverse, $L^\dagger$. Computing diag$(L^\dagger)$ exactly by pseudoinversion, however, is as expensive as dense matrix multiplication -- and the standard tools in practice even require cubic time. Moreover, the pseudoinverse requires quadratic space -- hardly feasible for large graphs. Resorting to approximation by, e.g., using the Johnson-Lindenstrauss transform, requires the solution of $O(\log |V| / \epsilon^2)$ Laplacian linear systems to guarantee a relative error, which is still very expensive for large inputs. In this paper, we present a novel approximation algorithm that requires the solution of only one Laplacian linear system. The remaining parts are purely combinatorial -- mainly sampling uniform spanning trees, which we relate to diag$(L^\dagger)$ via effective resistances. For small-world networks, our algorithm obtains a $\pm \epsilon$-approximation with high probability, in a time that is nearly-linear in $|E|$ and quadratic in $1 / \epsilon$. Another positive aspect of our algorithm is its parallel nature due to independent sampling. We thus provide two parallel implementations of our algorithm: one using OpenMP, one MPI + OpenMP. In our experiments against the state of the art, our algorithm (i) yields more accurate results, (ii) is much faster and more memory-efficient, and (iii) obtains good parallel speedups, in particular in the distributed setting.},
  file        = {:http\://arxiv.org/pdf/2006.13679v1:PDF},
  keywords    = {cs.DS, cs.SI},
}

@Article{Tsai2020a,
  author      = {Yuhsiang M. Tsai and Terry Cojean and Tobias Ribizel and Hartwig Anzt},
  date        = {2020-06-25},
  title       = {Preparing Ginkgo for AMD GPUs -- A Testimonial on Porting CUDA Code to HIP},
  eprint      = {2006.14290v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {With AMD reinforcing their ambition in the scientific high performance computing ecosystem, we extend the hardware scope of the Ginkgo linear algebra package to feature a HIP backend for AMD GPUs. In this paper, we report and discuss the porting effort from CUDA, the extension of the HIP framework to add missing features such as cooperative groups, the performance price of compiling HIP code for AMD architectures, and the design of a library providing native backends for NVIDIA and AMD GPUs while minimizing code duplication by using a shared code base.},
  file        = {:http\://arxiv.org/pdf/2006.14290v1:PDF},
  keywords    = {cs.MS},
}

@Article{Mangoubi2020,
  author      = {Oren Mangoubi and Nisheeth K. Vishnoi},
  date        = {2020-06-22},
  title       = {A Second-order Equilibrium in Nonconvex-Nonconcave Min-max Optimization: Existence and Algorithm},
  eprint      = {2006.12363v2},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Min-max optimization, with a nonconvex-nonconcave objective function $f: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$, arises in many areas, including optimization, economics, and deep learning. The nonconvexity-nonconcavity of $f$ means that the problem of finding a global $\varepsilon$-min-max point cannot be solved in $\mathrm{poly}(d, \frac{1}{\varepsilon})$ evaluations of $f$. Thus, most algorithms seek to obtain a certain notion of local min-max point where, roughly speaking, each player optimizes her payoff in a local sense. However, the classes of local min-max solutions which prior algorithms seek are only guaranteed to exist under very strong assumptions on $f$, such as convexity or monotonicity. We propose a notion of a greedy equilibrium point for min-max optimization and prove the existence of such a point for any function such that it and its first three derivatives are bounded. Informally, we say that a point $(x^\star, y^\star)$ is an $\varepsilon$-greedy min-max equilibrium point of a function $f: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ if $y^\star$ is a second-order local maximum for $f(x^\star,\cdot)$ and, roughly, $x^\star$ is a local minimum for a greedy optimization version of the function $\max_y f(x,y)$ which can be efficiently estimated using greedy algorithms. The existence follows from an algorithm that converges from any starting point to such a point in a number of gradient and function evaluations that is polynomial in $\frac{1}{\varepsilon}$, the dimension $d$, and the bounds on $f$ and its first three derivatives. Our results do not require convexity, monotonicity, or special starting points.},
  file        = {:http\://arxiv.org/pdf/2006.12363v2:PDF},
  keywords    = {cs.DS, cs.GT, cs.LG, math.OC, stat.ML},
}

@Article{Yuan2020,
  author      = {Rui Yuan and Alessandro Lazaric and Robert M. Gower},
  date        = {2020-06-22},
  title       = {Sketched Newton-Raphson},
  eprint      = {2006.12120v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {We propose a new globally convergent stochastic second order method. Our starting point is the development of a new Sketched Newton-Raphson (SNR) method for solving large scale nonlinear equations of the form $F(x)=0$ with $F: \mathbb{R}^d \rightarrow \mathbb{R}^d$. We then show how to design several stochastic second order optimization methods by re-writing the optimization problem of interest as a system of nonlinear equations and applying SNR. For instance, by applying SNR to find a stationary point of a generalized linear model (GLM), we derive completely new and scalable stochastic second order methods. We show that the resulting method is very competitive as compared to state-of-the-art variance reduced methods. Using a variable splitting trick, we also show that the Stochastic Newton method (SNM) is a special case of SNR, and use this connection to establish the first global convergence theory of SNM. Indeed, by showing that SNR can be interpreted as a variant of the stochastic gradient descent (SGD) method we are able to leverage proof techniques of SGD and establish a global convergence theory and rates of convergence for SNR. As a special case, our theory also provides a new global convergence theory for the original Newton-Raphson method under strictly weaker assumptions as compared to what is commonly used for global convergence. There are many ways to re-write an optimization problem as nonlinear equations. Each re-write would lead to a distinct method when using SNR. As such, we believe that SNR and its global convergence theory will open the way to designing and analysing a host of new stochastic second order methods.},
  file        = {:http\://arxiv.org/pdf/2006.12120v1:PDF},
  keywords    = {math.NA, cs.NA, math.OC},
}

@Article{Calandra2020,
  author    = {H. Calandra and S. Gratton and E. Riccietti and X. Vasseur},
  title     = {On a multilevel Levenberg{\textendash}Marquardt method for the training of artificial neural networks and its application to the solution of partial differential equations},
  doi       = {10.1080/10556788.2020.1775828},
  pages     = {1--26},
  abstract  = {In this paper, we propose a new multilevel Levenberg–Marquardt optimizer for the training of artificial neural networks with quadratic loss function. This setting allows us to get further insight into the potential of multilevel optimization methods. Indeed, when the least squares problem arises from the training of artificial neural networks, the variables subject to optimization are not related by any geometrical constraints and the standard interpolation and restriction operators cannot be employed any longer. A heuristic, inspired by algebraic multigrid methods, is then proposed to construct the multilevel transfer operators. We test the new optimizer on an important application: the approximate solution of partial differential equations by means of artificial neural networks. The learning problem is formulated as a least squares problem, choosing the nonlinear residual of the equation as a loss function, whereas the multilevel method is employed as a training method. Numerical experiments show encouraging results related to the efficiency of the new multilevel optimization method compared to the corresponding one-level procedure in this context.},
  journal   = {Optimization Methods and Software},
  month     = {6},
  publisher = {Informa {UK} Limited},
  year      = {2020},
}

@Book{Andrei2020,
  author    = {Neculai Andrei},
  title     = {Nonlinear Conjugate Gradient Methods for Unconstrained Optimization},
  doi       = {10.1007/978-3-030-42950-8},
  publisher = {Springer International Publishing},
  abstract  = {Two approaches are known for solving large-scale unconstrained optimization problems—the limited-memory quasi-Newton method (truncated Newton method) and the conjugate gradient method. This is the first book to detail conjugate gradient methods, showing their properties and convergence characteristics as well as their performance in solving large-scale unconstrained optimization problems and applications. Comparisons to the limited-memory and truncated Newton methods are also discussed. Topics studied in detail include: linear conjugate gradient methods, standard conjugate gradient methods, acceleration of conjugate gradient methods, hybrid, modifications of the standard scheme, memoryless BFGS preconditioned, and three-term. Other conjugate gradient methods with clustering the eigenvalues or with the minimization of the condition number of the iteration matrix, are also treated. For each method, the convergence analysis, the computational performances and the comparisons versus other conjugate gradient methods are given. \\ The theory behind the conjugate gradient algorithms presented as a methodology is developed with a clear, rigorous, and friendly exposition; the reader will gain an understanding of their properties and their convergence and will learn to develop and prove the convergence of his/her own methods. Numerous numerical studies are supplied with comparisons and comments on the behavior of conjugate gradient algorithms for solving a collection of 800 unconstrained optimization problems of different structures and complexities with the number of variables in the range [1000,10000].  The book is addressed to all those interested in developing and using new advanced techniques for solving unconstrained optimization complex problems. Mathematical programming researchers, theoreticians and practitioners in operations research, practitioners in engineering and industry researchers, as well as graduate students in mathematics, Ph.D. and master students in mathematical programming, will find plenty of information and practical applications for solving large-scale unconstrained optimization problems and applications by conjugate gradient methods.},
  year      = {2020},
}

@Article{MendlerDuenner2020,
  author      = {Celestine Mendler-Dünner and Aurelien Lucchi},
  date        = {2020-06-24},
  title       = {Randomized Block-Diagonal Preconditioning for Parallel Learning},
  eprint      = {2006.13591v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {We study preconditioned gradient-based optimization methods where the preconditioning matrix has block-diagonal form. Such a structural constraint comes with the advantage that the update computation can be parallelized across multiple independent tasks. Our main contribution is to demonstrate that the convergence of these methods can significantly be improved by a randomization technique which corresponds to repartitioning coordinates across tasks during the optimization procedure. We provide a theoretical analysis that accurately characterizes the expected convergence gains of repartitioning and validate our findings empirically on various traditional machine learning tasks. From an implementation perspective, block separable models are well suited for parallelization and, when shared memory is available, randomization can be implemented on top of existing methods very efficiently to improve convergence.},
  file        = {:http\://arxiv.org/pdf/2006.13591v1:PDF},
  keywords    = {cs.LG, cs.DC, stat.ML},
}

@Article{Chen2020d,
  author      = {Long Chen and Xiaozhe Hu and Huiwen Wu},
  date        = {2020-06-11},
  title       = {Randomized Fast Subspace Descent Methods},
  eprint      = {2006.06589v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Randomized Fast Subspace Descent (RFASD) Methods are developed and analyzed for smooth and non-constraint convex optimization problems. The efficiency of the method relies on a space decomposition which is stable in $A$-norm, and meanwhile, the condition number $\kappa_A$ measured in $A$-norm is small. At each iteration, the subspace is chosen randomly either uniformly or by a probability proportional to the local Lipschitz constants. Then in each chosen subspace, a preconditioned gradient descent method is applied. RFASD converges sublinearly for convex functions and linearly for strongly convex functions. Comparing with the randomized block coordinate descent methods, the convergence of RFASD is faster provided $\kappa_A$ is small and the subspace decomposition is $A$-stable. This improvement is supported by considering a multilevel space decomposition for Nesterov's `worst' problem.},
  file        = {:http\://arxiv.org/pdf/2006.06589v1:PDF},
  keywords    = {math.OC, cs.NA, math.NA, 65K05, 90C25},
}

@Article{Anzt2020b,
  author      = {Hartwig Anzt and Terry Cojean and Goran Flegar and Fritz Göbel and Thomas Grützmacher and Pratik Nayak and Tobias Ribizel and Yuhsiang Mike Tsai and Enrique S. Quintana-Ortí},
  date        = {2020-06-30},
  title       = {Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing},
  eprint      = {2006.16852v2},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo's design principle abstracts all functionality as "linear operators", motivating the notation of a "linear operator algebra library". Ginkgo's current focus is oriented towards providing sparse linear algebra functionality for high performance GPU architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific back ends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo's usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo's high performance on state-of-the-art GPU architectures.},
  file        = {:http\://arxiv.org/pdf/2006.16852v2:PDF},
  keywords    = {cs.MS, D.2; G.1.3; G.4},
}

@Article{DAmbra2020,
  author      = {Pasqua D'Ambra and Fabio Durastante and Salvatore Filippone},
  date        = {2020-06-29},
  title       = {AMG preconditioners for Linear Solvers towards Extreme Scale},
  eprint      = {2006.16147v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {Linear solvers for large and sparse systems are a key element of scientific applications, and their efficient implementation is necessary to harness the computational power of current computers. Algebraic Multigrid (AMG) Preconditioners are a popular ingredient of such linear solvers; this is the motivation for the present work where we examine some recent developments in a package of AMG preconditioners to improve efficiency, scalability, and robustness on extreme-scale problems. The main novelty is the design and implementation of a new parallel coarsening algorithm based on aggregation of unknowns employing weighted graph matching techniques; this is a completely automated procedure, requiring no information from the user, and applicable to general symmetric positive definite (s.p.d.) matrices. The new coarsening algorithm improves in terms of numerical scalability at low operator complexity over decoupled aggregation algorithms available in previous releases of the package. The preconditioners package is built on the parallel software framework PSBLAS, which has also been updated to progress towards exascale. We present weak scalability results on two of the most powerful supercomputers in Europe, for linear systems with sizes up to $O(10^{10})$ unknowns.},
  file        = {:http\://arxiv.org/pdf/2006.16147v1:PDF},
  keywords    = {math.NA, cs.NA, 65F08, 65F10, 65N55, 65Y05},
}

@Article{Demirci2020,
  author    = {Gunduz Vehbi Demirci and Cevdet Aykanat},
  title     = {Cartesian Partitioning Models for 2D and 3D Parallel {SpGEMM} Algorithms},
  doi       = {10.1109/tpds.2020.3000708},
  number    = {12},
  pages     = {2763--2775},
  volume    = {31},
  abstract  = {The focus is distributed-memory parallelization of sparse-general-matrix-multiplication (SpGEMM). Parallel SpGEMM algorithms are classified under one-dimensional (1D), 2D, and 3D categories denoting the number of dimensions by which the 3D sparse workcube representing the iteration space of SpGEMM is partitioned. Recently proposed successful 2D- and 3D-parallel SpGEMM algorithms benefit from upper bounds on communication overheads enforced by 2D and 3D cartesian partitioning of the workcube on 2D and 3D virtual processor grids, respectively. However, these methods are based on random cartesian partitioning and do not utilize sparsity patterns of SpGEMM instances for reducing the communication overheads. We propose hypergraph models for 2D and 3D cartesian partitioning of the workcube for further reducing the communication overheads of these 2D- and 3D- parallel SpGEMM algorithms. The proposed models utilize two- and three-phase partitioning that exploit multi-constraint hypergraph partitioning formulations. Extensive experimentation performed on 20 SpGEMM instances by using upto 900 processors demonstrate that proposed partitioning models significantly improve the scalability of 2D and 3D algorithms. For example, in 2D-parallel SpGEMM algorithm on 900 processors, the proposed partitioning model respectively achieves 85 and 42 percent decrease in total volume and total number of messages, leading to 1.63 times higher speedup compared to random partitioning, on average.},
  journal   = {{IEEE} Transactions on Parallel and Distributed Systems},
  month     = {12},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Slota2020,
  author    = {George M. Slota and Cameron Root and Karen Devine and Kamesh Madduri and Sivasankaran Rajamanickam},
  title     = {Scalable, Multi-Constraint, Complex-Objective Graph Partitioning},
  doi       = {10.1109/tpds.2020.3002150},
  number    = {12},
  pages     = {2789--2801},
  volume    = {31},
  abstract  = {We introduce XtraPuLP , a distributed-memory graph partitioner designed to process irregular trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated in various prior works as a viable means to produce high quality partitions of skewed and small-world graphs with minimal computation time. Our XtraPuLP implementation can also be generalized to compute partitions with an arbitrary number of constraints, and it can compute partitions with balanced communication load across all parts. On a collection of large sparse graphs, we show that XtraPuLP partitioning is considerably faster than state-of-the-art partitioning methods, while also demonstrating that XtraPuLP can produce partitions of real-world graphs with billion+ vertices and over a hundred billion edges in minutes. Additionally, we demonstrate XtraPuLP on a variety of applications, including large-scale graph analytics and sparse matrix-vector multiplication.},
  journal   = {{IEEE} Transactions on Parallel and Distributed Systems},
  month     = {12},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{GarciaGasulla2020,
  author   = {Marta Garcia-Gasulla and Fabio Banchelli and Kilian Peiro and Guillem Ramirez-Gargallo and Guillaume Houzeaux and Ismal Ben Hassan Saïdi and Christian Tenaud and Ivan Spisso and Filippo Mantovani},
  date     = {2020},
  title    = {A generic performance analysis technique appliedto different {CFD} methods for {HPC}},
  url      = {https://perso.limsi.fr/tenaud/Files/cfd-in-hpc_2020_accepted.pdf},
  abstract = {For complex engineering and scientific applications, Computational Fluid Dynamics simulations (CFD) require a huge amount of computational power. As such, it is of paramount importance to carefully assess the performance of CFD codes and to study them in depth for enabling optimization and portability. In this paper we study three complex CFD codes, OpenFOAM, Alya and CHORUS representing two numerical methods, namely the finite volume and finite element methods, on both structured and unstructured meshes. To all codes we apply a generic performance analysis method based on a set of metrics helping the code developer in spotting the critical points that can potentially limit the scalability of a parallel application. We show the root cause of the performance bottlenecks studying the three applications on the MareNostrum4 supercomputer. We conclude providing hints for improving the performance and the scalability of each application.},
}

@Article{Vanover2020,
  author       = {Jackson Vanover and Xuan Deng and Cindy Rubio-González},
  date         = {2020},
  journaltitle = {Proceedings of the International Symposium on Software Testing and Analysis},
  title        = {Discovering Discrepancies in Numerical Libraries},
  series       = {ISSTA'20},
  url          = {https://web.cs.ucdavis.edu/~rubio/includes/issta20.pdf},
  abstract     = {Numerical libraries constitute the building blocks for software applications that perform numerical calculations. Thus, it is paramount that such libraries provide accurate and consistent results. To that end, this paper addresses the problem of finding discrepancies between synonymous functions in different numerical libraries as a means of identifying incorrect behavior. Our approach automatically finds such synonymous functions, synthesizes testing drivers, and executes differential tests to discover meaningful discrepancies across numerical libraries. We implement our approach in a tool named FPDiff, and provide an evaluation on four popular numerical libraries: GNU Scientific Library (GSL), SciPy, mpmath, and jmat. FPDiff finds a total of 126 equivalence classes with a 95.8\% precision and 79.0\% recall, and discovers 655 instances in which an input produces a set of disagreeing outputs between function synonyms, 150 of which we found to represent 125 unique bugs. We have reported all bugs to library maintainers; so far, 30 bugs have been fixed, 9 have been found to be previously known, and 25 more have been acknowledged by developers.},
  publisher    = {{ACM}},
  year         = {2020},
}

@TechReport{Carson2020b,
  author      = {Erin Carson and Tomáš Gergelits},
  date        = {2020},
  institution = {LLNL-Charles University},
  title       = {Quarter I Report: Initial Exploration of the Use of Mixed Precision in Iterative Solvers},
  abstract    = {The first quarter of the project was primarily spent identifying potential projects at the intersection of finite precision analysis, mixed precision computation, and Krylov subspace methods. We summarize our findings in the remainder of the document. Other activities include attending biweekly xSDK meetings as well as contributing material to the technical report and journal versions of the multiprecision landscape paper.\\ The subsequent quarter will be spent selecting a subset of the described projects to focus on, performing initial numerical experiments to evaluate the potential for the use of mixed precision, and developing initial theoretical analysis.},
}

@Article{Leleux2020,
  author      = {Pierre Leleux and Sylvain Courtain and Guillaume Guex and Marco Saerens},
  date        = {2020-07-01},
  title       = {Sparse Randomized Shortest Paths Routing with Tsallis Divergence Regularization},
  eprint      = {2007.00419v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {This work elaborates on the important problem of (1) designing optimal randomized routing policies for reaching a target node t from a source note s on a weighted directed graph G and (2) defining distance measures between nodes interpolating between the least cost (based on optimal movements) and the commute-cost (based on a random walk on G), depending on a temperature parameter T. To this end, the randomized shortest path formalism (RSP, [2,99,124]) is rephrased in terms of Tsallis divergence regularization, instead of Kullback-Leibler divergence. The main consequence of this change is that the resulting routing policy (local transition probabilities) becomes sparser when T decreases, therefore inducing a sparse random walk on G converging to the least-cost directed acyclic graph when T tends to 0. Experimental comparisons on node clustering and semi-supervised classification tasks show that the derived dissimilarity measures based on expected routing costs provide state-of-the-art results. The sparse RSP is therefore a promising model of movements on a graph, balancing sparse exploitation and exploration in an optimal way.},
  file        = {:http\://arxiv.org/pdf/2007.00419v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Klockiewicz2020,
  author      = {Bazyli Klockiewicz and Léopold Cambier and Ryan Humble and Hamdi Tchelepi and Eric Darve},
  date        = {2020-07-01},
  title       = {Second Order Accurate Hierarchical Approximate Factorization of Sparse SPD Matrices},
  eprint      = {2007.00789v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {We describe a second-order accurate approach to sparsifying the off-diagonal blocks in approximate hierarchical matrix factorizations of sparse symmetric positive definite matrices. The norm of the error made by the new approach depends quadratically, not linearly, on the error in the low-rank approximation of the given block. The analysis of the resulting two-level preconditioner shows that the preconditioner is second-order accurate as well. We incorporate the new approach into the recent Sparsified Nested Dissection algorithm [SIAM J. Matrix Anal. Appl., 41 (2020), pp. 715-746], and test it on a wide range of problems. The new approach halves the number of Conjugate Gradient iterations needed for convergence, with almost the same factorization complexity, improving the total runtimes of the algorithm. Our approach can be incorporated into other rank-structured methods for solving sparse linear systems.},
  file        = {:http\://arxiv.org/pdf/2007.00789v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Raponi2020,
  author      = {Elena Raponi and Hao Wang and Mariusz Bujny and Simonetta Boria and Carola Doerr},
  date        = {2020-07-02},
  title       = {High Dimensional Bayesian Optimization Assisted by Principal Component Analysis},
  eprint      = {2007.00925v1},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  abstract    = {Bayesian Optimization (BO) is a surrogate-assisted global optimization technique that has been successfully applied in various fields, e.g., automated machine learning and design optimization. Built upon a so-called infill-criterion and Gaussian Process regression (GPR), the BO technique suffers from a substantial computational complexity and hampered convergence rate as the dimension of the search spaces increases. Scaling up BO for high-dimensional optimization problems remains a challenging task. In this paper, we propose to tackle the scalability of BO by hybridizing it with a Principal Component Analysis (PCA), resulting in a novel PCA-assisted BO (PCA-BO) algorithm. Specifically, the PCA procedure learns a linear transformation from all the evaluated points during the run and selects dimensions in the transformed space according to the variability of evaluated points. We then construct the GPR model, and the infill-criterion in the space spanned by the selected dimensions. We assess the performance of our PCA-BO in terms of the empirical convergence rate and CPU time on multi-modal problems from the COCO benchmark framework. The experimental results show that PCA-BO can effectively reduce the CPU time incurred on high-dimensional problems, and maintains the convergence rate on problems with an adequate global structure. PCA-BO therefore provides a satisfactory trade-off between the convergence rate and computational efficiency opening new ways to benefit from the strength of BO approaches in high dimensional numerical optimization.},
  file        = {:http\://arxiv.org/pdf/2007.00925v1:PDF},
  keywords    = {cs.NE},
}

@Article{Hu2020a,
  author      = {Xiaozhe Hu and Kaiyi Wu and Ludmil T. Zikatanov},
  date        = {2020-07-01},
  title       = {A Posteriori Error Estimates for Multilevel Methods for Graph Laplacians},
  eprint      = {2007.00189v1},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we study a posteriori error estimators which aid multilevel iterative solvers for linear systems of graph Laplacians. In earlier works such estimates were computed by solving a perturbed global optimization problem, which could be computationally expensive. We propose a novel strategy to compute these estimates by constructing a Helmholtz decomposition on the graph based on a spanning tree and the corresponding cycle space. To compute the error estimator, we solve efficiently a linear system on the spanning tree and then a least-squares problem on the cycle space. As we show, such estimator has a nearly-linear computational complexity for sparse graphs under certain assumptions. Numerical experiments are presented to demonstrate the efficacy of the proposed method.},
  file        = {:http\://arxiv.org/pdf/2007.00189v1:PDF},
  keywords    = {math.NA, cs.NA, G.1.8; G.1.3},
}

@Article{Liu2020a,
  author      = {Yang Liu and Pieter Ghysels and Lisa Claus and Xiaoye Sherry Li},
  date        = {2020-07-01},
  title       = {Sparse Approximate Multifrontal Factorization with Butterfly Compression for High Frequency Wave Equations},
  eprint      = {2007.00202v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {We present a fast and approximate multifrontal solver for large-scale sparse linear systems arising from finite-difference, finite-volume or finite-element discretization of high-frequency wave equations. The proposed solver leverages the butterfly algorithm and its hierarchical matrix extension for compressing and factorizing large frontal matrices via graph-distance guided entry evaluation or randomized matrix-vector multiplication-based schemes. Complexity analysis and numerical experiments demonstrate $\mathcal{O}(N\log^2 N)$ computation and $\mathcal{O}(N)$ memory complexity when applied to an $N\times N$ sparse system arising from 3D high-frequency Helmholtz and Maxwell problems.},
  file        = {:http\://arxiv.org/pdf/2007.00202v1:PDF},
  keywords    = {cs.MS, cs.CE, 15A23, 65F50, 65R10, 65R20},
}

@Article{Gaihre2020,
  author      = {Anil Gaihre and Xiaoye S. Li and Hang Liu},
  date        = {2020-07-02},
  title       = {{GSoFa}: Scalable Sparse {LU} Symbolic Factorization on {GPUs}},
  eprint      = {2007.00840v2},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Decomposing a matrix A into a lower matrix L and an upper matrix U, which is also known as LU decomposition, is an important operation in numerical linear algebra. For a sparse matrix, LU decomposition often introduces more nonzero entries in the L and U factors than the original matrix. Symbolic factorization step is needed to identify the nonzero structures of L and U matrices. Attracted by the enormous potentials of Graphics Processing Units (GPUs), an array of efforts has surged to deploy various steps of LU factorization on GPUs except, to the best of our knowledge, symbolic factorization.This paper introduces GSoFa, a GPU based Symbolic factorization design with the following three optimizations to enable scalable LU symbolic factorization for nonsymmetric pattern sparse matrices on GPUs. First, we introduce a novel fine-grained parallel symbolic factorization algorithm that is well suited for the Single Instruction Multiple Thread (SIMT) architecture of GPUs. Second, we propose multi-source concurrent symbolic factorization to improve the utilization of GPUs with focus on balancing the workload. Third, we introduce a three-pronged optimization to reduce the excessive space requirement faced by multi-source concurrent symbolic factorization. Taken together, this work scales LU symbolic factorization towards 1,000 GPUs with superior performance over the state-of-the-art CPU algorithm.},
  file        = {:http\://arxiv.org/pdf/2007.00840v2:PDF},
  keywords    = {cs.DC},
}

@Article{Lubbe2020,
  author    = {Retief Lubbe and Wen-Jie Xu and Daniel N. Wilke and Patrick Pizette and Nicolin Govender},
  title     = {Analysis of parallel spatial partitioning algorithms for {GPU} based {DEM}},
  doi       = {10.1016/j.compgeo.2020.103708},
  pages     = {103708},
  volume    = {125},
  abstract  = {The capability of solving a geotechnical discrete element method (DEM) applications is determined by the complexity of the simulation and its computational requirements. Collision detection algorithms are fundamental to resolve the mechanical collisions between millions of particles efficiently. These algorithms are a bottleneck for many DEM applications resulting in excessive memory usage or poor computational performance. In particular, for GPU based DEM, there are many factors for a user to consider when deciding on an algorithm. This study discusses a set of diverse classes of geotechnical problems and the impact of algorithm choice. Four factors were considered: i) the world domain size, number of particles and particle density, ii) polydispersity in size, iii) the time evolution and iv) the particle shape. This study shows that for spherical particles, the choice of broad-phase collision detection algorithm has the most impact on computational performance. The computational cost for convex polyhedral particles is dominated by the selection of the particles’ bounding volumes and their intersection tests over the selection of the broad-phase collision detection algorithm. On average for convex polyhedral particles, the broad-phase occupies at most 1.3\% of the total runtime, while the narrow-phase collision detection and collision response require more than 87\% of the runtime. A combination of bounding spheres and axis-aligned bounding boxes for use as bounding volumes of particles showed the best performance reducing the computational cost by 20\%. This study serves as a guide for further research in the field of GPU based DEM collision detection and the application in geotechnics.},
  journal   = {Computers and Geotechnics},
  month     = {9},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@InProceedings{Panagiotas2020,
  author     = {Ioannis Panagiotas and Bora Uçar},
  booktitle  = {Proceedings of the 2020 European Symposium on Algorithms},
  date       = {2020},
  title      = {Engineering fast almost optimal algorithms for bipartite graph matching},
  eprint     = {02463717},
  eprinttype = {HAL},
  url        = {https://hal.inria.fr/hal-02463717v3/document},
  abstract   = {We consider the maximum cardinality matching problem in bipartite graphs. There are a number of exact, deterministic algorithms for this purpose, whose complexities are high in practice. There are randomized approaches for special classes of bipartite graphs. Random 2-out bipartite graphs, where each vertex chooses two neighbors at random from the other side, form one class for which there is an $O(m + n log n)$-time Monte Carlo algorithm. Regular bipartite graphs, where all vertices have the same degree, form another class for which there is an expected $O(m + n log n)$-time Las Vegas algorithm. We investigate these two algorithms and turn them into practical heuristics with randomization. Experimental results show that the heuristics are fast and obtain near optimal matchings. They are also more robust than the state of the art heuristics used in the cardinality matching algorithms, and are generally more useful as initialization routines.},
}

@Article{Uribe2020,
  author      = {César A. Uribe and Ali Jadbabaie},
  date        = {2020-07-07},
  title       = {A Distributed Cubic-Regularized Newton Method for Smooth Convex Optimization over Networks},
  eprint      = {2007.03562v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We propose a distributed, cubic-regularized Newton method for large-scale convex optimization over networks. The proposed method requires only local computations and communications and is suitable for federated learning applications over arbitrary network topologies. We show a $O(k^{{-}3})$ convergence rate when the cost function is convex with Lipschitz gradient and Hessian, with $k$ being the number of iterations. We further provide network-dependent bounds for the communication required in each step of the algorithm. We provide numerical experiments that validate our theoretical results.},
  file        = {:http\://arxiv.org/pdf/2007.03562v1:PDF},
  keywords    = {math.OC, cs.LG, cs.MA, stat.ML},
}

@Article{Burke2020,
  author    = {James V. Burke and Frank E. Curtis and Hao Wang and Jiashan Wang},
  title     = {Inexact Sequential Quadratic Optimization with Penalty Parameter Updates within the {QP} Solver},
  doi       = {10.1137/18m1176488},
  number    = {3},
  pages     = {1822--1849},
  volume    = {30},
  abstract  = {This paper focuses on the design of sequential quadratic optimization (commonly known as SQP) methods for solving large-scale nonlinear optimization problems. The most computationally demanding aspect of such an approach is the computation of the search direction during each iteration, for which we consider the use of matrix-free methods. In particular, we develop a method that requires an inexact solve of a single QP subproblem to establish the convergence of the overall SQP method. It is known that SQP methods can be plagued by poor behavior of the global convergence mechanism. To confront this issue, we propose the use of an exact penalty function with a dynamic penalty parameter updating strategy to be employed within the subproblem solver in such a way that the resulting search direction predicts progress toward both feasibility and optimality. We present our parameter updating strategy and prove that, under reasonable assumptions, the strategy does not modify the penalty parameter unnecessarily. We close the paper with a discussion of the results of numerical experiments that illustrate the benefits of our proposed techniques.},
  journal   = {{SIAM} Journal on Optimization},
  month     = {1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year      = {2020},
}

@Article{Bullins2020,
  author      = {Brian Bullins and Kevin A. Lai},
  date        = {2020-07-09},
  title       = {Higher-order methods for convex-concave min-max optimization and monotone variational inequalities},
  eprint      = {2007.04528v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We provide improved convergence rates for constrained convex-concave min-max problems and monotone variational inequalities with higher-order smoothness. In min-max settings where the $p^{th}$-order derivatives are Lipschitz continuous, we give an algorithm HigherOrderMirrorProx that achieves an iteration complexity of $O(1/T^{\frac{p+1}{2}})$ when given access to an oracle for finding a fixed point of a $p^{th}$-order equation. We give analogous rates for the weak monotone variational inequality problem. For $p>2$, our results improve upon the iteration complexity of the first-order Mirror Prox method of Nemirovski [2004] and the second-order method of Monteiro and Svaiter [2012]. We further instantiate our entire algorithm in the unconstrained $p=2$ case.},
  file        = {:http\://arxiv.org/pdf/2007.04528v1:PDF},
  keywords    = {math.OC, cs.LG, stat.ML},
}

@Article{Gross2020,
  author      = {James C. Gross and Geoffrey T. Parks},
  date        = {2020-07-09},
  title       = {Optimization by moving ridge functions: Derivative-free optimization for computationally intensive functions},
  eprint      = {2007.04893v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {A novel derivative-free algorithm, optimization by moving ridge functions (OMoRF), for unconstrained and bound-constrained optimization is presented. This algorithm couples trust region methodologies with output-based dimension reduction to accelerate convergence of model-based optimization strategies. The dimension-reducing subspace is updated as the trust region moves through the design space, allowing OMoRF to be applied to functions with no known global low-dimensional structure. Furthermore, its low computational requirement allows it to make rapid progress when optimizing high-dimensional functions. Its performance is examined on a set of test problems of moderate to high dimension and a high-dimensional design optimization problem. The results show that OMoRF compares favourably to other common derivative-free optimization methods, particularly when very few function evaluations are available.},
  file        = {:http\://arxiv.org/pdf/2007.04893v1:PDF},
  keywords    = {math.OC},
}

@Article{Abdelfattah2020a,
  author      = {Ahmad Abdelfattah and Hartwig Anzt and Erik G. Boman and Erin Carson and Terry Cojean and Jack Dongarra and Mark Gates and Thomas Grützmacher and Nicholas J. Higham and Sherry Li and Neil Lindquist and Yang Liu and Jennifer Loe and Piotr Luszczek and Pratik Nayak and Sri Pranesh and Siva Rajamanickam and Tobias Ribizel and Barry Smith and Kasia Swirydowicz and Stephen Thomas and Stanimire Tomov and Yaohung M. Tsai and Ichitaro Yamazaki and Urike Meier Yang},
  date        = {2020-07-13},
  title       = {A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic},
  eprint      = {2007.06674v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {Within the past years, hardware vendors have started designing low precision special function units in response to the demand of the Machine Learning community and their demand for high compute power in low precision formats. Also the server-line products are increasingly featuring low-precision special function units, such as the NVIDIA tensor cores in ORNL's Summit supercomputer providing more than an order of magnitude higher performance than what is available in IEEE double precision. At the same time, the gap between the compute power on the one hand and the memory bandwidth on the other hand keeps increasing, making data access and communication prohibitively expensive compared to arithmetic operations. To start the multiprecision focus effort, we survey the numerical linear algebra community and summarize all existing multiprecision knowledge, expertise, and software capabilities in this landscape analysis report. We also include current efforts and preliminary results that may not yet be considered "mature technology," but have the potential to grow into production quality within the multiprecision focus effort. As we expect the reader to be familiar with the basics of numerical linear algebra, we refrain from providing a detailed background on the algorithms themselves but focus on how mixed- and multiprecision technology can help improving the performance of these methods and present highlights of application significantly outperforming the traditional fixed precision methods.},
  file        = {:http\://arxiv.org/pdf/2007.06674v1:PDF},
  keywords    = {cs.MS, cs.NA, math.NA, G.1.3; G.4},
}

@Article{Abdelfattah2020b,
  author    = {Ahmad Abdelfattah and Stanimire Tomov and Jack Dongarra},
  title     = {Matrix multiplication on batches of small matrices in half and half-complex precisions},
  doi       = {10.1016/j.jpdc.2020.07.001},
  pages     = {188--201},
  volume    = {145},
  abstract  = {Machine learning and artificial intelligence (AI) applications often rely on performing many small matrix operations -- in particular general matrix-matrix multiplication (GEMM). These operations are usually performed in a reduced precision, such as the 16-bit floating-point format (i.e., half precision or FP16). The GEMM operation is also very important for dense linear algebra algorithms, and half-precision GEMM operations can be used in mixed-precision linear solvers. Therefore, high-performance batched GEMM operations in reduced precision are significantly important, not only for deep learning frameworks, but also for scientific applications that rely on batched linear algebra, such as tensor contractions and sparse direct solvers.\\ This paper presents optimized batched GEMM kernels for graphics processing units (GPUs) in FP16 arithmetic. The paper addresses both real and complex half-precision computations on the GPU. The proposed design takes advantage of the Tensor Core technology that was recently introduced in CUDA-enabled GPUs. With eight tuning parameters introduced in the design, the developed kernels have a high degree of flexibility that overcomes the limitations imposed by the hardware and software (in the form of discrete configurations for the Tensor Core APIs). For real FP16 arithmetic, performance speedups are observed against cuBLAS for sizes up to 128, and range between $1.5\times$ and $2.5\times$. For the complex FP16 GEMM kernel, the speedups are between $1.7\times$ and $7\times$ thanks to a design that uses the standard interleaved matrix layout, in contrast with the planar layout required by the vendor's solution. The paper also discusses special optimizations for extremely small matrices, where even higher performance gains are achievable.},
  journal   = {Journal of Parallel and Distributed Computing},
  month     = {11},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Farhan2020,
  author    = {Mohammed Al Farhan and Ahmad Abdelfattah and Stanimire Tomov and Mark Gates and Dalal Sukkari and Azzam Haidar and Robert Rosenberg and Jack Dongarra},
  title     = {{MAGMA} templates for scalable linear algebra on emerging architectures},
  doi       = {10.1177/1094342020938421},
  pages     = {109434202093842},
  abstract  = {With the acquisition and widespread use of more resources that rely on accelerator/wide vector-based computing, there has been a strong demand for science and engineering applications to take advantage of these latest assets. This, however, has been extremely challenging due to the diversity of systems to support their extreme concurrency, complex memory hierarchies, costly data movement, and heterogeneous node architectures. To address these challenges, we design a programming model and describe its ease of use in the development of a new MAGMA Templates library that delivers high-performance scalable linear algebra portable on current and emerging architectures. MAGMA Templates derives its performance and portability by (1) building on existing state-of-the-art linear algebra libraries, like MAGMA, SLATE, Trilinos, and vendor-optimized math libraries, and (2) providing access (seamlessly to the users) to the latest algorithms and architecture-specific optimizations through a single, easy-to-use C++-based API.},
  journal   = {The International Journal of High Performance Computing Applications},
  month     = {7},
  publisher = {{SAGE} Publications},
  year      = {2020},
}

@InProceedings{Laguna2020,
  author    = {Ignacio Laguna},
  booktitle = {2020 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
  title     = {Varity: Quantifying Floating-Point Variations in {HPC} Systems Through Randomized Testing},
  doi       = {10.1109/ipdps47924.2020.00070},
  publisher = {{IEEE}},
  abstract  = {Floating-point arithmetic can be confusing and it is sometimes misunderstood by programmers. While numerical reproducibility is desirable in HPC, it is often unachievable due to the different ways compilers treat floating-point arithmetic and generate code around it. This reproducibility problem is exacerbated in heterogeneous HPC systems where code can be executed on different floating-point hardware, e.g., a host and a device architecture, producing in some situations different numerical results. We present VARITY, a tool to quantify floatingpoint variations in heterogeneous HPC systems. Our approach generates random test programs for multiple architectures (host and device) using the compilers that are available in the system. Using differential testing, it compares floating-point results and identifies unexpected variations in the program results. The results can guide programmers in choosing the compilers that produce the most similar results in a system, which is useful when numerical reproducibility is critical. By running 50,000 experiments with Varity on a system with IBM POWER9 CPUs, NVIDIA V100 GPUs, and four compilers (gcc, clang, xl, and nvcc), we identify and document several programs that produce significantly different results for a given input when different compilers or architectures are used, even when a similar optimization level is used everywhere.},
  month     = {5},
  year      = {2020},
}

@TechReport{Demmel2020,
  author      = {James Demmel and Jack Dongarra and Julie Langou and Julien Langou and Piotr Luszczek and Michael W. Mahoney},
  date        = {2020},
  institution = {University of Tennessee},
  title       = {Prospectus for the Next {LAPACK} and {ScaLAPACK} Libraries:Basic ALgebra LIbraries for Sustainable Technology withInterdisciplinary Collaboration {(BALLISTIC)}},
  url         = {https://www.icl.utk.edu/files/publications/2020/icl-utk-1391-2020.pdf},
  abstract    = {The convergence of several unprecedented changes, including formidable new system design constraints and revolutionary levels of heterogeneity, has made it clear that much of the essential software infrastructure of computational science and engineering is, or will soon be, obsolete. Math libraries have historically been in the vanguard of software that must be adapted first to such changes, both because these low-level workhorses are so critical to the accuracy and performance of so many different types of applications, and because they have proved to be outstanding vehicles for finding and implementing solutions to the problems that novel architectures pose. Under the Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC) project, the principal designers of the Linear Algebra PACKage (LAPACK) and the Scalable Linear Algebra PACKage (ScaLAPACK), the combination of which is abbreviated Sca/LAPACK, aim to enhance and update these libraries for the ongoing revolution in processor architecture, system design, and application requirements by incorporating them into a layered package of software components -- the BALLISTIC ecosystem -- that provides users seamless access to state-of-the-art solver implementations through familiar and improved Sca/LAPACK interfaces.\\ The set of innovations and improvements that will be made available through BALLISTIC is the result of a combination of inputs from a variety of sources: the authors' own algorithmic and software research, which attacks the challenges of multi-core, hybrid, and extreme-scale system designs; extensive interactions with users, vendors, and the management of large high-performance computing (HPC) facilities to help anticipate the demands and opportunities of new architectures and programming languages; and, finally, the enthusiastic participation of the research community in developing and offering enhanced versions of existing dense linear algebra software components. Aiming to help applications run portably at all levels of the platform pyramid, including in cloud-based systems, BALLISTIC's technical agenda includes: (1) adding new functionality requested by stakeholder communities; (2) incorporating vastly improved numerical methods and algorithms; (3) leveraging successful research results to transition Sca/LAPACK (interfaces) to multi-core and accelerator-enabled versions; (4) providing user-controllable autotuning for the deployed software; (5) introducing new interfaces and data structures to increase ease of use; (6) enhancing engineering for evolution via standards and community engagement; and (7) continuing to expand application community outreach. Enhanced engineering will also help keep the reference implementation for Sca/LAPACK efficient, maintainable, and testable at reasonable cost in the future.\\ The Sca/LAPACK libraries are the community standard for dense linear algebra. They have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. No other numerical library can claim this breadth of integration with the community. Consequently, enhancing these libraries with state-of-the-art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale and including cloud-based environments) is set to have a correspondingly large impact on the research and education community, government laboratories, and private industry.},
}

@Article{You2020a,
  author    = {Yang You and Yuxiong He and Samyam Rajbhandari and Wenhan Wang and Cho-Jui Hsieh and Kurt Keutzer and James Demmel},
  date      = {2020},
  title     = {Fast {LSTM} by dynamic decomposition on cloud and distributed systems},
  doi       = {10.1007/s10115-020-01487-8},
  abstract  = {Long short-term memory (LSTM) is a powerful deep learning technique that has been widely used in many real-world data-mining applications such as language modeling and machine translation. In this paper, we aim to minimize the latency of LSTM inference on cloud systems without losing accuracy. If an LSTM model does not fit in cache, the latency due to data movement will likely be greater than that due to computation. In this case, we reduce model parameters. If, as in most applications we consider, the LSTM models are able to fit the cache of cloud server processors, we focus on reducing the number of floating point operations, which has a corresponding linear impact on the latency of the inference calculation. Thus, in our system, we dynamically reduce model parameters or flops depending on which most impacts latency. Our inference system is based on singular value decomposition and canonical polyadic decomposition. Our system is accurate and low latency. We evaluate our system based on models from a series of real-world applications like language modeling, computer vision, question answering, and sentiment analysis. Users of our system can use either pre-trained models or start from scratch. Our system achieves $15\times$ average speedup for six real-world applications without losing accuracy in inference. We also design and implement a distributed optimization system with dynamic decomposition, which can significantly reduce the energy cost and accelerate the training process.},
  journal   = {Knowledge and Information Systems},
  month     = {7},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@InProceedings{Guo2020,
  author    = {Hui Guo and Cindy Rubio-González},
  booktitle = {Proceedings of the 42nd IEEE/ACM International Conference on Software Engineering},
  date      = {2020},
  title     = {Efficient Generation of Error-Inducing Floating-Point Inputsvia Symbolic Execution},
  series    = {ICSE'20},
  url       = {https://hguo15.github.io/huiguo.github.io/files/fpgen-icse20.pdf},
  abstract  = {Floating point is widely used in software to emulate arithmetic over reals. Unfortunately, floating point leads to rounding errors that propagate and accumulate during execution. Generating inputs to maximize the numerical error is critical when evaluating the accuracy of floating-point code. In this paper, we formulate the problem of generating high error-inducing floating-point inputs as a code coverage maximization problem solved using symbolic execution. Specifically, we define inaccuracy checks to detect large precision loss and cancellation. We inject these checks at strategic program locations to construct specialized branches that, when covered by a given input, are likely to lead to large errors in the result. We apply symbolic execution to generate inputs that exercise these specialized branches, and describe optimizations that make our approach practical. We implement a tool named FPGen and present an evaluation on 21 numerical programs including matrix computation and statistics libraries. We show that FPGen exposes errors for 20 of these programs and triggers errors that are, on average, over 2 orders of magnitude larger than the state of the art.},
}

@Article{Christlieb2020,
  author      = {Andrew J. Christlieb and Pierson T. Guthrey and William A. Sands and Mathialakan Thavappiragasm},
  date        = {2020-07-06},
  title       = {Parallel Algorithms for Successive Convolution},
  eprint      = {2007.03041v2},
  eprintclass = {physics.comp-ph},
  eprinttype  = {arXiv},
  abstract    = {In this work, we consider alternative discretizations for PDEs which use expansions involving integral operators to approximate spatial derivatives. These constructions use explicit information within the integral terms, but treat boundary data implicitly, which contributes to the overall speed of the method. This approach is provably unconditionally stable for linear problems and stability has been demonstrated experimentally for nonlinear problems. Additionally, it is matrix-free in the sense that it is not necessary to invert linear systems and iteration is not required for nonlinear terms. Moreover, the scheme employs a fast summation algorithm that yields a method with a computational complexity of $\mathcal{O}(N)$, where $N$ is the number of mesh points along a direction. While much work has been done to explore the theory behind these methods, their practicality in large scale computing environments is a largely unexplored topic. In this work, we explore the performance of these methods by developing a domain decomposition algorithm suitable for distributed memory systems along with shared memory algorithms. As a first pass, we derive an artificial CFL condition that enforces a nearest-neighbor communication pattern and briefly discuss possible generalizations. We also analyze several approaches for implementing the parallel algorithms by optimizing predominant loop structures and maximizing data reuse. Using a hybrid design that employs MPI and Kokkos for the distributed and shared memory components of the algorithms, respectively, we show that our methods are efficient and can sustain an update rate $> 1\times10^8$ DOF/node/s. We provide results that demonstrate the scalability and versatility of our algorithms using several different PDE test problems, including a nonlinear example, which employs an adaptive time-stepping rule.},
  file        = {:http\://arxiv.org/pdf/2007.03041v2:PDF},
  keywords    = {physics.comp-ph, cs.CE, cs.DC, cs.NA, math.NA},
}

@InProceedings{He2020a,
  author    = {Xiao He and Xingwei Wang and Jia Shi and Yi Liu},
  booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
  title     = {Testing high performance numerical simulation programs: experience, lessons learned, and open issues},
  doi       = {10.1145/3395363.3397382},
  publisher = {{ACM}},
  abstract  = {High performance numerical simulation programs are widely used to simulate actual physical processes on high performance computers for the analysis of various physical and engineering problems. They are usually regarded as non-testable due to their high complexity. This paper reports our real experience and lessons learned from testing five simulation programs that will be used to design and analyze nuclear power plants. We applied five testing approaches and found 33 bugs. We found that property-based testing and metamorphic testing are two effective methods. Nevertheless, we suffered from the lack of domain knowledge, the high test costs, the shortage of test cases, severe oracle issues, and inadequate automation support. Consequently, the five programs are not exhaustively tested from the perspective of software testing, and many existing software testing techniques and tools are not fully applicable due to scalability and portability issues. We need more collaboration and communication with other communities to promote the research and application of software testing techniques.},
  month     = {7},
  year      = {2020},
}

@InProceedings{Han2020,
  author    = {Jingoo Han and M. Mustafa Rafique and Luna Xu and Ali R. Butt and Seung-Hwan Lim and Sudharshan S. Vazhkudai},
  booktitle = {Proceedings of the 20th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Internet Computing},
  title     = {{MARBLE}: A Multi-{GPU} Aware Job Scheduler for Deep Learning on {HPC} Systems},
  doi       = {10.1109/ccgrid49817.2020.00-66},
  publisher = {{IEEE}},
  series    = {CCGRID '20},
  abstract  = {Deep learning (DL) has become a key tool for solving complex scientific problems. However, managing the multi-dimensional large-scale data associated with DL, especially atop extant multiple graphics processing units (GPUs) in modern supercomputers poses significant challenges. Moreover, the latest high-performance computing (HPC) architectures bring different performance trends in training throughput compared to the existing studies. Existing DL optimizations such as larger batch size and GPU locality-aware scheduling have little effect on improving DL training throughput performance due to fast CPU-to-GPU connections. Additionally, DL training on multiple GPUs scales sublinearly. Thus, simply adding more GPUs to a system is ineffective. To this end, we design MARBLE, a first-of-its-kind job scheduler, which considers the non-linear scalability of GPUs at the intra-node level to schedule an appropriate number of GPUs per node for a job. By sharing the GPU resources on a node with multiple DL jobs, MARBLE avoids low GPU utilization in current multi-GPU DL training on HPC systems. Our comprehensive evaluation in the Summit supercomputer shows that MARBLE is able to improve DL training performance by up to 48.3\% compared to the popular Platform Load Sharing Facility (LSF) scheduler. Compared to the state-of-the-art of DL scheduler, Optimus, MARBLE reduces the job completion time by up to 47\%.},
  month     = {5},
  year      = {2020},
}

@Article{Pachajoa2020,
  author      = {Carlos Pachajoa and Christina Pacher and Markus Levonyak and Wilfried N. Gansterer},
  date        = {2020-07-08},
  title       = {Algorithm-Based Checkpoint-Recovery for the Conjugate Gradient Method},
  eprint      = {2007.04066v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {As computers reach exascale and beyond, the incidence of faults will increase. Solutions to this problem are an active research topic. We focus on strategies to make the preconditioned conjugate gradient (PCG) solver resilient against node failures, specifically, the exact state reconstruction (ESR) method, which exploits redundancies in PCG. Reducing the frequency at which redundant information is stored lessens the runtime overhead. However, after the node failure, the solver must restart from the last iteration for which redundant information was stored, which increases recovery overhead. This formulation highlights the method's similarities to checkpoint-restart (CR). Thus, this method, which we call ESR with periodic storage (ESRP), can be considered a form of algorithm-based checkpoint-restart. The state is stored implicitly, by exploiting redundancy inherent to the algorithm, rather than explicitly as in CR. We also minimize the amount of data to be stored and retrieved compared to CR, but additional computation is required to reconstruct the solver's state. In this paper, we describe the necessary modifications to ESR to convert it into ESRP, and perform an experimental evaluation. We compare ESRP experimentally with previously-existing ESR and application-level in-memory CR. Our results confirm that the overhead for ESR is reduced significantly, both in the failure-free case, and if node failures are introduced. In the former case, the overhead of ESRP is usually lower than that of CR. However, CR is faster if node failures happen. We claim that these differences can be alleviated by the implementation of more appropriate preconditioners.},
  file        = {:http\://arxiv.org/pdf/2007.04066v1:PDF},
  keywords    = {cs.DC},
}

@InCollection{Ren2020,
  author    = {Yanfei Ren and David F. Gleich},
  booktitle = {Parallel Algorithms in Computational Science and Engineering},
  title     = {A Simple Study of Pleasing Parallelism on Multicore Computers},
  doi       = {10.1007/978-3-030-43736-7_11},
  pages     = {325--346},
  publisher = {Springer International Publishing},
  abstract  = {Pleasingly parallel computations are those that involve completely independent work. We investigate these in the context of a problem we call AllPageRank. The AllPageRank problem involves computing a subset of accurate PageRank entries for each possible seeded PageRank vector. AllPageRank is representative of a wider class of possible computational procedures that will run a large number of experiments on a single graph structure. Our study involves computing the AllPageRank vectors for a multi-million node graph within a reasonable timeframe on a modern shared memory, high-core count computer. For this setting, we parallelize over all of the seeded PageRank vector computations, which are all independent. The experiments demonstrate that there are non-trivial complexities in obtaining performance even in this ideal situation. For instance, threading computational environments gave scaling problems with a shared graph structure in memory. Also sparse matrix ordering techniques and multivector, or SIMD, optimizations were required to get a total runtime of a few days. We also show how different algorithms for PageRank that have different algorithmic advances and memory access patterns behave to guide future investigation of similar problems.},
  year      = {2020},
}

@InCollection{Manguoglu2020,
  author    = {Murat Manguoğlu and Eric Polizzi and Ahmed H. Sameh},
  booktitle = {Parallel Algorithms in Computational Science and Engineering},
  title     = {Parallel Hybrid Sparse Linear System Solvers},
  doi       = {10.1007/978-3-030-43736-7_4},
  pages     = {95--120},
  publisher = {Springer International Publishing},
  abstract  = {In this chapter, we present the SPIKE family of algorithms for solving banded linear systems and its multithreaded implementation as well as direct-iterative hybrid variants for solving general sparse linear system of equations.},
  year      = {2020},
}

@Article{Wang2020d,
  author    = {Meiqi Wang and Sixian Jia and Enli Chen and Shaopu Yang and Pengfei Liu and Zhuang Qi},
  title     = {A derived least square fast learning network model},
  doi       = {10.1007/s10489-020-01773-6},
  abstract  = {The extreme learning machine (ELM) requires a large number of hidden layer nodes in the training process. Thus, random parameters will exponentially increase and affect network stability. Moreover, the single activation function affects the generalization capability of the network. This paper proposes a derived least square fast learning network (DLSFLN) to solve the aforementioned problems. DLSFLN uses the inheritance of some functions to obtain various activation functions through continuous differentiation of functions. The types of activation functions were increased and the mapping capability of hidden layer neurons was enhanced when the random parameter dimension was maintained. DLSFLN randomly generates the input weights and hidden layer thresholds and uses the least square method to determine the connection weights between the output and the input layers and that between the output and the input nodes. The regression and classification experiments show that DLSFLN has a faster training speed and better training accuracy, generalization capability, and stability compared with other neural network algorithms, such as fast learning network(FLN).},
  journal   = {Applied Intelligence},
  month     = {7},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@Article{Lai2020,
  author      = {Ming-Jun Lai and Jiaxin Xie and Zhiqiang Xu},
  date        = {2020-07-14},
  title       = {Graph Sparsification by Universal Greedy Algorithms},
  eprint      = {2007.07161v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Graph sparsification is to approximate an arbitrary graph by a sparse graph and is useful in many applications, such as simplification of social networks, least squares problems, numerical solution of symmetric positive definite linear systems and etc. In this paper, inspired by the well-known sparse signal recovery algorithm called orthogonal matching pursuit (OMP), we introduce a deterministic, greedy edge selection algorithm called universal greedy algorithm(UGA) for graph sparsification. The UGA algorithm can output a $\frac{(1+\epsilon)^2}{(1-\epsilon)^2}$-spectral sparsifier with $\lceil\frac{n}{\epsilon^2}\rceil$ edges in $O(m+n^2/\epsilon^2)$ time with $m$ edges and $n$ vertices for a general random graph satisfying a mild sufficient condition. This is a linear time algorithm in terms of the number of edges that the community of graph sparsification is looking for. The best result in the literature to the knowledge of the authors is the existence of a deterministic algorithm which is almost linear, i.e. $O(m^{1+o(1)})$ for some $o(1)=O(\frac{(\log\log(m))^{2/3}}{\log^{1/3}(m)})$. We shall point out that several random graphs satisfy the sufficient condition and hence, can be sparsified in linear time. For a general spectral sparsification problem, e.g., positive subset selection problem, a nonnegative UGA algorithm is proposed which needs $O(mn^2+ n^3/\epsilon^2)$ time and the convergence is established.},
  file        = {:http\://arxiv.org/pdf/2007.07161v1:PDF},
  keywords    = {cs.DS},
}

@Article{Levy2020,
  author      = {Ryan Levy and Edgar Solomonik and Bryan K. Clark},
  date        = {2020-07-10},
  title       = {Distributed-Memory DMRG via Sparse and Dense Parallel Tensor Contractions},
  eprint      = {2007.05540v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {The Density Matrix Renormalization Group (DMRG) algorithm is a powerful tool for solving eigenvalue problems to model quantum systems. DMRG relies on tensor contractions and dense linear algebra to compute properties of condensed matter physics systems. However, its efficient parallel implementation is challenging due to limited concurrency, large memory footprint, and tensor sparsity. We mitigate these problems by implementing two new parallel approaches that handle block sparsity arising in DMRG, via Cyclops, a distributed memory tensor contraction library. We benchmark their performance on two physical systems using the Blue Waters and Stampede2 supercomputers. Our DMRG performance is improved by up to 5.9X in runtime and 99X in processing rate over ITensor, at roughly comparable computational resource use. This enables higher accuracy calculations via larger tensors for quantum state approximation. We demonstrate that despite having limited concurrency, DMRG is weakly scalable with the use of efficient parallel tensor contraction mechanisms.},
  file        = {:http\://arxiv.org/pdf/2007.05540v1:PDF},
  keywords    = {cs.DC, cond-mat.str-el, physics.comp-ph},
}

@Article{Booth2020a,
  author      = {Joshua Dennis Booth},
  date        = {2020-07-15},
  title       = {Auto Adaptive Irregular OpenMP Loops},
  eprint      = {2007.07977v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {OpenMP is a standard for the parallelization due to the ease in programming parallel-for loops in a fork-join manner. Many shared-memory applications are implemented using this model despite not being ideal for applications with high load imbalance, such as those that make irregular memory accesses. One parameter, i.e., chunk size, is made available to users in order to mitigate performance loss. However, this parameter is dependent on architecture, system load, application, and input; making it difficult to tune. We present an OpenMP scheduler that does an adaptive tuning for chunk size for unbalanced applications that make irregular memory accesses. In particular, this method(iCh) uses work-stealing for imbalance and adapts chunk size using a force-feedback model that approximates variance of task length in a chunk. This scheduler has low overhead and allows for active load balancing while the applications are running. We demonstrate this using both sparse matrix-vector multiplication (spmv) and Betweenness Centrality (bc) and show that iCh can achieve average speedups close (i.e., within 1.061x for spmv and 1.092x for bc) of either OpenMP loops scheduled with dynamic or work-stealing methods that had chunk size tuned offline.},
  file        = {:http\://arxiv.org/pdf/2007.07977v1:PDF},
  keywords    = {cs.DC},
}

@InCollection{Guo2020a,
  author    = {Mengyu Guo and Shaowen Wang},
  booktitle = {Geotechnologies and the Environment},
  title     = {Quantum Computing for Solving Spatial Optimization Problems},
  doi       = {10.1007/978-3-030-47998-5_6},
  pages     = {97--113},
  publisher = {Springer International Publishing},
  abstract  = {Ever since Shor'’'s quantum factoring algorithm was developed, quantum computing has been pursued as a promising and powerful approach to solving many computationally complex problems such as combinatorial optimization and machine learning. As an important quantum computing approach, quantum annealing (QA) has received considerable attention. Extensive research has shown that QA, exploiting quantum-mechanical effects such as tunneling, entanglement and superposition, could be much more efficient in solving hard combinatorial optimization problems than its classical counterpart -- simulated annealing. Recent advances in quantum annealing hardware open the possibility of empirical testing of QA against the most challenging computational problems arising in geospatial applications. This chapter demonstrates how to employ QA to solve NP-hard spatial optimization problems through an illustrative example of programming a p-median model and a case study on spatial supply chain optimization. The research findings also address the short- and long-term potential of quantum computing in the future development of high-performance computing for geospatial applications.},
  year      = {2020},
}

@Article{Schenker2020,
  author      = {Carla Schenker and Jeremy E. Cohen and Evrim Acar},
  date        = {2020-07-19},
  title       = {A Flexible Optimization Framework for Regularized Matrix-Tensor Factorizations with Linear Couplings},
  eprint      = {2007.09605v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Coupled matrix and tensor factorizations (CMTF) are frequently used to jointly analyze data from multiple sources, also called data fusion. However, different characteristics of datasets stemming from multiple sources pose many challenges in data fusion and require to employ various regularizations, constraints, loss functions and different types of coupling structures between datasets. In this paper, we propose a flexible algorithmic framework for coupled matrix and tensor factorizations which utilizes Alternating Optimization (AO) and the Alternating Direction Method of Multipliers (ADMM). The framework facilitates the use of a variety of constraints, loss functions and couplings with linear transformations in a seamless way. Numerical experiments on simulated and real datasets demonstrate that the proposed approach is accurate, and computationally efficient with comparable or better performance than available CMTF methods for Frobenius norm loss, while being more flexible. Using Kullback-Leibler divergence on count data, we demonstrate that the algorithm yields accurate results also for other loss functions.},
  file        = {:http\://arxiv.org/pdf/2007.09605v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Article{Sayegh2020,
  author    = {Ammar T. Al Sayegh and Elisa D. Sotelino},
  title     = {A new row-wise parallel finite element analysis algorithm with dynamic load balancing},
  doi       = {10.1504/ijeie.2020.108588},
  number    = {2},
  pages     = {120},
  volume    = {3},
  abstract  = {A parallel scheme is devised to efficiently parallelise all steps of parallel finite element analysis in this study. In addition, this scheme is based on a row-wise matrix distribution. A new row-wise parallel finite element analysis algorithm that exploits the nature of distributed compressed row sparse matrices and multivectors to improve concurrency is developed. A new dynamic load balancing technique has also been devised. The dynamic load balancing technique has been designed specifically to balance the computational workload among processors suitable for the analysis of nonlinear structures. This new algorithm has been implemented in ParaStruc, which is a parallel structural analysis system. Trilinos, a set of parallel numerical libraries developed by researchers in the Sandia National Laboratory has been used to build this algorithm. ParaStruc is a lightweight fully parallelised parallel finite element analysis system, which contains only three classes and a pre-processor. It is shown that this approach produces superior performance in terms of speedup, efficiency, and isoefficiency in the analysis of nonlinear structure response ranges when compared to parallel ABAQUS. The performance and efficiency of this algorithm has been verified with numerical simulations of a 200-metre 50-story 10-frame 10-bay 3D structure subjected to various load levels.},
  journal   = {International Journal of Earthquake and Impact Engineering},
  publisher = {Inderscience Publishers},
  year      = {2020},
}

@Article{Chen2020e,
  author    = {Shixiang Chen and Shiqian Ma and Lingzhou Xue and Hui Zou},
  title     = {An Alternating Manifold Proximal Gradient Method for Sparse Principal Component Analysis and Sparse Canonical Correlation Analysis},
  doi       = {10.1287/ijoo.2019.0032},
  pages     = {ijoo.2019.0032},
  abstract  = {Sparse principal component analysis and sparse canonical correlation analysis are two essential techniques from high-dimensional statistics and machine learning for analyzing large-scale data. Both problems can be formulated as an optimization problem with nonsmooth objective and nonconvex constraints. Because nonsmoothness and nonconvexity bring numerical difficulties, most algorithms suggested in the literature either solve some relaxations of them or are heuristic and lack convergence guarantees. In this paper, we propose a new alternating manifold proximal gradient method to solve these two high-dimensional problems and provide a unified convergence analysis. Numerical experimental results are reported to demonstrate the advantages of our algorithm.},
  journal   = {{INFORMS} Journal on Optimization},
  month     = {7},
  publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
  year      = {2020},
}

@InProceedings{Page2020,
  author    = {Brian A. Page and Peter M. Kogge},
  booktitle = {Proceedings of the 2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops},
  title     = {Scalability of Sparse Matrix Dense Vector Multiply ({SpMV}) on a Migrating Thread Architecture},
  doi       = {10.1109/ipdpsw50202.2020.00088},
  publisher = {{IEEE}},
  series    = {IPDPSW '20},
  abstract  = {Sparse matrix dense vector multiplication (SpMV), exhibits the memory bandwidth and communication driven nature of many sparse linear algebra operations. Irregular memory accesses from the non-zero structure within a sparse matrix wreak havoc on performance. This paper presents strong scaling for communication avoiding SpMV implementations on a migrating thread system intended to address the lack of locality in sparse problems. We developed communication avoiding SpMV code to attempt to reduce off-node thread migration by using the hypergraph partitioning package HYPE to determine workload distribution. Additionally, we investigate the performance impact of overlapping communication and computation through the use of remote memory operations supported by the architecture. Incorporating remote memory operations with hypergraph partitioning we achieved 6.18X speedup for overall performance.},
  month     = {5},
  year      = {2020},
}

@Article{Priest2020,
  author      = {Benjamin W. Priest and Alec Dunton and Geoffrey Sanders},
  date        = {2020-07-24},
  title       = {Scaling Graph Clustering with Distributed Sketches},
  eprint      = {2007.12669v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {The unsupervised learning of community structure, in particular the partitioning vertices into clusters or communities, is a canonical and well-studied problem in exploratory graph analysis. However, like most graph analyses the introduction of immense scale presents challenges to traditional methods. Spectral clustering in distributed memory, for example, requires hundreds of expensive bulk-synchronous communication rounds to compute an embedding of vertices to a few eigenvectors of a graph associated matrix. Furthermore, the whole computation may need to be repeated if the underlying graph changes some low percentage of edge updates. We present a method inspired by spectral clustering where we instead use matrix sketches derived from random dimension-reducing projections. We show that our method produces embeddings that yield performant clustering results given a fully-dynamic stochastic block model stream using both the fast Johnson-Lindenstrauss and CountSketch transforms. We also discuss the effects of stochastic block model parameters upon the required dimensionality of the subsequent embeddings, and show how random projections could significantly improve the performance of graph clustering in distributed memory.},
  file        = {:http\://arxiv.org/pdf/2007.12669v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Mentus2020,
  author      = {Cassidy Mentus and Marcus Roper},
  date        = {2020-07-27},
  title       = {Optimal Mixing in Transport Networks: Numerical Optimization and Analysis},
  eprint      = {2007.13637v1},
  eprintclass = {nlin.AO},
  eprinttype  = {arXiv},
  abstract    = {Many foraging microorganisms rely upon cellular transport networks to deliver nutrients, fluid and organelles between different parts of the organism. Networked organisms ranging from filamentous fungi to slime molds demonstrate a remarkable ability to mix or disperse molecules and organelles in their transport media. Here we introduce mathematical tools to analyze the structure of energy efficient transport networks that maximize mixing and sending signals originating from and arriving at each node. We define two types of entropies on flows to quantify mixing and develop numerical algorithms to optimize the combination of entropy and energy on networks, given constraints on the amount of available material. We present an in-depth exploration of optimal single source-sink networks on finite triangular grids, a fundamental setting for optimal transport networks in the plane. Using numerical simulations and rigorous proofs, we show that, if the constraint on conductances is strict, the optimal networks are paths of every possible length. If the constraint is relaxed, our algorithm produces loopy networks that fan out at the source and pour back into a single path that flows to the sink. Taken together, our results expand the class of optimal transportation networks that can be compared with real biological data, and highlight how real network morphologies may be shaped by tradeoffs between transport efficiency and the need to mix the transported matter.},
  file        = {:http\://arxiv.org/pdf/2007.13637v1:PDF},
  keywords    = {nlin.AO, q-bio.QM},
}

@InProceedings{Brock2020a,
  author    = {Benjamin Brock and Aydin Buluc and Timothy G. Mattson and Scott McMillan and Jose E. Moreira and Roger Pearce and Oguz Selvitopi and Trevor Steil},
  booktitle = {Proceedings of the 2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops},
  title     = {Considerations for a Distributed {GraphBLAS} {API}},
  doi       = {10.1109/ipdpsw50202.2020.00048},
  publisher = {{IEEE}},
  series    = {IPDPSW '20},
  abstract  = {The GraphBLAS emerged from an international effort to standardize linear-algebraic building blocks for computing on graphs and graph-structured data. The GraphBLAS is expressed as a C API and has paved the way for multiple implementations. The GraphBLAS C API, however, does not define how distributed-memory parallelism should be handled. This paper reviews various approaches for a GraphBLAS API for distributed computing. This work is guided by our experience with existing distributed memory libraries. Our goal for this paper is to highlight the pros and cons of different approaches rather than to advocate for one particular choice.},
  month     = {5},
  year      = {2020},
}

@InProceedings{Lopez2020,
  author    = {Florent Lopez and Edmond Chow and Stanimire Tomov and Jack Dongarra},
  booktitle = {Proceedings of the 2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops},
  title     = {Asynchronous {SGD} for {DNN} training on Shared-memory Parallel Architectures},
  doi       = {10.1109/ipdpsw50202.2020.00168},
  publisher = {{IEEE}},
  series    = {IPDPSW '20},
  abstract  = {We present a parallel asynchronous Stochastic Gradient Descent algorithm for shared memory architectures. Different from previous asynchronous algorithms, we consider the case where the gradient updates are not particularly sparse. In the context of the MagmaDNN framework, we compare the parallel efficiency of the asynchronous implementation with that of the traditional synchronous implementation. Tests are performed for training deep neural networks on multicore CPUs and GPU devices.},
  month     = {5},
  year      = {2020},
}

@InProceedings{Ozkaya2020,
  author    = {M. Yusuf Ozkaya and M. Fatih Balin and Ali Pinar and {\"U}mitV. Catalyurek},
  booktitle = {Proceedings of the 2020 {IEEE} International Parallel and Distributed Processing Symposium Workshops},
  title     = {A scalable graph generation algorithm to sample over a given shell distribution},
  doi       = {10.1109/ipdpsw50202.2020.00051},
  publisher = {{IEEE}},
  series    = {IPDPSW '20},
  abstract  = {Graphs are commonly used to model the relationships between various entities. These graphs can be enormously large and thus, scalable graph analysis has been the subject of many research efforts. To enable scalable analytics, many researchers have focused on generating realistic graphs that support controlled experiments for understanding how algorithms perform under changing graph features. Significant progress has been made on scalable graph generation which preserve some important graph properties (e.g., degree distribution, clustering coefficients). In this paper, we study how to sample a graph from the space of graphs with a given shell distribution. Shell distribution is related to the $k$-core, which is the largest subgraph where each vertex is connected to at least $k$other vertices. A $k$-shell is the subset of vertices that are in $k$-core but not $( k +1)$-core, and the shell distribution comprises the sizes of these shells. Core decompositions are widely used to extract information from graphs and to assist other computations. We present a scalable shared and distributed memory graph generator that, given a shell decomposition, generates a random graph that conforms to it. Our extensive experimental results show the efficiency and scalability of our methods. Our algorithm generates $2 ^{33}$ vertices and $2 ^{37}$ edges in less than 50 seconds on 384 cores.},
  month     = {5},
  year      = {2020},
}

@InProceedings{Alappat2020,
  author    = {Alappat, Christie L. and Alvermann, Andreas and Basermann, Achim and Fehske, Holger and Futamura, Yasunori and Galgon, Martin and Hager, Georg and Huber, Sarah and Imakura, Akira and Kawai, Masatoshi and Kreutzer, Moritz and Lang, Bruno and Nakajima, Kengo and R{\"o}hrig-Z{\"o}llner, Melven and Sakurai, Tetsuya and Shahzad, Faisal and Thies, Jonas and Wellein, Gerhard},
  booktitle = {Software for Exascale Computing -- SPPEXA 2016--2019},
  title     = {{ESSEX}: Equipping Sparse Solvers For Exascale},
  editor    = {Bungartz, Hans-Joachim and Reiz, Severin and Uekermann, Benjamin and Neumann, Philipp and Nagel, Wolfgang E.},
  isbn      = {978-3-030-47956-5},
  pages     = {143--187},
  publisher = {Springer International Publishing},
  abstract  = {The ESSEX project has investigated programming concepts, data structures, and numerical algorithms for scalable, efficient, and robust sparse eigenvalue solvers on future heterogeneous exascale systems. Starting without the burden of legacy code, a holistic performance engineering process could be deployed across the traditional software layers to identify efficient implementations and guide sustainable software development. At the basic building blocks level, a flexible MPI+X programming approach was implemented together with a new sparse data structure (SELL-C-$\sigma$) to support heterogeneous architectures by design. Furthermore, ESSEX focused on hardware-efficient kernels for all relevant architectures and efficient data structures for block vector formulations of the eigensolvers. The algorithm layer addressed standard, generalized, and nonlinear eigenvalue problems and provided some widely usable solver implementations including a block Jacobi--Davidson algorithm, contour-based integration schemes, and filter polynomial approaches. Adding to the highly efficient kernel implementations, algorithmic advances such as adaptive precision, optimized filtering coefficients, and preconditioning have further improved time to solution. These developments were guided by quantum physics applications, especially from the field of topological insulator- or graphene-based systems. For these, ScaMaC, a scalable matrix generation framework for a broad set of quantum physics problems, was developed. As the central software core of ESSEX, the PHIST library for sparse systems of linear equations and eigenvalue problems has been established. It abstracts algorithmic developments from low-level optimization. Finally, central ESSEX software components and solvers have demonstrated scalability and hardware efficiency on up to 256 K cores using million-way process/thread-level parallelism.},
  year      = {2020},
}

@Article{Wang2020e,
  author    = {Jiulin Wang and Yong Xia},
  title     = {Closing the Gap between Necessary and Sufficient Conditions for Local Nonglobal Minimizer of Trust Region Subproblem},
  doi       = {10.1137/19m1294459},
  number    = {3},
  pages     = {1980--1995},
  volume    = {30},
  abstract  = {The trust region subproblem has at most one local nonglobal minimizer. In characterizing this local solution, there is a clear gap between necessary and sufficient conditions. In this paper, we surprisingly show that the sufficient second-order optimality condition remains necessary. As an application, we improve the state-of-the-art algorithm for computing a candidate of the local nonglobal minimizer and then show that finding the local nonglobal minimizer or proving the nonexistence can be done in polynomial time.},
  journal   = {{SIAM} Journal on Optimization},
  month     = {1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year      = {2020},
}

@Article{Curtis2020a,
  author      = {Frank E. Curtis and Yutong Dai and Daniel P. Robinson},
  date        = {2020-07-29},
  title       = {A Subspace Acceleration Method for Minimization Involving a Group Sparsity-Inducing Regularizer},
  eprint      = {2007.14951v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We consider the problem of minimizing an objective function that is the sum of a convex function and a group sparsity-inducing regularizer. Problems that integrate such regularizers arise in modern machine learning applications, often for the purpose of obtaining models that are easier to interpret and that have higher predictive accuracy. We present a new method for solving such problems that utilize subspace acceleration, domain decomposition, and support identification. Our analysis shows, under common assumptions, that the iterate sequence generated by our framework is globally convergent, converges to an $\epsilon$-approximate solution in at most $O(\epsilon^{-(1+p)})$ (respectively, $O(\epsilon^{-(2+p)})$) iterations for all $\epsilon$ bounded above and large enough (respectively, all $\epsilon$ bounded above) where $p > 0$ is an algorithm parameter, and exhibits superlinear local convergence. Preliminary numerical results for the task of binary classification based on regularized logistic regression show that our approach is efficient and robust, with the ability to outperform a state-of-the-art method.},
  file        = {:http\://arxiv.org/pdf/2007.14951v1:PDF},
  keywords    = {math.OC, 49M37, 65K05, 65K10, 65Y20, 68Q25, 90C30, 90C60},
}

@Article{Li2020e,
  author    = {Huan Li and Cong Fang and Zhouchen Lin},
  title     = {Accelerated First-Order Optimization Algorithms for Machine Learning},
  doi       = {10.1109/jproc.2020.3007634},
  pages     = {1--16},
  abstract  = {Numerical optimization serves as one of the pillars of machine learning. To meet the demands of big data applications, lots of efforts have been put on designing theoretically and practically fast algorithms. This article provides a comprehensive survey on accelerated first-order algorithms with a focus on stochastic algorithms. Specifically, this article starts with reviewing the basic accelerated algorithms on deterministic convex optimization, then concentrates on their extensions to stochastic convex optimization, and at last introduces some recent developments on acceleration for nonconvex optimization.},
  journal   = {Proceedings of the {IEEE}},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Jakovetic2020a,
  author    = {Dusan Jakovetic and Dragana Bajovic and Joao Xavier and Jose M. F. Moura},
  title     = {Primal-Dual Methods for Large-Scale and Distributed Convex Optimization and Data Analytics},
  doi       = {10.1109/jproc.2020.3007395},
  pages     = {1--16},
  abstract  = {The augmented Lagrangian method (ALM) is a classical optimization tool that solves a given ``difficult'' (constrained) problem via finding solutions of a sequence of ``easier'' (often unconstrained) subproblems with respect to the original (primal) variable, wherein constraints satisfaction is controlled via the so-called dual variables. ALM is highly flexible with respect to how primal subproblems can be solved, giving rise to a plethora of different primal-dual methods. The powerful ALM mechanism has recently proved to be very successful in various large-scale and distributed applications. In addition, several significant advances have appeared, primarily on precise complexity results with respect to computational and communication costs in the presence of inexact updates and design and analysis of novel optimal methods for distributed consensus optimization. We provide a tutorial-style introduction to ALM and its variants for solving convex optimization problems in large-scale and distributed settings. We describe control-theoretic tools for the algorithms' analysis and design, survey recent results, and provide novel insights into the context of two emerging applications: federated learning and distributed energy trading.},
  journal   = {Proceedings of the {IEEE}},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year      = {2020},
}

@Article{Fasi2020b,
  author     = {Fasi, Massimiliano and Higham, Nicholas J.},
  date       = {2020},
  title      = {Matrices with Tunable Infinity-Norm Condition Number and No Need for Pivoting in {LU} Factorization},
  eprint     = {MIMS EPrint:2020.17},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/id/eprint/2775},
  abstract   = {We propose a two-parameter family of nonsymmetric dense $n \times n$ matrices $A(\alpha, \beta)$ for which LU factorization without pivoting is numerically stable, and we show how to choose $\alpha$ and $\beta$ to achieve any value of the $\infty$-norm condition number. The matrix $A(\alpha, \beta)$ can be formed from a simple formula in $O(n^2)$ flops. The matrix is suitable for use in the HPL-AI Mixed-Precision Benchmark, which requires an extreme scale test matrix (dimension $n > 10^7$) that has a controlled condition number and can be safely used in LU factorization without pivoting. It is also of interest as a general-purpose test matrix.},
}

@TechReport{Friedlander2002,
  author      = {Michael P. Friedlander and Michael A. Saunders},
  date        = {2002},
  institution = {Argonne National Laboratory -- Mathematics and Computer Science Division},
  title       = {A globally convergent {LCL} method for nonlinear optimization},
  eprint      = {ANL/MCS-P1015-1202},
  url         = {https://ftp.mcs.anl.gov/pub/tech_reports/reports/P1015.pdf},
  abstract    = {For optimization problems with nonlinear constraints, linearly constrained Lagrangian (LCL) methods sequentially minimize a Lagrangian function subject to linearized constraints. These methods converge rapidly near a solution but may not be reliable from arbitrary starting points. The well known example MINOS has proven effective on many large problems. Its success motivates us to propose a globally convergent variant. Our stabilized LCL method possesses two important properties: the subproblems are always feasible, and they may be solved inexactly. These features are present in MINOS only as heuristics.\\ The new algorithm has been implemented in Matlab, with the option to use either the MINOS or SNOPT Fortran codes to solve the linearly constrained subproblems. Only first derivatives are required. We present numerical results on a nonlinear subset of the COPS, CUTE, and HS testproblem sets, which include many large examples. The results demonstrate the robustness and efficiency of the stabilized LCL procedure.},
}

@Article{Kepner2020,
  author      = {Jeremy Kepner and Chad Meiners and Chansup Byun and Sarah McGuire and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Raul Harnasch and Matthew Hubbell and Micheal Houle and Micheal Jones and Andrew Kirby and Anna Klein and Lauren Milechin and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Doug Stetson and Adam Tse and Charles Yee and Peter Michaleas},
  date        = {2020-08-01},
  title       = {Multi-Temporal Analysis and Scaling Relations of 100,000,000,000 Network Packets},
  eprint      = {2008.00307v1},
  eprintclass = {cs.NI},
  eprinttype  = {arXiv},
  abstract    = {Our society has never been more dependent on computer networks. Effective utilization of networks requires a detailed understanding of the normal background behaviors of network traffic. Large-scale measurements of networks are computationally challenging. Building on prior work in interactive supercomputing and GraphBLAS hypersparse hierarchical traffic matrices, we have developed an efficient method for computing a wide variety of streaming network quantities on diverse time scales. Applying these methods to 100,000,000,000 anonymized source-destination pairs collected at a network gateway reveals many previously unobserved scaling relationships. These observations provide new insights into normal network background traffic that could be used for anomaly detection, AI feature engineering, and testing theoretical models of streaming networks.},
  file        = {:http\://arxiv.org/pdf/2008.00307v1:PDF},
  keywords    = {cs.NI, cs.DC, cs.SI},
}

@Article{Li2020f,
  author      = {Cheng Li and Min Tang and Ruofeng Tong and Ming Cai and Jieyi Zhao and Dinesh Manocha},
  date        = {2020-08-02},
  title       = {P-Cloth: Interactive Complex Cloth Simulation on Multi-GPU Systems using Dynamic Matrix Assembly and Pipelined Implicit Integrators},
  eprint      = {2008.00409v2},
  eprintclass = {cs.GR},
  eprinttype  = {arXiv},
  abstract    = {We present a novel parallel algorithm for cloth simulation that exploits multiple GPUs for fast computation and the handling of very high resolution meshes. To accelerate implicit integration, we describe new parallel algorithms for sparse matrix-vector multiplication (SpMV) and for dynamic matrix assembly on a multi-GPU workstation. Our algorithms use a novel work queue generation scheme for a fat-tree GPU interconnect topology. Furthermore, we present a novel collision handling scheme that uses spatial hashing for discrete and continuous collision detection along with a non-linear impact zone solver. Our parallel schemes can distribute the computation and storage overhead among multiple GPUs and enable us to perform almost interactive simulation on complex cloth meshes, which can hardly be handled on a single GPU due to memory limitations. We have evaluated the performance with two multi-GPU workstations (with 4 and 8 GPUs, respectively) on cloth meshes with 0.5-1.65M triangles. Our approach can reliably handle the collisions and generate vivid wrinkles and folds at 2-5 fps, which is significantly faster than prior cloth simulation systems. We observe almost linear speedups with respect to the number of GPUs.},
  file        = {:http\://arxiv.org/pdf/2008.00409v2:PDF},
  keywords    = {cs.GR},
}

@Article{Ahrens2020b,
  author      = {Peter Ahrens},
  date        = {2020-07-31},
  title       = {Load Plus Communication Balancing in Contiguous Partitions for Distributed Sparse Matrices: Linear-Time Algorithms},
  eprint      = {2007.16192v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {We study partitioning to parallelize multiplication of one or more dense vectors by a sparse matrix (SpMV or SpMM). We consider contiguous partitions, where the rows (or columns) of the matrix are split into $K$ parts without reordering. We present exact and approximate contiguous partitioning algorithms that minimize the runtime of the longest-running processor under cost models that combine work factors and hypergraph communication factors. This differs from traditional graph or hypergraph partitioning models which minimize total communication under a work balance constraint. We address regimes where partitions of the row space and column space are expected to match (the symmetric case) or are allowed to differ (the nonsymmetric case). Our algorithms use linear space. Our exact algorithm runs in linear time when $K^2$ is sublinear. Our $(1 + \epsilon)$-approximate algorithm runs in linear time when $K\log(1/\epsilon)$ is sublinear. We combine concepts from high-performance computing and computational geometry. Existing load balancing algorithms optimize a linear model of per-processor work. We make minor adaptations to optimize arbitrary nonuniform monotonic increasing or decreasing cost functions which may be expensive to evaluate. We then show that evaluating our model of communication is equivalent to planar dominance counting. We specialize Chazelle's dominance counting algorithm to points in the bounded integer plane and generalize it to trade reduced construction time for increased query time, since our partitioners make very few queries. Our algorithms split the original row (or column) ordering into parts to optimize diverse cost models. Combined with reordering or embedding techniques, our algorithms might be used to build more general heuristic partitioners, as they can optimally round one-dimensional embeddings of direct $K$-way noncontiguous partitioning problems.},
  file        = {:http\://arxiv.org/pdf/2007.16192v1:PDF},
  keywords    = {cs.DS},
}

@Article{Nolet2020,
  author      = {Corey J. Nolet and Victor Lafargue and Edward Raff and Thejaswi Nanditale and Tim Oates and John Zedlewski and Joshua Patterson},
  date        = {2020-08-01},
  title       = {Bringing UMAP Closer to the Speed of Light with GPU Acceleration},
  eprint      = {2008.00325v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsupervised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficent and inaccurate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs. Our implementation has been made publicly available as part of the open source RAPIDS cuML library(https://github.com/rapidsai/cuml).},
  file        = {:http\://arxiv.org/pdf/2008.00325v1:PDF},
  keywords    = {cs.LG, cs.DS, stat.ML},
}

@Article{Azad2020,
  author    = {Ariful Azad and Aydin Bulu{\c{c}} and Xiaoye S. Li and Xinliang Wang and Johannes Langguth},
  title     = {A Distributed-Memory Algorithm for Computing a Heavy-Weight Perfect Matching on Bipartite Graphs},
  doi       = {10.1137/18m1189348},
  number    = {4},
  pages     = {C143--C168},
  volume    = {42},
  abstract  = {We design and implement an efficient parallel algorithm for finding a perfect matching in a weighted bipartite graph such that weights on the edges of the matching are large. This problem differs from the maximum weight matching problem, for which scalable approximation algorithms are known. It is primarily motivated by finding good pivots in scalable sparse direct solvers before factorization. Due to the lack of scalable alternatives, distributed solvers use sequential implementations of maximum weight perfect matching algorithms, such as those available in MC64. To overcome this limitation, we propose a fully parallel distributed memory algorithm that first generates a perfect matching and then iteratively improves the weight of the perfect matching by searching for weight-increasing cycles of length 4 in parallel. For most practical problems the weights of the perfect matchings generated by our algorithm are very close to the optimum. An efficient implementation of the algorithm scales up to 256 nodes (17,408 cores) on a Cray XC40 supercomputer and can solve instances that are too large to be handled by a single node using the sequential algorithm.},
  journal   = {{SIAM} Journal on Scientific Computing},
  month     = {1},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  year      = {2020},
}

@InProceedings{Santos2020,
  author    = {Fernando Fernandes dos Santos and Marcelo Brandalero and Pedro Martins Basso and Michael Hubner and Luigi Carro and Paolo Rech},
  booktitle = {Proceedings of the 26th IEEE International Symposium on On-Line Testing and Robust System Design},
  title     = {Reduced-Precision {DWC} for Mixed-Precision {GPUs}},
  doi       = {10.1109/iolts50870.2020.9159748},
  publisher = {{IEEE}},
  series    = {IOLTS 2020},
  abstract  = {Duplication with Comparison (DWC) is an effective software-level solution to improve the reliability of computing systems, including Graphics Processing Units (GPUs). DWC, however, introduces performance and energy consumption overheads that could be unacceptable for High-Performance Computing (HPC) or real-time safety-critical applications. In this work, we propose Reduced-Precision DWC (RP-DWC): an improvement over the traditional DWC approach that uses mixed-precision GPUs hardware resources to implement fault detection. We investigate, through both fault injection campaigns and accelerated neutron beam experiments, the impact of RPDWC onto performance, energy consumption, and its fault detection capabilites. We show that RP-DWC achieves on average 74\% fault coverage (up to 86\%) with very small overheads (0.1\% time and 24\% energy consumption overhead, in the best case).},
  month     = {7},
  year      = {2020},
}

@InProceedings{Kreutzer2020,
  author    = {Patrick Kreutzer and Stefan Kraus and Michael Philippsen},
  booktitle = {Proceedings of the13th IEEE International Conference on Software Testing, Validation and Verification},
  title     = {Language-Agnostic Generation of Compilable Test Programs},
  doi       = {10.1109/icst46399.2020.00015},
  publisher = {{IEEE}},
  series    = {ICST 2020},
  abstract  = {Testing is an integral part of the development of compilers and other language processors. To automatically create large sets of test programs, random program generators, or fuzzers, have emerged. Unfortunately, existing approaches are either language-specific (and thus require a rewrite for each language) or may generate programs that violate rules of the respective programming language (which limits their usefulness). This work introduces *Smith, a language-agnostic framework for the generation of valid, compilable test programs. It takes as input an abstract attribute grammar that specifies the syntactic and semantic rules of a programming language. It then creates test programs that satisfy all these rules. By aggressively pruning the search space and keeping the construction as local as possible, *Smith can generate huge, complex test programs in short time. We present four case studies covering four real-world programming languages (C, Lua, SQL, and SMT-LIB 2) to show that *Smith is both efficient and effective, while being flexible enough to support programming languages that differ considerably. We found bugs in all four case studies. For example, *Smith detected 165 different crashes in older versions of GCC and LLVM. *Smith and the language grammars are available online.},
  month     = {10},
  year      = {2020},
}

@Article{Criscitiello2020,
  author      = {Chris Criscitiello and Nicolas Boumal},
  date        = {2020-08-05},
  title       = {An accelerated first-order method for non-convex optimization on manifolds},
  eprint      = {2008.02252v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We describe the first gradient methods on Riemannian manifolds to achieve accelerated rates in the non-convex case. Under Lipschitz assumptions on the Riemannian gradient and Hessian of the cost function, these methods find approximate first-order critical points strictly faster than regular gradient descent. A randomized version also finds approximate second-order critical points. Both the algorithms and their analyses build extensively on existing work in the Euclidean case. The basic operation consists in running the Euclidean accelerated gradient descent method (appropriately safe-guarded against non-convexity) in the current tangent space, then moving back to the manifold and repeating. This requires lifting the cost function from the manifold to the tangent space, which can be done for example through the Riemannian exponential map. For this approach to succeed, the lifted cost function (called the pullback) must retain certain Lipschitz properties. As a contribution of independent interest, we prove precise claims to that effect, with explicit constants. Those claims are affected by the Riemannian curvature of the manifold, which in turn affects the worst-case complexity bounds for our optimization algorithms.},
  file        = {:http\://arxiv.org/pdf/2008.02252v1:PDF},
  keywords    = {math.OC, cs.NA, math.DG, math.NA},
}

@Article{Hebling2020,
  author    = {Gustavo M. Hebling and Julio A.D. Massignan and João B.A. London Junior and Marcos H.M. Camillo},
  title     = {Sparse and numerically stable implementation of a distribution system state estimation based on Multifrontal {QR} factorization},
  doi       = {10.1016/j.epsr.2020.106734},
  pages     = {106734},
  volume    = {189},
  abstract  = {Enhancing situational awareness of distribution networks is a requirement of Smart Grids. In order to fulfill this requirement, specialized algorithms have been developed to perform Distribution System State Estimation (DSSE). Due to the particularities of such networks, those algorithms often rely on simplifications and approximations of the measurement model which make difficult to generalize their results. This paper presents a sparse and numerically stable implementation of an algorithm for DSSE, which does not require any additional assumption from the traditional state estimation formulation. The numerical stability is guaranteed by using Multifrontal QR factorization and an optimal ordering technique is evaluated to reduce fill-in. Simulation results are carried out with IEEE three-phase unbalanced test feeders to evaluate the algorithm.},
  journal   = {Electric Power Systems Research},
  month     = {12},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Uroic2020,
  author    = {Tessa Uroić and Hrvoje Jasak},
  title     = {Parallelisation of selective algebraic multigrid for block--pressure--velocity system in {OpenFOAM}},
  doi       = {10.1016/j.cpc.2020.107529},
  pages     = {107529},
  abstract  = {In the world of computational fluid dynamics (CFD), solving the governing equations of incompressible, turbulent, single--phase fluid flow still represents the basis of many industrial and academic applications. The implicitly coupled (monolithic) solution approach is still being developed and investigated for industrial--size applications. A parallel selection algebraic multigrid algorithm (AMG) based on the domain decomposition method is presented, applied for the solution of the linearised implicitly coupled pressure--velocity system discretised by the finite volume method, implemented in OpenFOAM. Since the setup phase of the selection AMG, i.e. sorting the equations into coarse and fine subsets is inherently sequential, it was decided to perform the setup phase locally on each processing unit. The prolongation matrix for transferring the correction from coarse to fine level and restriction matrix for transferring the residual from fine to coarse level are assembled locally as well. Parallel communication is necessary only for the calculation of the coarse level matrix, i.e. the matrix elements which describe the cross--coupling of equations located on different processing units. A localised version of the ILU factorisation based on Crout's algorithm is used as a smoother in the multigrid cycle. A detailed analysis of the coarse level matrix complexity is conducted in the context of the finite volume method in domain decomposition mode. The performance and scaling of our parallel implementation is investigated for two test cases and the possible drawbacks of the method are given.},
  journal   = {Computer Physics Communications},
  month     = {8},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Pham2020,
  author    = {Minh Pham and Anh Ninh and Hoang Le and Yufeng Liu},
  title     = {An efficient algorithm for minimizing multi non-smooth component functions},
  doi       = {10.1080/10618600.2020.1804390},
  pages     = {1--23},
  abstract  = {Many problems in statistics and machine learning can be formulated as an optimization problem of a finite sum of non-smooth convex functions. We propose an algorithm to minimize this type of objective functions based on the idea of alternating linearization. Our algorithm retains the simplicity of contemporary methods without any restrictive assumptions on the smoothness of the loss function. We apply our proposed method to solve two challenging problems: overlapping group Lasso and convex regression with sharp partitions (CRISP). Numerical experiments show that our method is superior to the state-of-the-art algorithms, many of which are based on the accelerated proximal gradient method.},
  journal   = {Journal of Computational and Graphical Statistics},
  month     = {8},
  publisher = {Informa {UK} Limited},
  year      = {2020},
}

@TechReport{Brown2020,
  author      = {Cade Brown and Ahmad Abdelfattah and Stanimire Tomov and Jack Dongarra},
  date        = {2020},
  institution = {University of Tennessee},
  title       = {Design, Optimization, and Benchmarking of Dense Linear Algebra Algorithms on {AMD GPUs}},
  url         = {https://www.icl.utk.edu/files/publications/2020/icl-utk-1405-2020.pdf},
  abstract    = {Dense linear algebra (DLA) has historically been in the vanguard of software that must be adapted first to hardware changes. This is because DLA is both critical to the accuracy and performance of so many different types of applications, and because they have proved to be outstanding vehicles for finding and implementing solutions to the problems that novel architectures pose. Therefore, in this paper we investigate the portability of the MAGMA DLA library to the latest AMD GPUs. We use auto tools to convert the CUDA code in MAGMA to the HeterogeneousComputing Interface for Portability (HIP) language. MAGMA provides LAPACK for GPUs and benchmarks for fundamental DLA routines ranging from BLAS to dense factorizations, linear systems and eigen-problem solvers. We port these routines to HIP and quantify currently achievable performance through the MAGMA benchmarks for the main workload algorithms on MI25 and MI50 AMD GPUs. Comparison with performance roofline models and theoretical expectations are used to identify current limitations and directions for future improvements.},
}

@TechReport{Archibald2020,
  author      = {Rick Archibald and Edmond Chow and Eduardo D'Azevedo and Jack Dongarra and Markus Eisenbach and Rocco Febbo and Florent Lopez and Daniel Nichols and Stanimire Tomov and Kwai Wong and Junqi Yin},
  date        = {2020},
  institution = {University of Tennessee},
  title       = {Integrating Deep Learning in Domain Sciencesat Exascale},
  url         = {https://www.icl.utk.edu/files/publications/2020/icl-utk-1403-2020.pdf},
  abstract    = {This paper presents some of the current challenges in designing deep learning artificial intelligence (AI) and integrating it with traditional high-performance computing (HPC) simulations. We evaluate existing packages for their ability to run deep learning models and applications on large-scale HPC systems eciently, identify challenges, and propose new asynchronous parallelization and optimization techniques for current large-scale heterogeneous systems and upcoming exascale systems. These developments, along with existing HPC AI software capabilities, have been integrated into MagmaDNN, an open-source HPC deep learning framework. Many deep learning frameworks are targeted at data scientists and fall short in providing quality integration into existing HPC workflows. This paper discusses the necessities of an HPC deep learning framework and how those needs can be provided (e.g., as in MagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA and its modular memory management, MPI, CuBLAS, CuDNN, MKL, and HIP. Advancements are also illustrated through the use of algorithmic enhancements in reduced- and mixed-precision, as well as asynchronous optimization methods. Finally, we present illustrations and potential solutions for enhancing traditional compute- and data-intensive applications at ORNL and UTK with AI. The approaches and future challenges are illustrated in materials science, imaging, and climate applications.},
}

@TechReport{Dongarra2020,
  author      = {Jack Dongarra and Mark Gates and Piotr Luszczek and Stanimire Tomov},
  institution = {University of Tennessee},
  title       = {Translational Process: Mathematical Software Perspective},
  abstract    = {Each successive generation of computer architecture has brought new challenges to achieving high performance mathematical solvers, necessitating development and analysis of new algorithms, which are then embodied in software libraries. These libraries hide architectural details from applications, allowing them to achieve a level of portability across platforms from desktops to worldclass high performance computing (HPC) systems. Thus there has been an informal translational computer science process of developing algorithms and distributing them in open source software libraries for adoption by applications and vendors. With the move to exascale, increasing intentionality about this process will benefit the long-term sustainability of the scientific software stack.},
  date        = {2020},
  url         = {https://www.icl.utk.edu/files/publications/2020/icl-utk-1404-2020.pdf},
}

@InProceedings{Lu2020a,
  author    = {Zhengyang Lu and Yuyao Niu and Weifeng Liu},
  booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
  title     = {Efficient Block Algorithms for Parallel Sparse Triangular Solve},
  doi       = {10.1145/3404397.3404413},
  publisher = {{ACM}},
  series    = {ICPP 2020},
  abstract  = {The sparse triangular solve (SpTRSV) kernel is an important building block for a number of linear algebra routines such as sparse direct and iterative solvers. The major challenge of accelerating SpTRSV lies in the difficulties of finding higher parallelism. Existing work mainly focuses on reducing dependencies and synchronizations in the level-set methods. However, the 2D block layout of the input matrix has been largely ignored in designing more efficient SpTRSV algorithms.\\ In this paper, we implement three block algorithms, i.e., column block, row block and recursive block algorithms, for parallel SpTRSV on modern GPUs, and propose an adaptive approach that can automatically select the best kernels according to input sparsity structures. By testing 159 sparse matrices on two high-end NVIDIA GPUs, the experimental results demonstrate that the recursive block algorithm has the best performance among the three block algorithms, and it is on average 4.72$\times$ (up to 72.03$\times$) and 9.95$\times$ (up to 61.08$\times$) faster than cuSPARSE v2 and Sync-free methods, respectively. Besides, our method merely needs moderate cost for preprocessing the input matrix, thus is highly efficient for multiple right-hand sides and iterative scenarios.},
  month     = {8},
  year      = {2020},
}

@InProceedings{Su2020,
  author    = {Jiya Su and Feng Zhang and Weifeng Liu and Bingsheng He and Ruofan Wu and Xiaoyong Du and Rujia Wang},
  booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
  title     = {{CapelliniSpTRSV}: A Thread-Level Synchronization-Free Sparse Triangular Solve on {GPUs}},
  doi       = {10.1145/3404397.3404400},
  publisher = {{ACM}},
  series    = {ICPP 2020},
  abstract  = {Sparse triangular solves (SpTRSVs) have been extensively used in linear algebra fields, and many GPU-based SpTRSV algorithms have been proposed. Synchronization-free SpTRSVs, due to their short preprocessing time and high performance, are currently the most popular SpTRSV algorithms. However, we observe that the performance of those SpTRSV algorithms on different matrices can vary greatly by 845 times. Our further studies show that when the average number of components per level is high and the average number of nonzero elements per row is low, those SpTRSVs exhibit extremely low performance. The reason is that, they use a warp on the GPU to process a row in sparse matrices, and such warp-level designs have severe underutilization of the GPU. To solve this problem, we propose CapelliniSpTRSV, a thread-level synchronization-free SpTRSV algorithm. Particularly, CapelliniSpTRSV has three novel features. First, unlike the previous studies, CapelliniSpTRSV does not need preprocessing to calculate levels. Second, CapelliniSpTRSV exhibits high performance on matrices that previous SpTRSVs cannot handle efficiently. Third, CapelliniSpTRSV's optimization does not rely on specific sparse matrix storage format. Instead, it can achieve very good performance on the most popular sparse matrix storage, compressed sparse row (CSR) format, and thus users do not need to conduct format conversion. We evaluate CapelliniSpTRSV with 245 matrices from the Florida Sparse Matrix Collection on three GPU platforms, and experiments show that our SpTRSV exhibits 6.84 GFLOPS/s, which is 4.97$\times$ speedup over the state-of-the-art synchronization-free SpTRSV algorithm, and 4.74$\times$ speedup over the SpTRSV in cuSPARSE. CapelliniSpTRSV is open-sourced in https://github.com/JiyaSu/CapelliniSpTRSV.},
  month     = {8},
  year      = {2020},
}

@InProceedings{Mishra2020,
  author    = {Ashirbad Mishra and Shad Kirmani and Kamesh Madduri},
  booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
  title     = {Fast Spectral Graph Layout on Multicore Platforms},
  doi       = {10.1145/3404397.3404471},
  publisher = {{ACM}},
  series    = {ICPP 2020},
  abstract  = {We present ParHDE, a shared-memory parallelization of the High-Dimensional Embedding (HDE) graph algorithm. Originally proposed as a graph drawing algorithm, HDE characterizes the global structure of a graph and is closely related to spectral graph computations such as computing the eigenvectors of the graph Laplacian. We identify compute- and memory-intensive steps in HDE and parallelize these steps for efficient execution on shared-memory multicore platforms. ParHDE can process graphs with billions of edges in minutes, is up to 18$\times$ faster than a prior parallel implementation of HDE, and achieves up to a 24$\times$ relative speedup on a 28-core system. We also implement several extensions of ParHDE and demonstrate its utility in diverse graph computation-related applications.},
  month     = {8},
  year      = {2020},
}

@Article{Nayak2020a,
  author    = {Pratik Nayak and Terry Cojean and Hartwig Anzt},
  title     = {Evaluating asynchronous Schwarz solvers on {GPUs}},
  doi       = {10.1177/1094342020946814},
  pages     = {109434202094681},
  abstract  = {With the commencement of the exascale computing era, we realize that the majority of the leadership supercomputers are heterogeneous and massively parallel. Even a single node can contain multiple co-processors such as GPUs and multiple CPU cores. For example, ORNL's Summit accumulates six NVIDIA Tesla V100 GPUs and 42 IBM Power9 cores on each node. Synchronizing across compute resources of multiple nodes can be prohibitively expensive. Hence, it is necessary to develop and study asynchronous algorithms that circumvent this issue of bulk-synchronous computing. In this study, we examine the asynchronous version of the abstract Restricted Additive Schwarz method as a solver. We do not explicitly synchronize, but allow the communication between the sub-domains to be completely asynchronous, thereby removing the bulk synchronous nature of the algorithm.\\ We accomplish this by using the one-sided Remote Memory Access (RMA) functions of the MPI standard. We study the benefits of using such an asynchronous solver over its synchronous counterpart. We also study the communication patterns governed by the partitioning and the overlap between the sub-domains on the global solver. Finally, we show that this concept can render attractive performance benefits over the synchronous counterparts even for a well-balanced problem.},
  journal   = {The International Journal of High Performance Computing Applications},
  month     = {8},
  publisher = {{SAGE} Publications},
  year      = {2020},
}

@Article{Anzt2020c,
  author       = {Hartwig Anzt and Terry Cojean and Yen-Chen Chen and Goran Flegar and Fritz Göbel and Thomas Grützmacher and Pratik Nayak and Tobias Ribizel and Yu-Hsiang Tsai},
  date         = {2020},
  journaltitle = {The Journal of Open Source Software},
  title        = {Ginkgo: A high performance numerical linear algebra library},
  doi          = {10.21105/joss.02260},
  abstract     = {Ginkgo is a production-ready sparse linear algebra library for high performance computing on GPU-centric architectures with a high level of performance portability and focuses on software sustainability.\\ The library focuses on solving sparse linear systems and accommodates a large variety of matrix formats, state-of-the-art iterative (Krylov) solvers and preconditioners, which make the library suitable for a variety of scientific applications. Ginkgo supports many architectures such as multi-threaded CPU, NVIDIA GPUs, and AMD GPUs. The heavy use of modern C++ features simplifies the addition of new executor paradigms and algorithmic functionality without introducing significant performance overhead.\\ Solving linear systems is usually one of the most computationally and memory intensive aspects of any application. Hence there has been a significant amount of effort in this direction with software libraries such as UMFPACK and CHOLMOD (“Suitesparse,” 2020) for solving linear systems with direct methods and PETSc (“PETSc,” 2020), Trilinos (“Trilinos,” 2020), Eigen (“Eigen,” 2020) and many more to solve linear systems with iterative methods. With Ginkgo, we aim to ensure high performance while not compromising portability. Hence, we provide very efficient low level kernels optimized for different architectures and separate these kernels from the algorithms thereby ensuring extensibility and ease of use.\\ Ginkgo is also a part of the xSDK effort (“xSDK,” 2020) and available as a Spack (Gamblin et al., 2015) package. xSDK aims to provide infrastructure for and interoperability between a collection of related and complementary software elements to foster rapid and efficient development of scientific applications using High Performance Computing. Within this effort, we provide interoperability with application libraries such as deal.ii (Arndt et al., 2019) and mfem (Anderson et al., 2020). Ginkgo provides wrappers within these two libraries so that they can take advantage of the features of Ginkgo},
}

@Article{Kronqvist2020,
  author    = {Jan Kronqvist and Ruth Misener},
  title     = {A disjunctive cut strengthening technique for convex {MINLP}},
  doi       = {10.1007/s11081-020-09551-6},
  abstract  = {Generating polyhedral outer approximations and solving mixed-integer linear relaxations remains one of the main approaches for solving convex mixed-integer nonlinear programming (MINLP) problems. There are several algorithms based on this concept, and the efficiency is greatly affected by the tightness of the outer approximation. In this paper, we present a new framework for strengthening cutting planes of nonlinear convex constraints, to obtain tighter outer approximations. The strengthened cuts can give a tighter continuous relaxation and an overall tighter representation of the nonlinear constraints. The cuts are strengthened by analyzing disjunctive structures in the MINLP problem, and we present two types of strengthened cuts. The first type of cut is obtained by reducing the right-hand side value of the original cut, such that it forms the tightest generally valid inequality for a chosen disjunction. The second type of cut effectively uses individual right-hand side values for each term of the disjunction. We prove that both types of cuts are valid and that the second type of cut can dominate both the first type and the original cut. We use the cut strengthening in conjunction with the extended supporting hyperplane algorithm, and numerical results show that the strengthening can significantly reduce both the number of iterations and the time needed to solve convex MINLP problems.},
  journal   = {Optimization and Engineering},
  month     = {8},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@Article{Tsai2020b,
  author      = {Yuhsiang Mike Tsai and Terry Cojean and Hartwig Anzt},
  date        = {2020-08-19},
  title       = {Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear Algebra Computations},
  eprint      = {2008.08478v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {GPU accelerators have become an important backbone for scientific high performance computing, and the performance advances obtained from adopting new GPU hardware are significant. In this paper we take a first look at NVIDIA's newest server line GPU, the A100 architecture part of the Ampere generation. Specifically, we assess its performance for sparse linear algebra operations that form the backbone of many scientific applications and assess the performance improvements over its predecessor.},
  file        = {:http\://arxiv.org/pdf/2008.08478v1:PDF},
  keywords    = {cs.MS, cs.PF},
}

@InCollection{Goebel2020,
  author    = {Fritz Goebel and Hartwig Anzt and Terry Cojean and Goran Flegar and Enrique S. Quintana-Ort{\'{\i}}},
  booktitle = {Euro-Par 2020: Parallel Processing},
  title     = {Multiprecision Block-Jacobi for Iterative Triangular Solves},
  doi       = {10.1007/978-3-030-57675-2_34},
  pages     = {546--560},
  publisher = {Springer International Publishing},
  abstract  = {Recent research efforts have shown that Jacobi and block-Jacobi relaxation methods can be used as an effective and highly parallel approach for the solution of sparse triangular linear systems arising in the application of ILU-type preconditioners. Simultaneously, a few independent works have focused on designing efficient high performance adaptive-precision block-Jacobi preconditioning (block-diagonal scaling), in the context of the iterative solution of sparse linear systems, on manycore architectures. In this paper, we bridge the gap between relaxation methods based on regular splittings and preconditioners by demonstrating that iterative refinement can be leveraged to construct a relaxation method from the preconditioner. In addition, we exploit this insight to construct a highly-efficient sparse triangular system solver for graphics processors that combines iterative refinement with the block-Jacobi preconditioner available in the Ginkgo library.},
  year      = {2020},
}

@InCollection{Gou2020,
  author    = {Changjiang Gou and Ali Al Zoobi and Anne Benoit and Mathieu Faverge and Loris Marchal and Gr{\'{e}}goire Pichon and Pierre Ramet},
  booktitle = {Euro-Par 2020: Parallel Processing},
  title     = {Improving Mapping for Sparse Direct~Solvers},
  doi       = {10.1007/978-3-030-57675-2_11},
  pages     = {167--182},
  publisher = {Springer International Publishing},
  abstract  = {In order to express parallelism, parallel sparse direct solvers take advantage of the elimination tree to exhibit tree-shaped task graphs, where nodes represent computational tasks and edges represent data dependencies. One of the pre-processing stages of sparse direct solvers consists of mapping computational resources (processors) to these tasks. The objective is to minimize the factorization time by exhibiting good data locality and load balancing. The proportional mapping technique is a widely used approach to solve this resource-allocation problem. It achieves good data locality by assigning the same processors to large parts of the elimination tree. However, it may limit load balancing in some cases. In this paper, we propose a dynamic mapping algorithm based on proportional mapping. This new approach, named Steal, relaxes the data locality criterion to improve load balancing. In order to validate the newly introduced method, we perform extensive experiments on the PaStiX sparse direct solver. It demonstrates that our algorithm enables better static scheduling of the numerical factorization while keeping good data locality.},
  year      = {2020},
}

@InCollection{Ahmad2020,
  author    = {Najeeb Ahmad and Buse Yilmaz and Didem Unat},
  booktitle = {Euro-Par 2020: Parallel Processing},
  title     = {A Prediction Framework for Fast Sparse Triangular Solves},
  doi       = {10.1007/978-3-030-57675-2_33},
  pages     = {529--545},
  publisher = {Springer International Publishing},
  abstract  = {Sparse triangular solve (SpTRSV) is an important linear algebra kernel, finding extensive uses in numerical and scientific computing. The parallel implementation of SpTRSV is a challenging task due to the sequential nature of the steps involved. This makes it, in many cases, one of the most time-consuming operations in an application. Many approaches for efficient SpTRSV on CPU and GPU systems have been proposed in the literature. However, no single implementation or platform (CPU or GPU) gives the fastest solution for all input sparse matrices. In this work, we propose a machine learning-based framework to predict the SpTRSV implementation giving the fastest execution time for a given sparse matrix based on its structural features. The framework is tested with six SpTRSV implementations on a state-of-the-art CPU-GPU machine (Intel Xeon Gold CPU, NVIDIA V100 GPU). Experimental results, with 998 matrices taken from the SuiteSparse Matrix Collection, show the classifier prediction accuracy of 87\% for the fastest SpTRSV algorithm for a given input matrix. Predicted SpTRSV implementations achieve average speedups (harmonic mean) in the range of 1.4--2.7$\times$ against the six SpTRSV implementations used in the evaluation.},
  year      = {2020},
}

@Article{Zhang2020f,
  author      = {Ying Zhang and Zhiqiang Zhao and Zhuo Feng},
  date        = {2020-08-17},
  title       = {SF-GRASS: Solver-Free Graph Spectral Sparsification},
  eprint      = {2008.07633v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Recent spectral graph sparsification techniques have shown promising performance in accelerating many numerical and graph algorithms, such as iterative methods for solving large sparse matrices, spectral partitioning of undirected graphs, vectorless verification of power/thermal grids, representation learning of large graphs, etc. However, prior spectral graph sparsification methods rely on fast Laplacian matrix solvers that are usually challenging to implement in practice. This work, for the first time, introduces a solver-free approach (SF-GRASS) for spectral graph sparsification by leveraging emerging spectral graph coarsening and graph signal processing (GSP) techniques. We introduce a local spectral embedding scheme for efficiently identifying spectrally-critical edges that are key to preserving graph spectral properties, such as the first few Laplacian eigenvalues and eigenvectors. Since the key kernel functions in SF-GRASS can be efficiently implemented using sparse-matrix-vector-multiplications (SpMVs), the proposed spectral approach is simple to implement and inherently parallel friendly. Our extensive experimental results show that the proposed method can produce a hierarchy of high-quality spectral sparsifiers in nearly-linear time for a variety of real-world, large-scale graphs and circuit networks when compared with the prior state-of-the-art spectral method.},
  file        = {:http\://arxiv.org/pdf/2008.07633v1:PDF},
  keywords    = {cs.DS, cs.LG, cs.NA, cs.SI, math.NA},
}

@Article{Ahmadi2020,
  author      = {Amir Ali Ahmadi and Jeffrey Zhang},
  date        = {2020-08-14},
  title       = {Complexity aspects of local minima and related notions},
  eprint      = {2008.06148v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We consider the notions of (i) critical points, (ii) second-order points, (iii) local minima, and (iv) strict local minima for multivariate polynomials. For each type of point, and as a function of the degree of the polynomial, we study the complexity of deciding (1) if a given point is of that type, and (2) if a polynomial has a point of that type. Our results characterize the complexity of these two questions for all degrees left open by prior literature. Our main contributions reveal that many of these questions turn out to be tractable for cubic polynomials. In particular, we present an efficiently-checkable necessary and sufficient condition for local minimality of a point for a cubic polynomial. We also show that a local minimum of a cubic polynomial can be efficiently found by solving semidefinite programs of size linear in the number of variables. By contrast, we show that it is strongly NP-hard to decide if a cubic polynomial has a critical point. We also prove that the set of second-order points of any cubic polynomial is a spectrahedron, and conversely that any spectrahedron is the projection of the set of second-order points of a cubic polynomial. In our final section, we briefly present a potential application of finding local minima of cubic polynomials to the design of a third-order Newton method.},
  file        = {:http\://arxiv.org/pdf/2008.06148v1:PDF},
  keywords    = {math.OC, cs.CC, cs.LG, 90C60 (Primary), 90C22, 90C30, 90C46 (Secondary)},
}

@Article{Scott2020,
  author       = {Jennifer Scott and Miroslav Tůma},
  date         = {2020},
  journaltitle = {SIAM Journal on Scientific Computing},
  title        = {A Null-Space Approach for Symmetric Saddle Point Systems With a Non Zero (2,2) Block},
  eprint       = {RAL-P-2020-003},
  url          = {https://www2.karlin.mff.cuni.cz/~mirektuma/ps/RAL-P-2020-003.pdf},
  abstract     = {Null-space methods have long been used to solve large-scale symmetric saddle point systems of equations in which the $k \times k$ $(2, 2)$ block is zero. This paper focuses on the case where the $(2, 2)$ block is non zero. A novel null-space approach is proposed to transform the saddle point system into another symmetric saddle point system of the same order but with a zero $(2, 2)$ block of order at most $2k$. Success of any null-space approach is dependent on the construction of a suitable null-space basis. The not uncommon case of the off-diagonal block being a wide matrix that has far fewer rows than columns and that may be dense is considered. A number of approaches are explored with the aim of balancing stability of the transformed system with sparsity. Linear least squares problems that contain a small number of dense rows arising from practical applications are used to illustrate our ideas and to explore their potential for solving large-scale systems.},
}

@Article{Malitsky2019,
  author       = {Yura Malitsky and Konstantin Mishchenko},
  date         = {2019-10-21},
  journaltitle = {Proceedings of the 37th International Conference on Machine Learning},
  title        = {Adaptive Gradient Descent without Descent},
  eprint       = {1910.09529v2},
  eprintclass  = {math.OC},
  eprinttype   = {arXiv},
  series       = {PLMR 2019},
  abstract     = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
  file         = {:http\://arxiv.org/pdf/1910.09529v2:PDF},
  keywords     = {math.OC, cs.LG, cs.NA, math.NA, stat.ML},
}

@Article{Huang2020a,
  author      = {Kevin Huang and Junyu Zhang and Shuzhong Zhang},
  date        = {2020-08-22},
  title       = {Cubic Regularized Newton Method for Saddle Point Models: a Global and Local Convergence Analysis},
  eprint      = {2008.09919v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a cubic regularized Newton (CRN) method for solving convex-concave saddle point problems (SPP). At each iteration, a cubic regularized saddle point subproblem is constructed and solved, which provides a search direction for the iterate. With properly chosen stepsizes, the method is shown to converge to the saddle point with global linear and local superlinear convergence rates, if the saddle point function is gradient Lipschitz and strongly-convex-strongly-concave. In the case that the function is merely convex-concave, we propose a homotopy continuation (or path-following) method. Under a Lipschitz-type error bound condition, we present an iteration complexity bound of $\mathcal{O}\left(\ln \left(1/\epsilon\right)\right)$ to reach an $\epsilon$-solution through a homotopy continuation approach, and the iteration complexity bound becomes $\mathcal{O}\left(\left(1/\epsilon\right)^{\frac{1-\theta}{\theta^2}}\right)$ under a H\"{o}lderian-type error bound condition involving a parameter $\theta$ ($0<\theta<1$).},
  file        = {:http\://arxiv.org/pdf/2008.09919v1:PDF},
  keywords    = {math.OC},
}

@Article{Hanzely2020a,
  author      = {Filip Hanzely},
  date        = {2020-08-26},
  title       = {Optimization for Supervised Machine Learning: Randomized Algorithms for Data and Parameters},
  doi         = {10.25781/KAUST-4F2DH},
  eprint      = {2008.11824},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Many key problems in machine learning and data science are routinely modeled as optimization problems and solved via optimization algorithms. With the increase of the volume of data and the size and complexity of the statistical models used to formulate these often ill-conditioned optimization tasks, there is a need for new efficient algorithms able to cope with these challenges. In this thesis, we deal with each of these sources of difficulty in a different way. To efficiently address the big data issue, we develop new methods which in each iteration examine a small random subset of the training data only. To handle the big model issue, we develop methods which in each iteration update a random subset of the model parameters only. Finally, to deal with ill-conditioned problems, we devise methods that incorporate either higher-order information or Nesterov's acceleration/momentum. In all cases, randomness is viewed as a powerful algorithmic tool that we tune, both in theory and in experiments, to achieve the best results. Our algorithms have their primary application in training supervised machine learning models via regularized empirical risk minimization, which is the dominant paradigm for training such models. However, due to their generality, our methods can be applied in many other fields, including but not limited to data science, engineering, scientific computing, and statistics.},
  file        = {:http\://arxiv.org/pdf/2008.11824v1:PDF},
  keywords    = {math.OC, cs.LG, cs.NA, math.NA},
}

@Article{Besta2020,
  author       = {Maciej Besta and Armon Carigiet and Zur Vonarburg-Shmaria and Kacper Janda and Lukas Gianinazzi and Torsten Hoefler},
  date         = {2020-08-26},
  journaltitle = {Proceedings of the ACM/IEEE International Conference on High Performance Computing, Networking, Storage and Analysis, November 2020},
  title        = {High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality},
  eprint       = {2008.11321},
  eprintclass  = {cs.DS},
  eprinttype   = {arXiv},
  abstract     = {We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining work-efficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.},
  file         = {:http\://arxiv.org/pdf/2008.11321v1:PDF},
  keywords     = {cs.DS, cs.DC, cs.PF},
}

@Article{Caliciotti2019,
  author   = {Andrea Caliciotti and Giovanni Fasano and Florian Potra and Massimo Roma},
  date     = {2019},
  title    = {Issues on the use of a modified {Bunch} and {Kaufmandecomposition} for large scale {Newton's} equation},
  url      = {https://arca.unive.it/retrieve/handle/10278/3729359/205899/CFPR-final2019_accepted.pdf},
  abstract = {In this work, we deal with Truncated Newton methods for solving large scale (possibly nonconvex) unconstrained optimization problems. In particular, we consider the use of a modified Bunch and Kaufman factorization for solving the Newton equation, at each (outer) iteration of the method. The Bunch and Kaufman factorization of a tridiagonal matrix is an effective and stable matrix decomposition, which is well exploited in the widely adopted SYMMBK [2, 5, 6, 19, 20] routine. It can be used to provide conjugate directions, both in the case of $1 \times 1$ and $2 \times 2$ pivoting steps. The main drawback is that the resulting solution of Newton's equation might not be gradient--related, in the case the objective function is nonconvex. Here we first focus on some theoretical properties, in order to ensure that at each iteration of the Truncated Newton method, the search direction obtained by using an adapted Bunch and Kaufman factorization is gradient--related. This allows to perform a standard Armijo-type linesearch procedure, using a bounded descent direction. Furthermore, the results of an extended numerical experience using large scale CUTEst problems is reported, showing the reliability and the efficiency of the proposed approach, both on convex and nonconvex problems.},
}

@Article{Amaral2020,
  author      = {V. S. Amaral and R. Andreani and E. G. Birgin and D. S. Marcondes and J. M. Martínez},
  date        = {2020-09-03},
  title       = {On complexity and convergence of high-order coordinate descent algorithms},
  eprint      = {2009.01811},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {Coordinate descent methods with high-order regularized models for box-constrained minimization are introduced. High-order stationarity asymptotic convergence and first-order stationarity worst-case evaluation complexity bounds are established. The computer work that is necessary for obtaining first-order $\varepsilon$-stationarity with respect to the variables of each coordinate-descent block is $O(\varepsilon^{-(p+1)/p})$ whereas the computer work for getting first-order $\varepsilon$-stationarity with respect to all the variables simultaneously is $O(\varepsilon^{-(p+1)})$. Numerical examples involving multidimensional scaling problems are presented. The numerical performance of the methods is enhanced by means of coordinate-descent strategies for choosing initial points.},
  file        = {:http\://arxiv.org/pdf/2009.01811v1:PDF},
  keywords    = {math.OC, 90C30, 65K05, 49M37, 90C60, 68Q25},
}

@Article{Schmidt2020,
  author      = {Drew Schmidt},
  date        = {2020-09-02},
  title       = {A Survey of Singular Value Decomposition Methods for Distributed Tall/Skinny Data},
  eprint      = {2009.00761},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {The Singular Value Decomposition (SVD) is one of the most important matrix factorizations, enjoying a wide variety of applications across numerous application domains. In statistics and data analysis, the common applications of SVD such as Principal Components Analysis (PCA) and linear regression. Usually these applications arise on data that has far more rows than columns, so-called "tall/skinny" matrices. In the big data analytics context, this may take the form of hundreds of millions to billions of rows with only a few hundred columns. There is a need, therefore, for fast, accurate, and scalable tall/skinny SVD implementations which can fully utilize modern computing resources. To that end, we present a survey of three different algorithms for computing the SVD for these kinds of tall/skinny data layouts using MPI for communication. We contextualize these with common big data analytics techniques, principally PCA. Finally, we present both CPU and GPU timing results from the Summit supercomputer, and discuss possible alternative approaches.},
  file        = {:http\://arxiv.org/pdf/2009.00761v1:PDF},
  keywords    = {cs.MS, stat.CO},
}

@TechReport{Devine2020,
  author      = {Karen Devine and Grey Ballard},
  date        = {2020},
  institution = {Sandia National Laboratories},
  title       = {{GentenMPI}: Distributed Memory Sparse Tensor Decomposition},
  eprint      = {SAND2020-8515},
  url         = {https://www.osti.gov/servlets/purl/1656940},
  abstract    = {GentenMPl is a toolkit of sparse canonical polyadic (CP) tensor decomposition algorithms that is designed to run effectively on distributed-memory high-performance computers. Its use of distributed-memory parallelism enables it to efficiently decompose tensors that are too large for a single compute node's memory. GentenMPl leverages Sandia's decades-long investment in the Trilinos solver framework for much of its parallel-computation capability. Trilinos contains numerical algorithms and linear algebra classes that have been optimized for parallel simulation of complex physical phenomena. This work applies these tools to the data science problem of sparse tensor decomposition. In this report, we describe the use of Trilinos in GentenMPl, extensions needed for sparse tensor decomposition, and implementations of the CP-ALS (CP via alternating least squares [4, 7]) and GCP-SGD (generalized CP via stochastic gradient descent [11, 12, 17]) sparse tensor decomposition algorithms. We show that GentenMPl can decompose sparse tensors of extreme size, e.g., a 12.6-terabyte tensor on 8192 computer cores. We demonstrate that the Trilinos backbone provides good strong and weak scaling of the tensor decomposition algorithms.},
}

@Article{Higham2020b,
  author     = {Nicholas J. Higham and Xiaobo Liu},
  date       = {2020},
  title      = {A Multiprecision Derivative-Free Schur-Parlett Algorithm for Computing Matrix Functions},
  eprint     = {MIMS EPrint:2020.19},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2781/1/paper.pdf},
  abstract   = {The Schur--Parlett algorithm, implemented in MATLAB as $funm$, computes a function $f(A)$ of an $n \times n$ matrix $A$ by using the Schur decomposition and a block recurrence of Parlett. The algorithm requires the ability to compute $f$ and its derivatives, and it requires that $f$ has a Taylor series expansion with a suitably large radius of convergence. We develop a version of the Schur--Parlett algorithm that requires only function values and uses higher precision arithmetic to evaluate $f$ on the diagonal blocks of order greater than $2$ (if there are any) of the reordered and blocked Schur form. The key idea is to compute by diagonalization the function of a small random diagonal perturbation of each triangular block, where the perturbation ensures that diagonalization will succeed. This multiprecision Schur--Parlett algorithm is applicable to arbitrary functions $f$ and, like the original Schur--Parlett algorithm, it generally behaves in a numerically stable fashion. Our algorithm is inspired by Davies's randomized approximate diagonalization method, but we explain why that is not a reliable numerical method for computing matrix functions. We apply our algorithm to the matrix Mittag--Leffler function and show that it yields results of accuracy similar to, and in some cases much greater than, the state of the art algorithm for this function. The algorithm will be useful for evaluating any matrix function for which the derivatives of the underlying function are not readily available or accurately computable.},
}

@InCollection{Georgakoudis2020,
  author    = {Giorgis Georgakoudis and Johannes Doerfert and Ignacio Laguna and Thomas R. W. Scogland},
  booktitle = {{OpenMP}: Portable Multi-Level Parallelism on Modern Systems},
  title     = {{FAROS}: A Framework to Analyze {OpenMP} Compilation Through Benchmarking and Compiler Optimization Analysis},
  doi       = {10.1007/978-3-030-58144-2_1},
  pages     = {3--17},
  publisher = {Springer International Publishing},
  abstract  = {Compilers optimize OpenMP programs differently than their serial elision. Early outlining of parallel regions and invocation of parallel code via OpenMP runtime functions are two of the most profound differences. Understanding the interplay between compiler optimizations, OpenMP compilation, and application performance is hard and usually requires specialized benchmarks and compilation analysis tools.\\
To this end, we present FAROS, an extensible framework to automate and structure the analysis of compiler optimization of OpenMP programs. FAROS provides a generic configuration interface to profile and analyze OpenMP applications with their native build configurations. Using FAROS on a set of 39 OpenMP programs, including HPC applications and kernels, we show that OpenMP compilation hinders optimization for the majority of programs. Comparing single-threaded OpenMP execution to its sequential counterpart, we observed slowdowns as much as 135.23\%. In some cases, however, OpenMP compilation speeds up execution as much as 25.48\% when OpenMP semantics help compiler optimization. Following analysis on compiler optimization reports enables us to pinpoint the reasons without in-depth knowledge of the compiler. The information can be used to improve compilers and also to bring performance on par through manual code refactoring.},
  year      = {2020},
}

@Article{Wilkinson2020,
  author      = {Leland Wilkinson and Hengrui Luo},
  date        = {2020-09-08},
  title       = {A Distance-preserving Matrix Sketch},
  eprint      = {2009.03979},
  eprintclass = {cs.HC},
  eprinttype  = {arXiv},
  abstract    = {Visualizing very large matrices involves many formidable problems. Various popular solutions to these problems involve sampling, clustering, projection, or feature selection to reduce the size and complexity of the original task. An important aspect of these methods is how to preserve relative distances between points in the higher-dimensional space after reducing rows and columns to fit in a lower dimensional space. This aspect is important because conclusions based on faulty visual reasoning can be harmful. Judging dissimilar points as similar or similar points as dissimilar on the basis of a visualization can lead to false conclusions. To ameliorate this bias and to make visualizations of very large datasets feasible, we introduce a new algorithm that selects a subset of rows and columns of a rectangular matrix. This selection is designed to preserve relative distances as closely as possible. We compare our matrix sketch to more traditional alternatives on a variety of artificial and real datasets.},
  file        = {:http\://arxiv.org/pdf/2009.03979v1:PDF},
  keywords    = {cs.HC, cs.LG, stat.ML},
}

@Article{Marin2020,
  author    = {Oana Marin and Emil Constantinescu and Barry Smith},
  title     = {A scalable matrix-free spectral element approach for unsteady {PDE} constrainedoptimization using {PETSc}/{TAO}},
  doi       = {10.1016/j.jocs.2020.101207},
  pages     = {101207},
  abstract  = {We provide a new approach for the efficient matrix-free application of the transpose of the Jacobian for the spectral element method for the adjoint-based solution of partial differential equation (PDE) constrained optimization. This results in optimizations of nonlinear PDEs using explicit integrators where the integration of the adjoint problem is not more expensive than the forward simulation. Solving PDE constrained optimization problems entails combining expertise from multiple areas, including simulation, computation of derivatives, and optimization. The Portable, Extensible Toolkit for Scientific computation (PETSc) together with its companion package, the Toolkit for Advanced Optimization (TAO), is an integrated numerical software library that contains an algorithmic/software stack for solving linear systems, nonlinear systems, ordinary differential equations, differential algebraic equations, and large-scale optimization problems and, as such, is an ideal tool for performing PDE-constrained optimization. This paper describes an efficient approach in which the software stack provided by PETSc/TAO can be used for large-scale nonlinear time-dependent problems. While time integration can involve a range of high-order methods, both implicit and explicit. The PDE-constrained optimization algorithm used is gradient-based and seamlessly integrated with the simulation of the physical problem.},
  journal   = {Journal of Computational Science},
  month     = {9},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@InProceedings{Gao2020c,
  author    = {G. Gao and Y. Wang and J. Vink and T. Wells and F. Saaf},
  booktitle = {Conference Proceedings, ECMOR},
  date      = {2020},
  title     = {Distributed Quasi-Newton Derivative-Free Optimization Method for Optimization Problems with Multiple Local Optima},
  pages     = {1--22},
  series    = {ECMOR XVII},
  url       = {https://www.earthdoc.org/content/papers/10.3997/2214-4609.202035131},
  abstract  = {For highly nonlinear problems, the objective function f(x) may have multiple local optima and it is desired to locate all of them. Analytical or adjoint-based derivatives may not be available for most real optimization problems, especially, when responses of a system are predicted by numerical simulations. The distributed-Gauss-Newton (DGN) optimization method performs quite efficiently and robustly for history-matching problems with multiple best matches. However, this method is not applicable for generic optimization problems, e.g., life-cycle production optimization or well location optimization.\\
In this paper, we generalized the distribution techniques of the DGN optimization method and developed a new distributed quasi-Newton (DQN) optimization method that is applicable to generic optimization problems. It can handle generalized objective functions $F(x,y(x))=f(x)$ with both explicit variables x and implicit variables, i.e., simulated responses, $y(x)$. The partial derivatives of $F(x,y)$ with respect to both x and y can be computed analytically, whereas the partial derivatives of $y(x)$ with respect to x (sensitivity matrix) is estimated by applying the same efficient information sharing mechanism implemented in the DGN optimization method. An ensemble of quasi-Newton optimization tasks is distributed among multiple high-performance-computing (HPC) cluster nodes. The simulation results generated from one optimization task are shared with others by updating a common set of training data points, which records simulated responses of all simulation jobs. The sensitivity matrix at the current best solution of each optimization task is approximated by either the linear-interpolation (LI) method or the support-vector-regression (SVR) method, using some or all training data points. The gradient of the objective function is then analytically computed using its partial derivatives with respect to x and y and the estimated sensitivities of y with respect to $x$. The Hessian is updated using the quasi-Newton formulation. A new search point for each distributed optimization task is generated by solving a quasi-Newton trust-region subproblem for the next iteration.\\
The proposed DQN method is first validated on a synthetic history matching problem and its performance is found to be comparable with the DGN optimizer. Then, the DQN method is tested on different optimization problems. For all test problems, the DQN method can find multiple optima of the objective function with reasonably small numbers of iterations (30 to 50). Compared to sequential model-based derivative-free optimization methods, the DQN method can reduce the computational cost, in terms of the number of simulations required for convergence, by a factor of 3 to 10.},
}

@Article{Awan2020,
  author    = {Muaaz G. Awan and Jack Deslippe and Aydin Buluc and Oguz Selvitopi and Steven Hofmeyr and Leonid Oliker and Katherine Yelick},
  title     = {{ADEPT}: a domain independent sequence alignment strategy for gpu architectures},
  doi       = {10.1186/s12859-020-03720-1},
  number    = {1},
  volume    = {21},
  abstract  = {Bioinformatic workflows frequently make use of automated genome assembly and protein clustering tools. At the core of most of these tools, a significant portion of execution time is spent in determining optimal local alignment between two sequences. This task is performed with the Smith-Waterman algorithm, which is a dynamic programming based method. With the advent of modern sequencing technologies and increasing size of both genome and protein databases, a need for faster Smith-Waterman implementations has emerged. Multiple SIMD strategies for the Smith-Waterman algorithm are available for CPUs. However, with the move of HPC facilities towards accelerator based architectures, a need for an efficient GPU accelerated strategy has emerged. Existing GPU based strategies have either been optimized for a specific type of characters (Nucleotides or Amino Acids) or for only a handful of application use-cases.},
  journal   = {{BMC} Bioinformatics},
  month     = {9},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@InCollection{Tupitsa2020,
  author    = {Nazarii Tupitsa and Alexander Gasnikov and Pavel Dvurechensky and Sergey Guminov},
  booktitle = {Mathematical Optimization Theory and Operations Research},
  title     = {Strongly Convex Optimization for the Dual Formulation of Optimal Transport},
  doi       = {10.1007/978-3-030-58657-7_17},
  pages     = {192--204},
  publisher = {Springer International Publishing},
  abstract  = {In this paper we experimentally check a hypothesis, that dual problem to discrete entropy regularized optimal transport problem possesses strong convexity on a certain compact set. We present a numerical estimation technique of parameter of strong convexity and show that such an estimate increases the performance of an accelerated alternating minimization algorithm for strongly convex functions applied to the considered problem.},
  year      = {2020},
}

@Article{Brayford2020,
  author      = {David Brayford and Christoph Bernau and Wolfram Hesse and Carla Guillen},
  date        = {2020-09-13},
  title       = {Analyzing Performance Properties Collected by the PerSyst Scalable HPC Monitoring Tool},
  eprint      = {2009.06061},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {The ability to understand how a scientific application is executed on a large HPC system is of great importance in allocating resources within the HPC data center. In this paper, we describe how we used system performance data to identify: execution patterns, possible code optimizations and improvements to the system monitoring. We also identify candidates for employing machine learning techniques to predict the performance of similar scientific codes.},
  file        = {:http\://arxiv.org/pdf/2009.06061v1:PDF},
  keywords    = {cs.DC},
}

@Article{Xin2020,
  author      = {Ran Xin and Shi Pu and Angelia Nedić and Usman A. Khan},
  date        = {2020-09-12},
  title       = {A general framework for decentralized optimization with first-order methods},
  eprint      = {2009.05837},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Decentralized optimization to minimize a finite sum of functions over a network of nodes has been a significant focus within control and signal processing research due to its natural relevance to optimal control and signal estimation problems. More recently, the emergence of sophisticated computing and large-scale data science needs have led to a resurgence of activity in this area. In this article, we discuss decentralized first-order gradient methods, which have found tremendous success in control, signal processing, and machine learning problems, where such methods, due to their simplicity, serve as the first method of choice for many complex inference and training tasks. In particular, we provide a general framework of decentralized first-order methods that is applicable to undirected and directed communication networks alike, and show that much of the existing work on optimization and consensus can be related explicitly to this framework. We further extend the discussion to decentralized stochastic first-order methods that rely on stochastic gradients at each node and describe how local variance reduction schemes, previously shown to have promise in the centralized settings, are able to improve the performance of decentralized methods when combined with what is known as gradient tracking. We motivate and demonstrate the effectiveness of the corresponding methods in the context of machine learning and signal processing problems that arise in decentralized environments.},
  file        = {:http\://arxiv.org/pdf/2009.05837v1:PDF},
  keywords    = {cs.LG, cs.MA, cs.SY, eess.SY, math.OC, stat.ML},
}

@Article{Pi2020,
  author    = {J. Pi and Honggang Wang and Panos M. Pardalos},
  title     = {A Dual Reformulation and Solution Framework for Regularized Convex Clustering Problems},
  doi       = {10.1016/j.ejor.2020.09.010},
  abstract  = {Clustering techniques are powerful tools commonly used in statistical learning and data analytics. Most of the past research formulates clustering tasks as a non-convex problem, where a global optimum often cannot be found. Recent studies show that hierarchical clustering and k-means clustering can be relaxed and analyzed as a convex problem. Moreover, sparse convex clustering algorithms are proposed to extend the convex clustering framework to high-dimensional space by introducing an adaptive group-Lasso penalty term. Due to the non-smoothness nature of the associated objective functions, there are still no efficient fast-convergent algorithms for clustering problems even with convexity. In this paper, we first review the structure of convex clustering problems and prove the differentiability of their dual problems. We then show that such reformulated dual problems can be efficiently solved by the accelerated first-order methods with the feasibility projection. Furthermore, we present a general framework for convex clustering with regularization terms and discuss a specific implementation of this framework using $L_{1,1}$-norm. We also derive the dual form for the regularized convex clustering problems and show that it can be efficiently solved by embedding a projection operator and a proximal operator in the accelerated gradient method. Finally, we compare our approach with several other co-clustering algorithms using a number of example clustering problems. Numerical results show that our models and solution methods outperform all the compared algorithms for both convex clustering and convex co-clustering.},
  journal   = {European Journal of Operational Research},
  month     = {9},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Gusmeroli2020,
  author      = {Nicolò Gusmeroli and Timotej Hrga and Borut Lužar and Janez Povh and Melanie Siebenhofer and Angelika Wiegele},
  date        = {2020-09-14},
  title       = {BiqBin: a parallel branch-and-bound solver for binary quadratic problems with linear constraints},
  eprint      = {2009.06240},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We present BiqBin, an exact solver for linearly constrained binary quadratic problems. Our approach is based on an exact penalty method to first efficiently transform the original problem into an instance of Max-Cut, and then to solve the Max-Cut problem by a branch-and-bound algorithm. All the main ingredients are carefully developed using new semidefinite programming relaxations obtained by strengthening the existing relaxations with a set of hypermetric inequalities, applying the bundle method as the bounding routine and using new strategies for exploring the branch-and-bound tree. Furthermore, an efficient C implementation of a sequential and a parallel branch-and-bound algorithm is presented. The latter is based on a load coordinator-worker scheme using MPI for multi-node parallelization and is evaluated on a high-performance computer. The new solver is benchmarked against BiqCrunch, GUROBI, and SCIP on four families of (linearly constrained) binary quadratic problems. Numerical results demonstrate that BiqBin is a highly competitive solver. The serial version outperforms the other three solvers on the majority of the benchmark instances. We also evaluate the parallel solver and show that it has good scaling properties. The general audience can use it as an on-line service available at http://www.biqbin.eu.},
  file        = {:http\://arxiv.org/pdf/2009.06240v1:PDF},
  keywords    = {math.OC},
}

@Article{Spiteri2020,
  author    = {Pierre Spiteri},
  title     = {Parallel asynchronous algorithms: A survey},
  doi       = {10.1016/j.advengsoft.2020.102896},
  pages     = {102896},
  volume    = {149},
  abstract  = {This paper deals with a synthetic presentation of parallel iterative asynchronous algorithms and their extensions for the solution of large sparse linear or pseudo-linear algebraic systems eventually constrained. The behavior of these iterative parallel asynchronous algorithms is studied by three distinct methods : contraction property, partial ordering property linked to the discrete maximum principle and nested sets; the link between these three kinds of analysis is presented. Stopping tests of the iterations are presented both from computer science and from numerical analysis approach including in this last case approximate contraction property, partial ordering property linked to the discrete maximum principle and nested sets. The principle of implementation of these parallel asynchronous iterative methods is described for subdomain method without overlapping and for subdomain method with overlapping; the use of load balancing approach for asynchronous parallel algorithms is also discussed. Various applications modelled by linear equations or pseudo linear equations and solved by such parallel algorithms are presented as well as the uses of these methods in computer security and Boolean calculation. The efficiency of parallel iterative asynchronous algorithms is also discussed.},
  journal   = {Advances in Engineering Software},
  month     = {11},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Iwashita2020,
  author      = {Takeshi Iwashita and Kengo Suzuki and Takeshi Fukaya},
  date        = {2020-09-16},
  title       = {An Integer Arithmetic-Based Sparse Linear Solver Using a GMRES Method and Iterative Refinement},
  eprint      = {2009.07495},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we develop a (preconditioned) GMRES solver based on integer arithmetic, and introduce an iterative refinement framework for the solver. We describe the data format for the coefficient matrix and vectors for the solver that is based on integer or fixed-point numbers. To avoid overflow in calculations, we introduce initial scaling and logical shifts (adjustments) of operands in arithmetic operations. We present the approach for operand shifts, considering the characteristics of the GMRES algorithm. Numerical tests demonstrate that the integer arithmetic-based solver with iterative refinement has comparable solver performance in terms of convergence to the standard solver based on floating-point arithmetic. Moreover, we show that preconditioning is important, not only for improving convergence but also reducing the risk of overflow.},
  file        = {:http\://arxiv.org/pdf/2009.07495v1:PDF},
  keywords    = {math.NA, cs.MS, cs.NA},
}

@Article{Blanco2020,
  author      = {Mark P. Blanco and Scott McMillan and Tze Meng Low},
  date        = {2020-09-16},
  title       = {Towards an Objective Metric for the Performance of Exact Triangle Count},
  eprint      = {2009.07935},
  eprintclass = {cs.PF},
  eprinttype  = {arXiv},
  abstract    = {The performance of graph algorithms is often measured in terms of the number of traversed edges per second (TEPS). However, this performance metric is inadequate for a graph operation such as exact triangle counting. In triangle counting, execution times on graphs with a similar number of edges can be distinctly different as demonstrated by results from the past Graph Challenge entries. We discuss the need for an objective performance metric for graph operations and the desired characteristics of such a metric such that it more accurately captures the interactions between the amount of work performed and the capabilities of the hardware on which the code is executed. Using exact triangle counting as an example, we derive a metric that captures how certain techniques employed in many implementations improve performance. We demonstrate that our proposed metric can be used to evaluate and compare multiple approaches for triangle counting, using a SIMD approach as a case study against a scalar baseline.},
  file        = {:http\://arxiv.org/pdf/2009.07935v2:PDF},
  keywords    = {cs.PF},
}

@Article{Yasar2020,
  author      = {Abdurrahman Yaşar and Muhammed Fatih Balin and Xiaojing An and Kaan Sancak and {\"U}mitV. Çatalyürek},
  date        = {2020-09-16},
  title       = {On Symmetric Rectilinear Matrix Partitioning},
  eprint      = {2009.07735},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Even distribution of irregular workload to processing units is crucial for efficient parallelization in many applications. In this work, we are concerned with a spatial partitioning called rectilinear partitioning (also known as generalized block distribution) of sparse matrices. More specifically, in this work, we address the problem of symmetric rectilinear partitioning of a square matrix. By symmetric, we mean the rows and columns of the matrix are identically partitioned yielding a tiling where the diagonal tiles (blocks) will be squares. We first show that the optimal solution to this problem is NP-hard, and we propose four heuristics to solve two different variants of this problem. We present a thorough analysis of the computational complexities of those proposed heuristics. To make the proposed techniques more applicable in real life application scenarios, we further reduce their computational complexities by utilizing effective sparsification strategies together with an efficient sparse prefix-sum data structure. We experimentally show the proposed algorithms are efficient and effective on more than six hundred test matrices. With sparsification, our methods take less than 3 seconds in the Twitter graph on a modern 24 core system and output a solution whose load imbalance is no worse than 1\%.},
  file        = {:http\://arxiv.org/pdf/2009.07735v1:PDF},
  keywords    = {cs.DS},
}

@Article{Zhang2020g,
  author      = {Chenglong Zhang},
  date        = {2020-09-16},
  title       = {A New Perspective of Graph Data and A Generic and Efficient Method for Large Scale Graph Data Traversal},
  eprint      = {2009.07463},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {The BFS algorithm is a basic graph data processing algorithm and many other graph data processing algorithms have similar architectural features with BFS algorithm and can be built on the basis of BFS algorithm model. We analyze the differences between graph algorithms and traditional high-performance algorithms in detail, propose a new way of classifying algorithms into data independent algorithm and data correlation algorithm based on their run-time correlation with data, and use this new classification to explain the validity of the methods proposed in this paper. Through a deeper analysis of graph data, we propose a new fundamental perspective on understanding graph data, establishing a link between two basic data structures, graph and tree, and viewing graph data as consisting of smaller subgraphs and edge trees. Small degree vertices are found to be one of important cause of random memory access. Based on this, we propose a general, easy to implement, and efficient method for graph data processing, with the basic idea of treating low-degree vertices and core subgraphs separately, thus significantly reducing the size of random memory access and improving the efficiency of memory access. Finally, we evaluated the performance of the method on three major data center computing platforms (Intel, AMD, and ARM), and the experiments showed that it brought 19.7\%, 31.8\% and 17.9\% performance improvement, respectively, with a performance-power ratio of 282.70 MTEPS/s on the ARM platform, ranking it among the Green graph500 in November 2019. World No. 1 on the big dataset list.},
  file        = {:http\://arxiv.org/pdf/2009.07463v2:PDF},
  keywords    = {cs.DC},
}

@Article{Sofranac2020,
  author      = {Boro Sofranac and Ambros Gleixner and Sebastian Pokutta},
  date        = {2020-09-16},
  title       = {Accelerating Domain Propagation: an Efficient GPU-Parallel Algorithm over Sparse Matrices},
  eprint      = {2009.07785},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Fast domain propagation of linear constraints has become a crucial component of today's best algorithms and solvers for mixed integer programming and pseudo-boolean optimization to achieve peak solving performance. Irregularities in the form of dynamic algorithmic behaviour, dependency structures, and sparsity patterns in the input data make efficient implementations of domain propagation on GPUs and, more generally, on parallel architectures challenging. This is one of the main reasons why domain propagation in state-of-the-art solvers is single thread only. In this paper, we present a new algorithm for domain propagation which (a) avoids these problems and allows for an efficient implementation on GPUs, and is (b) capable of running propagation rounds entirely on the GPU, without any need for synchronization or communication with the CPU. We present extensive computational results which demonstrate the effectiveness of our approach and show that ample speedups are possible on practically relevant problems: on state-of-the-art GPUs, our geometric mean speed-up for reasonably-large instances is around 10x to 20x and can be as high as 195x on favorably-large instances.},
  file        = {:http\://arxiv.org/pdf/2009.07785v2:PDF},
  keywords    = {cs.DC, cs.DM, cs.DS, cs.MS, math.OC},
}

@Article{Doikov2020b,
  author      = {Nikita Doikov and Yurii Nesterov},
  date        = {2020-09-18},
  title       = {Affine-invariant contracting-point methods for Convex Optimization},
  eprint      = {2009.08894},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we develop new affine-invariant algorithms for solving composite convex minimization problems with bounded domain. We present a general framework of Contracting-Point methods, which solve at each iteration an auxiliary subproblem restricting the smooth part of the objective function onto contraction of the initial domain. This framework provides us with a systematic way for developing optimization methods of different order, endowed with the global complexity bounds. We show that using an appropriate affine-invariant smoothness condition, it is possible to implement one iteration of the Contracting-Point method by one step of the pure tensor method of degree $p \geq 1$. The resulting global rate of convergence in functional residual is then ${\cal O}(1 / k^p)$, where $k$ is the iteration counter. It is important that all constants in our bounds are affine-invariant. For $p = 1$, our scheme recovers well-known Frank-Wolfe algorithm, providing it with a new interpretation by a general perspective of tensor methods. Finally, within our framework, we present efficient implementation and total complexity analysis of the inexact second-order scheme $(p = 2)$, called Contracting Newton method. It can be seen as a proper implementation of the trust-region idea. Preliminary numerical results confirm its good practical performance both in the number of iterations, and in computational time.},
  file        = {:http\://arxiv.org/pdf/2009.08894v1:PDF},
  keywords    = {math.OC},
}

@Article{Zounon2020,
  author     = {Zounon, Mawussi and Higham, Nicholas J. and Lucas,Craig and Tisseur, Françoise},
  date       = {2020},
  title      = {Performance Evaluation of Mixed PrecisionAlgorithms for Solving Sparse Linear Systems},
  eprint     = {MIMS EPrint:2020.21},
  eprinttype = {MIMS EPrint},
  url        = {http://eprints.maths.manchester.ac.uk/2783/1/paper.pdf},
  abstract   = {It is well established that mixed precision algorithms that factorize a matrix at a precision lower than the working precision can reduce the execution time and the energy consumption of parallel solvers for dense linear systems. Much less is known about the efficiency of mixed precision parallel algorithms for sparse linear systems, and existing work focuses on single core experiments. We evaluate the benefits of using single precision arithmetic in solving a double precision sparse linear systems using multiple cores, focusing on the key components of LU factorization and matrix–vector products. We find that single precision sparse LU factorization is prone to a severe loss of performance due to the intrusion of subnormal numbers. We identify a mechanism that allows cascading fill-ins to generate subnormal numbers and show that automatically flushing subnormals to zero avoids the performance penalties. Our results show that the anticipated speedup of 2 over a double precision LU factorization is obtained only for the very largest of our test problems. For iterative solvers, we find that for the majority of the matrices computing or applying incomplete factorization preconditioners in single precision does not present sufficient performance benefits to justify the loss of accuracy compared with the use of double precision. We also find that using single precision for the matrix–vector product kernels provides an average speedup of 1.5 over double precision kernels, but new mixed precision algorithms are needed to exploit this benefit without losing the performance gain in the process of refining the solution to double precision accuracy.},
}

@Article{Montoison2020,
  author      = {Alexis Montoison and Dominique Orban},
  date        = {2020-08-28},
  title       = {{TriCG} and {TriMR}: Two Iterative Methods for Symmetric Quasi-Definite Systems},
  doi         = {10.13140/RG.2.2.12344.16645},
  eprint      = {2008.12863},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {We introduce iterative methods named TriCG and TriMR for solving symmetric quasi-definite systems based on the orthogonal tridiagonalization process proposed by Saunders, Simon and Yip in 1988. TriCG and TriMR are tantamount to preconditioned block-CG and block-MINRES with two right-hand sides in which the two approximate solutions are summed at each iteration, but require less storage and work per iteration. We evaluate the performance of TriCG and TriMR on linear systems generated from the SuiteSparse Matrix Collection and from discretized and stablized Stokes equations. We compare TriCG and TriMR with SYMMLQ and MINRES, the recommended Krylov methods for symmetric and indefinite systems. In all our experiments, TriCG and TriMR terminate earlier than SYMMLQ and MINRES on a residual-based stopping condition with an improvement of up to 50\% in terms of number of iterations.},
  file        = {:http\://arxiv.org/pdf/2008.12863v1:PDF},
  keywords    = {math.NA, cs.MS, cs.NA, math.OC, 15A06, 65F10, 65F08, 65F22, 65F25, 65F35, 65F50, 90C06, 90C90},
}

@Article{Yasar2020a,
  author      = {Abdurrahman Yaşar and Sivasankaran Rajamanickam and Jonathan Berry and {\"U}mit V. Çatalyürek},
  date        = {2020-09-25},
  title       = {A Block-Based Triangle Counting Algorithm on Heterogeneous Environments},
  eprint      = {2009.12457},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  abstract    = {Triangle counting is a fundamental building block in graph algorithms. In this paper, we propose a block-based triangle counting algorithm to reduce data movement during both sequential and parallel execution. Our block-based formulation makes the algorithm naturally suitable for heterogeneous architectures. The problem of partitioning the adjacency matrix of a graph is well-studied. Our task decomposition goes one step further: it partitions the set of triangles in the graph. By streaming these small tasks to compute resources, we can solve problems that do not fit on a device. We demonstrate the effectiveness of our approach by providing an implementation on a compute node with multiple sockets, cores and GPUs. The current state-of-the-art in triangle enumeration processes the Friendster graph in 2.1 seconds, not including data copy time between CPU and GPU. Using that metric, our approach is 20 percent faster. When copy times are included, our algorithm takes 3.2 seconds. This is 5.6 times faster than the fastest published CPU-only time.},
  file        = {:http\://arxiv.org/pdf/2009.12457v1:PDF},
  keywords    = {cs.DS},
}

@InProceedings{DomenechAsensi2020,
  author    = {Gines Domenech-Asensi and Tom J. Kazmierski},
  booktitle = {Proceedings of the 2020 {IEEE} International Symposium on Circuits and Systems},
  title     = {Stability and Efficiency of Explicit Integration in Interconnect Analysis on {GPUs}},
  doi       = {10.1109/iscas45731.2020.9181157},
  publisher = {{IEEE}},
  series    = {ISCAS 2020},
  abstract  = {This paper presents a new high-performance technique to parallelise numeric integration of large VLSI interconnect analog models on a general purpose GPU. The technique is based on the combination of space-state formulation with an explicit integration method based on the Adams-Bashforth second order formula. The paper studies the stability of the variable step explicit method and proposes a technique to guarantee integration stability specifically for interconnect systems. Although explicit methods require smaller integration steps compared to those of the traditional implicit techniques, they avoid the complex calculations inherent to implicit integration. The proposed approach is demonstrated using an RC VLSI interconnect model and results are compared to those achieved by Ngspice, a state-of-the-art implicit integration solver, implemented on the same parallel hardware. The results show that the speed-up of the parallelised explicit solution reaches one order of magnitude for large systems and is increasing with the circuit size.},
  month     = {10},
  year      = {2020},
}

@Article{Aliaga2020,
  author      = {José I. Aliaga and Hartwig Anzt and Thomas Grützmacher and Enrique S. Quintana-Ortí and Andrés E. Tomás},
  date        = {2020-09-25},
  title       = {Compressed Basis GMRES on High Performance GPUs},
  eprint      = {2009.12101},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  abstract    = {Krylov methods provide a fast and highly parallel numerical tool for the iterative solution of many large-scale sparse linear systems. To a large extent, the performance of practical realizations of these methods is constrained by the communication bandwidth in all current computer architectures, motivating the recent investigation of sophisticated techniques to avoid, reduce, and/or hide the message-passing costs (in distributed platforms) and the memory accesses (in all architectures). This paper introduces a new communication-reduction strategy for the (Krylov) GMRES solver that advocates for decoupling the storage format (i.e., the data representation in memory) of the orthogonal basis from the arithmetic precision that is employed during the operations with that basis. Given that the execution time of the GMRES solver is largely determined by the memory access, the datatype transforms can be mostly hidden, resulting in the acceleration of the iterative step via a lower volume of bits being retrieved from memory. Together with the special properties of the orthonormal basis (whose elements are all bounded by 1), this paves the road toward the aggressive customization of the storage format, which includes some floating point as well as fixed point formats with little impact on the convergence of the iterative process. We develop a high performance implementation of the "compressed basis GMRES" solver in the Ginkgo sparse linear algebra library and using a large set of test problems from the SuiteSparse matrix collection we demonstrate robustness and performance advantages on a modern NVIDIA V100 GPU of up to 50\% over the standard GMRES solver that stores all data in IEEE double precision.},
  file        = {:http\://arxiv.org/pdf/2009.12101v1:PDF},
  keywords    = {cs.MS},
}

@Article{Facca2020,
  author      = {Enrico Facca and Michele Benzi},
  date        = {2020-09-28},
  title       = {Fast Iterative Solution of the Optimal Transport Problem on Graphs},
  eprint      = {2009.13478},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we address the numerical solution of the Optimal Transport Problem on undirected weighted graphs, taking the shortest path distance as transport cost. The optimal solution is obtained from the long-time limit of the gradient descent dynamics. Among different time stepping procedures for the discretization of this dynamics, a backward Euler time stepping scheme combined with the inexact Newton-Raphson method results in a robust and accurate approach for the solution of the Optimal Transport Problem on graphs. It is found experimentally that the algorithm requires solving between $\mathcal{O}(1)$ and $\mathcal{O}(M^{0.36})$ linear systems involving weighted Laplacian matrices, where $M$ is the number of edges. These linear systems are solved via algebraic multigrid methods, resulting in an efficient solver for the Optimal Transport Problem on graphs.},
  file        = {:http\://arxiv.org/pdf/2009.13478v1:PDF},
  keywords    = {math.NA, cs.NA},
}

@Article{Roy2020,
  author      = {Abhishek Roy and Krishnakumar Balasubramanian and Saeed Ghadimi and Prasant Mohapatra},
  date        = {2020-09-28},
  title       = {Escaping Saddle-Points Faster under Interpolation-like Conditions},
  eprint      = {2009.13016},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we show that under over-parametrization several standard stochastic optimization algorithms escape saddle-points and converge to local-minimizers much faster. One of the fundamental aspects of over-parametrized models is that they are capable of interpolating the training data. We show that, under interpolation-like assumptions satisfied by the stochastic gradients in an over-parametrization setting, the first-order oracle complexity of Perturbed Stochastic Gradient Descent (PSGD) algorithm to reach an $\epsilon$-local-minimizer, matches the corresponding deterministic rate of $\tilde{\mathcal{O}}(1/\epsilon^{2})$. We next analyze Stochastic Cubic-Regularized Newton (SCRN) algorithm under interpolation-like conditions, and show that the oracle complexity to reach an $\epsilon$-local-minimizer under interpolation-like conditions, is $\tilde{\mathcal{O}}(1/\epsilon^{2.5})$. While this obtained complexity is better than the corresponding complexity of either PSGD, or SCRN without interpolation-like assumptions, it does not match the rate of $\tilde{\mathcal{O}}(1/\epsilon^{1.5})$ corresponding to deterministic Cubic-Regularized Newton method. It seems further Hessian-based interpolation-like assumptions are necessary to bridge this gap. We also discuss the corresponding improved complexities in the zeroth-order settings.},
  file        = {:http\://arxiv.org/pdf/2009.13016v1:PDF},
  keywords    = {stat.ML, cs.LG, math.OC, math.ST, stat.TH},
}

@Article{Zachariadis2020,
  author    = {Orestis Zachariadis and Nitin Satpute and Juan G{\'{o}}mez-Luna and Joaqu{\'{\i}}n Olivares},
  title     = {Accelerating sparse matrix{\textendash}matrix multiplication with {GPU} Tensor Cores},
  doi       = {10.1016/j.compeleceng.2020.106848},
  pages     = {106848},
  volume    = {88},
  abstract  = {parse general matrix–matrix multiplication (spGEMM) is an essential component in many scientific and data analytics applications. However, the sparsity pattern of the input matrices and the interaction of their patterns make spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which specialize in dense matrix multiplication. Our aim is to re-purpose TCUs for sparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply sparse rectangular blocks using the mixed precision mode of TCUs. tSparse partitions the input matrices into tiles and operates only on tiles which contain one or more elements. It creates a task list of the tiles, and performs matrix multiplication of these tiles using TCUs. To the best of our knowledge, this is the first time that TCUs are used in the context of spGEMM. We show that spGEMM, with our tiling approach, benefits from TCUs. Our approach significantly improves the performance of spGEMM in comparison to cuSPARSE, CUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.},
  journal   = {Computers {\&} Electrical Engineering},
  month     = {12},
  publisher = {Elsevier {BV}},
  year      = {2020},
}

@Article{Herholz2020,
  author       = {Philipp Herholz and Olga Sorkine-Hornung},
  date         = {2020},
  journaltitle = {ACM Transaction son Computer Graphics},
  title        = {Sparse Cholesky Updates for Interactive Mesh Parameterization},
  number       = {6},
  url          = {https://igl.ethz.ch/projects/sparse-cholesky-update/sparse-cholesky-update-paper.pdf},
  volume       = {39},
  abstract     = {We present a novel linear solver for interactive parameterization tasks. Our method is based on the observation that quasi-conformal parameterizations of a triangle mesh are largely determined by boundary conditions. These boundary conditions are typically constructed interactively by users, who have to take several artistic and geometric constraints into account while introducing cuts on the geometry. Commonly, the main computational burden in these methods is solving a linear system every time new boundary conditions are imposed. The core of our solver is a novel approach to efficiently update the Cholesky factorization of the linear system to reflect new boundary conditions, thereby enabling a seamless and interactive workflow even for large meshes consisting of several millions of vertices},
}

@Article{Anwer2020,
  author       = {Abdul Rehman Anwer and Guanpeng Li and Karthik Pattabiraman and Michael Sullivan and Timothy Tsai and Siva Kumar Sastry Hari},
  date         = {2020},
  journaltitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage, and Analysis},
  title        = {{GPU}-Trident: Efficient Modeling of ErrorPropagation in {GPU} Programs},
  issn         = {978-1-7281-9998-6/20/31.00},
  series       = {SC 20},
  url          = {http://blogs.ubc.ca/karthik/files/2020/08/SC2020-final.pdf},
  abstract     = {Fault injection (FI) techniques are typically used to determine the reliability profiles of programs under soft errors. However, these techniques are highly resource- and time-intensive. Prior research developed a model, TRIDENT to analytically predict Silent Data Corruption (SDC, i.e., incorrect output without any indication) probabilities of single-threaded CPU applications without requiring FIs. Unfortunately, TRIDENT is incompatible with GPU programs, due to their high degree of parallelism and different memory architectures than CPU programs. The main challenge is that modeling error propagation across thousands of threads in a GPU kernel requires enormous amounts of data to be profiled and analyzed, posing a major scalability bottleneck for HPC applications.\\ In this paper, we propose GPU-TRIDENT, an accurate and scalable technique for modeling error propagation in GPU programs. We find that GPU-TRIDENT is 2 orders of magnitude faster than FI-based approaches, and nearly as accurate in determining the SDC rate of GPU programs.},
}

@InProceedings{Lee2020,
  author    = {Hochan Lee and David Wong and Loc Hoang and Roshan Dathathri and Gurbinder Gill and Vishwesh Jatala and David Kuck and Keshav Pingali},
  booktitle = {Proceedings of the 2020 IEEE International Symposium on Workload Characterization},
  date      = {2020},
  title     = {A Study of APIs for Graph Analytics Workloads},
  series    = {IISWC 2020},
  abstract  = {Traditionally, parallel graph analytics workloads have been implemented in systems like Pregel, GraphLab, Galois, and Ligra that support graph data structures and graph operations directly. An alternative approach is to express graph workloads in terms of sparse matrix kernels such as sparse matrix-vector and matrix-matrix multiplication. An API for these kernels has been defined by the GraphBLAS project. The SuiteSparse project has implemented this API on shared-memory platforms, and the LAGraph project is building a library of graph algorithms using this API.\\ How does the matrix-based approach perform compared to the graph-based approach? Our experiments on a 56 core CPU show that for representative graph workloads, LAGraph/SuiteSparse solutions are 5$\times$ slower on the average than Galois solutions. We argue that this performance gap arises from inherent limitations of a matrix-based API: regardless of which architecture a matrixbased algorithm is run on, it is subject to the same inherent limitations of the matrix-based API.},
}

@Article{Selvitopi2020a,
  author      = {Oguz Selvitopi and Saliya Ekanayake and Giulia Guidi and Georgios Pavlopoulos and Ariful Azad and Aydin Buluc},
  date        = {2020-09-30},
  title       = {Distributed Many-to-Many Protein Sequence Alignment using Sparse Matrices},
  eprint      = {2009.14467},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Identifying similar protein sequences is a core step in many computational biology pipelines such as detection of homologous protein sequences, generation of similarity protein graphs for downstream analysis, functional annotation and gene location. Performance and scalability of protein similarity searches have proven to be a bottleneck in many bioinformatics pipelines due to increases in cheap and abundant sequencing data. This work presents a new distributed-memory software, PASTIS. PASTIS relies on sparse matrix computations for efficient identification of possibly similar proteins. We use distributed sparse matrices for scalability and show that the sparse matrix infrastructure is a great fit for protein similarity searches when coupled with a fully-distributed dictionary of sequences that allows remote sequence requests to be fulfilled. Our algorithm incorporates the unique bias in amino acid sequence substitution in searches without altering the basic sparse matrix model, and in turn, achieves ideal scaling up to millions of protein sequences.},
  file        = {:http\://arxiv.org/pdf/2009.14467v1:PDF},
  keywords    = {cs.DC, q-bio.GN},
}

@InProceedings{Azad2020a,
  author    = {Ariful Azad and Mohsen Mahmoudi Aznaveh and Scott Beamer and Mark Blanco and Jinhao Chen and Luke D’Alessandro and Roshan Dathathri and Tim Davis and Kevin Deweese and Jesun Firoz and Henry A Gabb and Gurbinder Gill and Balint Hegyi and Scott Kolodzie and Tze Meng Low and Andrew Lumsdaine and Tugsbayasgalan Manlaibaatar and Timothy G Mattson and Scott McMillan and Ramesh Peri and Keshav Pingali and Upasana Sridhar and Gabor Szarnyas and Yunming Zhang and Yongzhe Zhang},
  booktitle = {Proceedings of the 2020 IEEE International Symposium on Workload Characterization},
  date      = {2020},
  title     = {Evaluation of Graph Analytics Frameworks Using the {GAP} Benchmark Suite,},
  series    = {IISWC 2020},
  url       = {https://www.cs.utexas.edu/~roshan/GraphAnalyticsFrameworksStudy.pdf},
  abstract  = {Graphs play a key role in data analytics. Graphs and the software systems used to work with them are highly diverse. Algorithms interact with hardware in different ways and which graph solution works best on a given platform changes with the structure of the graph. This makes it difficult to decide which graph programming framework is the best for a given situation. In this paper, we try to make sense of this diverse landscape. We evaluate five different frameworks for graph analytics: SuiteSparse GraphBLAS, Galois, the NWGraph library, the Graph Kernel Collection (GKC), and GraphIt. We use the GAP Benchmark Suite to evaluate each framework. GAP consists of 30 tests: six graph algorithms (breadth-first search, single-source shortest path, PageRank, betweenness centrality, connected components, and triangle counting) on five graphs. The GAP Benchmark Suite includes high-performance reference implementations to provide a performance baseline for comparison. Our results show the relative strengths of each framework, but also serve as a case study for the challenges of establishing objective measures for comparing graph frameworks.},
}

@Article{Gratton2020,
  author    = {Serge Gratton and Ehouarn Simon and David Titley-Peloquin and Philippe L. Toint},
  title     = {Minimizing convex quadratics with variable precision conjugate gradients},
  doi       = {10.1002/nla.2337},
  abstract  = {We investigate the method of conjugate gradients, exploiting inaccurate matrix‐vector products, for the solution of convex quadratic optimization problems. Theoretical performance bounds are derived, and the necessary quantities occurring in the theoretical bounds estimated, leading to a practical algorithm. Numerical experiments suggest that this approach has significant potential, including in the steadily more important context of multiprecision computations.},
  journal   = {Numerical Linear Algebra with Applications},
  month     = {10},
  publisher = {Wiley},
  year      = {2020},
}

@Article{Hermans2020,
  author      = {Ben Hermans and Andreas Themelis and Panagiotis Patrinos},
  date        = {2020-10-06},
  title       = {QPALM: A Proximal Augmented Lagrangian Method for Nonconvex Quadratic Programs},
  eprint      = {2010.02653},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We propose QPALM, a nonconvex quadratic programming (QP) solver based on the proximal augmented Lagrangian method. This method solves a sequence of inner subproblems which can be enforced to be strongly convex and which therefore admit of a unique solution. The resulting steps are shown to be equivalent to inexact proximal point iterations on the extended-real-valued cost function. Furthermore, we prove global convergence of such iterations to a stationary point at an R-linear rate in the specific case of a (possibly nonconvex) QP. The QPALM algorithm solves the subproblems iteratively using semismooth Newton directions and an exact linesearch. The former can be computed efficiently in most iterations by making use of suitable factorization update routines, while the latter requires the zero of a monotone, piecewise affine function. QPALM is implemented in open-source C code, with tailored linear algebra routines for the factorization in a self-written package LADEL. The resulting implementation is shown to be extremely robust in numerical simulations, solving all of the Maros-Meszaros problems and finding a stationary point for most of the nonconvex QPs in the Cutest test set. Furthermore, it is shown to be competitive against state-of-the-art convex QP solvers in typical QPs arising from application domains such as portfolio optimization and model predictive control. As such, QPALM strikes a unique balance between solving both easy and hard problems efficiently.},
  file        = {:http\://arxiv.org/pdf/2010.02653v1:PDF},
  keywords    = {math.OC, 90C05, 90C20, 90C26, 49J53, 49M15},
}

@Article{Gower2020a,
  author      = {Robert M. Gower and Mark Schmidt and Francis Bach and Peter Richtarik},
  date        = {2020-10-02},
  title       = {Variance-Reduced Methods for Machine Learning},
  eprint      = {2010.00892},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a method introduced over 60 years ago. The last 8 years have seen an exciting new development: variance reduction (VR) for stochastic optimization methods. These VR methods excel in settings where more than one pass through the training data is allowed, achieving a faster convergence than SGD in theory as well as practice. These speedups underline the surge of interest in VR methods and the fast-growing body of work on this topic. This review covers the key principles and main developments behind VR methods for optimization with finite data sets and is aimed at non-expert readers. We focus mainly on the convex setting, and leave pointers to readers interested in extensions for minimizing non-convex functions.},
  file        = {:http\://arxiv.org/pdf/2010.00892v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML, 65K05, 68T99, G.1.6},
}

@TechReport{Nesi2020,
  author      = {Lucas Leandro Nesi and Vinicius Garcia Pinto and Marcelo Cogo Miletto and Lucas Mello Schnorr},
  date        = {2020},
  institution = {Université Grenoble Alpes, CNRS, Inria},
  title       = {Distributed-memory multi-{GPU} block-sparse tensor contraction for electronic structure},
  eprint      = {02960848},
  eprinttype  = {HAL},
  url         = {https://hal.inria.fr/hal-02960848/document},
  abstract    = {High-performance computing (HPC) applications enable the solution of computeintensive problems in feasible time. Among many HPC paradigms, task-based programming has gathered community attention in recent years. This paradigm enables constructing an HPC application using a more declarative approach, structuring it in a direct acyclic graph (DAG). The performance evaluation of these applications is as hard as in any other programming paradigm. Understanding how to analyze these applications, employing the DAG and runtime metrics, presents opportunities to improve its performance. This article describes the StarVZ R-package available on CRAN for performance analysis of task-based applications. StarVZ enables transforms runtime trace data into different visualizations of the application behavior. An analyst can understand their applications’ performance limitations and compare multiple executions. StarVZ has been successfully applied to several study-cases, showing its applicability in a number of scenarios.},
}

@TechReport{Quirynen2020,
  author      = {Rien Quirynen and Stefano Di Cairano},
  date        = {2020},
  institution = {Mitsubishi Electric Research Laboratories (MERL)},
  title       = {Block-Structured Preconditioning of Iterative Solverswithin a Primal Active-Set Method for fast {MPC}},
  eprint      = {TR2020-134},
  eprinttype  = {TR},
  url         = {https://www.merl.com/publications/docs/TR2020-134.pdf},
  abstract    = {Model predictive control (MPC) for linear dynamical systems requires solving an optimal control structured quadratic program (QP) at each sampling instant. This paper proposes a primal active-set strategy, called PRESAS, for the efficient solution of such block-sparse QPs, based on a preconditioned iterative solver to compute the search direction in each iteration. Rank-one factorization updates of the preconditioner result in a per-iteration computational complexity of $\mathcal{O}(Nm^2)$, where m denotes the number of state and control variables and N the number of control intervals. Three different block-structured preconditioning techniques are presented and their numerical properties are studied further. In addition, an augmented Lagrangian based implementation is proposed to avoid a costly initialization procedure to find a primal feasible starting point. Based on a standalone C code implementation, we illustrate the computational performance of PRESAS against current state of the art QP solvers for multiple linear and nonlinear MPC case studies. We also show that the solver is real-time feasible on a dSPACE MicroAutoBox-II rapid prototyping unit for vehicle control applications, and numerical reliability is illustrated based on experimental results from a testbench of small-scale autonomous vehicles.},
}

@PhdThesis{Edwars2020,
  author      = {James Alexander Edwars},
  date        = {2020},
  institution = {University of Maryland, Institute for Advanced Computer Studies and Department of Electrical and Computer Engineering},
  title       = {Study of fine-grained, irregular parallel applications on a many-core processor},
  url         = {https://drum.lib.umd.edu/bitstream/handle/1903/26626/Edwards_umd_0117E_21139.pdf},
  abstract    = {This dissertation demonstrates the possibility of obtaining strong speedups for a variety of parallel applications versus the best serial and parallel implementations on commodity platforms. These results were obtained using the PRAM-inspired Explicit Multi-Threading (XMT) many-core computing platform, which is designed to efficiently support execution of both serial and parallel code and switching between the two.},
}

@Article{Chennupati2020,
  author      = {Gopinath Chennupati and Nandakishore Santhi and Phill Romero and Stephan Eidenbenz},
  date        = {2020-10-08},
  title       = {Machine Learning Enabled Scalable Performance Prediction of Scientific Codes},
  eprint      = {2010.04212},
  eprintclass = {cs.PF},
  eprinttype  = {arXiv},
  abstract    = {We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input, predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks, finally present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of scientific application (radiation transport), SNAP. We analyze the application of multi-variate regression models that accurately predict the reuse profiles and the basic block counts. The predicted runtimes of SNAP when compared to that of actual times are accurate.},
  file        = {:http\://arxiv.org/pdf/2010.04212v1:PDF},
  keywords    = {cs.PF, cs.AR},
}

@Article{Kwasniewski2020,
  author      = {Grzegorz Kwasniewski and Tal Ben-Nun and Alexandros Nikolaos Ziogas and Timo Schneider and Maciej Besta and Torsten Hoefler},
  date        = {2020-10-12},
  title       = {On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal LU Factorization},
  eprint      = {2010.05975},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Dense linear algebra kernels, such as linear solvers or tensor contractions, are fundamental components of many scientific computing applications. In this work, we present a novel method of deriving parallel I/O lower bounds for this broad family of programs. Based on the X-partitioning abstraction, our method explicitly captures inter-statement dependencies. Applying our analysis to LU factorization, we derive COnfLUX, an LU algorithm with the parallel I/O cost of $N^3 / (P \sqrt{M})$ communicated elements per processor -- only $1/3\times$ over our established lower bound. We evaluate COnfLUX on various problem sizes, demonstrating empirical results that match our theoretical analysis, communicating asymptotically less than Cray ScaLAPACK or SLATE, and outperforming the asymptotically-optimal CANDMC library. Running on $1$,$024$ nodes of Piz Daint, COnfLUX communicates 1.6$\times$ less than the second-best implementation and is expected to communicate 2.1$\times$ less on a full-scale run on Summit.},
  file        = {:http\://arxiv.org/pdf/2010.05975v1:PDF},
  keywords    = {cs.DC},
}

@Article{Garmanjani2020,
  author    = {R. Garmanjani},
  title     = {A note on the worst-case complexity of nonlinear stepsize control methods for convex smooth unconstrained optimization},
  doi       = {10.1080/02331934.2020.1830088},
  pages     = {1--11},
  abstract  = {In this paper, we analyse the worst-case complexity of nonlinear stepsize control (NSC) algorithms for solving convex smooth unconstrained optimization problems. We show that, to drive the norm of the gradient below some given positive $\epsilon$, such methods take at most $O(\epsilon-1)$ iterations, which shows that the complexity bound for these methods is in parity with that of gradient descent methods for the same class of problems. As NSC algorithm is a generalization of several methods such as trust-region and adaptive cubic with regularization methods, such bound holds automatically for these methods as well.},
  journal   = {Optimization},
  month     = {10},
  publisher = {Informa {UK} Limited},
  year      = {2020},
}

@Article{Mor2020,
  author      = {Uria Mor and Haim Avron},
  date        = {2020-10-15},
  title       = {Solving Trust Region Subproblems Using Riemannian Optimization},
  eprint      = {2010.07547},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {The Trust Region Subproblem is a fundamental optimization problem that takes a pivotal role in Trust Region Methods. However, the problem, and variants of it, also arise in quite a few other applications. In this article, we present a family of globally convergent iterative Riemannian optimization algorithms for a variant of the Trust Region Subproblem that replaces the inequality constraint with an equality constraint. Our approach uses either a trivial or a non-trivial Riemannian geometry of the search-space, and requires only minimal spectral information about the quadratic component of the objective function. We further show how the theory of Riemannian optimization promotes a deeper understanding of the Trust Region Subproblem and its difficulties, e.g. a deep connection between the Trust Region Subproblem and the problem of finding affine eigenvectors, and a new examination of the so-called hard case in light of the condition number of the Riemannian Hessian operator at a global optimum. Finally, we propose to incorporate preconditioning via a careful selection of a variable Riemannian metric, and establish bounds on the asymptotic convergence rate in terms of how well the preconditioner approximates the input matrix.},
  file        = {:http\://arxiv.org/pdf/2010.07547v1:PDF},
  keywords    = {math.OC, cs.NA, math.NA},
}

@Article{Kornowski2020,
  author      = {Guy Kornowski and Ohad Shamir},
  date        = {2020-10-13},
  title       = {High-Order Oracle Complexity of Smooth and Strongly Convex Optimization},
  eprint      = {2010.06642},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this note, we consider the complexity of optimizing a highly smooth (Lipschitz $k$-th order derivative) and strongly convex function, via calls to a $k$-th order oracle which returns the value and first $k$ derivatives of the function at a given point, and where the dimension is unrestricted. Extending the techniques introduced in Arjevani et al. [2019], we prove that the worst-case oracle complexity for any fixed $k$ to optimize the function up to accuracy $\epsilon$ is on the order of $\left(\frac{\mu_k D^{k-1}}{\lambda}\right)^{\frac{2}{3k+1}}+\log\log\left(\frac{1}{\epsilon}\right)$ (up to log factors independent of $\epsilon$), where $\mu_k$ is the Lipschitz constant of the $k$-th derivative, $D$ is the initial distance to the optimum, and $\lambda$ is the strong convexity parameter.},
  file        = {:http\://arxiv.org/pdf/2010.06642v1:PDF},
  keywords    = {math.OC, cs.LG},
}

@Article{Hrga2020,
  author      = {Timotej Hrga and Janez Povh},
  date        = {2020-10-15},
  title       = {{MADAM}: A parallel exact solver for Max-Cut based on semidefinite programming and {ADMM}},
  eprint      = {2010.07839},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {We present MADAM, a parallel semidefinite based exact solver for Max-Cut, a problem of finding the cut with maximum weight in a given graph. The algorithm uses branch and bound paradigm that applies alternating direction method of multipliers as the bounding routine to solve the basic semidefinite relaxation strengthened by a subset of hypermetric inequalities. The benefit of the new approach is less computationally expensive update rule for the dual variable with respect to the inequality constraints. We provide theoretical convergence of the algorithm, as well as extensive computational experiments with this method, to show that our algorithm outperformes current state-of-the-art approaches. Furthermore, by combining algorithmic ingredients from the serial algorithm we develop an efficient distributed parallel solver based on MPI.},
  file        = {:http\://arxiv.org/pdf/2010.07839v1:PDF},
  keywords    = {math.OC},
}

@Article{Han2020a,
  author    = {Qingchang Han and Hailong Yang and Ming Dun and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
  title     = {Towards efficient tile low-rank {GEMM} computation on sunway many-core processors},
  doi       = {10.1007/s11227-020-03444-2},
  abstract  = {Tile low-rank general matrix multiplication (TLR GEMM) is a novel method of matrix multiplication on large data-sparse matrices, which can significantly reduce storage footprint and arithmetic complexity under given accuracy. To implement high-performance TLR GEMM on Sunway many-core processor, the following challenges remain to be addressed: 1) design an efficient parallel scheme; 2) provide an efficient kernel library of math functions commonly used in TLR GEMM. This paper proposes swTLR GEMM, an efficient implementation of TLR GEMM. We assign LR GEMM computation to a single computing processing element (CPE) and use grouped task queue to process different data tiles of the TLR matrix. Moreover, we implement an efficient kernel library (swLR Kernels) for low-rank matrix operations. To scale to massive (CGs), we organize the CGs into the CG grid and partition the matrices into blocks accordingly. We also apply Cannon’s algorithm to enable efficient communication when processing the matrix blocks across CGs simultaneously. The experiment results show that the DGEMM kernel in swLR Kernels achieves 102$\times$ speedup on average. In terms of overall performance, swTLR GEMM-LLD and swTLR GEMM-LLL achieve 91$\times$ and 20.1$\times$ speedup on average, respectively. In addition, our implementation of swTLR GEMM exhibits good scalability when running on 1,024 CGs of Sunway processors (66,560 cores in total).},
  journal   = {The Journal of Supercomputing},
  month     = {10},
  publisher = {Springer Science and Business Media {LLC}},
  year      = {2020},
}

@PhdThesis{SoltanMohammadi2020,
  author      = {Soltan Mohammadi, Mahdi},
  date        = {2020},
  institution = {Department of Computer Science, The University of Arizona},
  title       = {Automatic Sparse Computation Parallelization By Utilizing Domain-Specific Knowledge In Data Dependence Analysis},
  eprint      = {28149621},
  eprintclass = {ProQuest},
  abstract    = {Sparse vectors, matrices, and tensors are commonly used to compress nonzero values of big data manipulated in data analytics, scientific simulations, and machine learning computations. As with general computations, parallelization of loops in sparse computations, codes manipulating sparse structures, is essential to efficiently utilize available parallel architectures. The sparse computations often exhibit partial parallelism in loops that are sequential in the corresponding dense computation due to sparsity of data dependencies coming from indirect memory access through index arrays, e.g. col in val[col[j]]. Such dependencies can only be discovered at runtime when content of index arrays are available. Consequently, performance programmers typically use the inspector/executor strategy to take advantage of partial parallelism in sparse computation. There, programmers implement an inspector code that creates iteration dependency graph at runtime from which wavefronts of iterations are extracted and fed into a parallel version of the computation called an executor. The executor executes iteration waves sequentially to respect sparse dependencies while executing iterations inside each wavefront in parallel.\\ To automate the generation of the inspector and executor code, compiler-based loop-carried data dependency analysis is needed. However, straightforward automatically generated inspectors typically have significantly higher overhead than hand written optimized ones. Consequently, the specific problem that I am addressing in this dissertation is how can we automate the strategies used by expert programmers to generate efficient runtime inspectors for parallelizing sparse computation.\\ The overarching contribution of this dissertation is an approach for encoding index array properties for individual index arrays and relationships between index arrays as universally quantified constraints and using them in compiler-based data dependence analysis. The dependence analysis is then evaluated in the context of finding wavefront parallelism in sparse computations. More specifically, one contribution is an approach to automatically use index array properties to prove more data dependencies unsatisfiable, removing the need for inspecting them at runtime. Other contributions are methods to use the same properties to simplify compiletime-satisfiable dependences by finding equalities and subset relationships enabling generation of faster runtime inspectors. The last contribution includes compile-time methods for expanding opportunities for array privatization in sparse computations by defining an array as private if its contents start and end each iteration with the same value. Evaluation results show my approach is able to find seven fully parallel loops in seven sparse computations where previous compiler-based approach could not, and efficiently extract partial parallelism from outer most loops of five out of six sparse computations.},
}

@Article{Scott2020a,
  author       = {J. Scott and M Tůma},
  date         = {2020},
  journaltitle = {ACM Transactions on Mathematical Software},
  title        = {A computational study of using black-box {QR} solvers for large-scale sparse-dense linear least squares problems},
  eprint       = {RAL-P-2020-004},
  url          = {http://purl.org/net/epubs/manifestation/47616417/RAL-P-2020-004.pdf},
  abstract     = {Large-scale overdetermined linear least squares problems arise in many practical applications, both as subproblems of nonlinear least squares problems and in their own right. One popular solution method is based on the backward stable QR factorization of the system matrix A. This paper focuses on sparse-dense linear least squares problems, that is, problems where A is sparse except from a small number of rows that are considered to be dense. For large-scale problems, the direct application of a QR solver will fail because of a lack of memory or will be unacceptably slow. We study a number of approaches for solving such problems using a sparse QR solver without modication. We consider the case where the sparse part of A is rank-decient and show that either preprocessing A using partial matrix stretching or using regularization and employing a direct-iterative approach can be seamlessly combined with a black-box QR solver. Furthermore, we propose extending the augmented system formulation with iterative renement for sparse problems to sparse-dense problems and demonstrate experimentally that multi-precision variants can be successfully used.},
}

@Article{Li2020g,
  author      = {Min Li and Chuanfu Xiao and Chao Yang},
  date        = {2020-10-20},
  title       = {a-Tucker: Input-Adaptive and Matricization-Free Tucker Decomposition for Dense Tensors on CPUs and GPUs},
  eprint      = {2010.10131},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Tucker decomposition is one of the most popular models for analyzing and compressing large-scale tensorial data. Existing Tucker decomposition algorithms usually rely on a single solver to compute the factor matrices and core tensor, and are not flexible enough to adapt with the diversities of the input data and the hardware. Moreover, to exploit highly efficient GEMM kernels, most Tucker decomposition implementations make use of explicit matricizations, which could introduce extra costs in terms of data conversion and memory usage. In this paper, we present a-Tucker, a new framework for input-adaptive and matricization-free Tucker decomposition of dense tensors. A mode-wise flexible Tucker decomposition algorithm is proposed to enable the switch of different solvers for the factor matrices and core tensor, and a machine-learning adaptive solver selector is applied to automatically cope with the variations of both the input data and the hardware. To further improve the performance and enhance the memory efficiency, we implement a-Tucker in a fully matricization-free manner without any conversion between tensors and matrices. Experiments with a variety of synthetic and real-world tensors show that a-Tucker can substantially outperform existing works on both CPUs and GPUs.},
  file        = {:http\://arxiv.org/pdf/2010.10131v1:PDF},
  keywords    = {cs.DC, cs.AI},
}

@Article{Gottesbueren2020,
  author      = {Lars Gottesbüren and Tobias Heuer and Peter Sanders and Sebastian Schlag},
  date        = {2020-10-20},
  title       = {Scalable Shared-Memory Hypergraph Partitioning},
  eprint      = {2010.10272},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Hypergraph partitioning is an important preprocessing step for optimizing data placement and minimizing communication volumes in high-performance computing applications. To cope with ever growing problem sizes, it has become increasingly important to develop fast parallel partitioning algorithms whose solution quality is competitive with existing sequential algorithms. To this end, we present Mt-KaHyPar, the first shared-memory multilevel hypergraph partitioner with parallel implementations of many techniques used by the sequential, high-quality partitioning systems: a parallel coarsening algorithm that uses parallel community detection as guidance, initial partitioning via parallel recursive bipartitioning with work-stealing, a scalable label propagation refinement algorithm, and the first fully-parallel direct $k$-way formulation of the classical FM algorithm. Experiments performed on a large benchmark set of instances from various application domains demonstrate the scalability and effectiveness of our approach. With 64 cores, we observe self-relative speedups of up to 51 and a harmonic mean speedup of 23.5. In terms of solution quality, we outperform the distributed hypergraph partitioner Zoltan on 95\% of the instances while also being a factor of 2.1 faster. With just four cores,Mt-KaHyPar is also slightly faster than the fastest sequential multilevel partitioner PaToH while producing better solutions on 83\% of all instances. The sequential high-quality partitioner KaHyPar still finds better solutions than our parallel approach, especially when using max-flow-based refinement. This, however, comes at the cost of considerably longer running times.},
  file        = {:http\://arxiv.org/pdf/2010.10272v2:PDF},
  keywords    = {cs.DC},
}

@InProceedings{Harris2020,
  author    = {Steven Harris and Roger D. Chamberlain and Christopher Gill},
  booktitle = {Proceedings of the IEEE High-Performance Extreme Computing Conference},
  date      = {2020},
  title     = {{OpenCL} Performance on the Intel Heterogeneous Architecture Research Platform},
  series    = {HPEC 2020},
  url       = {https://www.cse.wustl.edu/~roger/papers/hcg20.pdf},
  abstract  = {The fundamental operation of matrix multiplication is ubiquitous across a myriad of disciplines. Yet, the identification of new optimizations for matrix multiplication remains relevant for emerging hardware architectures and heterogeneous systems. Frameworks such as OpenCL enable computation orchestration on existing systems, and its availability using the Intel High Level Synthesis compiler allows users to architect new designs for reconfigurable hardware using C/C++. Using the HARPv2 as a vehicle for exploration, we investigate the utility of several traditional matrix multiplication optimizations to better understand the performance portability of OpenCL and the implications for such optimizations on cache coherent heterogeneous architectures. Our results give targeted insights into the applicability of best practices that were designed for existing architectures when used on emerging heterogeneous systems.},
}

@Article{Liang2020,
  author      = {Ling Liang and Defeng Sun and Kim-Chuan Toh},
  date        = {2020-10-17},
  title       = {An Inexact Augmented Lagrangian Method for Second-order Cone Programming with Applications},
  eprint      = {2010.08772},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we adopt the augmented Lagrangian method (ALM) to solve convex quadratic second-order cone programming problems (SOCPs). Fruitful results on the efficiency of the ALM have been established in the literature. Recently, it has been shown by Cui et al. (2019) that, if the quadratic growth condition holds at an optimal solution for the dual problem, then the KKT residual converges to zero R-superlinearly when the ALM is applied to the primal problem. Moreover, Cui et al (2017) provided sufficient conditions for the quadratic growth condition to hold under the metric subregularity and bounded linear regularity conditions for solving composite matrix optimization problems involving spectral functions. Here, we adopt these recent ideas to analyze the convergence properties of the ALM when applied to SOCPs. To the best of our knowledge, no similar work has been done for SOCPs so far. In our paper, we first provide sufficient conditions to ensure the quadratic growth condition for SOCPs. With these elegant theoretical guarantees, we then design an SOCP solver and apply it to solve various classes of SOCPs such as minimal enclosing ball problems, classic trust-region subproblems, square-root Lasso problems, and DIMACS Challenge problems. Numerical results show that the proposed ALM based solver is efficient and robust compared to the existing highly developed solvers such as Mosek and SDPT3.},
  file        = {:http\://arxiv.org/pdf/2010.08772v1:PDF},
  keywords    = {math.OC, 90C06, 90C22, 90C25},
}

@Article{Hussain2020,
  author      = {Md Taufique Hussain and Oguz Selvitopi and Aydin Buluç and Ariful Azad},
  date        = {2020-10-16},
  title       = {Communication-Avoiding and Memory-Constrained Sparse Matrix-Matrix Multiplication at Extreme Scale},
  eprint      = {2010.08526},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {Sparse matrix-matrix multiplication (SpGEMM) is a widely used kernel in various graph, scientific computing and machine learning algorithms. In this paper, we consider SpGEMMs performed on hundreds of thousands of processors generating trillions of nonzeros in the output matrix. Distributed SpGEMM at this extreme scale faces two key challenges: (1) high communication cost and (2) inadequate memory to generate the output. We address these challenges with an integrated communication-avoiding and memory-constrained SpGEMM algorithm that scales to 262,144 cores (more than 1 million hardware threads) and can multiply sparse matrices of any size as long as inputs and a fraction of output fit in the aggregated memory. As we go from 16,384 cores to 262,144 cores on a Cray XC40 supercomputer, the new SpGEMM algorithm runs 10x faster when multiplying large-scale protein-similarity matrices.},
  file        = {:http\://arxiv.org/pdf/2010.08526v1:PDF},
  keywords    = {cs.DC},
}

@Article{Guidi2020,
  author      = {Giulia Guidi and Oguz Selvitopi and Marquita Ellis and Leonid Oliker and Katherine Yelick and Aydin Buluc},
  date        = {2020-10-20},
  title       = {Parallel String Graph Construction and Transitive Reduction for De Novo Genome Assembly},
  eprint      = {2010.10055},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  abstract    = {One of the most computationally intensive tasks in computational biology is de novo genome assembly, the decoding of the sequence of an unknown genome from redundant and erroneous short sequences. A common assembly paradigm identifies overlapping sequences, simplifies their layout, and creates consensus. Despite many algorithms developed in the literature, the efficient assembly of large genomes is still an open problem. In this work, we introduce new distributed-memory parallel algorithms for overlap detection and layout simplification steps of de novo genome assembly, and implement them in the diBELLA 2D pipeline. Our distributed memory algorithms for both overlap detection and layout simplification are based on linear-algebra operations over semirings using 2D distributed sparse matrices. Our layout step consists of performing a transitive reduction from the overlap graph to a string graph. We provide a detailed communication analysis of the main stages of our new algorithms. diBELLA 2D achieves near linear scaling with over 80\% parallel efficiency for the human genome, reducing the runtime for overlap detection by 1.2-1.3$\times$ for the human genome and 1.5-1.9$\times$ for C. elegans compared to the state-of-the-art. Our transitive reduction algorithm outperforms an existing distributed-memory implementation by 10.5-13.3$\times$ for the human genome and 18-29$\times$ for the C. elegans. Our work paves the way for efficient de novo assembly of large genomes using long reads in distributed memory.},
  file        = {:http\://arxiv.org/pdf/2010.10055v1:PDF},
  keywords    = {cs.DC, q-bio.GN},
}

@Manual{Lourenco2020,
  author   = {Christopher Lourenco and Jinhao Chen and Erick Moreno-Centeno and Timothy A. Davis},
  date     = {2020},
  title    = {User Guide for {SLIP} {LU}, A Sparse Left-Looking Integer-Preserving {LU} Factorization},
  version  = {1.0.2},
  abstract = {SLIP LU is a software package designed to exactly solve unsymmetric sparse linear systems, $Ax = b$, where $A \in \mathbb{Q}^{n \times n}$ , $b \in \mathbb{Q}^{n \times r}$ , and $x \in \mathbb{Q}^{n \times r}$ . This package performs a left-looking, roundoff-error-free (REF) LU factorization $PAQ = LDU$, where $L$ and $U$ are integer, $D$ is diagonal, and $P$ and $Q$ are row and column permutations, respectively. Note that the matrix $D$ is never explicitly computed nor needed; thus this package uses only the matrices $L$ and $U$. The theory associated with this code is the Sparse Left-looking Integer-Preserving (SLIP) LU factorization [8]. Aside from solving sparse linear systems exactly, one of the key goals of this package is to provide a framework for other solvers to benchmark the reliability and stability of their linear solvers, as our final solution vector $x$ is guaranteed to be exact. In addition, SLIP LU provides a wrapper class for the GNU Multiple Precision Arithmetic (GMP) and GNU Multiple Precision Floating Point Reliable (MPFR) libraries in order to prevent memory leaks and improve the overall stability of these external libraries. SLIP LU is written in ANSI C and is accompanied by a MATLAB interface.},
}

@Article{Zhang2020h,
  author    = {Guanglu Zhang and Douglas Allaire and Jonathan Cagan},
  title     = {Taking the Guess Work Out of the Initial Guess: A Solution Interval Method for Least Squares Parameter Estimation in Nonlinear Models},
  doi       = {10.1115/1.4048811},
  pages     = {1--61},
  abstract  = {Fitting a specified model to data is critical in many science and engineering fields. A major task in fitting a specified model to data is to estimate the value of each parameter in the model. Iterative local methods, such as the Gauss-Newton method and the Levenberg-Marquardt method, are often employed for parameter estimation in nonlinear models. However, practitioners must guess the initial value for each parameter to initialize these iterative local methods. A poor initial guess can contribute to non-convergence of these methods or lead these methods to converge to a wrong or inferior solution. In this paper, a solution interval method is introduced to find the optimal estimator for each parameter in a nonlinear model that minimizes the squared error of the fit. The method includes three algorithms that require different level of computational power to find the optimal parameter estimators. The method constructs a solution interval for each parameter in the model. These solution intervals significantly reduce the search space for optimal parameter estimators. The method also provides an empirical probability distribution for each parameter, which is valuable for parameter uncertainty assessment. The solution interval method is validated through two case studies in which the Michaelis-Menten model and Fick's second law are fit to experimental data sets, respectively. These case studies show that the solution interval method can find optimal parameter estimators efficiently. A four-step procedure for implementing the solution interval method in practice is also outlined.},
  journal   = {Journal of Computing and Information Science in Engineering},
  month     = {10},
  publisher = {{ASME} International},
  year      = {2020},
}

@TechReport{Marchal2020,
  author      = {Loris Marchal and Thibault Marette and Grégoire Pichon and Frédéric Vivien},
  date        = {2020},
  institution = {Inria},
  title       = {Trading Performance for Memoryin Sparse Direct Solvers using Low-rank Compression},
  eprint      = {02976233},
  eprinttype  = {HAL},
  url         = {https://hal.inria.fr/hal-02976233/document},
  abstract    = {Sparse direct solvers using Block Low-Rank compression have been proven efficient to solve problems arising in many real-life applications. Improving those solvers is crucial for being able to 1) solve larger problems and 2) speed up computations. A main characteristic of a sparse direct solver using low-rank compression is when compression is performed. There are two distinct approaches: (1) all blocks are compressed before starting the factorization, which reduces the memory as much as possible, or (2) each block is compressed as late as possible, which usually leads to better speedup. The objective of this paper is to design a composite approach, to speedup computations while staying under a given memory limit. This should allow to solve large problems that cannot be solved with Approach 2 while reducing the execution time compared to Approach 1. We propose a memory-aware strategy where each block can be compressed either at the beginning or as late as possible. We first consider the problem of choosing when to compress each block, under the assumption that all information on blocks is perfectly known, i.e., memory requirement and execution time of a block when compressed or not. We show that this problem is a variant of the NP-complete Knapsack problem, and adapt an existing 2-approximation algorithm for our problem. Unfortunately, the required information on blocks depends on numerical properties and in practice cannot be known in advance. We thus introduce models to estimate those values. Experiments on the PaStiX solver demonstrate that our new approach can achieve an excellent trade-off between memory consumption and computational cost. For instance on matrix Geo1438, Approach 2 uses three times as much memory as Approach 1 while being three times faster. Our new approach leads to an execution time only 30\% larger than Approach 2 when given a memory 30\% larger than the one needed by Approach 1},
}

@Article{Swirydowicz2020,
  author    = {Katarzyna {\'{S}}wirydowicz and Julien Langou and Shreyas Ananthan and Ulrike Yang and Stephen Thomas},
  title     = {Low synchronization Gram{\textendash}Schmidt and generalized minimal residual algorithms},
  doi       = {10.1002/nla.2343},
  abstract  = {The Gram–Schmidt process uses orthogonal projection to construct the $A = QR$ factorization of a matrix. When $Q$ has linearly independent columns, the operator $P = I - Q(Q^T Q)^{-1} Q^T$ defines an orthogonal projection onto $Q^\bot$. In finite precision, $Q$ loses orthogonality as the factorization progresses. A family of approximate projections is derived with the form $P = I - QT Q^T$, with correction matrix $T$. When $T = (Q^T Q)^{-1}$, and $T$ is triangular, it is postulated that the best achievable orthogonality is $\mathcal{O}(\varepsilon)\kappa(A)$. We present new variants of modified (MGS) and classical Gram–Schmidt algorithms that require one global reduction step. An interesting form of the projector leads to a compact $WY$ representation for MGS. In particular, the inverse compact $WY$ MGS algorithm is equivalent to a lower triangular solve. Our main contribution is to introduce a backward normalization lag into the compact $WY$ representation, resulting in a $\mathcal{O}(\varepsilon)\kappa([r_0, A V_m])$ stable Generalized Minimal Residual Method (GMRES) algorithm that requires only one global reduce per iteration. Further improvements in performance are achieved by accelerating GMRES on GPUs.},
  journal   = {Numerical Linear Algebra with Applications},
  month     = {10},
  publisher = {Wiley},
  year      = {2020},
}

@Book{Bueler2020,
  author    = {Ed Bueler},
  title     = {{PETSc} for Partial Differential Equations: Numerical Solutions in C and Python},
  doi       = {10.1137/1.9781611976311},
  publisher = {Society for Industrial and Applied Mathematics},
  abstract  = {The Portable, Extensible Toolkit for Scientific Computation (PETSc) is an open-source library of advanced data structures and methods for solving linear and nonlinear equations and for managing discretizations. This book uses these modern numerical tools to demonstrate how to solve nonlinear partial differential equations (PDEs) in parallel. It starts from key mathematical concepts, such as Krylov space methods, preconditioning, multigrid, and Newton's method. In PETSc these components are composed at run time into fast solvers.\\ Discretizations are introduced from the beginning, with an emphasis on finite difference and finite element methodologies. The example C programs of the first 12 chapters, listed on the inside front cover, solve (mostly) elliptic and parabolic PDE problems. Discretization leads to large, sparse, and generally nonlinear systems of algebraic equations. For such problems, mathematical solver concepts are explained and illustrated through the examples, with sufficient context to speed further development.\\
PETSc for Partial Differential Equations\begin{itemize}
\item addresses both discretization and fast solvers for PDEs;
\item emphasizes practice more than theory;
\item contains well-structured examples, with advice on run-time solver choices;
\item demonstrates how to achieve high performance and parallel scalability; and
\item builds on the reader's understanding of fast solver concepts when applying the Firedrake
\end{itemize}
Python finite element solver library in the last two chapters.\\ This textbook, the first to cover PETSc programming for nonlinear PDEs, provides an on-ramp for graduate students and researchers to a major area of high-performance computing for science and engineering. It is suitable as a supplement for courses in scientific computing or numerical methods for differential equations.},
  month     = {1},
  year      = {2020},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
}
